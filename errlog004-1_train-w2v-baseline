DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=hydra_train
INFO:fairseq.distributed.utils:distributed init (rank 2): tcp://localhost:25846
[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:25846 (errno: 99 - Cannot assign requested address).
INFO:fairseq.distributed.utils:distributed init (rank 3): tcp://localhost:25846
[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:25846 (errno: 99 - Cannot assign requested address).
INFO:fairseq.distributed.utils:distributed init (rank 1): tcp://localhost:25846
[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:25846 (errno: 99 - Cannot assign requested address).
INFO:fairseq.distributed.utils:distributed init (rank 0): tcp://localhost:25846
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 0
INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 2
INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 3
INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 1
[2023-09-12 23:21:38,139][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 8, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:25846', 'distributed_port': 25846, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1900000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1900000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.1, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'audio_pretraining', 'data': '/home/Workspace/fairseq/data', 'labels': None, 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': 250000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': none, 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 8}, 'criterion': {'_name': 'wav2vec', 'infonce': True, 'loss_weights': [0.1, 10.0], 'log_keys': ['prob_perplexity', 'code_perplexity', 'temp']}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2023-09-12 23:21:39,517][fairseq_cli.train][INFO] - Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
        (3): GELU(approximate='none')
      )
      (1-4): 4 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
      (5-6): 2 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.1, inplace=False)
  (dropout_features): Dropout(p=0.1, inplace=False)
  (quantizer): GumbelVectorQuantizer(
    (weight_proj): Linear(in_features=512, out_features=640, bias=True)
  )
  (project_q): Linear(in_features=256, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): Linear(in_features=768, out_features=256, bias=True)
)
[2023-09-12 23:21:39,519][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2023-09-12 23:21:39,519][fairseq_cli.train][INFO] - model: Wav2Vec2Model
[2023-09-12 23:21:39,519][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion
[2023-09-12 23:21:39,520][fairseq_cli.train][INFO] - num. shared model params: 95,045,248 (num. trained: 95,045,248)
[2023-09-12 23:21:39,520][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2023-09-12 23:21:39,527][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 5474, skipped 93 samples
[2023-09-12 23:21:39,547][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0
[2023-09-12 23:21:39,713][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-12 23:21:39,713][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias
[2023-09-12 23:21:39,713][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias
[2023-09-12 23:21:39,713][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias
[2023-09-12 23:21:39,713][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias
[2023-09-12 23:21:39,713][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias
[2023-09-12 23:21:39,714][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias
[2023-09-12 23:21:39,816][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2023-09-12 23:21:39,816][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 23:21:39,816][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 23:21:39,816][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 23:21:39,816][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 23.648 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 23:21:39,816][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2023-09-12 23:21:39,817][fairseq_cli.train][INFO] - training on 4 devices (GPUs/TPUs)
[2023-09-12 23:21:39,817][fairseq_cli.train][INFO] - max tokens per device = 1900000 and max sentences per device = None
[2023-09-12 23:21:39,818][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2023-09-12 23:21:39,818][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2023-09-12 23:21:39,818][fairseq.trainer][INFO] - loading train data for epoch 1
[2023-09-12 23:21:40,081][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 280531, skipped 710 samples
[2023-09-12 23:21:40,153][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 23:21:40,153][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2023-09-12 23:21:40,153][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2023-09-12 23:21:40,153][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2023-09-12 23:21:40,153][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2023-09-12 23:21:40,452][fairseq_cli.train][INFO] - begin dry-run validation on "valid" subset
[2023-09-12 23:21:40,453][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 23:21:40,453][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2023-09-12 23:21:40,453][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2023-09-12 23:21:40,453][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2023-09-12 23:21:40,453][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2023-09-12 23:21:57,386][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 23:21:57,393][fairseq.trainer][INFO] - begin training epoch 1
[2023-09-12 23:21:57,394][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 23:22:19,204][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
loss: tensor(12220.9717, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5840, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12347.3506, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6821, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11675.2979, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8521, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6881.9536, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2051, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12726.4004, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7021, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6594.6743, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6953, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11672.8545, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3633, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11903.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1641, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11979.5547, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5176, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 23:35:17,330][train_inner][INFO] - {"epoch": 1, "update": 0.386, "loss": "9.076", "ntokens": "149652", "nsentences": "539.78", "prob_perplexity": "356.828", "code_perplexity": "342.569", "temp": "1.999", "loss_0": "6.684", "loss_1": "0.064", "loss_2": "2.328", "accuracy": "0.01198", "wps": "38502.5", "ups": "0.26", "wpb": "149652", "bsz": "539.8", "num_updates": "200", "lr": "3.125e-06", "gnorm": "1.252", "loss_scale": "64", "train_wall": "783", "gb_free": "12.9", "wall": "818"}
loss: tensor(11730.8027, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7207, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 23:48:29,461][train_inner][INFO] - {"epoch": 1, "update": 0.77, "loss": "6.957", "ntokens": "149589", "nsentences": "540.235", "prob_perplexity": "538.64", "code_perplexity": "523.216", "temp": "1.997", "loss_0": "6.66", "loss_1": "0.023", "loss_2": "0.275", "accuracy": "0.01195", "wps": "37768.7", "ups": "0.25", "wpb": "149589", "bsz": "540.2", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.122", "loss_scale": "128", "train_wall": "791", "gb_free": "13", "wall": "1610"}
[2023-09-12 23:56:17,964][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 23:56:17,965][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 23:56:18,042][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2023-09-12 23:56:41,843][valid][INFO] - {"epoch": 1, "valid_loss": "6.703", "valid_ntokens": "7874.17", "valid_nsentences": "55.2525", "valid_prob_perplexity": "587.625", "valid_code_perplexity": "569.928", "valid_temp": "1.995", "valid_loss_0": "6.657", "valid_loss_1": "0.011", "valid_loss_2": "0.035", "valid_accuracy": "0.01467", "valid_wps": "33059.3", "valid_wpb": "7874.2", "valid_bsz": "55.3", "valid_num_updates": "520"}
[2023-09-12 23:56:41,845][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 520 updates
[2023-09-12 23:56:41,846][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-12 23:56:43,688][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-12 23:56:44,570][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 520 updates, score 6.703) (writing took 2.725212568999268 seconds)
[2023-09-12 23:56:44,570][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2023-09-12 23:56:44,571][train][INFO] - {"epoch": 1, "train_loss": "7.72", "train_ntokens": "149436", "train_nsentences": "538.404", "train_prob_perplexity": "480.341", "train_code_perplexity": "465.132", "train_temp": "1.997", "train_loss_0": "6.669", "train_loss_1": "0.036", "train_loss_2": "1.015", "train_accuracy": "0.01196", "train_wps": "37635.9", "train_ups": "0.25", "train_wpb": "149436", "train_bsz": "538.4", "train_num_updates": "520", "train_lr": "8.125e-06", "train_gnorm": "0.533", "train_loss_scale": "256", "train_train_wall": "2042", "train_gb_free": "15.8", "train_wall": "2105"}
[2023-09-12 23:56:44,572][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 23:56:44,658][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2023-09-12 23:56:44,909][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 23:56:44,912][fairseq.trainer][INFO] - begin training epoch 2
[2023-09-12 23:56:44,913][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-13 00:02:01,578][train_inner][INFO] - {"epoch": 2, "update": 1.154, "loss": "6.715", "ntokens": "149220", "nsentences": "535.635", "prob_perplexity": "592.005", "code_perplexity": "575.866", "temp": "1.995", "loss_0": "6.658", "loss_1": "0.011", "loss_2": "0.046", "accuracy": "0.01196", "wps": "36748.3", "ups": "0.25", "wpb": "149220", "bsz": "535.6", "num_updates": "600", "lr": "9.375e-06", "gnorm": "0.015", "loss_scale": "256", "train_wall": "784", "gb_free": "12.7", "wall": "2422"}
loss: tensor(11467.9238, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7305, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11501.4521, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.1289, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:08:56,354][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
[2023-09-13 00:15:14,480][train_inner][INFO] - {"epoch": 2, "update": 1.539, "loss": "6.577", "ntokens": "149727", "nsentences": "540.1", "prob_perplexity": "516.217", "code_perplexity": "501.516", "temp": "1.993", "loss_0": "6.532", "loss_1": "0.028", "loss_2": "0.017", "accuracy": "0.02083", "wps": "37766.9", "ups": "0.25", "wpb": "149727", "bsz": "540.1", "num_updates": "800", "lr": "1.25e-05", "gnorm": "0.17", "loss_scale": "128", "train_wall": "792", "gb_free": "13", "wall": "3215"}
loss: tensor(10568.4043, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6211, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:17:06,649][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
loss: tensor(11218.5859, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8101, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10832.1123, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9150, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12337.4014, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8750, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11898.5498, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0742, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(11602.1621, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0391, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11887.7227, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7559, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(11308.2949, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.1445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10759.3867, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.1445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12220.0156, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.2656, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10743.3623, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.5273, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7311.0918, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.0039, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8394.4199, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5996, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10850.9834, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0508, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(10537.9219, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8164, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10175.7471, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2773, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:22:31,861][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
loss: tensor(7737.3433, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4290, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:28:31,941][train_inner][INFO] - {"epoch": 2, "update": 1.927, "loss": "6.155", "ntokens": "149693", "nsentences": "539.97", "prob_perplexity": "261.917", "code_perplexity": "256.938", "temp": "1.991", "loss_0": "6.051", "loss_1": "0.085", "loss_2": "0.019", "accuracy": "0.05632", "wps": "37542.3", "ups": "0.25", "wpb": "149693", "bsz": "540", "num_updates": "1000", "lr": "1.5625e-05", "gnorm": "0.492", "loss_scale": "32", "train_wall": "796", "gb_free": "12.8", "wall": "4012"}
loss: tensor(9095.1973, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6416, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:30:57,390][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 00:30:57,391][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 00:30:57,605][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2023-09-13 00:31:21,260][valid][INFO] - {"epoch": 2, "valid_loss": "5.765", "valid_ntokens": "7927.98", "valid_nsentences": "55.2525", "valid_prob_perplexity": "122.642", "valid_code_perplexity": "118.926", "valid_temp": "1.99", "valid_loss_0": "5.63", "valid_loss_1": "0.116", "valid_loss_2": "0.019", "valid_accuracy": "0.12539", "valid_wps": "33220.3", "valid_wpb": "7928", "valid_bsz": "55.3", "valid_num_updates": "1038", "valid_best_loss": "5.765"}
[2023-09-13 00:31:21,262][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 1038 updates
[2023-09-13 00:31:21,263][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 00:31:23,691][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 00:31:24,996][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 1038 updates, score 5.765) (writing took 3.7342237810371444 seconds)
[2023-09-13 00:31:24,997][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2023-09-13 00:31:24,997][train][INFO] - {"epoch": 2, "train_loss": "6.387", "train_ntokens": "149440", "train_nsentences": "538.521", "train_prob_perplexity": "404.055", "train_code_perplexity": "393.813", "train_temp": "1.992", "train_loss_0": "6.314", "train_loss_1": "0.053", "train_loss_2": "0.02", "train_accuracy": "0.03796", "train_wps": "37208.8", "train_ups": "0.25", "train_wpb": "149440", "train_bsz": "538.5", "train_num_updates": "1038", "train_lr": "1.62188e-05", "train_gnorm": "0.304", "train_loss_scale": "32", "train_train_wall": "2049", "train_gb_free": "13.1", "train_wall": "4185"}
[2023-09-13 00:31:25,001][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 00:31:25,084][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2023-09-13 00:31:25,326][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 00:31:25,329][fairseq.trainer][INFO] - begin training epoch 3
[2023-09-13 00:31:25,329][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-13 00:40:09,351][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
loss: tensor(9180.1152, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3184, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:42:14,730][train_inner][INFO] - {"epoch": 3, "update": 2.313, "loss": "5.817", "ntokens": "148946", "nsentences": "539.84", "prob_perplexity": "116.464", "code_perplexity": "114.421", "temp": "1.989", "loss_0": "5.68", "loss_1": "0.118", "loss_2": "0.019", "accuracy": "0.1102", "wps": "36205.2", "ups": "0.24", "wpb": "148946", "bsz": "539.8", "num_updates": "1200", "lr": "1.875e-05", "gnorm": "0.633", "loss_scale": "32", "train_wall": "793", "gb_free": "12.9", "wall": "4835"}
loss: tensor(9903.7314, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4189, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:55:26,790][train_inner][INFO] - {"epoch": 3, "update": 2.697, "loss": "5.536", "ntokens": "150010", "nsentences": "537.625", "prob_perplexity": "61.83", "code_perplexity": "61.196", "temp": "1.987", "loss_0": "5.387", "loss_1": "0.13", "loss_2": "0.018", "accuracy": "0.16663", "wps": "37878.4", "ups": "0.25", "wpb": "150010", "bsz": "537.6", "num_updates": "1400", "lr": "2.1875e-05", "gnorm": "0.689", "loss_scale": "32", "train_wall": "791", "gb_free": "12.7", "wall": "5627"}
loss: tensor(8838.8154, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4385, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 00:56:42,836][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
loss: tensor(12658.8574, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1240, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7764.3628, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7422, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11823.1006, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9199, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11818.1338, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1367, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11532.9824, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8828, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12187.2344, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3867, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10890.7734, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.9688, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10984.1621, device='cuda:1')
loss_ent_max: tensor(-4.7148, device='cuda:1', dtype=torch.float16)
loss: tensor(8309.0547, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.1641, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11567.1436, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.9336, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(12330.8535, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.1016, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10154.3604, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.0156, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12133.4473, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7148, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8929.4941, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9400.8457, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4473, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11093.7627, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0254, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11482.7646, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0371, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9478.0029, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0520, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9651.7666, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5981, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6136.7783, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6367, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9621.3633, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3418, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9084.4463, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2041, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6747.4688, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9819, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8864.1504, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3994, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 01:05:46,854][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 01:05:46,855][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 01:05:47,025][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
loss: tensor(5767.2065, device='cuda:0')
loss_ent_max: tensor(-1.6309, device='cuda:0', dtype=torch.float16)
[2023-09-13 01:06:10,708][valid][INFO] - {"epoch": 3, "valid_loss": "5.088", "valid_ntokens": "7924.16", "valid_nsentences": "55.2525", "valid_prob_perplexity": "48.891", "valid_code_perplexity": "48.648", "valid_temp": "1.985", "valid_loss_0": "4.939", "valid_loss_1": "0.133", "valid_loss_2": "0.015", "valid_accuracy": "0.23953", "valid_wps": "33248.8", "valid_wpb": "7924.2", "valid_bsz": "55.3", "valid_num_updates": "1557", "valid_best_loss": "5.088"}
[2023-09-13 01:06:10,710][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 1557 updates
[2023-09-13 01:06:10,711][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 01:06:13,187][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 01:06:14,533][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 1557 updates, score 5.088) (writing took 3.823583675082773 seconds)
[2023-09-13 01:06:14,534][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2023-09-13 01:06:14,535][train][INFO] - {"epoch": 3, "train_loss": "5.566", "train_ntokens": "149571", "train_nsentences": "538.38", "train_prob_perplexity": "73.096", "train_code_perplexity": "72.18", "train_temp": "1.987", "train_loss_0": "5.42", "train_loss_1": "0.128", "train_loss_2": "0.018", "train_accuracy": "0.15793", "train_wps": "37150.6", "train_ups": "0.25", "train_wpb": "149571", "train_bsz": "538.4", "train_num_updates": "1557", "train_lr": "2.43281e-05", "train_gnorm": "0.679", "train_loss_scale": "16", "train_train_wall": "2058", "train_gb_free": "13.4", "train_wall": "6275"}
[2023-09-13 01:06:14,538][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 01:06:14,629][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2023-09-13 01:06:14,860][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 01:06:14,863][fairseq.trainer][INFO] - begin training epoch 4
[2023-09-13 01:06:14,863][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-13 01:09:04,270][train_inner][INFO] - {"epoch": 4, "update": 3.083, "loss": "5.359", "ntokens": "149061", "nsentences": "535.925", "prob_perplexity": "52.261", "code_perplexity": "51.934", "temp": "1.985", "loss_0": "5.21", "loss_1": "0.132", "loss_2": "0.016", "accuracy": "0.1923", "wps": "36468.4", "ups": "0.24", "wpb": "149061", "bsz": "535.9", "num_updates": "1600", "lr": "2.5e-05", "gnorm": "0.724", "loss_scale": "16", "train_wall": "788", "gb_free": "12.9", "wall": "6444"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(9802.2793, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0820, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5692.4238, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9736, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10243.5762, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4102, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5186.5015, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1299, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8929.0273, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2441, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9314.9541, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2373, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8708.6904, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5215, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4207.7017, device='cuda:2')
loss_ent_max: tensor(-0.9170, device='cuda:2', dtype=torch.float16)
loss: tensor(9284.5381, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4121, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6920.0161, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6021, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5495.6982, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8105, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(9029.7666, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3652, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6432.4087, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6089, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 01:22:13,281][train_inner][INFO] - {"epoch": 4, "update": 3.466, "loss": "5.217", "ntokens": "149748", "nsentences": "538.84", "prob_perplexity": "51.819", "code_perplexity": "51.587", "temp": "1.983", "loss_0": "5.07", "loss_1": "0.133", "loss_2": "0.014", "accuracy": "0.20725", "wps": "37958.4", "ups": "0.25", "wpb": "149748", "bsz": "538.8", "num_updates": "1800", "lr": "2.8125e-05", "gnorm": "0.707", "loss_scale": "32", "train_wall": "788", "gb_free": "12.9", "wall": "7233"}
loss: tensor(7080.1006, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3115, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7088.9497, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6621, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8966.1953, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3398, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7717.3677, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6162, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 01:31:04,282][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2023-09-13 01:31:57,253][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
loss: tensor(6322.5801, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9639, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 01:35:40,381][train_inner][INFO] - {"epoch": 4, "update": 3.854, "loss": "5.096", "ntokens": "149744", "nsentences": "538.025", "prob_perplexity": "53.023", "code_perplexity": "52.839", "temp": "1.981", "loss_0": "4.95", "loss_1": "0.132", "loss_2": "0.013", "accuracy": "0.21802", "wps": "37106.7", "ups": "0.25", "wpb": "149744", "bsz": "538", "num_updates": "2000", "lr": "3.125e-05", "gnorm": "0.703", "loss_scale": "16", "train_wall": "806", "gb_free": "12.9", "wall": "8041"}
[2023-09-13 01:40:41,771][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 01:40:41,772][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 01:40:41,958][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2023-09-13 01:41:05,826][valid][INFO] - {"epoch": 4, "valid_loss": "4.739", "valid_ntokens": "7887.98", "valid_nsentences": "55.2525", "valid_prob_perplexity": "52.402", "valid_code_perplexity": "52.206", "valid_temp": "1.979", "valid_loss_0": "4.594", "valid_loss_1": "0.132", "valid_loss_2": "0.012", "valid_accuracy": "0.28173", "valid_wps": "32978.9", "valid_wpb": "7888", "valid_bsz": "55.3", "valid_num_updates": "2076", "valid_best_loss": "4.739"}
[2023-09-13 01:41:05,828][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 2076 updates
[2023-09-13 01:41:05,829][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 01:41:08,276][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 01:41:09,606][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 2076 updates, score 4.739) (writing took 3.778094638022594 seconds)
[2023-09-13 01:41:09,606][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2023-09-13 01:41:09,607][train][INFO] - {"epoch": 4, "train_loss": "5.15", "train_ntokens": "149497", "train_nsentences": "538.331", "train_prob_perplexity": "52.581", "train_code_perplexity": "52.374", "train_temp": "1.982", "train_loss_0": "5.004", "train_loss_1": "0.132", "train_loss_2": "0.014", "train_accuracy": "0.2131", "train_wps": "37033.9", "train_ups": "0.25", "train_wpb": "149497", "train_bsz": "538.3", "train_num_updates": "2076", "train_lr": "3.24375e-05", "train_gnorm": "0.718", "train_loss_scale": "16", "train_train_wall": "2063", "train_gb_free": "13.5", "train_wall": "8370"}
[2023-09-13 01:41:09,611][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 01:41:09,712][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2023-09-13 01:41:09,958][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 01:41:09,961][fairseq.trainer][INFO] - begin training epoch 5
[2023-09-13 01:41:09,961][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(8948.4805, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9390, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8041.7866, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0732, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5879.7739, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6235, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 01:49:19,895][train_inner][INFO] - {"epoch": 5, "update": 4.238, "loss": "5.007", "ntokens": "149065", "nsentences": "536.41", "prob_perplexity": "54.373", "code_perplexity": "54.213", "temp": "1.979", "loss_0": "4.863", "loss_1": "0.132", "loss_2": "0.012", "accuracy": "0.22638", "wps": "36379", "ups": "0.24", "wpb": "149065", "bsz": "536.4", "num_updates": "2200", "lr": "3.4375e-05", "gnorm": "0.716", "loss_scale": "32", "train_wall": "790", "gb_free": "12.6", "wall": "8860"}
loss: tensor(8337.8330, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1230, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8981.5635, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2920, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9049.3721, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4233, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8408.6650, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8921, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 02:02:32,308][train_inner][INFO] - {"epoch": 5, "update": 4.622, "loss": "4.935", "ntokens": "149679", "nsentences": "536.925", "prob_perplexity": "55.817", "code_perplexity": "55.663", "temp": "1.977", "loss_0": "4.791", "loss_1": "0.132", "loss_2": "0.012", "accuracy": "0.2334", "wps": "37778.1", "ups": "0.25", "wpb": "149679", "bsz": "536.9", "num_updates": "2400", "lr": "3.75e-05", "gnorm": "0.671", "loss_scale": "32", "train_wall": "791", "gb_free": "12.7", "wall": "9652"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(5906.9214, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8970, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8200.4980, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4365, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8671.6738, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9521, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8197.8066, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7080, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8624.7822, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7666, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8788.2734, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4102, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8461.1289, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4329, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10103.9355, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2334, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5055.0474, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1006, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4235.3467, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0527, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7564.8818, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0883, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8369.1240, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2233, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(8813.6680, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2081, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8551.1074, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1099, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 02:06:16,441][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
loss: tensor(11919.5557, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3799, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12421.3027, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6650, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10751.5859, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6426, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7496.6577, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0078, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11892.9473, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0391, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11886.3926, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8242, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10725.1475, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8320, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9300.1514, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5996, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(11809.8184, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4258, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10267.6357, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7437, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10160.3389, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9653, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10130.5479, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6060, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6905.6870, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6680, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8074.4849, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8643, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8559.0938, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8530, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8616.9326, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6846, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7303.0337, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9321, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9012.7246, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2703, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8752.2773, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7891, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6804.6670, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5293, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8904.9521, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8545, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7361.8071, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4355, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6285.7144, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6689, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8396.8564, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1514, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9521.2920, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2617, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5448.2134, device='cuda:3')
loss_ent_max: tensor(-0.6904, device='cuda:3', dtype=torch.float16)
loss: tensor(6621.3682, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2795, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8895.6943, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0049, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8810.8379, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7734, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8575.7207, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0494, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8393.1748, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2705, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7048.4893, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1621, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9223.9072, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5449, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7961.0405, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4297, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7400.0552, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6113, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7133.0830, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2461, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7939.2432, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0352, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8698.9375, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9385, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8493.1807, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4004, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8599.6455, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5576, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7929.9380, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4746, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5984.0967, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5835, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7817.3813, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4685, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8038.1738, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6646, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9144.8135, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0898, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4656.2393, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4773, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5645.9321, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3586, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9099.6992, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1768, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: loss: tensor(8575.6582, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1055, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9212.1465, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4463, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7741.0806, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7729, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 02:15:33,096][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 02:15:33,097][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 02:15:33,173][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
loss: tensor(6025.5869, device='cuda:0')
loss_ent_max: tensor(-0.6338, device='cuda:0', dtype=torch.float16)
loss: tensor(7834.7959, device='cuda:0')
loss_ent_max: tensor(-1.0938, device='cuda:0', dtype=torch.float16)
[2023-09-13 02:15:57,003][valid][INFO] - {"epoch": 5, "valid_loss": "4.54", "valid_ntokens": "7892.98", "valid_nsentences": "55.2525", "valid_prob_perplexity": "57.104", "valid_code_perplexity": "56.943", "valid_temp": "1.974", "valid_loss_0": "4.398", "valid_loss_1": "0.131", "valid_loss_2": "0.011", "valid_accuracy": "0.29393", "valid_wps": "33056.6", "valid_wpb": "7893", "valid_bsz": "55.3", "valid_num_updates": "2596", "valid_best_loss": "4.54"}
[2023-09-13 02:15:57,006][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 2596 updates
[2023-09-13 02:15:57,007][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 02:15:59,459][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 02:16:00,778][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 2596 updates, score 4.54) (writing took 3.7719086579745635 seconds)
[2023-09-13 02:16:00,778][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2023-09-13 02:16:00,778][train][INFO] - {"epoch": 5, "train_loss": "4.925", "train_ntokens": "149408", "train_nsentences": "538.288", "train_prob_perplexity": "56.152", "train_code_perplexity": "56", "train_temp": "1.977", "train_loss_0": "4.782", "train_loss_1": "0.132", "train_loss_2": "0.012", "train_accuracy": "0.23401", "train_wps": "37152.5", "train_ups": "0.25", "train_wpb": "149408", "train_bsz": "538.3", "train_num_updates": "2596", "train_lr": "4.05625e-05", "train_gnorm": "0.686", "train_loss_scale": "32", "train_train_wall": "2060", "train_gb_free": "15", "train_wall": "10461"}
[2023-09-13 02:16:00,780][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 02:16:00,867][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2023-09-13 02:16:01,123][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 02:16:01,127][fairseq.trainer][INFO] - begin training epoch 6
[2023-09-13 02:16:01,127][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-13 02:16:16,643][train_inner][INFO] - {"epoch": 6, "update": 5.008, "loss": "4.873", "ntokens": "149002", "nsentences": "539.5", "prob_perplexity": "57.491", "code_perplexity": "57.345", "temp": "1.975", "loss_0": "4.731", "loss_1": "0.131", "loss_2": "0.011", "accuracy": "0.23855", "wps": "36150.8", "ups": "0.24", "wpb": "149002", "bsz": "539.5", "num_updates": "2600", "lr": "4.0625e-05", "gnorm": "0.708", "loss_scale": "32", "train_wall": "795", "gb_free": "12.7", "wall": "10477"}
loss: tensor(7922.9463, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8936, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6760.5693, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0488, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8865.5459, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6123, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 02:23:44,934][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2023-09-13 02:24:04,275][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2023-09-13 02:26:38,118][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2023-09-13 02:29:42,531][train_inner][INFO] - {"epoch": 6, "update": 5.397, "loss": "4.827", "ntokens": "149477", "nsentences": "535.98", "prob_perplexity": "58.958", "code_perplexity": "58.824", "temp": "1.973", "loss_0": "4.685", "loss_1": "0.131", "loss_2": "0.011", "accuracy": "0.24215", "wps": "37096.4", "ups": "0.25", "wpb": "149478", "bsz": "536", "num_updates": "2800", "lr": "4.375e-05", "gnorm": "0.654", "loss_scale": "8", "train_wall": "805", "gb_free": "12.9", "wall": "11283"}
loss: tensor(8147.2544, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2920, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7439.8721, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3738, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7730.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4014, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 02:43:05,018][train_inner][INFO] - {"epoch": 6, "update": 5.781, "loss": "4.777", "ntokens": "149983", "nsentences": "541.63", "prob_perplexity": "60.468", "code_perplexity": "60.34", "temp": "1.971", "loss_0": "4.636", "loss_1": "0.131", "loss_2": "0.01", "accuracy": "0.24684", "wps": "37379.5", "ups": "0.25", "wpb": "149983", "bsz": "541.6", "num_updates": "3000", "lr": "4.6875e-05", "gnorm": "0.645", "loss_scale": "8", "train_wall": "801", "gb_free": "12.7", "wall": "12085"}
[2023-09-13 02:50:35,995][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 02:50:35,996][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 02:50:36,172][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2023-09-13 02:50:59,976][valid][INFO] - {"epoch": 6, "valid_loss": "4.428", "valid_ntokens": "7926.82", "valid_nsentences": "55.2525", "valid_prob_perplexity": "60.772", "valid_code_perplexity": "60.628", "valid_temp": "1.969", "valid_loss_0": "4.288", "valid_loss_1": "0.131", "valid_loss_2": "0.01", "valid_accuracy": "0.3116", "valid_wps": "33139.4", "valid_wpb": "7926.8", "valid_bsz": "55.3", "valid_num_updates": "3114", "valid_best_loss": "4.428"}
[2023-09-13 02:50:59,977][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 3114 updates
[2023-09-13 02:50:59,978][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 02:51:02,410][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 02:51:03,738][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 3114 updates, score 4.428) (writing took 3.7605642459820956 seconds)
[2023-09-13 02:51:03,738][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2023-09-13 02:51:03,739][train][INFO] - {"epoch": 6, "train_loss": "4.789", "train_ntokens": "149482", "train_nsentences": "538.384", "train_prob_perplexity": "60.102", "train_code_perplexity": "59.972", "train_temp": "1.972", "train_loss_0": "4.648", "train_loss_1": "0.131", "train_loss_2": "0.01", "train_accuracy": "0.24564", "train_wps": "36820.3", "train_ups": "0.25", "train_wpb": "149482", "train_bsz": "538.4", "train_num_updates": "3114", "train_lr": "4.86563e-05", "train_gnorm": "0.655", "train_loss_scale": "16", "train_train_wall": "2071", "train_gb_free": "15.4", "train_wall": "12564"}
[2023-09-13 02:51:03,741][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 02:51:03,860][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2023-09-13 02:51:04,087][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 02:51:04,090][fairseq.trainer][INFO] - begin training epoch 7
[2023-09-13 02:51:04,091][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(7802.4976, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2256, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8410.3662, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4609, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 02:56:44,447][train_inner][INFO] - {"epoch": 7, "update": 6.165, "loss": "4.732", "ntokens": "149094", "nsentences": "538.79", "prob_perplexity": "61.67", "code_perplexity": "61.542", "temp": "1.969", "loss_0": "4.592", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.25163", "wps": "36389.8", "ups": "0.24", "wpb": "149094", "bsz": "538.8", "num_updates": "3200", "lr": "5e-05", "gnorm": "0.648", "loss_scale": "16", "train_wall": "790", "gb_free": "12.6", "wall": "12905"}
loss: tensor(7903.9082, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0732, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7681.1670, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6743, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 03:01:34,929][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6484.1328, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8882, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7399.6055, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8579, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8146.6621, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4612, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8796.6299, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8198, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6724.6240, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2275, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7793.1318, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2673, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9181.8906, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7095, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7441.5552, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2256, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8542.8223, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4963, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6687.8359, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3721, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8826.8027, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5693, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7689.3848, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2316, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 03:09:58,524][train_inner][INFO] - {"epoch": 7, "update": 6.551, "loss": "4.667", "ntokens": "149600", "nsentences": "541.7", "prob_perplexity": "62.767", "code_perplexity": "62.625", "temp": "1.967", "loss_0": "4.527", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.26092", "wps": "37679", "ups": "0.25", "wpb": "149600", "bsz": "541.7", "num_updates": "3400", "lr": "5.3125e-05", "gnorm": "0.64", "loss_scale": "16", "train_wall": "793", "gb_free": "12.8", "wall": "13699"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6202.6958, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2188, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8425.1426, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7910, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7618.7544, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9561, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 03:18:45,605][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2023-09-13 03:23:12,843][train_inner][INFO] - {"epoch": 7, "update": 6.937, "loss": "4.601", "ntokens": "149829", "nsentences": "536.485", "prob_perplexity": "63.726", "code_perplexity": "63.586", "temp": "1.965", "loss_0": "4.461", "loss_1": "0.13", "loss_2": "0.009", "accuracy": "0.26919", "wps": "37725.1", "ups": "0.25", "wpb": "149829", "bsz": "536.5", "num_updates": "3600", "lr": "5.625e-05", "gnorm": "0.655", "loss_scale": "16", "train_wall": "793", "gb_free": "12.6", "wall": "14493"}
[2023-09-13 03:25:20,180][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 03:25:20,181][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 03:25:20,265][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2023-09-13 03:25:43,830][valid][INFO] - {"epoch": 7, "valid_loss": "4.327", "valid_ntokens": "7899.77", "valid_nsentences": "55.2525", "valid_prob_perplexity": "62.782", "valid_code_perplexity": "62.67", "valid_temp": "1.964", "valid_loss_0": "4.188", "valid_loss_1": "0.13", "valid_loss_2": "0.009", "valid_accuracy": "0.32222", "valid_wps": "33199.6", "valid_wpb": "7899.8", "valid_bsz": "55.3", "valid_num_updates": "3633", "valid_best_loss": "4.327"}
[2023-09-13 03:25:43,832][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 3633 updates
[2023-09-13 03:25:43,833][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 03:25:46,481][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 03:25:47,892][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 3633 updates, score 4.327) (writing took 4.0605399899650365 seconds)
[2023-09-13 03:25:47,893][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2023-09-13 03:25:47,894][train][INFO] - {"epoch": 7, "train_loss": "4.645", "train_ntokens": "149453", "train_nsentences": "538.416", "train_prob_perplexity": "63.086", "train_code_perplexity": "62.948", "train_temp": "1.967", "train_loss_0": "4.505", "train_loss_1": "0.13", "train_loss_2": "0.01", "train_accuracy": "0.26354", "train_wps": "37217.2", "train_ups": "0.25", "train_wpb": "149453", "train_bsz": "538.4", "train_num_updates": "3633", "train_lr": "5.67656e-05", "train_gnorm": "0.649", "train_loss_scale": "16", "train_train_wall": "2052", "train_gb_free": "13.3", "train_wall": "14648"}
[2023-09-13 03:25:47,896][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 03:25:47,996][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2023-09-13 03:25:48,229][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 03:25:48,232][fairseq.trainer][INFO] - begin training epoch 8
[2023-09-13 03:25:48,233][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(8553.6992, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2324, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 03:33:02,576][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2023-09-13 03:36:52,843][train_inner][INFO] - {"epoch": 8, "update": 7.322, "loss": "4.566", "ntokens": "148944", "nsentences": "539.685", "prob_perplexity": "64.606", "code_perplexity": "64.473", "temp": "1.963", "loss_0": "4.427", "loss_1": "0.13", "loss_2": "0.009", "accuracy": "0.27265", "wps": "36327.7", "ups": "0.24", "wpb": "148944", "bsz": "539.7", "num_updates": "3800", "lr": "5.9375e-05", "gnorm": "0.631", "loss_scale": "8", "train_wall": "790", "gb_free": "13", "wall": "15313"}
loss: tensor(6905.4883, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6958, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7501.7725, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2527, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 03:50:10,322][train_inner][INFO] - {"epoch": 8, "update": 7.706, "loss": "4.536", "ntokens": "149708", "nsentences": "536.87", "prob_perplexity": "65.593", "code_perplexity": "65.464", "temp": "1.961", "loss_0": "4.398", "loss_1": "0.129", "loss_2": "0.009", "accuracy": "0.27515", "wps": "37545.3", "ups": "0.25", "wpb": "149708", "bsz": "536.9", "num_updates": "4000", "lr": "6.25e-05", "gnorm": "0.613", "loss_scale": "16", "train_wall": "796", "gb_free": "12.7", "wall": "16111"}
[2023-09-13 03:50:18,223][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(8031.4648, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1367, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8520.1719, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7578, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6242.0684, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3730, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6589.2227, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3049, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7235.9141, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3513, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7031.7241, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2979, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6929.7163, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2510, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7429.6147, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4385, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5496.9043, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0615, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8576.2197, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1017, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8512.3711, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0225, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3987.2329, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3127, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6546.4644, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6416, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6847.9380, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6089, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7068.1387, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1393, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8979.5918, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2563, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8875.6973, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4749, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8925.4092, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6191, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7772.5400, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8438, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8335.8398, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3340, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8245.2441, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1670, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7847.9297, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1338, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8103.4893, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1888, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7812.2363, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0127, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4385.7559, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6787, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8319.2793, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1006, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6435.9883, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1660, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7830.1367, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1807, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8352.1104, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0146, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7895.8486, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4082, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7045.4956, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8242, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8196.7793, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8042, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7527.5811, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7905, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7556.4780, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4160, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5590.7891, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4238, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6774.1206, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9155, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8234.3760, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1621, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8198.1094, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5840, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(4422.8994, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0726, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7748.5620, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0108, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:00:13,146][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 04:00:13,148][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 04:00:13,346][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2023-09-13 04:00:36,987][valid][INFO] - {"epoch": 8, "valid_loss": "4.261", "valid_ntokens": "7890.49", "valid_nsentences": "55.2525", "valid_prob_perplexity": "65.423", "valid_code_perplexity": "65.322", "valid_temp": "1.959", "valid_loss_0": "4.123", "valid_loss_1": "0.13", "valid_loss_2": "0.009", "valid_accuracy": "0.3291", "valid_wps": "33280.6", "valid_wpb": "7890.5", "valid_bsz": "55.3", "valid_num_updates": "4152", "valid_best_loss": "4.261"}
[2023-09-13 04:00:36,989][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 4152 updates
[2023-09-13 04:00:36,990][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 04:00:39,407][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 04:00:40,733][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 4152 updates, score 4.261) (writing took 3.743774772970937 seconds)
[2023-09-13 04:00:40,733][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2023-09-13 04:00:40,734][train][INFO] - {"epoch": 8, "train_loss": "4.537", "train_ntokens": "149397", "train_nsentences": "538.395", "train_prob_perplexity": "65.542", "train_code_perplexity": "65.414", "train_temp": "1.961", "train_loss_0": "4.398", "train_loss_1": "0.129", "train_loss_2": "0.009", "train_accuracy": "0.27516", "train_wps": "37048.8", "train_ups": "0.25", "train_wpb": "149397", "train_bsz": "538.4", "train_num_updates": "4152", "train_lr": "6.4875e-05", "train_gnorm": "0.617", "train_loss_scale": "8", "train_train_wall": "2061", "train_gb_free": "13.4", "train_wall": "16741"}
[2023-09-13 04:00:40,736][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 04:00:40,819][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2023-09-13 04:00:41,046][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 04:00:41,049][fairseq.trainer][INFO] - begin training epoch 9
[2023-09-13 04:00:41,050][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(5233.4917, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2578, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6364.9443, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0907, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7366.6685, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2734, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7479.7588, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4836, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:03:51,239][train_inner][INFO] - {"epoch": 9, "update": 8.092, "loss": "4.511", "ntokens": "148842", "nsentences": "537.625", "prob_perplexity": "66.497", "code_perplexity": "66.375", "temp": "1.959", "loss_0": "4.372", "loss_1": "0.129", "loss_2": "0.009", "accuracy": "0.2775", "wps": "36262.6", "ups": "0.24", "wpb": "148842", "bsz": "537.6", "num_updates": "4200", "lr": "6.5625e-05", "gnorm": "0.618", "loss_scale": "8", "train_wall": "791", "gb_free": "12.8", "wall": "16931"}
[2023-09-13 04:05:13,867][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
loss: tensor(3832.6509, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0176, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7960.0142, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0651, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:17:07,892][train_inner][INFO] - {"epoch": 9, "update": 8.478, "loss": "4.486", "ntokens": "149800", "nsentences": "539.86", "prob_perplexity": "67.712", "code_perplexity": "67.591", "temp": "1.957", "loss_0": "4.348", "loss_1": "0.129", "loss_2": "0.009", "accuracy": "0.27952", "wps": "37607.3", "ups": "0.25", "wpb": "149800", "bsz": "539.9", "num_updates": "4400", "lr": "6.875e-05", "gnorm": "0.58", "loss_scale": "4", "train_wall": "795", "gb_free": "12.9", "wall": "17728"}
loss: tensor(7034.8154, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0880, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7974.7627, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3376, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:24:23,609][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
loss: tensor(8503.2500, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1743, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3629.5923, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0809, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7863.1899, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5986, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:30:26,736][train_inner][INFO] - {"epoch": 9, "update": 8.864, "loss": "4.462", "ntokens": "149770", "nsentences": "538.93", "prob_perplexity": "68.894", "code_perplexity": "68.773", "temp": "1.956", "loss_0": "4.325", "loss_1": "0.129", "loss_2": "0.009", "accuracy": "0.2812", "wps": "37496.6", "ups": "0.25", "wpb": "149770", "bsz": "538.9", "num_updates": "4600", "lr": "7.1875e-05", "gnorm": "0.571", "loss_scale": "4", "train_wall": "798", "gb_free": "13", "wall": "18527"}
[2023-09-13 04:30:57,490][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
tensor(8869.0312, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0059, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7519.9966, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5938, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8622.6777, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2402, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6548.2432, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6719, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7612.0625, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8389, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7817.1040, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5894, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8927.9658, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9873, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7234.4077, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0461, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8363.9404, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7866, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8557.6416, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7964, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7324.3916, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3760, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6989.8013, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2031, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6218.7935, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1260, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7049.7524, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0391, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8573.2842, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7739, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7826.9360, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4824, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6987.0405, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7988, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7858.7114, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3301, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5779.2324, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9272, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8533.3340, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3601, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7224.1992, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7646, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4134.2075, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0205, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6387.5703, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3203, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8219.6016, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7900, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8164.5381, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0485, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(9201.1143, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0044, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6966.7109, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6548, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5501.2915, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1366, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8183.9492, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1726, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6589.8789, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1344, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7433.4121, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1142, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8316.5732, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1602, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(8014.0098, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0084, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:35:03,254][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 04:35:03,255][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 04:35:03,379][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2023-09-13 04:35:26,959][valid][INFO] - {"epoch": 9, "valid_loss": "4.228", "valid_ntokens": "7881.95", "valid_nsentences": "55.2525", "valid_prob_perplexity": "69.607", "valid_code_perplexity": "69.51", "valid_temp": "1.954", "valid_loss_0": "4.09", "valid_loss_1": "0.129", "valid_loss_2": "0.009", "valid_accuracy": "0.32997", "valid_wps": "33313.6", "valid_wpb": "7881.9", "valid_bsz": "55.3", "valid_num_updates": "4670", "valid_best_loss": "4.228"}
[2023-09-13 04:35:26,960][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 4670 updates
[2023-09-13 04:35:26,961][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 04:35:29,588][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 04:35:30,996][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 4670 updates, score 4.228) (writing took 4.035282504046336 seconds)
[2023-09-13 04:35:30,996][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2023-09-13 04:35:30,997][train][INFO] - {"epoch": 9, "train_loss": "4.475", "train_ntokens": "149462", "train_nsentences": "538.344", "train_prob_perplexity": "68.325", "train_code_perplexity": "68.204", "train_temp": "1.956", "train_loss_0": "4.337", "train_loss_1": "0.129", "train_loss_2": "0.009", "train_accuracy": "0.28026", "train_wps": "37039.1", "train_ups": "0.25", "train_wpb": "149462", "train_bsz": "538.3", "train_num_updates": "4670", "train_lr": "7.29687e-05", "train_gnorm": "0.582", "train_loss_scale": "2", "train_train_wall": "2059", "train_gb_free": "12.8", "train_wall": "18831"}
[2023-09-13 04:35:31,002][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 04:35:31,095][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2023-09-13 04:35:31,323][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 04:35:31,326][fairseq.trainer][INFO] - begin training epoch 10
[2023-09-13 04:35:31,326][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7142.7573, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1021, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(4842.5308, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1792, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4174.4224, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1484, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7653.8271, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5000, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: tensor(7324.8911, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1225, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5597.8555, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0147, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6382.5059, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2036, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7270.2300, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9307, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7108.5278, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3105, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7634.3833, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1077, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6899.6934, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3721, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5488.9175, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0681, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7833.6240, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3564, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7200.4287, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5181, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7223.6294, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4922, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7096.3418, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0359, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7636.7793, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4136, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7275.9902, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4636, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6257.8662, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7164.9751, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1904, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6624.4648, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4700, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:44:06,253][train_inner][INFO] - {"epoch": 10, "update": 9.25, "loss": "4.448", "ntokens": "149245", "nsentences": "535.6", "prob_perplexity": "69.947", "code_perplexity": "69.83", "temp": "1.954", "loss_0": "4.311", "loss_1": "0.129", "loss_2": "0.009", "accuracy": "0.28256", "wps": "36422.8", "ups": "0.24", "wpb": "149246", "bsz": "535.6", "num_updates": "4800", "lr": "7.5e-05", "gnorm": "0.578", "loss_scale": "2", "train_wall": "790", "gb_free": "12.6", "wall": "19346"}
loss: tensor(6913.2456, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6821, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:53:15,149][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
loss: tensor(5332.1919, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6953, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7006.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3621, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 04:57:26,530][train_inner][INFO] - {"epoch": 10, "update": 9.635, "loss": "4.413", "ntokens": "149677", "nsentences": "540.975", "prob_perplexity": "71.044", "code_perplexity": "70.922", "temp": "1.952", "loss_0": "4.276", "loss_1": "0.128", "loss_2": "0.009", "accuracy": "0.28633", "wps": "37406.2", "ups": "0.25", "wpb": "149676", "bsz": "541", "num_updates": "5000", "lr": "7.8125e-05", "gnorm": "0.561", "loss_scale": "2", "train_wall": "799", "gb_free": "12.9", "wall": "20147"}
loss: tensor(6836.9961, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5879, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3795.8755, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6548, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 05:09:57,642][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 05:09:57,643][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 05:09:57,830][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2023-09-13 05:10:21,577][valid][INFO] - {"epoch": 10, "valid_loss": "4.161", "valid_ntokens": "7867.53", "valid_nsentences": "55.2525", "valid_prob_perplexity": "69.588", "valid_code_perplexity": "69.474", "valid_temp": "1.949", "valid_loss_0": "4.023", "valid_loss_1": "0.129", "valid_loss_2": "0.009", "valid_accuracy": "0.33921", "valid_wps": "33030.9", "valid_wpb": "7867.5", "valid_bsz": "55.3", "valid_num_updates": "5190", "valid_best_loss": "4.161"}
[2023-09-13 05:10:21,579][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 5190 updates
[2023-09-13 05:10:21,580][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 05:10:24,062][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 05:10:25,416][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 5190 updates, score 4.161) (writing took 3.837249518954195 seconds)
[2023-09-13 05:10:25,416][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2023-09-13 05:10:25,417][train][INFO] - {"epoch": 10, "train_loss": "4.413", "train_ntokens": "149490", "train_nsentences": "538.273", "train_prob_perplexity": "71.079", "train_code_perplexity": "70.957", "train_temp": "1.951", "train_loss_0": "4.276", "train_loss_1": "0.128", "train_loss_2": "0.009", "train_accuracy": "0.28619", "train_wps": "37115.1", "train_ups": "0.25", "train_wpb": "149490", "train_bsz": "538.3", "train_num_updates": "5190", "train_lr": "8.10938e-05", "train_gnorm": "0.564", "train_loss_scale": "2", "train_train_wall": "2063", "train_gb_free": "13.8", "train_wall": "20926"}
[2023-09-13 05:10:25,419][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 05:10:25,510][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2023-09-13 05:10:25,750][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 05:10:25,753][fairseq.trainer][INFO] - begin training epoch 11
[2023-09-13 05:10:25,754][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-13 05:11:04,261][train_inner][INFO] - {"epoch": 11, "update": 10.019, "loss": "4.393", "ntokens": "148941", "nsentences": "534.55", "prob_perplexity": "71.71", "code_perplexity": "71.583", "temp": "1.95", "loss_0": "4.256", "loss_1": "0.128", "loss_2": "0.009", "accuracy": "0.28801", "wps": "36428", "ups": "0.24", "wpb": "148941", "bsz": "534.5", "num_updates": "5200", "lr": "8.125e-05", "gnorm": "0.579", "loss_scale": "4", "train_wall": "788", "gb_free": "13.4", "wall": "20964"}
loss: tensor(5913.2222, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9097, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7650.2319, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2781, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 05:15:08,460][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
loss: tensor(7849.7256, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5605, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6997.9272, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6758, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 05:24:22,785][train_inner][INFO] - {"epoch": 11, "update": 10.405, "loss": "4.371", "ntokens": "150031", "nsentences": "541.47", "prob_perplexity": "72.484", "code_perplexity": "72.356", "temp": "1.948", "loss_0": "4.234", "loss_1": "0.128", "loss_2": "0.009", "accuracy": "0.29047", "wps": "37577.2", "ups": "0.25", "wpb": "150031", "bsz": "541.5", "num_updates": "5400", "lr": "8.4375e-05", "gnorm": "0.543", "loss_scale": "2", "train_wall": "797", "gb_free": "12.5", "wall": "21763"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6540.8398, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9878, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6873.8057, device='cuda:2')
loss_ent_max: tensor(-1.3232, device='cuda:2', dtype=torch.float16)
loss: tensor(7576.9487, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1162, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8534.9326, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0596, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8097.8716, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0112, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6611.5122, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7871, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7205.9150, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2046, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8453.3906, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0918, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8353.3066, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5312, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5938.0322, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8574, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7844.3613, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7871, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8114.0171, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9062, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7675.7480, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7090, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7369.8545, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0439, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6992.6045, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7290, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7559.3413, device='cuda:2', grad_fn=<NllLossBackward0>)
loss: tensor(5928.5454, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7388, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8298.2256, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6182, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7034.2485, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4456, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5921.4526, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5942, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7957.4082, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9106, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8197.9854, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1953, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 05:33:39,050][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2023-09-13 05:37:38,801][train_inner][INFO] - {"epoch": 11, "update": 10.791, "loss": "4.341", "ntokens": "149531", "nsentences": "537.925", "prob_perplexity": "73.194", "code_perplexity": "73.054", "temp": "1.946", "loss_0": "4.204", "loss_1": "0.128", "loss_2": "0.009", "accuracy": "0.29412", "wps": "37569.9", "ups": "0.25", "wpb": "149531", "bsz": "537.9", "num_updates": "5600", "lr": "8.75e-05", "gnorm": "0.557", "loss_scale": "2", "train_wall": "795", "gb_free": "13", "wall": "22559"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6123.3550, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4502, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8139.4482, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0723, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6071.5415, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7959, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6682.5493, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2822, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6436.3867, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2223, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7004.5229, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3223, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6722.7563, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9028, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7605.9849, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6294, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3978.8589, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2976, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4139.2378, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8096, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5206.6528, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0146, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6631.7920, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9937, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3069.1294, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8755, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6776.9644, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3467, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6564.1050, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2217, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6149.1143, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6777, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: tensor(-0.3977, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6967.2568, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1543, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6999.3237, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6709, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6903.9751, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5454, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5127.1514, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6572, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7290.2256, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1614, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5137.5610, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9077, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6914.3530, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4490, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8205.3848, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0938, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7350.8423, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3896, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5376.9902, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5371, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6535.4961, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3125, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7702.2441, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7402, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7098.5747, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8931, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7801.5234, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3633, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7359.6118, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9868, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 05:44:39,559][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 05:44:39,560][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 05:44:39,673][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2023-09-13 05:45:03,225][valid][INFO] - {"epoch": 11, "valid_loss": "4.12", "valid_ntokens": "7893.55", "valid_nsentences": "55.2525", "valid_prob_perplexity": "70.703", "valid_code_perplexity": "70.572", "valid_temp": "1.944", "valid_loss_0": "3.982", "valid_loss_1": "0.128", "valid_loss_2": "0.01", "valid_accuracy": "0.33884", "valid_wps": "33445.6", "valid_wpb": "7893.5", "valid_bsz": "55.3", "valid_num_updates": "5709", "valid_best_loss": "4.12"}
[2023-09-13 05:45:03,226][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 5709 updates
[2023-09-13 05:45:03,227][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 05:45:05,687][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 05:45:07,057][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 5709 updates, score 4.12) (writing took 3.831224269932136 seconds)
[2023-09-13 05:45:07,058][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2023-09-13 05:45:07,059][train][INFO] - {"epoch": 11, "train_loss": "4.351", "train_ntokens": "149450", "train_nsentences": "538.358", "train_prob_perplexity": "72.983", "train_code_perplexity": "72.845", "train_temp": "1.946", "train_loss_0": "4.214", "train_loss_1": "0.128", "train_loss_2": "0.009", "train_accuracy": "0.29301", "train_wps": "37261.4", "train_ups": "0.25", "train_wpb": "149450", "train_bsz": "538.4", "train_num_updates": "5709", "train_lr": "8.92031e-05", "train_gnorm": "0.559", "train_loss_scale": "2", "train_train_wall": "2050", "train_gb_free": "13.6", "train_wall": "23007"}
[2023-09-13 05:45:07,062][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 05:45:07,145][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2023-09-13 05:45:07,373][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 05:45:07,377][fairseq.trainer][INFO] - begin training epoch 12
[2023-09-13 05:45:07,377][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(5572.3130, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4277, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 05:51:05,862][train_inner][INFO] - {"epoch": 12, "update": 11.175, "loss": "4.325", "ntokens": "149015", "nsentences": "535.595", "prob_perplexity": "73.768", "code_perplexity": "73.612", "temp": "1.944", "loss_0": "4.188", "loss_1": "0.128", "loss_2": "0.01", "accuracy": "0.29601", "wps": "36927.8", "ups": "0.25", "wpb": "149015", "bsz": "535.6", "num_updates": "5800", "lr": "9.0625e-05", "gnorm": "0.568", "loss_scale": "4", "train_wall": "778", "gb_free": "12.6", "wall": "23366"}
loss_ent_max: tensor(-0.5381, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7033.2271, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0840, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7374.6147, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4194, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5079.5879, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7910, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5839.7002, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1709, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8438.7070, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0537, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8167.4023, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7871, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4769.9106, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3730, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6472.8657, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7480, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7926.6567, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5811, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7816.7119, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0162, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: [2023-09-13 05:52:14,895][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
loss: tensor(7574.8452, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9277, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6550.3330, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3252, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3197.3420, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4084, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6204.4585, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0596, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 06:04:23,453][train_inner][INFO] - {"epoch": 12, "update": 11.56, "loss": "4.294", "ntokens": "149856", "nsentences": "539.605", "prob_perplexity": "74.621", "code_perplexity": "74.449", "temp": "1.942", "loss_0": "4.157", "loss_1": "0.127", "loss_2": "0.01", "accuracy": "0.29963", "wps": "37577.1", "ups": "0.25", "wpb": "149856", "bsz": "539.6", "num_updates": "6000", "lr": "9.375e-05", "gnorm": "0.538", "loss_scale": "2", "train_wall": "796", "gb_free": "12.4", "wall": "24164"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7661.0474, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8242, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3902.3000, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8184, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8035.6265, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9141, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7397.1040, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0347, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7300.3711, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2367, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7848.3057, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5327, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8187.0361, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2520, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6976.5947, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2249, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3719.3452, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7812, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7156.5864, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3196, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7175.1855, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1465, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7331.4404, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3252, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6355.3633, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0615, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7365.0923, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3508, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7805.7539, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4873, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: loss: tensor(7724.8169, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8267, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 06:11:02,278][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2023-09-13 06:11:57,006][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(7164.3608, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0417, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 06:17:38,481][train_inner][INFO] - {"epoch": 12, "update": 11.948, "loss": "4.273", "ntokens": "149580", "nsentences": "540.185", "prob_perplexity": "75.47", "code_perplexity": "75.269", "temp": "1.94", "loss_0": "4.136", "loss_1": "0.127", "loss_2": "0.01", "accuracy": "0.30185", "wps": "37629", "ups": "0.25", "wpb": "149580", "bsz": "540.2", "num_updates": "6200", "lr": "9.6875e-05", "gnorm": "0.544", "loss_scale": "1", "train_wall": "794", "gb_free": "12.8", "wall": "24959"}
[2023-09-13 06:19:21,156][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 06:19:21,157][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 06:19:21,353][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2023-09-13 06:19:44,972][valid][INFO] - {"epoch": 12, "valid_loss": "4.025", "valid_ntokens": "7901.38", "valid_nsentences": "55.2525", "valid_prob_perplexity": "72.216", "valid_code_perplexity": "72.079", "valid_temp": "1.939", "valid_loss_0": "3.887", "valid_loss_1": "0.128", "valid_loss_2": "0.01", "valid_accuracy": "0.35172", "valid_wps": "33348.8", "valid_wpb": "7901.4", "valid_bsz": "55.3", "valid_num_updates": "6227", "valid_best_loss": "4.025"}
[2023-09-13 06:19:44,974][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 6227 updates
[2023-09-13 06:19:44,975][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 06:19:47,486][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 06:19:48,857][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 6227 updates, score 4.025) (writing took 3.8834308579098433 seconds)
[2023-09-13 06:19:48,858][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2023-09-13 06:19:48,858][train][INFO] - {"epoch": 12, "train_loss": "4.291", "train_ntokens": "149485", "train_nsentences": "538.459", "train_prob_perplexity": "74.873", "train_code_perplexity": "74.691", "train_temp": "1.941", "train_loss_0": "4.153", "train_loss_1": "0.127", "train_loss_2": "0.01", "train_accuracy": "0.29994", "train_wps": "37195.4", "train_ups": "0.25", "train_wpb": "149485", "train_bsz": "538.5", "train_num_updates": "6227", "train_lr": "9.72969e-05", "train_gnorm": "0.55", "train_loss_scale": "1", "train_train_wall": "2050", "train_gb_free": "14", "train_wall": "25089"}
[2023-09-13 06:19:48,860][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 06:19:48,956][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2023-09-13 06:19:49,197][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 06:19:49,200][fairseq.trainer][INFO] - begin training epoch 13
[2023-09-13 06:19:49,201][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(5362.1387, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6592, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 06:31:11,677][train_inner][INFO] - {"epoch": 13, "update": 12.332, "loss": "4.26", "ntokens": "149095", "nsentences": "536.48", "prob_perplexity": "75.957", "code_perplexity": "75.737", "temp": "1.938", "loss_0": "4.122", "loss_1": "0.127", "loss_2": "0.011", "accuracy": "0.30333", "wps": "36669", "ups": "0.25", "wpb": "149095", "bsz": "536.5", "num_updates": "6400", "lr": "0.0001", "gnorm": "0.567", "loss_scale": "2", "train_wall": "784", "gb_free": "12.9", "wall": "25772"}
loss: tensor(6109.9897, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6787, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7605.6055, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0609, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8021.7407, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6855, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 06:38:26,286][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6618.2607, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6338, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5980.1274, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2168, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7607.2505, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1709, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7377.9102, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0378, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6795.0645, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8994, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7123.4985, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5059, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7727.3555, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3469, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7278.3115, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1124, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6814.3291, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0204, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3427.8601, device='cuda:3')
loss_ent_max: tensor(-0.7378, device='cuda:3', dtype=torch.float16)
loss: tensor(7661.6704, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9707, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6761.1221, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0980, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6874.4580, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3918, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6318.5415, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0225, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8210.8545, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1995, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: [2023-09-13 06:44:30,579][train_inner][INFO] - {"epoch": 13, "update": 12.718, "loss": "4.244", "ntokens": "149926", "nsentences": "537.685", "prob_perplexity": "76.909", "code_perplexity": "76.612", "temp": "1.936", "loss_0": "4.106", "loss_1": "0.127", "loss_2": "0.011", "accuracy": "0.30497", "wps": "37533.1", "ups": "0.25", "wpb": "149926", "bsz": "537.7", "num_updates": "6600", "lr": "0.000103125", "gnorm": "0.546", "loss_scale": "1", "train_wall": "798", "gb_free": "12.8", "wall": "26571"}
loss: tensor(6201.3071, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3616, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6826.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3538, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6440.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7656, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7236.1348, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6284, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6445.2319, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7612, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 06:54:12,647][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 06:54:12,648][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 06:54:12,779][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2023-09-13 06:54:36,389][valid][INFO] - {"epoch": 13, "valid_loss": "3.969", "valid_ntokens": "7921.27", "valid_nsentences": "55.2525", "valid_prob_perplexity": "72.763", "valid_code_perplexity": "72.487", "valid_temp": "1.934", "valid_loss_0": "3.829", "valid_loss_1": "0.128", "valid_loss_2": "0.012", "valid_accuracy": "0.36121", "valid_wps": "33352.8", "valid_wpb": "7921.3", "valid_bsz": "55.3", "valid_num_updates": "6747", "valid_best_loss": "3.969"}
[2023-09-13 06:54:36,391][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 6747 updates
[2023-09-13 06:54:36,392][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 06:54:38,801][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 06:54:40,180][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 6747 updates, score 3.969) (writing took 3.7886774350190535 seconds)
[2023-09-13 06:54:40,180][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2023-09-13 06:54:40,181][train][INFO] - {"epoch": 13, "train_loss": "4.245", "train_ntokens": "149572", "train_nsentences": "538.415", "train_prob_perplexity": "76.932", "train_code_perplexity": "76.636", "train_temp": "1.936", "train_loss_0": "4.107", "train_loss_1": "0.127", "train_loss_2": "0.011", "train_accuracy": "0.30487", "train_wps": "37190.5", "train_ups": "0.25", "train_wpb": "149572", "train_bsz": "538.4", "train_num_updates": "6747", "train_lr": "0.000105422", "train_gnorm": "0.548", "train_loss_scale": "1", "train_train_wall": "2060", "train_gb_free": "13.5", "train_wall": "27180"}
[2023-09-13 06:54:40,183][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 06:54:40,269][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2023-09-13 06:54:40,515][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 06:54:40,519][fairseq.trainer][INFO] - begin training epoch 14
[2023-09-13 06:54:40,519][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(5943.3472, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1631, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 06:58:07,794][train_inner][INFO] - {"epoch": 14, "update": 13.102, "loss": "4.229", "ntokens": "149040", "nsentences": "538.795", "prob_perplexity": "78.054", "code_perplexity": "77.679", "temp": "1.934", "loss_0": "4.091", "loss_1": "0.127", "loss_2": "0.012", "accuracy": "0.30641", "wps": "36475.2", "ups": "0.24", "wpb": "149040", "bsz": "538.8", "num_updates": "6800", "lr": "0.00010625", "gnorm": "0.551", "loss_scale": "2", "train_wall": "788", "gb_free": "13", "wall": "27388"}
loss: tensor(7703.1968, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0610, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6204.4824, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1915, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 07:01:03,091][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8520.8721, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3782, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6836.8999, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1143, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4341.7466, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1514, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5341.0918, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1035, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5191.3232, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1663, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7606.9263, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2389, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5110.2720, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2159, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4708.6357, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3586, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5226.5024, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4041, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6834.5122, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1603, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7481.5029, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0684, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6159.7246, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0713, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8091.0928, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0647, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3631.1472, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3296, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5399.1387, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7026, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8055.3862, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2935, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6390.7393, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8774, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6922.5659, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8906, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7362.7314, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4897, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7859.8149, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8096, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7927.1123, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0533, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 07:11:23,592][train_inner][INFO] - {"epoch": 14, "update": 13.488, "loss": "4.207", "ntokens": "149834", "nsentences": "541.1", "prob_perplexity": "80.024", "code_perplexity": "79.534", "temp": "1.932", "loss_0": "4.068", "loss_1": "0.126", "loss_2": "0.012", "accuracy": "0.30858", "wps": "37656.3", "ups": "0.25", "wpb": "149834", "bsz": "541.1", "num_updates": "7000", "lr": "0.000109375", "gnorm": "0.524", "loss_scale": "1", "train_wall": "795", "gb_free": "12.9", "wall": "28184"}
loss: tensor(7916.0425, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7573, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7978.9751, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1715, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4869.5234, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2693, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5857.4824, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2827, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 07:22:39,376][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2023-09-13 07:24:44,085][train_inner][INFO] - {"epoch": 14, "update": 13.873, "loss": "4.194", "ntokens": "149831", "nsentences": "536.995", "prob_perplexity": "82.333", "code_perplexity": "81.716", "temp": "1.93", "loss_0": "4.055", "loss_1": "0.126", "loss_2": "0.013", "accuracy": "0.30951", "wps": "37434.6", "ups": "0.25", "wpb": "149831", "bsz": "537", "num_updates": "7200", "lr": "0.0001125", "gnorm": "0.547", "loss_scale": "1", "train_wall": "799", "gb_free": "12.8", "wall": "28984"}
loss: tensor(7414.5137, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0381, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 07:29:00,162][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 07:29:00,163][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 07:29:00,329][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2023-09-13 07:29:24,059][valid][INFO] - {"epoch": 14, "valid_loss": "3.952", "valid_ntokens": "7900.29", "valid_nsentences": "55.2525", "valid_prob_perplexity": "76.959", "valid_code_perplexity": "76.471", "valid_temp": "1.929", "valid_loss_0": "3.811", "valid_loss_1": "0.127", "valid_loss_2": "0.014", "valid_accuracy": "0.36403", "valid_wps": "33124.4", "valid_wpb": "7900.3", "valid_bsz": "55.3", "valid_num_updates": "7266", "valid_best_loss": "3.952"}
[2023-09-13 07:29:24,061][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 7266 updates
[2023-09-13 07:29:24,062][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 07:29:26,555][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 07:29:27,923][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 7266 updates, score 3.952) (writing took 3.8618451840011403 seconds)
[2023-09-13 07:29:27,923][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2023-09-13 07:29:27,924][train][INFO] - {"epoch": 14, "train_loss": "4.202", "train_ntokens": "149487", "train_nsentences": "538.459", "train_prob_perplexity": "81.196", "train_code_perplexity": "80.646", "train_temp": "1.931", "train_loss_0": "4.064", "train_loss_1": "0.126", "train_loss_2": "0.013", "train_accuracy": "0.30874", "train_wps": "37161.5", "train_ups": "0.25", "train_wpb": "149487", "train_bsz": "538.5", "train_num_updates": "7266", "train_lr": "0.000113531", "train_gnorm": "0.548", "train_loss_scale": "1", "train_train_wall": "2056", "train_gb_free": "16.2", "train_wall": "29268"}
[2023-09-13 07:29:27,925][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 07:29:28,007][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2023-09-13 07:29:28,254][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 07:29:28,257][fairseq.trainer][INFO] - begin training epoch 15
[2023-09-13 07:29:28,258][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(6591.2661, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4448, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6765.2158, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2507, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6950.5850, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6387, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 07:38:18,072][train_inner][INFO] - {"epoch": 15, "update": 14.257, "loss": "4.188", "ntokens": "148794", "nsentences": "539.75", "prob_perplexity": "84.557", "code_perplexity": "83.862", "temp": "1.928", "loss_0": "4.049", "loss_1": "0.125", "loss_2": "0.014", "accuracy": "0.30957", "wps": "36559.4", "ups": "0.25", "wpb": "148794", "bsz": "539.8", "num_updates": "7400", "lr": "0.000115625", "gnorm": "0.559", "loss_scale": "1", "train_wall": "784", "gb_free": "12.8", "wall": "29798"}
tensor(6249.1304, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2393, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4814.0317, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2588, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7470.2993, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7192, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7124.4341, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1614, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6481.7676, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2271, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5241.6675, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6987, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(8081.2114, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0541, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7716.0220, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0482, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5950.7002, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8569, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7076.8579, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3250, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5119.6196, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5229, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5404.7905, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6631, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6466.7246, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0815, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7572.2222, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0279, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7462.1782, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0080, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6686.8745, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7324, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6965.2861, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1124, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7714.5391, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5723, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4243.6919, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3420, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4572.3765, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2737, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7312.6768, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3040, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6252.3818, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7422, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6936.7749, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4509, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6174.3711, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0927, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5081.7603, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0527, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6047.3613, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5479, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7459.4585, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3384, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7377.9985, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1785, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6936.9819, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3125, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7270.6167, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0646, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7111.0967, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5347, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7288.9736, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0232, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8339.8750, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(7607.3022, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4592, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7054.5444, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5537, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6276.3496, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4497, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 07:44:56,135][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(5953.3828, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2363, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 07:51:32,595][train_inner][INFO] - {"epoch": 15, "update": 14.643, "loss": "4.167", "ntokens": "149735", "nsentences": "540.435", "prob_perplexity": "87.087", "code_perplexity": "86.255", "temp": "1.926", "loss_0": "4.028", "loss_1": "0.125", "loss_2": "0.014", "accuracy": "0.31146", "wps": "37691.9", "ups": "0.25", "wpb": "149735", "bsz": "540.4", "num_updates": "7600", "lr": "0.00011875", "gnorm": "0.547", "loss_scale": "1", "train_wall": "793", "gb_free": "13", "wall": "30593"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6813.1147, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7246, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5097.6816, device='cuda:3')
loss_ent_max: tensor(-0.5396, device='cuda:3', dtype=torch.float16)
loss: tensor(7582.3066, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1217, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7196.2314, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0862, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4350.5532, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1943, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5814.6816, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3770, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6439.6841, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1807, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8218.1719, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4268, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5298.3325, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6772, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6796.3359, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6924, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6322.9883, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5981, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7335.7788, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6006, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6161.6143, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9482, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6047.9971, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2764, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4139.3179, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3425, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7580.5146, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7490, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4058.1709, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9810, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6371.0806, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1631, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7586.8315, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6880, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6484.2061, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4900, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4792.5205, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7139, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7287.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3311, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 08:03:29,898][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2023-09-13 08:03:46,114][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 08:03:46,115][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 08:03:46,215][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2023-09-13 08:04:09,959][valid][INFO] - {"epoch": 15, "valid_loss": "3.948", "valid_ntokens": "7827.61", "valid_nsentences": "55.2525", "valid_prob_perplexity": "85.446", "valid_code_perplexity": "84.489", "valid_temp": "1.924", "valid_loss_0": "3.808", "valid_loss_1": "0.125", "valid_loss_2": "0.016", "valid_accuracy": "0.36117", "valid_wps": "32692.1", "valid_wpb": "7827.6", "valid_bsz": "55.3", "valid_num_updates": "7785", "valid_best_loss": "3.948"}
[2023-09-13 08:04:09,961][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 7785 updates
[2023-09-13 08:04:09,962][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 08:04:12,403][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 08:04:13,768][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 7785 updates, score 3.948) (writing took 3.807124678976834 seconds)
[2023-09-13 08:04:13,768][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2023-09-13 08:04:13,769][train][INFO] - {"epoch": 15, "train_loss": "4.169", "train_ntokens": "149497", "train_nsentences": "538.52", "train_prob_perplexity": "87.517", "train_code_perplexity": "86.663", "train_temp": "1.926", "train_loss_0": "4.03", "train_loss_1": "0.125", "train_loss_2": "0.014", "train_accuracy": "0.31107", "train_wps": "37198", "train_ups": "0.25", "train_wpb": "149497", "train_bsz": "538.5", "train_num_updates": "7785", "train_lr": "0.000121641", "train_gnorm": "0.554", "train_loss_scale": "1", "train_train_wall": "2054", "train_gb_free": "16", "train_wall": "31354"}
[2023-09-13 08:04:13,771][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 08:04:13,852][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2023-09-13 08:04:14,081][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 08:04:14,084][fairseq.trainer][INFO] - begin training epoch 16
[2023-09-13 08:04:14,085][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-13 08:05:14,033][train_inner][INFO] - {"epoch": 16, "update": 15.029, "loss": "4.166", "ntokens": "149276", "nsentences": "534.365", "prob_perplexity": "89.805", "code_perplexity": "88.816", "temp": "1.924", "loss_0": "4.027", "loss_1": "0.124", "loss_2": "0.015", "accuracy": "0.31051", "wps": "36345.1", "ups": "0.24", "wpb": "149276", "bsz": "534.4", "num_updates": "7800", "lr": "0.000121875", "gnorm": "0.593", "loss_scale": "1", "train_wall": "792", "gb_free": "12.4", "wall": "31414"}
loss: tensor(7056.7407, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4182, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7378.4966, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2871, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4345.7324, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9790, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7698.2842, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0312, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(6370.4858, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7583, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5839.4243, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8857, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 08:18:24,666][train_inner][INFO] - {"epoch": 16, "update": 15.413, "loss": "4.141", "ntokens": "149760", "nsentences": "541.965", "prob_perplexity": "92.822", "code_perplexity": "91.698", "temp": "1.923", "loss_0": "4.002", "loss_1": "0.123", "loss_2": "0.016", "accuracy": "0.31312", "wps": "37883.6", "ups": "0.25", "wpb": "149760", "bsz": "542", "num_updates": "8000", "lr": "0.000125", "gnorm": "0.515", "loss_scale": "1", "train_wall": "789", "gb_free": "13", "wall": "32205"}
loss: tensor(7559.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4551, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 08:20:58,761][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7436.3657, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1941, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8214.6699, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0272, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6529.4590, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0527, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8475.9199, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2207, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6402.0049, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9438, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7062.4941, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6904, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7498.0684, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8491, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7468.7070, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7441, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7330.4395, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6401, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5529.7002, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7681, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4764.7319, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3898.2876, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1484, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5361.9414, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3528, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6419.0659, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4102, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5823.8916, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3809, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: loss: tensor(5233.4595, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4150, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7125.8418, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1875, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 08:31:31,743][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2023-09-13 08:31:43,261][train_inner][INFO] - {"epoch": 16, "update": 15.8, "loss": "4.126", "ntokens": "149681", "nsentences": "539.985", "prob_perplexity": "96.096", "code_perplexity": "94.768", "temp": "1.921", "loss_0": "3.988", "loss_1": "0.123", "loss_2": "0.016", "accuracy": "0.3137", "wps": "37486", "ups": "0.25", "wpb": "149681", "bsz": "540", "num_updates": "8200", "lr": "0.000128125", "gnorm": "0.557", "loss_scale": "0.5", "train_wall": "797", "gb_free": "13", "wall": "33003"}
loss: tensor(7502.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6924, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7202.4375, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9014, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6396.8730, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1650, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 08:38:29,767][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 08:38:29,768][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 08:38:29,957][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2023-09-13 08:38:53,761][valid][INFO] - {"epoch": 16, "valid_loss": "3.983", "valid_ntokens": "7888.31", "valid_nsentences": "55.2525", "valid_prob_perplexity": "80.927", "valid_code_perplexity": "79.71", "valid_temp": "1.919", "valid_loss_0": "3.84", "valid_loss_1": "0.126", "valid_loss_2": "0.017", "valid_accuracy": "0.36114", "valid_wps": "32869", "valid_wpb": "7888.3", "valid_bsz": "55.3", "valid_num_updates": "8304", "valid_best_loss": "3.948"}
[2023-09-13 08:38:53,762][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 8304 updates
[2023-09-13 08:38:53,763][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_last.pt
[2023-09-13 08:38:56,292][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_last.pt
[2023-09-13 08:38:56,355][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 16 @ 8304 updates, score 3.983) (writing took 2.5927714420249686 seconds)
[2023-09-13 08:38:56,356][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2023-09-13 08:38:56,356][train][INFO] - {"epoch": 16, "train_loss": "4.135", "train_ntokens": "149482", "train_nsentences": "538.308", "train_prob_perplexity": "95.176", "train_code_perplexity": "93.902", "train_temp": "1.921", "train_loss_0": "3.996", "train_loss_1": "0.123", "train_loss_2": "0.016", "train_accuracy": "0.31295", "train_wps": "37252.3", "train_ups": "0.25", "train_wpb": "149482", "train_bsz": "538.3", "train_num_updates": "8304", "train_lr": "0.00012975", "train_gnorm": "0.55", "train_loss_scale": "0.5", "train_train_wall": "2052", "train_gb_free": "14.3", "train_wall": "33437"}
[2023-09-13 08:38:56,358][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 08:38:56,459][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2023-09-13 08:38:56,713][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 08:38:56,716][fairseq.trainer][INFO] - begin training epoch 17
[2023-09-13 08:38:56,716][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(6727.3564, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4355, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 08:45:14,391][train_inner][INFO] - {"epoch": 17, "update": 16.184, "loss": "4.126", "ntokens": "149280", "nsentences": "532.145", "prob_perplexity": "99.039", "code_perplexity": "97.548", "temp": "1.919", "loss_0": "3.987", "loss_1": "0.122", "loss_2": "0.017", "accuracy": "0.31282", "wps": "36808", "ups": "0.25", "wpb": "149280", "bsz": "532.1", "num_updates": "8400", "lr": "0.00013125", "gnorm": "0.552", "loss_scale": "0.5", "train_wall": "783", "gb_free": "12.9", "wall": "33815"}
tensor(-0.7471, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4848.4995, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7510, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6023.9590, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4387, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6727.6074, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4395, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4767.7256, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9995, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6748.0894, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9917, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6369.4609, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1758, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6898.7017, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6328, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5424.2686, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1650, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6950.5767, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7925, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5172.7808, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2881, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6583.9795, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5107, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6516.0601, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9453, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5143.7915, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1504, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7153.8467, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2070, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5224.2925, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8926, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5432.5874, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2793, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7009.9736, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2266, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6794.2402, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6855, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4743.8794, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7324, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6681.6123, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2480, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7062.6255, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0557, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6244.3193, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3789, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 08:58:18,779][train_inner][INFO] - {"epoch": 17, "update": 16.568, "loss": "4.098", "ntokens": "149457", "nsentences": "539.185", "prob_perplexity": "102.992", "code_perplexity": "101.338", "temp": "1.917", "loss_0": "3.96", "loss_1": "0.121", "loss_2": "0.017", "accuracy": "0.31537", "wps": "38108", "ups": "0.25", "wpb": "149457", "bsz": "539.2", "num_updates": "8600", "lr": "0.000134375", "gnorm": "0.562", "loss_scale": "1", "train_wall": "783", "gb_free": "13.1", "wall": "34599"}
loss: tensor(6846.6743, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3359, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7505.5854, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7363, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7682.1875, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2402, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 09:05:36,344][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2023-09-13 09:06:59,569][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2023-09-13 09:11:43,806][train_inner][INFO] - {"epoch": 17, "update": 16.956, "loss": "4.079", "ntokens": "149933", "nsentences": "541.32", "prob_perplexity": "106.52", "code_perplexity": "104.762", "temp": "1.915", "loss_0": "3.942", "loss_1": "0.12", "loss_2": "0.017", "accuracy": "0.3168", "wps": "37249.3", "ups": "0.25", "wpb": "149933", "bsz": "541.3", "num_updates": "8800", "lr": "0.0001375", "gnorm": "0.548", "loss_scale": "0.5", "train_wall": "804", "gb_free": "12.6", "wall": "35404"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(5271.6670, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1436, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8239.9111, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4917, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7128.2842, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5024, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5201.9971, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4270, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6719.5664, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5791, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3961.0977, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7402, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7435.5693, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4758, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6029.6772, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6934, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6980.9780, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2871, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7228.3101, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9424, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7892.3101, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3779, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6391.4062, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5557, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7353.1338, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8184, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5701.7871, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8242, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4314.2095, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9648, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7379.2925, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2109, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5916.3018, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7393, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5550.3350, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0625, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7097.9692, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6440, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4778.9331, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1562, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7147.4985, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0527, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6433.3130, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7891, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7869.8428, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4688, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6249.2246, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9424, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6526.6670, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4414, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7072.0176, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2910, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7344.3525, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7812, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7225.4082, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8232, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5079.2681, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0293, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6897.6401, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4551, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6776.7759, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1680, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 09:13:10,071][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 09:13:10,072][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 09:13:10,250][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2023-09-13 09:13:33,818][valid][INFO] - {"epoch": 17, "valid_loss": "3.865", "valid_ntokens": "7908.25", "valid_nsentences": "55.2525", "valid_prob_perplexity": "94.046", "valid_code_perplexity": "92.176", "valid_temp": "1.914", "valid_loss_0": "3.725", "valid_loss_1": "0.123", "valid_loss_2": "0.017", "valid_accuracy": "0.36852", "valid_wps": "33492.1", "valid_wpb": "7908.3", "valid_bsz": "55.3", "valid_num_updates": "8823", "valid_best_loss": "3.865"}
[2023-09-13 09:13:33,819][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 8823 updates
[2023-09-13 09:13:33,820][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 09:13:36,263][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 09:13:37,598][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 8823 updates, score 3.865) (writing took 3.7784688270185143 seconds)
[2023-09-13 09:13:37,598][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2023-09-13 09:13:37,599][train][INFO] - {"epoch": 17, "train_loss": "4.096", "train_ntokens": "149484", "train_nsentences": "538.341", "train_prob_perplexity": "103.894", "train_code_perplexity": "102.225", "train_temp": "1.916", "train_loss_0": "3.958", "train_loss_1": "0.121", "train_loss_2": "0.017", "train_accuracy": "0.31531", "train_wps": "37276.8", "train_ups": "0.25", "train_wpb": "149484", "train_bsz": "538.3", "train_num_updates": "8823", "train_lr": "0.000137859", "train_gnorm": "0.556", "train_loss_scale": "0.5", "train_train_wall": "2050", "train_gb_free": "13.9", "train_wall": "35518"}
[2023-09-13 09:13:37,600][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 09:13:37,759][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2023-09-13 09:13:37,989][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 09:13:37,992][fairseq.trainer][INFO] - begin training epoch 18
[2023-09-13 09:13:37,992][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(3925.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0215, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7359.9375, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2617, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7472.8359, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5234, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 09:25:12,493][train_inner][INFO] - {"epoch": 18, "update": 17.34, "loss": "4.077", "ntokens": "148766", "nsentences": "538.465", "prob_perplexity": "109.225", "code_perplexity": "107.378", "temp": "1.913", "loss_0": "3.94", "loss_1": "0.12", "loss_2": "0.018", "accuracy": "0.31633", "wps": "36792.1", "ups": "0.25", "wpb": "148766", "bsz": "538.5", "num_updates": "9000", "lr": "0.000140625", "gnorm": "0.578", "loss_scale": "1", "train_wall": "779", "gb_free": "12.5", "wall": "36213"}
tensor(5579.7124, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2666, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3647.6401, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3477, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6023.9478, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8594, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7910.7959, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2627, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6671.9062, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5439, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(3499.6975, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0234, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5864.2686, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6055, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6341.1562, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5830, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7043.6938, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1543, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7949.6792, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0508, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7523.7129, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6250, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7928.4712, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8740, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6813.0811, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5801, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7007.4824, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0059, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7579.1245, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(5121.6523, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5078, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6944.7935, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7822, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 09:38:18,907][train_inner][INFO] - {"epoch": 18, "update": 17.724, "loss": "4.051", "ntokens": "149659", "nsentences": "537.785", "prob_perplexity": "113.088", "code_perplexity": "111.091", "temp": "1.911", "loss_0": "3.914", "loss_1": "0.119", "loss_2": "0.018", "accuracy": "0.31879", "wps": "38061.2", "ups": "0.25", "wpb": "149659", "bsz": "537.8", "num_updates": "9200", "lr": "0.00014375", "gnorm": "0.57", "loss_scale": "1", "train_wall": "785", "gb_free": "12.4", "wall": "36999"}
loss: tensor(6103.2920, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4453, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5133.8145, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7383, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7367.0352, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3027, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 09:41:15,985][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2023-09-13 09:46:21,567][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2023-09-13 09:47:46,088][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 09:47:46,089][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 09:47:46,219][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2023-09-13 09:48:09,726][valid][INFO] - {"epoch": 18, "valid_loss": "3.88", "valid_ntokens": "7913.87", "valid_nsentences": "55.2525", "valid_prob_perplexity": "95.178", "valid_code_perplexity": "93.091", "valid_temp": "1.909", "valid_loss_0": "3.739", "valid_loss_1": "0.123", "valid_loss_2": "0.018", "valid_accuracy": "0.36536", "valid_wps": "33505.7", "valid_wpb": "7913.9", "valid_bsz": "55.3", "valid_num_updates": "9342", "valid_best_loss": "3.865"}
[2023-09-13 09:48:09,727][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 9342 updates
[2023-09-13 09:48:09,728][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_last.pt
[2023-09-13 09:48:12,159][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_last.pt
[2023-09-13 09:48:12,211][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 18 @ 9342 updates, score 3.88) (writing took 2.4839881679508835 seconds)
[2023-09-13 09:48:12,212][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2023-09-13 09:48:12,212][train][INFO] - {"epoch": 18, "train_loss": "4.056", "train_ntokens": "149483", "train_nsentences": "538.239", "train_prob_perplexity": "112.809", "train_code_perplexity": "110.84", "train_temp": "1.911", "train_loss_0": "3.919", "train_loss_1": "0.119", "train_loss_2": "0.018", "train_accuracy": "0.31826", "train_wps": "37395.8", "train_ups": "0.25", "train_wpb": "149483", "train_bsz": "538.2", "train_num_updates": "9342", "train_lr": "0.000145969", "train_gnorm": "0.568", "train_loss_scale": "0.5", "train_train_wall": "2044", "train_gb_free": "14.4", "train_wall": "37592"}
[2023-09-13 09:48:12,214][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 09:48:12,417][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2023-09-13 09:48:12,686][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 09:48:12,691][fairseq.trainer][INFO] - begin training epoch 19
[2023-09-13 09:48:12,691][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(7216.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4961, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 09:52:04,792][train_inner][INFO] - {"epoch": 19, "update": 18.111, "loss": "4.043", "ntokens": "149384", "nsentences": "535.575", "prob_perplexity": "116.347", "code_perplexity": "114.3", "temp": "1.909", "loss_0": "3.906", "loss_1": "0.118", "loss_2": "0.019", "accuracy": "0.31904", "wps": "36175.5", "ups": "0.24", "wpb": "149384", "bsz": "535.6", "num_updates": "9400", "lr": "0.000146875", "gnorm": "0.559", "loss_scale": "0.5", "train_wall": "798", "gb_free": "12.7", "wall": "37825"}
loss: tensor(4308.7153, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7930, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6019.3652, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4180, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-13 10:02:57,586][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
[2023-09-13 10:05:20,174][train_inner][INFO] - {"epoch": 19, "update": 18.497, "loss": "4.013", "ntokens": "149660", "nsentences": "537.5", "prob_perplexity": "121.205", "code_perplexity": "119.054", "temp": "1.907", "loss_0": "3.877", "loss_1": "0.117", "loss_2": "0.019", "accuracy": "0.32178", "wps": "37632.1", "ups": "0.25", "wpb": "149660", "bsz": "537.5", "num_updates": "9600", "lr": "0.00015", "gnorm": "0.538", "loss_scale": "0.25", "train_wall": "794", "gb_free": "13", "wall": "38620"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7036.3940, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2285, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5300.8423, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2363, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7943.9761, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2480, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7372.5264, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1895, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7781.3013, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3555, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7199.4736, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2100, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6212.6499, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6973, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6666.8462, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5664, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6520.9053, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5566, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6597.4409, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9854, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5366.2012, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3750, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6673.6567, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3945, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3964.1790, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0859, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6622.9453, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2051, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6806.2881, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4590, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6545.9727, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1367, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6506.4053, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8857, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7509.8442, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4424, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6643.7954, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9707, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7027.3862, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0723, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6928.7539, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7617, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6881.8984, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1582, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5926.9370, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3086, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7129.2344, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8867, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7128.7197, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5312, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7066.2949, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9648, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(3739.9099, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0176, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5179.6191, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1270, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5148.5259, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7070, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7208.8979, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0586, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 10:18:26,480][train_inner][INFO] - {"epoch": 19, "update": 18.881, "loss": "3.998", "ntokens": "149777", "nsentences": "540.58", "prob_perplexity": "125.031", "code_perplexity": "122.8", "temp": "1.905", "loss_0": "3.863", "loss_1": "0.116", "loss_2": "0.019", "accuracy": "0.32295", "wps": "38096.4", "ups": "0.25", "wpb": "149777", "bsz": "540.6", "num_updates": "9800", "lr": "0.000153125", "gnorm": "0.546", "loss_scale": "0.25", "train_wall": "785", "gb_free": "13.1", "wall": "39407"}
loss: tensor(6212.1567, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3418, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 10:22:32,388][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-13 10:22:32,389][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 10:22:32,508][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
loss: tensor(4524.9028, device='cuda:0')
loss_ent_max: tensor(-2.6934, device='cuda:0', dtype=torch.float16)
[2023-09-13 10:22:56,067][valid][INFO] - {"epoch": 19, "valid_loss": "3.719", "valid_ntokens": "7852.62", "valid_nsentences": "55.2525", "valid_prob_perplexity": "111.473", "valid_code_perplexity": "108.107", "valid_temp": "1.904", "valid_loss_0": "3.58", "valid_loss_1": "0.119", "valid_loss_2": "0.02", "valid_accuracy": "0.38871", "valid_wps": "33228.7", "valid_wpb": "7852.6", "valid_bsz": "55.3", "valid_num_updates": "9862", "valid_best_loss": "3.719"}
[2023-09-13 10:22:56,069][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 9862 updates
[2023-09-13 10:22:56,070][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 10:22:58,571][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/23-21-32/checkpoints/checkpoint_best.pt
[2023-09-13 10:22:59,951][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 19 @ 9862 updates, score 3.719) (writing took 3.8826155960559845 seconds)
[2023-09-13 10:22:59,952][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2023-09-13 10:22:59,952][train][INFO] - {"epoch": 19, "train_loss": "4.009", "train_ntokens": "149508", "train_nsentences": "538.348", "train_prob_perplexity": "122.814", "train_code_perplexity": "120.639", "train_temp": "1.906", "train_loss_0": "3.873", "train_loss_1": "0.117", "train_loss_2": "0.019", "train_accuracy": "0.32203", "train_wps": "37238.4", "train_ups": "0.25", "train_wpb": "149508", "train_bsz": "538.3", "train_num_updates": "9862", "train_lr": "0.000154094", "train_gnorm": "0.544", "train_loss_scale": "0.5", "train_train_wall": "2056", "train_gb_free": "13.5", "train_wall": "39680"}
[2023-09-13 10:22:59,954][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-13 10:23:00,037][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2023-09-13 10:23:00,267][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-13 10:23:00,270][fairseq.trainer][INFO] - begin training epoch 20
[2023-09-13 10:23:00,270][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(4287.8135, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4297, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 10:32:05,179][train_inner][INFO] - {"epoch": 20, "update": 19.265, "loss": "3.985", "ntokens": "148984", "nsentences": "537.735", "prob_perplexity": "127.923", "code_perplexity": "125.703", "temp": "1.903", "loss_0": "3.85", "loss_1": "0.115", "loss_2": "0.02", "accuracy": "0.32451", "wps": "36395.4", "ups": "0.24", "wpb": "148984", "bsz": "537.7", "num_updates": "10000", "lr": "0.00015625", "gnorm": "0.528", "loss_scale": "0.5", "train_wall": "789", "gb_free": "12.7", "wall": "40225"}
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6353.3730, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0449, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5653.3979, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3047, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6625.3120, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0137, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: tensor(-2.6113, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4885.8203, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2773, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6551.8960, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6543, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7961.9785, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2930, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5913.9028, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7140.3555, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7490, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7146.5029, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2617, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7066.5811, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0430, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6623.9897, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9648, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3871.8232, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4082, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4787.7251, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1934, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5369.0732, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8262, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(5641.8608, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7949, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5938.8105, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7246, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6643.6436, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0820, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3977.8960, device='cuda:2', grad_fn=<NllLossBackward0>)loss: tensor(6746.9619, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2734, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5494.5498, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0859, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-13 10:43:34,346][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2023-09-13 10:45:17,632][train_inner][INFO] - {"epoch": 20, "update": 19.651, "loss": "3.953", "ntokens": "150011", "nsentences": "538.355", "prob_perplexity": "132.067", "code_perplexity": "129.808", "temp": "1.902", "loss_0": "3.819", "loss_1": "0.114", "loss_2": "0.019", "accuracy": "0.32785", "wps": "37860", "ups": "0.25", "wpb": "150011", "bsz": "538.4", "num_updates": "10200", "lr": "0.000159375", "gnorm": "0.549", "loss_scale": "0.5", "train_wall": "791", "gb_free": "12.6", "wall": "41018"}
Traceback (most recent call last):
  File "/root/anaconda3/bin/fairseq-hydra-train", line 8, in <module>
    sys.exit(cli_main())
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 87, in cli_main
    hydra_main()
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/main.py", line 32, in decorated_main
    _run_hydra(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 346, in _run_hydra
    run_and_report(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 198, in run_and_report
    return func()
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 347, in <lambda>
    lambda: hydra.run(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 107, in run
    return run_job(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/core/utils.py", line 129, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 27, in hydra_main
    _hydra_main(cfg)
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 56, in _hydra_main
    distributed_utils.call_main(cfg, pre_main, **kwargs)
  File "/home/Workspace/fairseq/fairseq/distributed/utils.py", line 379, in call_main
    torch.multiprocessing.spawn(
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 109, in join
    ready = multiprocessing.connection.wait(
  File "/root/anaconda3/lib/python3.9/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/root/anaconda3/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
Process SpawnProcess-4:
Process SpawnProcess-3:
Process SpawnProcess-1:
Process SpawnProcess-2:
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt

loss_ent_max: tensor(-3.2754, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6512.4917, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8555, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6847.0684, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3926, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4698.8525, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6621, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6197.5259, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6348, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
loss: tensor(4752.9883, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6836, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(4631.3564, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4648, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7006.1733, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3320, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6249.2769, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9766, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5513.7681, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5801, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6531.5093, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9785, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5911.0044, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6504, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
tensor(6638.6704, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7988, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6437.4800, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3926, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3905.0803, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9785, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6542.3936, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5137, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4484.9976, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2637, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6711.4722, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8145, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7107.5269, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9121, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7516.9399, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1465, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4111.0107, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6172, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6200.2197, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0586, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7137.9873, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6289, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4480.6699, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3887, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
/root/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
