DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=hydra_train
INFO:fairseq.distributed.utils:distributed init (rank 2): tcp://localhost:51446
[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:51446 (errno: 99 - Cannot assign requested address).
INFO:fairseq.distributed.utils:distributed init (rank 0): tcp://localhost:51446
INFO:fairseq.distributed.utils:distributed init (rank 1): tcp://localhost:51446
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1
INFO:fairseq.distributed.utils:distributed init (rank 3): tcp://localhost:51446
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 0
INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 1
INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 2
INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 3
[2023-09-11 23:32:11,878][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:51446', 'distributed_port': 51446, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1900000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1900000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.1, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'audio_pretraining', 'data': '/home/Workspace/fairseq/data', 'labels': None, 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': 250000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': none, 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 1}, 'criterion': {'_name': 'wav2vec', 'infonce': True, 'loss_weights': [0.1, 10.0], 'log_keys': ['prob_perplexity', 'code_perplexity', 'temp']}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2023-09-11 23:32:13,319][fairseq_cli.train][INFO] - Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
        (3): GELU(approximate='none')
      )
      (1-4): 4 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
      (5-6): 2 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.1, inplace=False)
  (dropout_features): Dropout(p=0.1, inplace=False)
  (quantizer): GumbelVectorQuantizer(
    (weight_proj): Linear(in_features=512, out_features=640, bias=True)
  )
  (project_q): Linear(in_features=256, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): Linear(in_features=768, out_features=256, bias=True)
)
[2023-09-11 23:32:13,321][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2023-09-11 23:32:13,321][fairseq_cli.train][INFO] - model: Wav2Vec2Model
[2023-09-11 23:32:13,321][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion
[2023-09-11 23:32:13,322][fairseq_cli.train][INFO] - num. shared model params: 95,454,848 (num. trained: 95,454,848)
[2023-09-11 23:32:13,323][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2023-09-11 23:32:13,330][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 5474, skipped 93 samples
[2023-09-11 23:32:13,401][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0
[2023-09-11 23:32:13,401][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-11 23:32:13,401][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias
[2023-09-11 23:32:13,402][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias
[2023-09-11 23:32:13,402][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias
[2023-09-11 23:32:13,402][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias
[2023-09-11 23:32:13,402][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias
[2023-09-11 23:32:13,402][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias
[2023-09-11 23:32:13,510][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2023-09-11 23:32:13,510][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-11 23:32:13,510][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-11 23:32:13,510][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-11 23:32:13,510][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 23.648 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-11 23:32:13,510][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2023-09-11 23:32:13,510][fairseq_cli.train][INFO] - training on 4 devices (GPUs/TPUs)
[2023-09-11 23:32:13,511][fairseq_cli.train][INFO] - max tokens per device = 1900000 and max sentences per device = None
[2023-09-11 23:32:13,511][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2023-09-11 23:32:13,512][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2023-09-11 23:32:13,512][fairseq.trainer][INFO] - loading train data for epoch 1
[2023-09-11 23:32:13,775][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 280531, skipped 710 samples
[2023-09-11 23:32:13,852][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-11 23:32:13,852][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2023-09-11 23:32:13,852][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2023-09-11 23:32:13,852][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2023-09-11 23:32:13,853][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2023-09-11 23:32:14,155][fairseq_cli.train][INFO] - begin dry-run validation on "valid" subset
[2023-09-11 23:32:14,156][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-11 23:32:14,157][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2023-09-11 23:32:14,157][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2023-09-11 23:32:14,157][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2023-09-11 23:32:14,157][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2023-09-11 23:32:30,068][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-11 23:32:30,073][fairseq.trainer][INFO] - begin training epoch 1
[2023-09-11 23:32:30,074][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-11 23:32:52,251][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
self.scaling_factor_for_vector: Parameter containing:
tensor([ 1.4305e-06,  1.4305e-06,  6.7949e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.5497e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,
         1.4305e-06,  1.6689e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.6093e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.5497e-06,  1.4305e-06,  1.5497e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  4.8280e-06,  1.4305e-06,  1.4305e-06,
         1.6093e-06,  1.4305e-06,  1.5497e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  2.0862e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,
         1.1623e-05,  1.4305e-06,  1.4305e-06,  2.5630e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.6093e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  4.7088e-06,  1.4305e-06,  1.4305e-06,
         1.9670e-06,  1.6689e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4901e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         2.0862e-06,  1.2398e-05,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  2.3842e-06,  1.4305e-06,  8.0466e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         4.4703e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  3.0398e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.3649e-05,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.0788e-05,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.5497e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.7285e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         8.8215e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4901e-06,  2.3842e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         2.4438e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         2.3246e-06,  1.4305e-06,  1.4305e-06,  1.7881e-06,  1.4305e-06,
         5.2452e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         3.6359e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.6093e-06,  1.4901e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  3.8743e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  7.3910e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  3.3379e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  2.0266e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  2.6822e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.3232e-05,  1.4305e-06,
         9.5963e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  2.5630e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4961e-05,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,
         1.4305e-06,  1.4901e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  7.9870e-06,  1.4305e-06, -1.4901e-05,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  7.0930e-06,
         1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.6093e-06,  1.4305e-06,  1.4305e-06,
         2.9802e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.5497e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.1683e-05,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.7285e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  5.0664e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  2.1458e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.8477e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06,  1.4305e-06,
         1.4305e-06,  1.4305e-06,  1.4305e-06,  1.4901e-06,  1.4305e-06],
       device='cuda:2', dtype=torch.float16, requires_grad=True)self.scaling_factor_for_vector: Parameter containing:
tensor([ 1.5497e-05,  1.5497e-05,  6.6042e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,  1.7345e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5557e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5557e-05,
         1.5676e-05,  1.5676e-05,  1.5736e-05,  1.7107e-05,  1.5497e-05,
         1.5497e-05,  1.9550e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5557e-05,  1.5557e-05,  1.5497e-05,  2.0206e-05,  1.5497e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5616e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.7226e-05,  1.5497e-05,  1.8954e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.6034e-05,  1.5497e-05,  1.5497e-05,
         1.5557e-05,  1.5676e-05,  4.8757e-05,  1.5497e-05,  1.5497e-05,
         2.0742e-05,  1.5497e-05,  1.7166e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5557e-05,  2.1815e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.6809e-05,  1.5497e-05,
         1.5497e-05,  1.5616e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.9550e-05,  1.5497e-05,
         8.8274e-05,  1.5497e-05,  1.5497e-05,  2.9385e-05,  1.5497e-05,
         1.5557e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.7166e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.6034e-05,  1.5616e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.7047e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  4.7803e-05,  1.5497e-05,  1.5616e-05,
         1.8537e-05,  1.6868e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.6212e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.6332e-05,  1.5736e-05,  1.5795e-05,  1.5497e-05,
         2.6703e-05,  6.5565e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5676e-05,
         1.5497e-05,  1.5497e-05,  4.0054e-05,  1.5497e-05,  3.2902e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.6212e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         4.3333e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,  2.6882e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  6.0439e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  9.9659e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.8299e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  2.1338e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5736e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5914e-05,  1.5497e-05,
         6.7174e-05,  1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.6809e-05,  3.3379e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5914e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         2.0444e-05,  1.5497e-05,  1.5497e-05,  1.5736e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5914e-05,  1.5497e-05,  1.5557e-05,
         2.4557e-05,  1.5497e-05,  1.5497e-05,  1.8775e-05,  1.5497e-05,
         4.9412e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5557e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         4.6849e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5557e-05,  1.5497e-05,  1.6153e-05,  1.5974e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5795e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  2.2352e-05,  1.6212e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  3.8326e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  5.7936e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  2.2888e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  2.2173e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  4.2737e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  7.2420e-05,  1.5497e-05,
         9.0003e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  3.7968e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5616e-05,
         1.5497e-05,  1.5497e-05,  1.5795e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.1015e-04,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5616e-05,  1.5497e-05,  1.5497e-05,  1.5795e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.6391e-05,
         1.5497e-05,  1.9550e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.6630e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5557e-05,  1.5497e-05,
         1.5497e-05,  1.5736e-05,  1.5497e-05,  1.5497e-05,  1.5676e-05,
         1.5497e-05,  1.5497e-05,  5.5552e-05,  1.5497e-05, -1.0687e-04,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  5.1439e-05,
         1.5497e-05,  1.5497e-05,  1.6034e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.8239e-05,  1.5497e-05,  1.5497e-05,
         3.8445e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  2.3663e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5676e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5795e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  9.8348e-05,  1.5497e-05,
         1.5557e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5557e-05,
         1.5557e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.6928e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  5.0485e-05,
         1.5497e-05,  1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5557e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5557e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5557e-05,  1.5497e-05,  2.7835e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.8358e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.6510e-05,  1.5497e-05,  1.5497e-05,
         1.5497e-05,  1.5497e-05,  1.5497e-05,  1.6451e-05,  1.5497e-05],
       device='cuda:3', dtype=torch.float16, requires_grad=True)loss: tensor(10248.5459, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0449, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11501.2305, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0935, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11639.2510, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5728, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-11 23:46:05,465][train_inner][INFO] - {"epoch": 1, "update": 0.386, "loss": "9.43", "ntokens": "149790", "nsentences": "536.19", "prob_perplexity": "387.5", "code_perplexity": "371.792", "temp": "1.999", "loss_0": "6.679", "loss_1": "0.057", "loss_2": "2.694", "accuracy": "0.01202", "wps": "37804.3", "ups": "0.25", "wpb": "149790", "bsz": "536.2", "num_updates": "200", "lr": "3.125e-06", "gnorm": "1.458", "loss_scale": "64", "train_wall": "798", "gb_free": "12.5", "wall": "832"}
loss: tensor(8312.6377, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5225, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12184.3193, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0625, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11018.8271, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4375, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10758.7529, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4883, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12244.3105, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0312, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-11 23:59:11,271][train_inner][INFO] - {"epoch": 1, "update": 0.77, "loss": "6.996", "ntokens": "149665", "nsentences": "540.735", "prob_perplexity": "541.187", "code_perplexity": "525.315", "temp": "1.997", "loss_0": "6.657", "loss_1": "0.022", "loss_2": "0.316", "accuracy": "0.01194", "wps": "38092.1", "ups": "0.25", "wpb": "149665", "bsz": "540.7", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.136", "loss_scale": "128", "train_wall": "785", "gb_free": "12.9", "wall": "1618"}
loss: tensor(11569.9043, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2461, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11181.2490, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0352, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 8.5068e-04,  9.9945e-04,  4.5562e-04,  9.0790e-04,  8.9502e-04,
         8.0442e-04,  8.4591e-04,  8.0585e-04,  8.9121e-04,  1.0080e-03,
         8.1730e-04,  9.0313e-04,  9.3031e-04,  8.0681e-04,  8.8263e-04,
         8.7833e-04,  9.2077e-04,  9.9850e-04, -2.7823e-04,  8.0681e-04,
         8.0204e-04,  1.0481e-03,  8.7976e-04,  8.8644e-04,  9.3651e-04,
         1.0576e-03,  9.0170e-04,  8.0347e-04,  8.1062e-04,  8.0252e-04,
         8.0824e-04,  8.7309e-04,  7.7343e-04,  8.3637e-04,  9.6273e-04,
         8.2779e-04,  9.4271e-04,  8.0299e-04,  9.8419e-04,  1.0395e-03,
         8.0442e-04,  8.3733e-04,  8.0729e-04,  9.7513e-04,  8.4877e-04,
         8.3160e-04,  8.4972e-04,  8.0776e-04,  9.0361e-04,  9.2554e-04,
         8.1062e-04,  8.1539e-04,  9.5415e-04,  9.0790e-04,  8.0204e-04,
         5.8556e-04,  9.1791e-04,  1.0347e-03,  8.0156e-04,  8.0013e-04,
         8.0538e-04,  8.0490e-04,  8.0538e-04,  8.0252e-04,  8.1158e-04,
         8.6451e-04,  8.1062e-04,  1.0395e-03,  8.0729e-04,  8.1015e-04,
         8.0538e-04,  8.0490e-04,  8.3303e-04,  8.0204e-04,  1.0538e-03,
         9.8896e-04,  8.2159e-04, -4.7994e-04,  8.0299e-04,  8.8072e-04,
         8.0633e-04,  1.0023e-03,  8.8167e-04,  8.2397e-04,  8.0252e-04,
         9.8801e-04,  8.4734e-04,  9.7275e-04,  8.1873e-04,  8.9169e-04,
         8.2350e-04,  1.0347e-03,  9.1982e-04,  8.0204e-04,  1.0662e-03,
         9.8324e-04,  9.7036e-04,  1.0300e-03,  8.4734e-04,  8.8692e-04,
         1.0118e-03,  8.0776e-04,  1.0357e-03,  8.6260e-04,  8.5640e-04,
         8.0156e-04,  8.0681e-04,  9.3508e-04,  9.5797e-04,  9.4891e-04,
         8.2827e-04,  8.0442e-04,  8.8215e-04,  8.6927e-04,  8.0204e-04,
         1.0548e-03,  8.0204e-04,  8.0395e-04,  8.1062e-04,  8.1015e-04,
         8.0442e-04,  8.0776e-04,  8.0347e-04,  8.3637e-04, -2.8443e-04,
         1.0366e-03,  8.0347e-04,  8.0299e-04,  8.0442e-04,  8.1348e-04,
         9.2840e-04,  8.0442e-04,  1.0777e-03,  9.3937e-04,  8.6308e-04,
         9.9945e-04,  8.1110e-04,  8.4257e-04,  7.5436e-04,  8.3351e-04,
         8.4591e-04,  8.5497e-04,  1.0262e-03,  1.0548e-03,  9.9468e-04,
         8.6689e-04,  8.0347e-04,  1.0624e-03,  9.8324e-04,  1.0633e-03,
         8.8549e-04,  1.0719e-03,  1.0796e-03,  9.9659e-04,  8.1635e-04,
         8.4019e-04,  9.2220e-04,  8.8072e-04,  9.8801e-04,  8.0824e-04,
         9.1076e-04,  9.4414e-04,  8.0585e-04,  8.0729e-04,  8.0347e-04,
         1.0338e-03,  1.0662e-03,  1.0414e-03,  9.2268e-04,  1.0433e-03,
         8.0919e-04,  9.2411e-04,  8.5592e-04,  8.1825e-04,  8.7976e-04,
         8.0490e-04,  8.2731e-04,  1.0881e-03,  1.0719e-03,  8.5354e-04,
         1.0548e-03,  9.4748e-04,  8.0442e-04,  9.8419e-04,  8.7261e-04,
         8.0967e-04,  8.0156e-04,  8.0538e-04,  8.0252e-04,  8.7261e-04,
         1.0471e-03,  8.4352e-04,  8.3494e-04,  8.6975e-04,  8.0299e-04,
         7.6914e-04,  8.0204e-04,  8.9502e-04,  8.4591e-04,  8.2636e-04,
         7.7784e-05,  8.1396e-04,  1.0796e-03,  8.0872e-04,  9.6607e-04,
         8.0252e-04,  8.0252e-04,  7.4148e-04,  8.1587e-04,  8.1015e-04,
         7.0810e-04,  8.1539e-04,  8.5735e-04,  8.1015e-04,  8.6308e-04,
         8.1110e-04,  8.2016e-04,  8.3876e-04,  8.0919e-04,  8.2731e-04,
         9.0647e-04,  9.0933e-04,  1.0653e-03,  8.0204e-04,  8.0156e-04,
         9.7275e-04,  8.0204e-04,  9.3460e-04,  8.1015e-04,  8.5068e-04,
         8.1015e-04,  9.0694e-04,  8.5306e-04,  8.3733e-04,  8.4686e-04,
         8.1110e-04,  8.5878e-04,  8.0681e-04,  6.9141e-04,  1.0462e-03,
         8.0633e-04,  8.8120e-04,  8.0442e-04,  9.9754e-04,  8.0872e-04,
         8.0347e-04,  8.1253e-04,  6.3276e-04,  8.0299e-04,  8.0204e-04,
         8.0681e-04,  8.0347e-04,  8.0347e-04,  8.4066e-04,  9.4986e-04,
         8.0299e-04,  8.0252e-04,  8.4352e-04,  1.0157e-03,  8.4972e-04,
         9.2077e-04,  8.0585e-04,  9.6273e-04,  9.7656e-04,  8.2159e-04,
         8.0585e-04,  8.3160e-04,  8.0729e-04,  8.1158e-04,  8.2588e-04,
         1.0509e-03,  1.0166e-03,  8.0681e-04,  8.0347e-04,  9.8133e-04,
         8.0395e-04,  8.0585e-04,  9.5987e-04,  8.8072e-04,  8.0919e-04,
         8.0919e-04,  8.0442e-04, -1.8024e-04,  8.3685e-04,  8.2350e-04,
         1.0700e-03,  8.4734e-04,  1.0738e-03,  8.1921e-04,  8.0776e-04,
         8.0538e-04,  8.9264e-04,  8.6260e-04,  8.9359e-04,  8.0347e-04,
         8.0490e-04,  8.0490e-04,  8.0633e-04,  9.8419e-04,  8.0156e-04,
         1.0834e-03, -4.6802e-04,  8.0156e-04,  8.4543e-04,  8.0633e-04,
         8.8406e-04,  8.3447e-04,  8.0872e-04,  8.0347e-04,  8.0299e-04,
         8.6403e-04,  8.7452e-04,  8.6689e-04,  8.0490e-04,  8.0252e-04,
         1.0262e-03,  8.5020e-04,  9.7084e-04,  8.8644e-04,  8.0729e-04,
         8.8406e-04,  8.4829e-04,  8.0204e-04,  8.0156e-04,  8.0729e-04,
         8.5783e-04,  9.4748e-04,  9.9754e-04,  8.0347e-04,  8.5211e-04,
         8.1968e-04,  8.0824e-04,  8.2588e-04,  8.0538e-04,  9.4795e-04,
         9.2554e-04,  9.6750e-04,  4.0627e-04,  9.0075e-04,  8.0252e-04,
         8.4496e-04,  8.7023e-04,  8.4925e-04,  8.0442e-04,  8.5068e-04,
        -4.3416e-04,  1.2684e-04,  9.1362e-04,  8.4925e-04,  8.0395e-04,
         8.0204e-04,  8.0347e-04,  1.0185e-03,  8.0204e-04,  8.6737e-04,
         9.0933e-04,  8.0299e-04,  8.0967e-04,  8.1635e-04,  8.0156e-04,
         8.8549e-04,  8.3637e-04,  8.0204e-04,  8.0681e-04,  8.0872e-04,
         1.0509e-03,  8.0919e-04,  8.0776e-04,  8.0729e-04,  1.0033e-03,
         8.2636e-04,  8.0967e-04,  8.7833e-04,  8.0967e-04,  8.0681e-04,
         7.9966e-04,  8.0395e-04,  8.1348e-04,  8.0585e-04, -2.7490e-04,
         8.0204e-04,  8.0585e-04,  8.0967e-04,  8.0776e-04,  8.0776e-04,
         8.3542e-04,  8.0729e-04,  7.9918e-04,  1.0118e-03,  8.1015e-04,
         8.0442e-04,  8.0299e-04,  8.9121e-04,  8.2350e-04,  8.0681e-04,
         8.0395e-04,  8.4543e-04,  8.5974e-04,  9.0694e-04,  8.2493e-04,
         8.0538e-04,  8.0299e-04,  8.6832e-04,  1.0071e-03,  8.6880e-04,
         8.1444e-04,  8.0156e-04,  1.0481e-03,  8.6641e-04,  8.5497e-04,
         8.4352e-04,  8.7452e-04,  8.0156e-04,  8.1205e-04,  1.0614e-03,
         9.0122e-04,  8.0872e-04,  9.3079e-04,  8.1921e-04,  8.0490e-04,
         8.1015e-04,  8.0585e-04,  9.9754e-04,  9.0408e-04,  8.0156e-04,
         1.0824e-03,  8.1015e-04,  8.6498e-04,  8.7023e-04,  8.3828e-04,
         8.0872e-04,  1.0109e-03,  8.5354e-04,  3.6836e-04,  8.0156e-04,
         9.9564e-04,  8.0299e-04,  7.5340e-04,  8.0204e-04,  9.3651e-04,
         9.1219e-04,  8.0299e-04,  9.3222e-04,  7.3338e-04,  8.8215e-04,
         2.9612e-04,  1.0691e-03,  8.0585e-04,  1.0395e-03,  7.6342e-04,
         8.4686e-04,  8.0156e-04,  8.0299e-04,  8.0776e-04,  8.0395e-04,
         8.0252e-04,  8.0490e-04,  9.4986e-04,  8.7452e-04,  9.4795e-04,
         8.0681e-04,  1.0061e-03,  9.9659e-04,  8.0395e-04, -1.1110e-04,
         8.7452e-04,  2.3341e-04,  9.2840e-04,  8.5068e-04,  8.9979e-04,
         8.9884e-04,  8.9979e-04,  9.2268e-04,  8.1396e-04,  9.4843e-04,
         8.8549e-04,  1.0500e-03,  8.5545e-04,  8.0299e-04,  1.0128e-03,
         8.0252e-04,  6.9475e-04,  8.3542e-04,  8.3065e-04,  8.9264e-04,
         8.3113e-04,  7.9346e-04,  8.5497e-04,  1.0166e-03,  8.5354e-04,
         8.0204e-04,  8.5497e-04,  8.0824e-04,  8.0538e-04,  8.0252e-04,
         8.3923e-04,  8.0347e-04,  8.0442e-04,  8.9455e-04,  8.0395e-04,
         8.0967e-04,  8.2064e-04,  1.0729e-03,  8.3256e-04,  9.3174e-04,
         8.0538e-04,  8.4591e-04,  1.1024e-03,  8.0585e-04, -6.8998e-04,
         8.0872e-04,  8.1062e-04,  8.1825e-04,  8.0776e-04,  8.3256e-04,
         8.2254e-04,  8.2541e-04,  8.8310e-04,  8.5878e-04,  8.6546e-04,
         1.0490e-03,  8.0538e-04,  8.1873e-04,  8.0490e-04,  9.0599e-04,
         8.0156e-04,  8.1778e-04,  8.8644e-04,  8.0395e-04,  1.0328e-03,
         9.0361e-04,  9.8419e-04,  8.4305e-04,  8.0729e-04,  8.2874e-04,
         8.9264e-04,  8.8930e-04,  8.0729e-04,  4.0221e-04,  9.2125e-04,
         8.4639e-04,  8.0252e-04,  8.0299e-04,  8.0824e-04,  8.6689e-04,
         8.3590e-04,  9.6369e-04,  8.8739e-04,  8.1110e-04, -7.1466e-05,
         8.1491e-04,  8.0538e-04,  8.0919e-04,  8.0776e-04,  8.2874e-04,
         1.0490e-03,  8.0538e-04,  8.0204e-04,  6.6471e-04,  8.1062e-04,
         8.0204e-04,  1.0443e-03,  8.0299e-04,  8.0252e-04,  8.0252e-04,
         8.9788e-04,  8.0490e-04,  8.7166e-04, -1.7226e-04,  1.0405e-03,
        -1.4603e-04,  8.3256e-04,  8.0252e-04,  9.0837e-04,  1.0481e-03,
         8.1921e-04,  8.0919e-04,  1.0643e-03,  8.0967e-04,  9.5367e-04,
         8.1158e-04,  9.1028e-04,  8.1825e-04,  8.0538e-04,  8.0729e-04,
         8.0633e-04,  8.0776e-04,  9.0218e-04,  8.0872e-04,  9.2220e-04,
         8.0538e-04,  8.0776e-04,  8.1968e-04,  8.1587e-04,  8.7690e-04,
         1.0185e-03,  8.6641e-04,  8.6594e-04,  1.0033e-03,  8.0252e-04,
        -5.1689e-04,  8.0824e-04,  9.0075e-04,  8.0872e-04,  9.2173e-04,
         8.1348e-04,  8.2493e-04,  9.9659e-04,  8.7738e-04,  8.1873e-04,
         9.6607e-04,  8.0585e-04,  8.9550e-04,  1.0042e-03,  8.0585e-04,
         8.1301e-04,  8.5592e-04,  5.1260e-04,  8.3399e-04,  8.7309e-04,
         8.0252e-04,  8.8120e-04,  8.4448e-04,  7.1812e-04,  8.0156e-04,
         8.0585e-04,  8.4352e-04,  1.0719e-03,  8.9884e-04,  8.3447e-04,
         1.0357e-03,  8.0204e-04,  8.1968e-04,  8.1968e-04,  8.6021e-04,
         9.0408e-04,  8.8072e-04,  8.1015e-04,  9.1600e-04,  8.0490e-04],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(12008.4102, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3926, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10675.0225, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5063, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10823.6416, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7852, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11701.7217, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7402, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10764.0723, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1045, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11157.0605, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4980, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8715.5732, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0273, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0010,  0.0011,  0.0005,  0.0011,  0.0006,  0.0009,  0.0010,  0.0010,
         0.0011,  0.0012,  0.0010,  0.0011,  0.0011,  0.0010,  0.0010,  0.0010,
         0.0011,  0.0012, -0.0006,  0.0009,  0.0009,  0.0012,  0.0010,  0.0011,
         0.0011,  0.0012,  0.0011,  0.0009,  0.0010,  0.0009,  0.0010,  0.0010,
         0.0008,  0.0010,  0.0012,  0.0010,  0.0010,  0.0009,  0.0012,  0.0012,
         0.0009,  0.0010,  0.0010,  0.0011,  0.0010,  0.0010,  0.0010,  0.0010,
         0.0011,  0.0011,  0.0010,  0.0010,  0.0012,  0.0010,  0.0009,  0.0006,
         0.0011,  0.0010,  0.0009,  0.0009,  0.0010,  0.0009,  0.0009,  0.0009,
         0.0010,  0.0010,  0.0010,  0.0012,  0.0010,  0.0010,  0.0009,  0.0009,
         0.0010,  0.0009,  0.0012,  0.0011,  0.0010, -0.0008,  0.0009,  0.0010,
         0.0010,  0.0011,  0.0010,  0.0010,  0.0009,  0.0012,  0.0010,  0.0011,
         0.0010,  0.0011,  0.0009,  0.0012,  0.0011,  0.0009,  0.0012,  0.0012,
         0.0012,  0.0012,  0.0010,  0.0010,  0.0012,  0.0010,  0.0012,  0.0010,
         0.0010,  0.0009,  0.0010,  0.0011,  0.0012,  0.0012,  0.0010,  0.0010,
         0.0007,  0.0010,  0.0009,  0.0012,  0.0009,  0.0010,  0.0009,  0.0010,
         0.0009,  0.0009,  0.0009,  0.0010, -0.0003,  0.0012,  0.0009,  0.0009,
         0.0009,  0.0010,  0.0011,  0.0009,  0.0012,  0.0011,  0.0008,  0.0011,
         0.0010,  0.0010,  0.0008,  0.0010,  0.0009,  0.0006,  0.0012,  0.0012,
         0.0012,  0.0010,  0.0009,  0.0012,  0.0011,  0.0012,  0.0011,  0.0012,
         0.0012,  0.0012,  0.0010,  0.0010,  0.0011,  0.0010,  0.0012,  0.0010,
         0.0010,  0.0011,  0.0010,  0.0010,  0.0009,  0.0012,  0.0012,  0.0012,
         0.0011,  0.0012,  0.0010,  0.0011,  0.0010,  0.0010,  0.0010,  0.0009,
         0.0010,  0.0012,  0.0012,  0.0011,  0.0012,  0.0011,  0.0010,  0.0012,
         0.0011,  0.0010,  0.0009,  0.0010,  0.0009,  0.0011,  0.0012,  0.0010,
         0.0010,  0.0010,  0.0009,  0.0008,  0.0009,  0.0011,  0.0010,  0.0010,
        -0.0002,  0.0010,  0.0012,  0.0010,  0.0011,  0.0009,  0.0009,  0.0008,
         0.0010,  0.0010,  0.0005,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,
         0.0010,  0.0010,  0.0010,  0.0010,  0.0011,  0.0011,  0.0013,  0.0009,
         0.0009,  0.0011,  0.0009,  0.0011,  0.0010,  0.0010,  0.0010,  0.0011,
         0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0008,  0.0012,
         0.0009,  0.0010,  0.0010,  0.0012,  0.0010,  0.0010,  0.0010,  0.0007,
         0.0009,  0.0009,  0.0010,  0.0009,  0.0009,  0.0010,  0.0011,  0.0009,
         0.0009,  0.0010,  0.0012,  0.0011,  0.0010,  0.0009,  0.0011,  0.0012,
         0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0012,  0.0011,
         0.0010,  0.0009,  0.0012,  0.0009,  0.0009,  0.0011,  0.0010,  0.0010,
         0.0010,  0.0009, -0.0005,  0.0010,  0.0010,  0.0012,  0.0010,  0.0012,
         0.0010,  0.0010,  0.0009,  0.0010,  0.0010,  0.0011,  0.0010,  0.0009,
         0.0009,  0.0009,  0.0012,  0.0009,  0.0013, -0.0006,  0.0009,  0.0009,
         0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0009,  0.0010,  0.0010,
         0.0010,  0.0010,  0.0009,  0.0012,  0.0010,  0.0012,  0.0010,  0.0010,
         0.0010,  0.0009,  0.0009,  0.0009,  0.0009,  0.0010,  0.0011,  0.0012,
         0.0009,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0011,  0.0011,
         0.0012,  0.0003,  0.0010,  0.0009,  0.0010,  0.0010,  0.0010,  0.0009,
         0.0010, -0.0005, -0.0001,  0.0011,  0.0010,  0.0009,  0.0009,  0.0009,
         0.0012,  0.0009,  0.0010,  0.0010,  0.0009,  0.0010,  0.0010,  0.0009,
         0.0010,  0.0010,  0.0009,  0.0010,  0.0010,  0.0012,  0.0010,  0.0010,
         0.0009,  0.0011,  0.0010,  0.0010,  0.0011,  0.0010,  0.0009,  0.0008,
         0.0009,  0.0010,  0.0010, -0.0005,  0.0009,  0.0009,  0.0010,  0.0010,
         0.0010,  0.0010,  0.0010,  0.0009,  0.0011,  0.0010,  0.0010,  0.0009,
         0.0011,  0.0010,  0.0009,  0.0009,  0.0010,  0.0009,  0.0010,  0.0010,
         0.0009,  0.0009,  0.0011,  0.0011,  0.0010,  0.0010,  0.0009,  0.0012,
         0.0009,  0.0010,  0.0010,  0.0010,  0.0009,  0.0010,  0.0012,  0.0011,
         0.0010,  0.0007,  0.0010,  0.0009,  0.0010,  0.0009,  0.0009,  0.0011,
         0.0009,  0.0013,  0.0010,  0.0010,  0.0010,  0.0010,  0.0010,  0.0012,
         0.0010,  0.0004,  0.0009,  0.0012,  0.0009,  0.0006,  0.0009,  0.0011,
         0.0011,  0.0009,  0.0011,  0.0009,  0.0011,  0.0003,  0.0012,  0.0009,
         0.0012,  0.0009,  0.0010,  0.0009,  0.0009,  0.0010,  0.0010,  0.0009,
         0.0010,  0.0012,  0.0011,  0.0011,  0.0010,  0.0012,  0.0011,  0.0009,
        -0.0003,  0.0011,  0.0002,  0.0007,  0.0010,  0.0011,  0.0010,  0.0011,
         0.0011,  0.0010,  0.0011,  0.0011,  0.0012,  0.0011,  0.0009,  0.0011,
         0.0009,  0.0008,  0.0010,  0.0010,  0.0010,  0.0010,  0.0009,  0.0010,
         0.0012,  0.0010,  0.0009,  0.0010,  0.0010,  0.0009,  0.0009,  0.0010,
         0.0010,  0.0009,  0.0010,  0.0009,  0.0010,  0.0010,  0.0012,  0.0010,
         0.0011,  0.0009,  0.0010,  0.0013,  0.0010, -0.0007,  0.0010,  0.0010,
         0.0010,  0.0010,  0.0010,  0.0010,  0.0009,  0.0010,  0.0010,  0.0010,
         0.0012,  0.0009,  0.0010,  0.0010,  0.0008,  0.0009,  0.0010,  0.0010,
         0.0010,  0.0012,  0.0010,  0.0012,  0.0010,  0.0010,  0.0010,  0.0011,
         0.0011,  0.0010,  0.0004,  0.0011,  0.0010,  0.0009,  0.0009,  0.0010,
         0.0010,  0.0010,  0.0011,  0.0011,  0.0010, -0.0004,  0.0010,  0.0009,
         0.0010,  0.0010,  0.0010,  0.0012,  0.0009,  0.0009,  0.0007,  0.0010,
         0.0009,  0.0012,  0.0009,  0.0009,  0.0009,  0.0011,  0.0009,  0.0011,
        -0.0002,  0.0012, -0.0003,  0.0010,  0.0009,  0.0010,  0.0012,  0.0010,
         0.0010,  0.0012,  0.0010,  0.0011,  0.0010,  0.0011,  0.0010,  0.0009,
         0.0009,  0.0010,  0.0010,  0.0011,  0.0010,  0.0010,  0.0009,  0.0010,
         0.0010,  0.0010,  0.0010,  0.0012,  0.0010,  0.0010,  0.0012,  0.0009,
        -0.0007,  0.0010,  0.0010,  0.0010,  0.0011,  0.0010,  0.0010,  0.0012,
         0.0011,  0.0010,  0.0011,  0.0009,  0.0011,  0.0012,  0.0009,  0.0010,
         0.0010,  0.0003,  0.0010,  0.0011,  0.0009,  0.0010,  0.0010,  0.0008,
         0.0009,  0.0010,  0.0010,  0.0012,  0.0010,  0.0010,  0.0012,  0.0009,
         0.0010,  0.0010,  0.0010,  0.0011,  0.0010,  0.0010,  0.0011,  0.0009],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(11557.4961, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5781, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11113.7490, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.2031, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10627.5938, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7539, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: loss: tensor(11448.1680, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5234, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 00:07:04,100][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 00:07:04,102][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 00:07:04,271][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2023-09-12 00:07:27,920][valid][INFO] - {"epoch": 1, "valid_loss": "6.707", "valid_ntokens": "7893.53", "valid_nsentences": "55.2525", "valid_prob_perplexity": "587.5", "valid_code_perplexity": "569.517", "valid_temp": "1.995", "valid_loss_0": "6.657", "valid_loss_1": "0.011", "valid_loss_2": "0.039", "valid_accuracy": "0.015", "valid_wps": "33135", "valid_wpb": "7893.5", "valid_bsz": "55.3", "valid_num_updates": "520"}
[2023-09-12 00:07:27,922][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 520 updates
[2023-09-12 00:07:27,923][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 00:07:29,814][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 00:07:30,623][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 520 updates, score 6.707) (writing took 2.7009918940020725 seconds)
[2023-09-12 00:07:30,624][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2023-09-12 00:07:30,624][train][INFO] - {"epoch": 1, "train_loss": "7.873", "train_ntokens": "149485", "train_nsentences": "538.312", "train_prob_perplexity": "493.039", "train_code_perplexity": "477.147", "train_temp": "1.997", "train_loss_0": "6.665", "train_loss_1": "0.033", "train_loss_2": "1.175", "train_accuracy": "0.01197", "train_wps": "37413.4", "train_ups": "0.25", "train_wpb": "149485", "train_bsz": "538.3", "train_num_updates": "520", "train_lr": "8.125e-06", "train_gnorm": "0.618", "train_loss_scale": "256", "train_train_wall": "2055", "train_gb_free": "14.1", "train_wall": "2117"}
[2023-09-12 00:07:30,626][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 00:07:30,705][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2023-09-12 00:07:30,964][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 00:07:30,967][fairseq.trainer][INFO] - begin training epoch 2
[2023-09-12 00:07:30,967][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 00:12:46,184][train_inner][INFO] - {"epoch": 2, "update": 1.154, "loss": "6.718", "ntokens": "149101", "nsentences": "538.095", "prob_perplexity": "591.864", "code_perplexity": "575.572", "temp": "1.995", "loss_0": "6.655", "loss_1": "0.011", "loss_2": "0.052", "accuracy": "0.01203", "wps": "36593.1", "ups": "0.25", "wpb": "149101", "bsz": "538.1", "num_updates": "600", "lr": "9.375e-06", "gnorm": "0.017", "loss_scale": "256", "train_wall": "787", "gb_free": "13.2", "wall": "2433"}
self.scaling_factor_for_vector: Parameter containing:
tensor([ 2.0657e-03,  2.0885e-03,  7.5722e-04,  2.2888e-03, -8.9884e-04,
         1.8673e-03,  1.8597e-03,  1.8959e-03,  2.2068e-03,  2.2774e-03,
         2.1076e-03,  2.3117e-03,  2.2106e-03,  1.8835e-03,  1.9646e-03,
         1.9608e-03,  2.2812e-03,  2.1458e-03, -1.8425e-03,  1.8797e-03,
         1.8930e-03,  2.0561e-03,  2.0065e-03,  2.2850e-03,  2.2697e-03,
         2.3899e-03,  2.2373e-03,  1.9245e-03,  1.8740e-03,  1.8692e-03,
         1.8797e-03,  2.0695e-03,  1.4467e-03,  1.9045e-03,  2.0847e-03,
         2.0142e-03,  1.7166e-03,  1.8778e-03,  2.2068e-03,  2.2182e-03,
         1.8787e-03,  2.2354e-03,  1.9217e-03,  1.7452e-03,  1.9083e-03,
         2.2259e-03,  1.8950e-03,  1.9083e-03,  2.1458e-03,  2.2717e-03,
         1.9035e-03,  2.1706e-03,  2.3403e-03,  1.8673e-03,  1.9217e-03,
         1.6804e-03,  2.0504e-03, -7.4959e-04,  1.8873e-03,  1.8854e-03,
         2.0924e-03,  1.8845e-03,  1.8826e-03,  1.8778e-03,  2.0504e-03,
         2.0142e-03,  1.9035e-03,  2.2964e-03,  1.8988e-03,  1.8787e-03,
         1.8702e-03,  2.0447e-03,  1.8921e-03,  1.9913e-03,  1.8835e-03,
         2.2259e-03,  1.9331e-03, -2.3193e-03,  1.9627e-03,  2.1114e-03,
         1.8787e-03,  2.0676e-03,  1.9608e-03,  2.0905e-03,  1.8730e-03,
         1.9894e-03,  1.8997e-03,  1.9159e-03,  1.8911e-03,  1.9951e-03,
         1.8568e-03,  2.1267e-03,  2.2030e-03,  2.0294e-03,  2.0256e-03,
         2.0809e-03,  5.6839e-04,  2.1229e-03,  1.9760e-03,  1.9045e-03,
         2.3232e-03,  2.0733e-03,  2.3861e-03,  2.2068e-03,  1.9321e-03,
         1.8654e-03,  2.0370e-03,  2.3556e-03,  2.3079e-03,  2.2755e-03,
         2.1324e-03,  2.0618e-03, -9.1124e-04,  1.9112e-03,  1.9121e-03,
         2.2106e-03,  1.8673e-03,  1.9455e-03,  1.6651e-03,  2.1095e-03,
         1.8740e-03,  1.8759e-03,  1.8740e-03,  1.8911e-03,  8.3447e-06,
         2.3899e-03,  1.9951e-03,  1.8721e-03,  1.9608e-03,  1.9341e-03,
         1.9484e-03,  1.8864e-03, -1.3208e-04,  2.0847e-03,  1.6356e-03,
         1.9264e-03,  2.0237e-03,  1.9588e-03,  1.3762e-03,  1.9798e-03,
         1.6727e-03, -1.2760e-03,  2.3994e-03,  2.1420e-03,  2.2488e-03,
         2.2717e-03,  1.8787e-03,  2.1400e-03,  2.1267e-03,  2.3022e-03,
         2.2449e-03,  2.3365e-03,  2.3060e-03,  1.6947e-03,  1.8797e-03,
         1.9026e-03,  1.9588e-03,  1.9464e-03,  1.7471e-03,  1.8702e-03,
         1.8997e-03,  2.0580e-03,  2.0275e-03,  2.2297e-03,  1.8730e-03,
         2.2411e-03,  2.2602e-03,  2.0599e-03,  2.2812e-03,  2.2717e-03,
         1.8883e-03,  1.9417e-03,  2.1076e-03,  1.9178e-03,  1.9426e-03,
         1.9016e-03,  1.9293e-03,  2.2068e-03,  2.2202e-03, -5.4121e-04,
         2.1038e-03,  2.0103e-03,  1.9140e-03,  2.2430e-03,  2.2774e-03,
         1.9455e-03,  1.8864e-03,  1.8997e-03,  1.8864e-03,  2.2945e-03,
         2.3327e-03,  1.8911e-03,  1.9016e-03,  1.6756e-03,  1.8692e-03,
         3.9148e-04,  1.8578e-03,  2.0981e-03,  2.2621e-03,  2.1839e-03,
        -1.8578e-03,  1.9932e-03,  2.2259e-03,  1.9035e-03,  2.2068e-03,
         2.0161e-03,  1.8768e-03,  1.3571e-03,  2.1267e-03,  1.8787e-03,
         5.9748e-04,  1.9379e-03,  1.9026e-03,  1.8940e-03,  2.0714e-03,
         1.8797e-03,  1.9131e-03,  2.0676e-03,  1.8730e-03,  1.8806e-03,
         2.3022e-03,  2.2182e-03,  2.2964e-03,  1.8702e-03,  1.8673e-03,
         1.9283e-03,  1.8654e-03,  1.9646e-03,  2.0828e-03,  1.8988e-03,
         1.8835e-03,  2.1591e-03,  2.0599e-03,  1.9369e-03,  2.0504e-03,
         2.2259e-03,  1.9722e-03,  1.9741e-03,  1.6937e-03, -2.0266e-04,
         1.8702e-03,  2.0313e-03,  2.1420e-03,  1.1225e-03,  1.8759e-03,
         1.9836e-03,  1.9665e-03,  1.4753e-03,  1.8721e-03,  1.8768e-03,
         1.8778e-03,  1.9054e-03,  1.9646e-03,  1.9417e-03,  2.1172e-03,
         1.9083e-03,  2.0618e-03,  1.9474e-03,  2.2717e-03,  2.3098e-03,
         2.0866e-03,  1.8702e-03,  1.8501e-03,  2.1629e-03,  1.7529e-03,
         1.9302e-03,  2.1381e-03,  1.8740e-03,  1.8721e-03,  2.2793e-03,
         2.1019e-03,  1.9855e-03,  1.8864e-03,  1.7366e-03,  2.2449e-03,
         1.8711e-03,  1.8835e-03,  1.9588e-03,  1.9102e-03,  1.9169e-03,
         1.8721e-03,  1.8711e-03, -1.4420e-03,  1.9054e-03,  1.9426e-03,
         2.2163e-03,  1.9131e-03,  2.0390e-03,  1.8835e-03,  2.1858e-03,
         1.8730e-03,  1.8768e-03,  2.1248e-03,  2.3079e-03,  1.9369e-03,
         1.8759e-03,  1.8702e-03,  1.9798e-03, -8.6486e-05,  1.8654e-03,
         2.3441e-03, -1.4067e-03,  1.9274e-03,  1.7519e-03,  1.9608e-03,
         1.9054e-03,  2.2964e-03,  1.8883e-03,  2.0962e-03,  1.9970e-03,
         2.1114e-03,  1.9875e-03,  2.0294e-03,  1.9627e-03,  1.9989e-03,
         2.1706e-03,  1.9121e-03,  2.3441e-03,  2.0008e-03,  1.9779e-03,
         1.8749e-03,  1.8597e-03,  1.8673e-03,  1.8663e-03,  1.9064e-03,
         2.1439e-03,  2.0885e-03,  2.3098e-03,  1.8682e-03,  1.9197e-03,
         2.2564e-03,  1.8997e-03,  1.9550e-03,  1.8921e-03,  2.2755e-03,
         2.0676e-03,  2.2297e-03,  8.2397e-04,  1.8415e-03,  1.9436e-03,
         2.0847e-03,  1.9798e-03,  1.8959e-03,  1.8721e-03,  1.9188e-03,
        -4.7898e-04, -1.0157e-03,  2.2583e-03,  2.0733e-03,  1.8749e-03,
         1.9007e-03,  1.8692e-03,  2.2526e-03,  1.8930e-03,  1.9474e-03,
         1.9503e-03,  1.9331e-03,  1.8711e-03,  1.8845e-03,  1.8654e-03,
         1.9293e-03,  2.0084e-03,  1.8806e-03,  2.1229e-03,  1.8911e-03,
         2.3041e-03,  1.8835e-03,  1.8902e-03,  1.8682e-03,  1.5202e-03,
         1.9140e-03,  1.8816e-03,  2.2392e-03,  1.9112e-03,  1.8692e-03,
         1.4210e-03,  1.9836e-03,  1.9455e-03,  1.8864e-03, -1.1292e-03,
         1.8740e-03,  1.9836e-03,  1.8806e-03,  1.8911e-03,  1.8730e-03,
         1.9283e-03,  1.9102e-03,  2.5868e-04,  1.9245e-03,  1.8826e-03,
         2.1496e-03,  1.8749e-03,  2.1648e-03,  1.9026e-03,  1.8864e-03,
         1.8854e-03,  2.0943e-03,  1.9464e-03,  1.9121e-03,  2.0466e-03,
         1.8854e-03,  1.8711e-03,  2.2697e-03,  2.0447e-03,  1.9360e-03,
         1.8930e-03,  1.8682e-03,  2.3346e-03,  1.5202e-03,  2.3117e-03,
         2.0027e-03,  1.9312e-03,  1.8959e-03,  1.8816e-03,  2.2392e-03,
         2.0237e-03,  1.8911e-03, -7.9775e-04,  1.9312e-03,  1.9550e-03,
         1.8845e-03,  1.8711e-03,  4.8018e-04,  2.0752e-03,  1.8950e-03,
         1.9855e-03,  1.8768e-03,  1.9369e-03,  2.2831e-03,  1.8921e-03,
         1.9035e-03,  6.8331e-04,  2.0561e-03,  7.7963e-04,  1.9226e-03,
         2.3899e-03,  1.8768e-03,  1.1463e-03,  1.9169e-03,  2.3327e-03,
         2.0390e-03,  1.9970e-03,  2.2602e-03,  1.8320e-03,  2.1420e-03,
         6.2704e-04,  2.3518e-03,  1.8845e-03,  2.1973e-03,  1.7328e-03,
         1.9798e-03,  1.8663e-03,  1.9064e-03,  1.9035e-03,  1.9741e-03,
         1.8978e-03,  2.0199e-03,  2.3689e-03,  2.2106e-03,  2.1877e-03,
         2.0275e-03,  2.1477e-03,  2.1343e-03,  1.8711e-03, -1.0891e-03,
         2.1210e-03,  3.0851e-04, -1.1253e-03,  1.8940e-03,  2.1133e-03,
         1.6890e-03,  2.2049e-03,  2.3308e-03,  1.9016e-03,  2.2011e-03,
         2.0504e-03,  2.2087e-03,  7.4911e-04,  1.8682e-03,  2.0027e-03,
         1.9054e-03,  1.0185e-03,  1.9388e-03,  1.9007e-03,  1.9207e-03,
         2.1400e-03,  1.7424e-03,  2.1915e-03,  2.0237e-03,  1.9760e-03,
         2.0027e-03,  1.9684e-03,  1.8778e-03,  1.9102e-03,  1.9646e-03,
         2.0046e-03,  1.9703e-03,  1.8721e-03,  2.0657e-03,  1.9016e-03,
         1.8826e-03,  1.8883e-03,  2.2373e-03,  1.9455e-03,  1.9646e-03,
         1.8768e-03,  2.1172e-03,  2.2697e-03,  2.0199e-03, -6.6996e-04,
         1.8749e-03,  1.8768e-03,  1.9836e-03,  1.8787e-03,  1.9026e-03,
         2.1095e-03,  1.6937e-03,  2.0180e-03,  1.9102e-03,  1.9321e-03,
         2.1725e-03,  1.8778e-03,  1.8806e-03,  1.9588e-03, -3.1734e-04,
         1.8702e-03,  1.9112e-03,  1.9646e-03,  2.1477e-03,  2.3556e-03,
         1.8711e-03,  1.8454e-03,  1.9102e-03,  1.8816e-03,  2.2259e-03,
         2.2926e-03,  2.0275e-03,  1.8864e-03,  8.1253e-04,  2.3212e-03,
         2.1706e-03,  1.8749e-03,  1.8730e-03,  1.8797e-03,  2.0027e-03,
         2.0065e-03,  2.0313e-03,  2.2945e-03,  1.8845e-03, -1.8806e-03,
         2.2049e-03,  1.8902e-03,  1.8873e-03,  1.8959e-03,  1.8873e-03,
         2.2850e-03,  1.8711e-03,  1.9064e-03,  1.5488e-03,  1.8845e-03,
         1.8787e-03,  1.8921e-03,  1.8711e-03,  1.8663e-03,  1.8873e-03,
         2.2793e-03,  1.8902e-03,  2.2488e-03,  2.2674e-04,  2.3499e-03,
        -1.8759e-03,  2.2011e-03,  1.8797e-03,  1.9512e-03,  2.3117e-03,
         1.9722e-03,  1.9054e-03,  2.2469e-03,  2.1000e-03,  1.9264e-03,
         2.0123e-03,  2.0790e-03,  1.9760e-03,  2.0409e-03,  1.8711e-03,
         1.9188e-03,  1.8778e-03,  2.0008e-03,  1.8740e-03,  2.1820e-03,
         1.8749e-03,  1.9026e-03,  1.9665e-03,  1.8826e-03,  1.9341e-03,
         2.3575e-03,  2.0065e-03,  2.1152e-03,  2.2354e-03,  1.8921e-03,
        -1.7128e-03,  1.8997e-03,  2.0161e-03,  1.8730e-03,  2.2984e-03,
         1.9255e-03,  2.1706e-03,  2.2583e-03,  2.2888e-03,  2.0447e-03,
         1.9436e-03,  1.8911e-03,  2.2869e-03,  2.1305e-03,  1.8711e-03,
         2.1019e-03,  2.1706e-03,  5.4407e-04,  1.9159e-03,  2.2335e-03,
         1.8778e-03,  2.0504e-03,  2.0428e-03,  1.4687e-03,  1.8740e-03,
         1.9150e-03,  1.9150e-03,  2.2888e-03,  1.9531e-03,  2.1896e-03,
         2.0294e-03,  1.9045e-03,  1.8845e-03,  1.9255e-03,  1.9064e-03,
         2.3460e-03,  2.0199e-03,  1.8921e-03,  2.1210e-03,  1.8702e-03],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 00:17:36,717][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
tensor(11114.3213, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.9102, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12114.0479, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7852, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11500.1162, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.0430, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12470.5273, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8164, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10449.0908, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6758, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9451.5000, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.2422, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 2.3575e-03,  2.3632e-03,  9.3317e-04,  2.6035e-03, -1.0614e-03,
         2.1458e-03,  2.1591e-03,  2.1782e-03,  2.5501e-03,  2.6169e-03,
         2.4128e-03,  2.6207e-03,  2.5291e-03,  2.1629e-03,  2.2717e-03,
         2.2316e-03,  2.5558e-03,  2.4128e-03, -2.3479e-03,  2.1572e-03,
         2.1992e-03,  2.3804e-03,  2.2964e-03,  2.6798e-03,  2.7256e-03,
         2.7180e-03,  2.5558e-03,  2.2488e-03,  2.1534e-03,  2.1496e-03,
         2.1572e-03,  2.4357e-03,  1.7986e-03,  2.1801e-03,  2.3422e-03,
         2.2945e-03,  2.0218e-03,  2.1572e-03,  2.4872e-03,  2.5234e-03,
         2.1591e-03,  2.6455e-03,  2.1954e-03,  2.0065e-03,  2.1858e-03,
         2.5082e-03,  2.1744e-03,  2.1896e-03,  2.4128e-03,  2.6302e-03,
         2.1820e-03,  2.5349e-03,  2.7409e-03,  2.1877e-03,  2.2030e-03,
         2.0943e-03,  2.3613e-03, -1.1368e-03,  2.1839e-03,  2.2945e-03,
         2.4929e-03,  2.1648e-03,  2.1629e-03,  2.1572e-03,  2.3632e-03,
         2.3232e-03,  2.1839e-03,  2.6379e-03,  2.1820e-03,  2.1572e-03,
         2.1477e-03,  2.3708e-03,  2.1687e-03,  2.2736e-03,  2.1801e-03,
         2.5368e-03,  2.2068e-03, -2.7771e-03,  2.2526e-03,  2.4261e-03,
         2.1591e-03,  2.3575e-03,  2.2469e-03,  2.3842e-03,  2.1515e-03,
         2.3479e-03,  2.1801e-03,  2.2030e-03,  2.1839e-03,  2.2774e-03,
         2.1362e-03,  2.5578e-03,  2.5196e-03,  2.3174e-03,  2.3003e-03,
         2.3537e-03,  6.1512e-04,  2.3918e-03,  2.2545e-03,  2.1820e-03,
         2.6188e-03,  2.3632e-03,  2.7809e-03,  2.5806e-03,  2.2221e-03,
         2.1439e-03,  2.3422e-03,  2.7752e-03,  2.6493e-03,  2.5578e-03,
         2.5082e-03,  2.3460e-03, -1.1797e-03,  2.1896e-03,  2.1954e-03,
         2.6245e-03,  2.1458e-03,  2.2335e-03,  1.9760e-03,  2.5444e-03,
         2.1515e-03,  2.1534e-03,  2.1534e-03,  2.1706e-03,  2.8777e-04,
         2.7332e-03,  2.2755e-03,  2.1648e-03,  2.2469e-03,  2.2335e-03,
         2.2221e-03,  2.1629e-03, -3.2663e-04,  2.4872e-03,  2.0561e-03,
         2.2354e-03,  2.3766e-03,  2.3079e-03,  1.6308e-03,  2.2621e-03,
         1.9436e-03, -1.8291e-03,  2.7771e-03,  2.4261e-03,  2.6722e-03,
         2.6398e-03,  2.1553e-03,  2.4796e-03,  2.4471e-03,  2.6817e-03,
         2.5597e-03,  2.6340e-03,  2.7027e-03,  2.0866e-03,  2.1572e-03,
         2.1820e-03,  2.2335e-03,  2.2202e-03,  2.1839e-03,  2.1515e-03,
         2.1763e-03,  2.3232e-03,  2.3174e-03,  2.6302e-03,  2.1515e-03,
         2.5215e-03,  2.5482e-03,  2.3212e-03,  2.6340e-03,  2.5673e-03,
         2.1687e-03,  2.2182e-03,  2.5616e-03,  2.1954e-03,  2.2278e-03,
         2.1915e-03,  2.2163e-03,  2.4605e-03,  2.4815e-03, -8.6355e-04,
         2.3823e-03,  2.2907e-03,  2.1000e-03,  2.6588e-03,  2.5921e-03,
         2.2373e-03,  2.1667e-03,  2.1801e-03,  2.1687e-03,  2.6150e-03,
         2.7275e-03,  2.1706e-03,  2.1801e-03,  1.9341e-03,  2.1477e-03,
         6.8128e-05,  2.1343e-03,  2.3899e-03,  2.5940e-03,  2.5578e-03,
        -2.0981e-03,  2.3308e-03,  2.5654e-03,  2.2125e-03,  2.4834e-03,
         2.3212e-03,  2.1553e-03,  1.6127e-03,  2.4757e-03,  2.1572e-03,
         9.1648e-04,  2.2297e-03,  2.1858e-03,  2.1744e-03,  2.3499e-03,
         2.1572e-03,  2.2392e-03,  2.3441e-03,  2.1515e-03,  2.1591e-03,
         2.6302e-03,  2.5730e-03,  2.5711e-03,  2.1477e-03,  2.1439e-03,
         2.1973e-03,  2.1420e-03,  2.2392e-03,  2.3632e-03,  2.1763e-03,
         2.1610e-03,  2.4471e-03,  2.3327e-03,  2.2106e-03,  2.3232e-03,
         2.5692e-03,  2.2545e-03,  2.2964e-03,  1.9760e-03, -3.2568e-04,
         2.1496e-03,  2.3956e-03,  2.5482e-03,  1.2131e-03,  2.1534e-03,
         2.2888e-03,  2.2488e-03,  1.7462e-03,  2.1515e-03,  2.1610e-03,
         2.1572e-03,  2.1935e-03,  2.3766e-03,  2.2392e-03,  2.4586e-03,
         2.1877e-03,  2.3403e-03,  2.2240e-03,  2.6798e-03,  2.6283e-03,
         2.3937e-03,  2.1477e-03,  2.1286e-03,  2.4281e-03,  1.6613e-03,
         2.2144e-03,  2.4891e-03,  2.1534e-03,  2.1515e-03,  2.6989e-03,
         1.9131e-03,  2.2774e-03,  2.1667e-03,  2.0103e-03,  2.2984e-03,
         2.1515e-03,  2.1629e-03,  2.2316e-03,  2.1896e-03,  2.2011e-03,
         2.1515e-03,  2.1515e-03, -1.5554e-03,  2.1877e-03,  2.2411e-03,
         2.5768e-03,  2.1992e-03,  2.3499e-03,  2.1648e-03,  2.4815e-03,
         2.1496e-03,  2.1572e-03,  2.5425e-03,  2.5959e-03,  2.2182e-03,
         2.1534e-03,  2.1477e-03,  2.2812e-03, -5.8699e-04,  2.1420e-03,
         2.6779e-03, -1.4362e-03,  2.2221e-03,  2.0485e-03,  2.2793e-03,
         2.1820e-03,  2.5940e-03,  2.1648e-03,  2.3594e-03,  2.3308e-03,
         2.3994e-03,  2.2621e-03,  2.3651e-03,  2.2926e-03,  2.2984e-03,
         2.5101e-03,  2.1877e-03,  2.7008e-03,  2.3022e-03,  2.3155e-03,
         2.1572e-03,  2.1744e-03,  2.1458e-03,  2.1439e-03,  2.1896e-03,
         2.5253e-03,  2.3994e-03,  2.5864e-03,  2.1477e-03,  2.2087e-03,
         2.5425e-03,  2.1801e-03,  2.2564e-03,  2.1687e-03,  2.6073e-03,
         2.3651e-03,  2.5978e-03,  1.1282e-03,  2.1133e-03,  2.2850e-03,
         2.4033e-03,  2.2526e-03,  2.1744e-03,  2.1496e-03,  2.1935e-03,
        -4.0984e-04, -1.0996e-03,  2.7027e-03,  2.4281e-03,  2.1534e-03,
         2.2087e-03,  2.1477e-03,  2.5692e-03,  2.1687e-03,  2.2621e-03,
         2.2316e-03,  2.2507e-03,  2.1496e-03,  2.1648e-03,  2.1439e-03,
         2.2068e-03,  2.3079e-03,  2.1782e-03,  2.4414e-03,  2.1687e-03,
         2.7065e-03,  2.1648e-03,  2.1706e-03,  2.1458e-03,  1.8473e-03,
         2.1915e-03,  2.1610e-03,  2.6054e-03,  2.1935e-03,  2.1477e-03,
         1.7328e-03,  2.2926e-03,  2.2202e-03,  2.1667e-03, -1.1425e-03,
         2.1553e-03,  2.3270e-03,  2.1572e-03,  2.1687e-03,  2.1515e-03,
         2.2049e-03,  2.2240e-03, -3.2735e-04,  2.2831e-03,  2.1610e-03,
         2.4338e-03,  2.1572e-03,  2.4452e-03,  2.1820e-03,  2.1687e-03,
         2.1667e-03,  2.3689e-03,  2.3117e-03,  2.1915e-03,  2.4014e-03,
         2.1648e-03,  2.1496e-03,  2.5940e-03,  2.3460e-03,  2.2144e-03,
         2.1801e-03,  2.1458e-03,  2.7409e-03,  1.7767e-03,  2.6855e-03,
         2.2755e-03,  2.2144e-03,  2.1782e-03,  2.1591e-03,  2.5578e-03,
         2.2869e-03,  2.1667e-03, -9.7370e-04,  2.2087e-03,  2.2640e-03,
         2.1610e-03,  2.1496e-03,  7.2956e-04,  2.3689e-03,  2.1801e-03,
         2.0084e-03,  2.1553e-03,  2.2221e-03,  2.6855e-03,  2.1706e-03,
         2.1839e-03,  7.4863e-04,  2.3460e-03,  1.0605e-03,  2.2106e-03,
         2.7428e-03,  2.1553e-03,  1.5326e-03,  2.2202e-03,  2.7275e-03,
         2.3174e-03,  2.3136e-03,  2.5711e-03,  2.1133e-03,  2.4185e-03,
         8.9121e-04,  2.7790e-03,  2.1629e-03,  2.5864e-03,  2.0962e-03,
         2.2583e-03,  2.1439e-03,  2.2240e-03,  2.1858e-03,  2.2697e-03,
         2.1725e-03,  2.3022e-03,  2.6836e-03,  2.5177e-03,  2.5024e-03,
         2.3384e-03,  2.4662e-03,  2.4643e-03,  2.1648e-03, -1.0462e-03,
         2.3994e-03,  3.6263e-04, -1.3914e-03,  2.1725e-03,  2.5463e-03,
         1.9875e-03,  2.6207e-03,  2.6379e-03,  2.1820e-03,  2.5578e-03,
         2.3193e-03,  2.6321e-03,  4.6587e-04,  2.1458e-03,  2.3479e-03,
         2.1877e-03,  5.3453e-04,  2.2125e-03,  2.1820e-03,  2.1973e-03,
         2.4681e-03,  2.0199e-03,  2.4776e-03,  2.2888e-03,  2.2564e-03,
         2.3155e-03,  2.2678e-03,  2.1553e-03,  2.1915e-03,  2.2354e-03,
         2.3327e-03,  2.2659e-03,  2.1515e-03,  2.4395e-03,  2.1877e-03,
         2.1591e-03,  2.1667e-03,  2.6226e-03,  2.2373e-03,  2.2545e-03,
         2.1553e-03,  2.5082e-03,  2.5501e-03,  2.4014e-03, -6.5470e-04,
         2.1553e-03,  2.1553e-03,  2.2926e-03,  2.1591e-03,  2.1782e-03,
         2.3956e-03,  1.9493e-03,  2.3003e-03,  2.1915e-03,  2.2106e-03,
         2.4109e-03,  2.1553e-03,  2.1572e-03,  2.2297e-03, -2.9254e-04,
         2.1477e-03,  2.1877e-03,  2.3289e-03,  2.4281e-03,  2.6398e-03,
         2.1515e-03,  2.0428e-03,  2.1858e-03,  2.1610e-03,  2.6093e-03,
         2.6569e-03,  2.2907e-03,  2.1667e-03,  1.0262e-03,  2.6646e-03,
         2.6150e-03,  2.1572e-03,  2.1515e-03,  2.1610e-03,  2.4681e-03,
         2.2945e-03,  2.2984e-03,  2.6169e-03,  2.1610e-03, -2.1591e-03,
         2.5101e-03,  2.1725e-03,  2.1648e-03,  2.1763e-03,  2.1648e-03,
         2.6207e-03,  2.1496e-03,  2.2011e-03,  1.8263e-03,  2.1629e-03,
         2.1572e-03,  2.1553e-03,  2.1515e-03,  2.1458e-03,  2.1687e-03,
         2.6989e-03,  2.2812e-03,  2.5597e-03,  6.0511e-04,  2.7199e-03,
        -2.2144e-03,  2.5864e-03,  2.1610e-03,  2.2278e-03,  2.6455e-03,
         2.2602e-03,  2.1858e-03,  2.5196e-03,  2.5101e-03,  2.2068e-03,
         2.3079e-03,  2.4033e-03,  2.2602e-03,  2.3899e-03,  2.1477e-03,
         2.2030e-03,  2.1572e-03,  2.3174e-03,  2.1515e-03,  2.5311e-03,
         2.1534e-03,  2.1820e-03,  2.2602e-03,  2.1610e-03,  2.2125e-03,
         2.7714e-03,  2.3727e-03,  2.4071e-03,  2.5997e-03,  2.1839e-03,
        -1.9207e-03,  2.1801e-03,  2.3785e-03,  2.1515e-03,  2.6913e-03,
         2.2011e-03,  2.5654e-03,  2.5425e-03,  2.5902e-03,  2.3365e-03,
         2.2106e-03,  2.1763e-03,  2.5959e-03,  2.3975e-03,  2.1496e-03,
         2.4014e-03,  2.5234e-03,  8.7500e-04,  2.1992e-03,  2.6112e-03,
         2.1610e-03,  2.4052e-03,  2.4376e-03,  1.7672e-03,  2.1553e-03,
         2.1954e-03,  2.1935e-03,  2.6817e-03,  2.2278e-03,  2.4853e-03,
         2.4433e-03,  2.1915e-03,  2.1629e-03,  2.1992e-03,  2.1839e-03,
         2.7447e-03,  2.2888e-03,  2.1725e-03,  2.4490e-03,  2.1477e-03],
       device='cuda:3', dtype=torch.float16, requires_grad=True)[2023-09-12 00:21:02,715][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0

loss: tensor(12193.4170, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7124, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9014.1768, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0273, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11379.8066, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8364, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11474.9961, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1582, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9307.5830, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6953, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9009.8086, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8789, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11308.6641, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2188, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11706.7773, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3672, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11444.5107, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3555, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11558.5186, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7070, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11963.5127, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6641, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11850.2100, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6758, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12109.1377, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7734, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10041.0830, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7695, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9521.8555, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.0391, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11495.3330, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3867, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11306.5361, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3125, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11353.9561, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8574, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11882.7832, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5938, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11518.5176, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3887, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 3.3283e-03,  3.3531e-03,  1.8463e-03,  3.5076e-03, -5.6744e-04,
         3.1662e-03,  3.1815e-03,  3.1929e-03,  3.8605e-03,  3.5877e-03,
         3.3627e-03,  3.5038e-03,  3.7193e-03,  3.1815e-03,  3.2749e-03,
         3.2501e-03,  3.4771e-03,  3.4237e-03, -2.2068e-03,  3.1776e-03,
         3.2310e-03,  3.8376e-03,  3.3493e-03,  4.0245e-03,  3.9253e-03,
         3.6125e-03,  3.4714e-03,  3.2711e-03,  3.1738e-03,  3.1700e-03,
         3.2406e-03,  3.8013e-03,  2.6894e-03,  3.1986e-03,  3.4351e-03,
         3.2883e-03,  3.1719e-03,  3.1757e-03,  3.4637e-03,  3.4790e-03,
         3.1776e-03,  3.7746e-03,  3.2101e-03,  3.0155e-03,  3.2043e-03,
         3.4332e-03,  3.1929e-03,  3.2024e-03,  3.3894e-03,  4.2343e-03,
         3.1986e-03,  3.4275e-03,  3.6240e-03,  3.1490e-03,  3.2120e-03,
         2.9869e-03,  3.3779e-03,  6.3717e-05,  3.2635e-03,  3.1891e-03,
         3.3951e-03,  3.1815e-03,  3.1815e-03,  3.1757e-03,  3.3226e-03,
         3.3245e-03,  3.1986e-03,  4.1847e-03,  3.1948e-03,  3.1776e-03,
         3.1700e-03,  3.3207e-03,  3.1891e-03,  3.2635e-03,  3.1719e-03,
         3.5286e-03,  3.2215e-03, -2.5578e-03,  3.2444e-03,  3.4599e-03,
         3.1776e-03,  3.4351e-03,  3.2730e-03,  3.3436e-03,  3.1719e-03,
         3.2845e-03,  3.1986e-03,  3.1872e-03,  3.6755e-03,  3.6869e-03,
         3.1567e-03,  3.5381e-03,  3.7594e-03,  3.2921e-03,  3.6411e-03,
         3.6564e-03,  1.4410e-03,  3.3741e-03,  3.2654e-03,  3.2005e-03,
         3.5362e-03,  3.3398e-03,  3.6297e-03,  3.8738e-03,  3.3264e-03,
         3.1643e-03,  3.3150e-03,  3.7289e-03,  3.9711e-03,  3.4866e-03,
         4.1580e-03,  3.3207e-03,  7.7307e-05,  3.2101e-03,  3.2063e-03,
         3.4676e-03,  3.1662e-03,  3.2387e-03,  3.1433e-03,  4.0588e-03,
         3.1719e-03,  3.1738e-03,  3.1776e-03,  3.1910e-03,  1.6603e-03,
         3.6449e-03,  3.2673e-03,  3.1872e-03,  3.2406e-03,  3.2425e-03,
         3.2444e-03,  3.1815e-03,  6.1631e-05,  3.6201e-03,  3.1986e-03,
         3.5496e-03,  3.4523e-03,  3.6621e-03,  2.6379e-03,  3.2654e-03,
         2.9621e-03, -1.3199e-03,  3.8738e-03,  3.3779e-03,  3.4332e-03,
         3.5133e-03,  3.1757e-03,  3.5000e-03,  3.6812e-03,  3.5191e-03,
         3.5477e-03,  3.5229e-03,  4.1161e-03,  2.9793e-03,  3.1776e-03,
         3.3512e-03,  3.3474e-03,  3.2387e-03,  3.6392e-03,  3.1719e-03,
         3.1967e-03,  3.3283e-03,  3.2978e-03,  3.7231e-03,  3.1719e-03,
         3.4676e-03,  3.5248e-03,  3.3016e-03,  3.5915e-03,  3.4866e-03,
         3.1872e-03,  3.2368e-03,  3.5591e-03,  3.2101e-03,  3.2463e-03,
         3.3436e-03,  3.2368e-03,  3.4199e-03,  3.6068e-03, -7.9155e-04,
         3.3588e-03,  3.2310e-03,  2.8610e-03,  4.3602e-03,  3.4847e-03,
         3.2539e-03,  3.1834e-03,  3.1948e-03,  3.1853e-03,  3.7041e-03,
         4.1275e-03,  3.1853e-03,  3.1986e-03,  3.0327e-03,  3.1681e-03,
         1.3380e-03,  3.1567e-03,  3.3684e-03,  3.4714e-03,  3.9024e-03,
        -1.9836e-03,  3.3550e-03,  3.4771e-03,  3.2139e-03,  3.4485e-03,
         3.4885e-03,  3.1738e-03,  2.6112e-03,  3.4447e-03,  3.1776e-03,
         1.7405e-03,  3.2310e-03,  3.2845e-03,  3.1910e-03,  3.3379e-03,
         3.1853e-03,  3.4084e-03,  3.3340e-03,  3.2005e-03,  3.2253e-03,
         3.5210e-03,  3.4447e-03,  3.5019e-03,  3.1681e-03,  3.7346e-03,
         3.2120e-03,  3.1643e-03,  3.2558e-03,  3.3321e-03,  3.6030e-03,
         3.1948e-03,  3.4180e-03,  3.3417e-03,  3.2272e-03,  3.3169e-03,
         3.8338e-03,  3.2635e-03,  3.5114e-03,  3.1338e-03,  1.0653e-03,
         3.1910e-03,  3.4294e-03,  3.9330e-03,  2.0275e-03,  3.1738e-03,
         3.2730e-03,  3.4542e-03,  2.7676e-03,  3.1719e-03,  3.1776e-03,
         3.1776e-03,  3.2024e-03,  4.1466e-03,  3.2768e-03,  3.8090e-03,
         3.1986e-03,  3.3073e-03,  3.2368e-03,  4.2877e-03,  3.5248e-03,
         4.0474e-03,  3.1700e-03,  3.1414e-03,  3.5172e-03,  2.9182e-03,
         3.3741e-03,  3.4065e-03,  3.1738e-03,  3.1719e-03,  3.6564e-03,
         3.0270e-03,  3.4466e-03,  3.1853e-03,  3.0308e-03,  3.4466e-03,
         3.1700e-03,  3.1815e-03,  3.2501e-03,  3.2101e-03,  3.2082e-03,
         3.1719e-03,  3.1700e-03, -4.7016e-04,  3.2024e-03,  3.4180e-03,
         4.2381e-03,  3.2635e-03,  3.7231e-03,  3.1834e-03,  3.3836e-03,
         3.1719e-03,  3.1929e-03,  3.4962e-03,  3.4924e-03,  3.2387e-03,
         3.1738e-03,  3.1681e-03,  3.2616e-03,  5.9128e-04,  3.2406e-03,
         3.5725e-03, -1.1683e-03,  3.2539e-03,  3.0746e-03,  3.2978e-03,
         3.2101e-03,  3.4714e-03,  3.1853e-03,  3.3283e-03,  3.2864e-03,
         3.3741e-03,  3.4542e-03,  3.3855e-03,  3.7003e-03,  3.2730e-03,
         3.4637e-03,  3.2082e-03,  3.5973e-03,  3.3092e-03,  3.3836e-03,
         3.6106e-03,  3.9520e-03,  3.1681e-03,  3.1681e-03,  3.2005e-03,
         4.1313e-03,  3.3741e-03,  3.5419e-03,  3.1681e-03,  3.3646e-03,
         3.4447e-03,  3.1948e-03,  3.3951e-03,  3.1872e-03,  3.5095e-03,
         3.3493e-03,  3.4370e-03,  1.9369e-03,  3.1242e-03,  3.4618e-03,
         3.4275e-03,  3.3588e-03,  3.1948e-03,  3.1700e-03,  3.3169e-03,
         1.1406e-03, -2.8312e-05,  4.5357e-03,  4.1504e-03,  3.1719e-03,
         3.2234e-03,  3.1700e-03,  3.7079e-03,  3.1853e-03,  4.0092e-03,
         3.2463e-03,  3.2387e-03,  3.1815e-03,  3.1834e-03,  3.1662e-03,
         3.2272e-03,  3.3283e-03,  3.5477e-03,  3.3722e-03,  3.1872e-03,
         4.0512e-03,  3.1834e-03,  3.1872e-03,  3.1681e-03,  3.6259e-03,
         3.2082e-03,  3.1796e-03,  3.5191e-03,  3.2063e-03,  3.1700e-03,
         3.3226e-03,  3.2692e-03,  3.2291e-03,  3.1834e-03, -4.3726e-04,
         3.1738e-03,  4.1275e-03,  3.1776e-03,  3.1872e-03,  3.1719e-03,
         3.2196e-03,  3.3379e-03, -2.0370e-03,  3.6621e-03,  3.1796e-03,
         3.3760e-03,  3.1738e-03,  3.4332e-03,  3.1986e-03,  3.1834e-03,
         3.1834e-03,  3.3474e-03,  3.2578e-03,  3.2101e-03,  4.0665e-03,
         3.1815e-03,  3.1700e-03,  3.5534e-03,  3.3665e-03,  3.2330e-03,
         3.2234e-03,  3.1681e-03,  3.9368e-03,  2.7866e-03,  3.6030e-03,
         3.2768e-03,  3.2482e-03,  3.1910e-03,  3.1796e-03,  3.4828e-03,
         3.2978e-03,  3.1853e-03, -5.6601e-04,  3.2215e-03,  3.2539e-03,
         3.1815e-03,  3.1700e-03,  2.1305e-03,  3.3531e-03,  3.2215e-03,
         3.0785e-03,  3.1757e-03,  3.2349e-03,  3.6526e-03,  3.2825e-03,
         3.1986e-03,  1.5478e-03,  3.3379e-03,  2.4395e-03,  3.2158e-03,
         3.5954e-03,  3.1757e-03,  2.4662e-03,  3.9215e-03,  3.9825e-03,
         3.5896e-03,  3.2806e-03,  3.5114e-03,  3.1261e-03,  3.3894e-03,
         2.6741e-03,  4.0817e-03,  3.1834e-03,  4.0855e-03,  3.8795e-03,
         3.2635e-03,  3.1681e-03,  3.7193e-03,  3.1986e-03,  3.9253e-03,
         3.1910e-03,  3.2864e-03,  3.5534e-03,  3.4485e-03,  3.4370e-03,
         3.3073e-03,  3.4370e-03,  3.3455e-03,  3.1834e-03, -7.7963e-05,
         3.3760e-03,  8.4257e-04, -1.2827e-03,  3.1929e-03,  4.1428e-03,
         3.0251e-03,  4.4327e-03,  3.5267e-03,  3.1986e-03,  3.8261e-03,
         3.3188e-03,  3.4771e-03,  1.2798e-03,  3.1681e-03,  3.3226e-03,
         3.2005e-03, -5.5265e-04,  3.2310e-03,  3.1986e-03,  3.2673e-03,
         3.4027e-03,  3.0384e-03,  3.4580e-03,  3.2997e-03,  3.2921e-03,
         3.6659e-03,  3.3054e-03,  3.1757e-03,  3.2024e-03,  3.2387e-03,
         3.3665e-03,  3.2654e-03,  3.1700e-03,  3.4389e-03,  3.1986e-03,
         3.1796e-03,  3.1948e-03,  3.5076e-03,  3.2730e-03,  3.2673e-03,
         3.1757e-03,  3.7842e-03,  3.5057e-03,  3.7136e-03, -5.0068e-04,
         3.1757e-03,  3.1757e-03,  3.2730e-03,  3.1891e-03,  3.1986e-03,
         3.4256e-03,  2.9507e-03,  3.3016e-03,  3.2082e-03,  3.2310e-03,
         3.3684e-03,  3.1738e-03,  3.1796e-03,  3.2387e-03,  4.8542e-04,
         3.5381e-03,  3.2043e-03,  3.5267e-03,  3.3684e-03,  3.5419e-03,
         3.5076e-03,  3.3092e-03,  3.2043e-03,  3.1796e-03,  3.6240e-03,
         3.6068e-03,  3.3016e-03,  3.1853e-03,  1.9169e-03,  3.6697e-03,
         3.4924e-03,  3.1796e-03,  3.1719e-03,  3.1796e-03,  3.3913e-03,
         3.2845e-03,  3.3054e-03,  3.4943e-03,  3.1815e-03, -2.0390e-03,
         3.4142e-03,  3.1872e-03,  3.1853e-03,  3.1929e-03,  3.1853e-03,
         3.6983e-03,  3.1719e-03,  3.2387e-03,  2.8381e-03,  3.1834e-03,
         3.1757e-03,  3.1662e-03,  3.1700e-03,  3.1662e-03,  3.1834e-03,
         4.5433e-03,  3.8166e-03,  3.4676e-03,  2.3022e-03,  3.1357e-03,
        -1.5221e-03,  3.8376e-03,  3.1776e-03,  3.2425e-03,  3.5782e-03,
         3.2864e-03,  3.2005e-03,  3.5744e-03,  3.6335e-03,  3.2234e-03,
         3.2845e-03,  4.1389e-03,  3.2558e-03,  3.9139e-03,  3.1738e-03,
         3.3264e-03,  3.1776e-03,  4.0588e-03,  3.1719e-03,  3.4485e-03,
         3.1738e-03,  3.1967e-03,  3.8433e-03,  3.1815e-03,  3.2330e-03,
         3.5515e-03,  3.3474e-03,  3.3741e-03,  3.7251e-03,  3.7289e-03,
        -1.5726e-03,  3.1948e-03,  3.8643e-03,  3.1719e-03,  3.5286e-03,
         3.2272e-03,  4.2839e-03,  3.4943e-03,  3.4733e-03,  3.3169e-03,
         3.2310e-03,  3.1910e-03,  3.6354e-03,  3.4027e-03,  3.1700e-03,
         3.5229e-03,  3.8757e-03,  1.7653e-03,  3.2158e-03,  3.5095e-03,
         3.1776e-03,  4.0665e-03,  3.5133e-03,  3.1681e-03,  3.1738e-03,
         3.6602e-03,  3.4065e-03,  3.8433e-03,  3.2463e-03,  3.4199e-03,
         4.1885e-03,  3.6507e-03,  3.1834e-03,  3.2158e-03,  3.2043e-03,
         3.5820e-03,  3.2959e-03,  3.1891e-03,  4.1275e-03,  3.1681e-03],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 3.3340e-03,  3.3588e-03,  1.8520e-03,  3.5133e-03, -5.6410e-04,
         3.1719e-03,  3.1872e-03,  3.1986e-03,  3.8643e-03,  3.5934e-03,
         3.3684e-03,  3.5076e-03,  3.7251e-03,  3.1872e-03,  3.2806e-03,
         3.2558e-03,  3.4828e-03,  3.4294e-03, -2.2049e-03,  3.1834e-03,
         3.2368e-03,  3.8433e-03,  3.3569e-03,  4.0283e-03,  3.9291e-03,
         3.6182e-03,  3.4771e-03,  3.2768e-03,  3.1796e-03,  3.1757e-03,
         3.2482e-03,  3.8090e-03,  2.6913e-03,  3.2043e-03,  3.4409e-03,
         3.2940e-03,  3.1757e-03,  3.1815e-03,  3.4714e-03,  3.4847e-03,
         3.1834e-03,  3.7804e-03,  3.2158e-03,  3.0212e-03,  3.2101e-03,
         3.4389e-03,  3.1986e-03,  3.2082e-03,  3.3951e-03,  4.2419e-03,
         3.2043e-03,  3.4332e-03,  3.6278e-03,  3.1548e-03,  3.2177e-03,
         2.9926e-03,  3.3836e-03,  6.9916e-05,  3.2692e-03,  3.1929e-03,
         3.4008e-03,  3.1872e-03,  3.1872e-03,  3.1815e-03,  3.3283e-03,
         3.3302e-03,  3.2043e-03,  4.1962e-03,  3.2005e-03,  3.1834e-03,
         3.1757e-03,  3.3264e-03,  3.1948e-03,  3.2692e-03,  3.1757e-03,
         3.5324e-03,  3.2272e-03, -2.5558e-03,  3.2501e-03,  3.4676e-03,
         3.1834e-03,  3.4409e-03,  3.2787e-03,  3.3493e-03,  3.1776e-03,
         3.2921e-03,  3.2043e-03,  3.1929e-03,  3.6831e-03,  3.6926e-03,
         3.1624e-03,  3.5400e-03,  3.7670e-03,  3.2978e-03,  3.6449e-03,
         3.6640e-03,  1.4458e-03,  3.3798e-03,  3.2711e-03,  3.2063e-03,
         3.5419e-03,  3.3455e-03,  3.6335e-03,  3.8795e-03,  3.3340e-03,
         3.1719e-03,  3.3207e-03,  3.7327e-03,  3.9787e-03,  3.4924e-03,
         4.1618e-03,  3.3264e-03,  7.9870e-05,  3.2158e-03,  3.2120e-03,
         3.4714e-03,  3.1738e-03,  3.2444e-03,  3.1452e-03,  4.0627e-03,
         3.1776e-03,  3.1815e-03,  3.1834e-03,  3.1967e-03,  1.6651e-03,
         3.6507e-03,  3.2730e-03,  3.1929e-03,  3.2482e-03,  3.2482e-03,
         3.2501e-03,  3.1872e-03,  6.4611e-05,  3.6240e-03,  3.2024e-03,
         3.5496e-03,  3.4561e-03,  3.6640e-03,  2.6436e-03,  3.2711e-03,
         2.9678e-03, -1.3170e-03,  3.8776e-03,  3.3836e-03,  3.4351e-03,
         3.5191e-03,  3.1815e-03,  3.5038e-03,  3.6850e-03,  3.5229e-03,
         3.5534e-03,  3.5286e-03,  4.1199e-03,  2.9850e-03,  3.1834e-03,
         3.3569e-03,  3.3531e-03,  3.2444e-03,  3.6430e-03,  3.1776e-03,
         3.2024e-03,  3.3340e-03,  3.3035e-03,  3.7270e-03,  3.1776e-03,
         3.4733e-03,  3.5305e-03,  3.3073e-03,  3.5973e-03,  3.4924e-03,
         3.1929e-03,  3.2425e-03,  3.5591e-03,  3.2158e-03,  3.2520e-03,
         3.3531e-03,  3.2425e-03,  3.4256e-03,  3.6125e-03, -7.9107e-04,
         3.3665e-03,  3.2368e-03,  2.8648e-03,  4.3678e-03,  3.4904e-03,
         3.2597e-03,  3.1891e-03,  3.2005e-03,  3.1910e-03,  3.7098e-03,
         4.1313e-03,  3.1929e-03,  3.2043e-03,  3.0384e-03,  3.1738e-03,
         1.3390e-03,  3.1624e-03,  3.3741e-03,  3.4771e-03,  3.9101e-03,
        -1.9817e-03,  3.3607e-03,  3.4828e-03,  3.2196e-03,  3.4542e-03,
         3.4962e-03,  3.1796e-03,  2.6169e-03,  3.4504e-03,  3.1834e-03,
         1.7462e-03,  3.2368e-03,  3.2883e-03,  3.1967e-03,  3.3436e-03,
         3.1910e-03,  3.4142e-03,  3.3398e-03,  3.2063e-03,  3.2310e-03,
         3.5267e-03,  3.4485e-03,  3.5076e-03,  3.1738e-03,  3.7441e-03,
         3.2177e-03,  3.1700e-03,  3.2616e-03,  3.3379e-03,  3.6087e-03,
         3.2005e-03,  3.4237e-03,  3.3474e-03,  3.2330e-03,  3.3226e-03,
         3.8376e-03,  3.2692e-03,  3.5133e-03,  3.1395e-03,  1.0738e-03,
         3.1967e-03,  3.4351e-03,  3.9368e-03,  2.0332e-03,  3.1796e-03,
         3.2787e-03,  3.4618e-03,  2.7733e-03,  3.1776e-03,  3.1834e-03,
         3.1834e-03,  3.2082e-03,  4.1542e-03,  3.2825e-03,  3.8109e-03,
         3.2063e-03,  3.3131e-03,  3.2425e-03,  4.2953e-03,  3.5305e-03,
         4.0550e-03,  3.1757e-03,  3.1471e-03,  3.5267e-03,  2.9221e-03,
         3.3798e-03,  3.4122e-03,  3.1796e-03,  3.1796e-03,  3.6583e-03,
         3.0308e-03,  3.4523e-03,  3.1910e-03,  3.0365e-03,  3.4504e-03,
         3.1776e-03,  3.1872e-03,  3.2558e-03,  3.2158e-03,  3.2139e-03,
         3.1776e-03,  3.1776e-03, -4.6802e-04,  3.2082e-03,  3.4218e-03,
         4.2458e-03,  3.2692e-03,  3.7270e-03,  3.1891e-03,  3.3894e-03,
         3.1776e-03,  3.1986e-03,  3.4981e-03,  3.4981e-03,  3.2444e-03,
         3.1796e-03,  3.1757e-03,  3.2673e-03,  5.9414e-04,  3.2463e-03,
         3.5782e-03, -1.1663e-03,  3.2597e-03,  3.0804e-03,  3.3054e-03,
         3.2158e-03,  3.4771e-03,  3.1910e-03,  3.3340e-03,  3.2921e-03,
         3.3798e-03,  3.4561e-03,  3.3913e-03,  3.7079e-03,  3.2787e-03,
         3.4695e-03,  3.2139e-03,  3.6011e-03,  3.3150e-03,  3.3894e-03,
         3.6125e-03,  3.9711e-03,  3.1738e-03,  3.1738e-03,  3.2063e-03,
         4.1389e-03,  3.3798e-03,  3.5477e-03,  3.1738e-03,  3.3703e-03,
         3.4504e-03,  3.2005e-03,  3.4008e-03,  3.1929e-03,  3.5152e-03,
         3.3550e-03,  3.4409e-03,  1.9417e-03,  3.1300e-03,  3.4695e-03,
         3.4332e-03,  3.3646e-03,  3.2005e-03,  3.1757e-03,  3.3226e-03,
         1.1520e-03, -2.5034e-05,  4.5509e-03,  4.1656e-03,  3.1776e-03,
         3.2291e-03,  3.1757e-03,  3.7193e-03,  3.1910e-03,  4.0169e-03,
         3.2520e-03,  3.2444e-03,  3.1872e-03,  3.1891e-03,  3.1719e-03,
         3.2330e-03,  3.3340e-03,  3.5515e-03,  3.3779e-03,  3.1929e-03,
         4.0550e-03,  3.1891e-03,  3.1929e-03,  3.1738e-03,  3.6373e-03,
         3.2139e-03,  3.1853e-03,  3.5248e-03,  3.2120e-03,  3.1757e-03,
         3.3340e-03,  3.2749e-03,  3.2349e-03,  3.1891e-03, -4.3392e-04,
         3.1796e-03,  4.1389e-03,  3.1834e-03,  3.1929e-03,  3.1776e-03,
         3.2253e-03,  3.3436e-03, -2.0409e-03,  3.6678e-03,  3.1853e-03,
         3.3817e-03,  3.1815e-03,  3.4389e-03,  3.2043e-03,  3.1910e-03,
         3.1891e-03,  3.3531e-03,  3.2635e-03,  3.2158e-03,  4.0741e-03,
         3.1891e-03,  3.1757e-03,  3.5610e-03,  3.3722e-03,  3.2387e-03,
         3.2291e-03,  3.1738e-03,  3.9406e-03,  2.7924e-03,  3.6068e-03,
         3.2825e-03,  3.2539e-03,  3.1967e-03,  3.1853e-03,  3.4885e-03,
         3.3035e-03,  3.1929e-03, -5.6267e-04,  3.2272e-03,  3.2597e-03,
         3.1872e-03,  3.1757e-03,  2.1381e-03,  3.3588e-03,  3.2272e-03,
         3.0823e-03,  3.1815e-03,  3.2406e-03,  3.6583e-03,  3.2883e-03,
         3.2043e-03,  1.5526e-03,  3.3436e-03,  2.4490e-03,  3.2215e-03,
         3.6011e-03,  3.1834e-03,  2.4719e-03,  3.9330e-03,  3.9864e-03,
         3.5954e-03,  3.2864e-03,  3.5172e-03,  3.1319e-03,  3.3951e-03,
         2.6913e-03,  4.0855e-03,  3.1891e-03,  4.0932e-03,  3.8948e-03,
         3.2692e-03,  3.1738e-03,  3.7231e-03,  3.2043e-03,  3.9330e-03,
         3.1967e-03,  3.2921e-03,  3.5591e-03,  3.4542e-03,  3.4428e-03,
         3.3131e-03,  3.4428e-03,  3.3512e-03,  3.1891e-03, -7.3552e-05,
         3.3817e-03,  8.4639e-04, -1.2817e-03,  3.1986e-03,  4.1466e-03,
         3.0308e-03,  4.4403e-03,  3.5324e-03,  3.2043e-03,  3.8300e-03,
         3.3245e-03,  3.4809e-03,  1.2827e-03,  3.1738e-03,  3.3283e-03,
         3.2063e-03, -5.5790e-04,  3.2368e-03,  3.2043e-03,  3.2730e-03,
         3.4084e-03,  3.0441e-03,  3.4657e-03,  3.3054e-03,  3.2978e-03,
         3.6716e-03,  3.3112e-03,  3.1815e-03,  3.2101e-03,  3.2444e-03,
         3.3722e-03,  3.2711e-03,  3.1776e-03,  3.4447e-03,  3.2043e-03,
         3.1853e-03,  3.2005e-03,  3.5133e-03,  3.2787e-03,  3.2730e-03,
         3.1815e-03,  3.7918e-03,  3.5114e-03,  3.7289e-03, -4.9925e-04,
         3.1815e-03,  3.1815e-03,  3.2787e-03,  3.1948e-03,  3.2043e-03,
         3.4313e-03,  2.9564e-03,  3.3073e-03,  3.2139e-03,  3.2368e-03,
         3.3722e-03,  3.1815e-03,  3.1853e-03,  3.2444e-03,  4.9067e-04,
         3.5458e-03,  3.2101e-03,  3.5324e-03,  3.3741e-03,  3.5477e-03,
         3.5267e-03,  3.3131e-03,  3.2101e-03,  3.1853e-03,  3.6297e-03,
         3.6125e-03,  3.3073e-03,  3.1910e-03,  1.9226e-03,  3.6774e-03,
         3.4847e-03,  3.1853e-03,  3.1776e-03,  3.1853e-03,  3.3932e-03,
         3.2921e-03,  3.3112e-03,  3.5000e-03,  3.1872e-03, -2.0390e-03,
         3.4199e-03,  3.1948e-03,  3.1910e-03,  3.1986e-03,  3.1910e-03,
         3.7041e-03,  3.1776e-03,  3.2444e-03,  2.8439e-03,  3.1891e-03,
         3.1815e-03,  3.1719e-03,  3.1776e-03,  3.1719e-03,  3.1891e-03,
         4.5547e-03,  3.8261e-03,  3.4733e-03,  2.3060e-03,  3.1147e-03,
        -1.5144e-03,  3.8414e-03,  3.1834e-03,  3.2482e-03,  3.5839e-03,
         3.2921e-03,  3.2063e-03,  3.5801e-03,  3.6354e-03,  3.2291e-03,
         3.2902e-03,  4.1542e-03,  3.2616e-03,  3.9215e-03,  3.1796e-03,
         3.3321e-03,  3.1834e-03,  4.0741e-03,  3.1776e-03,  3.4542e-03,
         3.1796e-03,  3.2024e-03,  3.8548e-03,  3.1872e-03,  3.2387e-03,
         3.5553e-03,  3.3531e-03,  3.3798e-03,  3.7289e-03,  3.7384e-03,
        -1.5812e-03,  3.2005e-03,  3.8757e-03,  3.1776e-03,  3.5343e-03,
         3.2330e-03,  4.2953e-03,  3.5000e-03,  3.4790e-03,  3.3226e-03,
         3.2368e-03,  3.1967e-03,  3.6430e-03,  3.4084e-03,  3.1757e-03,
         3.5305e-03,  3.8795e-03,  1.7710e-03,  3.2215e-03,  3.5152e-03,
         3.1834e-03,  4.0779e-03,  3.5191e-03,  3.1738e-03,  3.1796e-03,
         3.6716e-03,  3.4122e-03,  3.8490e-03,  3.2520e-03,  3.4256e-03,
         4.1962e-03,  3.6602e-03,  3.1891e-03,  3.2215e-03,  3.2101e-03,
         3.5877e-03,  3.3016e-03,  3.1948e-03,  4.1351e-03,  3.1738e-03],
       device='cuda:2', dtype=torch.float16, requires_grad=True)loss: tensor(11249.7500, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5967, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 00:26:04,292][train_inner][INFO] - {"epoch": 2, "update": 1.541, "loss": "6.522", "ntokens": "149732", "nsentences": "539.26", "prob_perplexity": "476.02", "code_perplexity": "462.39", "temp": "1.993", "loss_0": "6.465", "loss_1": "0.037", "loss_2": "0.02", "accuracy": "0.02562", "wps": "37521.7", "ups": "0.25", "wpb": "149732", "bsz": "539.3", "num_updates": "800", "lr": "1.25e-05", "gnorm": "0.247", "loss_scale": "64", "train_wall": "797", "gb_free": "12.7", "wall": "3231"}

self.scaling_factor_for_vector: Parameter containing:
tensor([ 3.4161e-03,  3.4409e-03,  1.9321e-03,  3.5915e-03, -5.2118e-04,
         3.2539e-03,  3.2673e-03,  3.2806e-03,  3.9291e-03,  3.6678e-03,
         3.4485e-03,  3.5877e-03,  3.7937e-03,  3.2692e-03,  3.3627e-03,
         3.3379e-03,  3.5629e-03,  3.5095e-03, -2.1706e-03,  3.2654e-03,
         3.3169e-03,  3.9177e-03,  3.4504e-03,  4.0665e-03,  3.9787e-03,
         3.6945e-03,  3.5572e-03,  3.3550e-03,  3.2616e-03,  3.2578e-03,
         3.3283e-03,  3.9139e-03,  2.7084e-03,  3.2864e-03,  3.5172e-03,
         3.3760e-03,  3.2043e-03,  3.2635e-03,  3.5534e-03,  3.5648e-03,
         3.2654e-03,  3.8471e-03,  3.2978e-03,  3.1033e-03,  3.2921e-03,
         3.5191e-03,  3.2806e-03,  3.2902e-03,  3.4771e-03,  4.3297e-03,
         3.2864e-03,  3.5114e-03,  3.6755e-03,  3.2349e-03,  3.2997e-03,
         3.0575e-03,  3.4618e-03,  1.3447e-04,  3.3455e-03,  3.2501e-03,
         3.4733e-03,  3.2692e-03,  3.2692e-03,  3.2635e-03,  3.4084e-03,
         3.4103e-03,  3.2864e-03,  4.3373e-03,  3.2825e-03,  3.2654e-03,
         3.2578e-03,  3.4084e-03,  3.2768e-03,  3.3493e-03,  3.2444e-03,
         3.6087e-03,  3.3092e-03, -2.5291e-03,  3.3321e-03,  3.5610e-03,
         3.2654e-03,  3.5152e-03,  3.3588e-03,  3.4294e-03,  3.2597e-03,
         3.3722e-03,  3.2864e-03,  3.2730e-03,  3.7518e-03,  3.7556e-03,
         3.2444e-03,  3.5686e-03,  3.8586e-03,  3.3798e-03,  3.6812e-03,
         3.7632e-03,  1.5173e-03,  3.4599e-03,  3.3531e-03,  3.2883e-03,
         3.6221e-03,  3.4256e-03,  3.7060e-03,  3.9444e-03,  3.4218e-03,
         3.2539e-03,  3.4008e-03,  3.8033e-03,  4.0474e-03,  3.5725e-03,
         4.2191e-03,  3.4065e-03,  1.0669e-04,  3.2978e-03,  3.2940e-03,
         3.5095e-03,  3.2558e-03,  3.3264e-03,  3.1853e-03,  4.1161e-03,
         3.2616e-03,  3.2635e-03,  3.2654e-03,  3.2787e-03,  1.7223e-03,
         3.7422e-03,  3.3531e-03,  3.2768e-03,  3.3283e-03,  3.3302e-03,
         3.3321e-03,  3.2692e-03,  1.0586e-04,  3.6716e-03,  3.2444e-03,
         3.5686e-03,  3.5191e-03,  3.6964e-03,  2.7256e-03,  3.3531e-03,
         3.0499e-03, -1.2846e-03,  3.9368e-03,  3.4637e-03,  3.4695e-03,
         3.5954e-03,  3.2635e-03,  3.5572e-03,  3.7460e-03,  3.5992e-03,
         3.6316e-03,  3.6087e-03,  4.1809e-03,  3.0670e-03,  3.2654e-03,
         3.4294e-03,  3.4294e-03,  3.3264e-03,  3.6774e-03,  3.2597e-03,
         3.2845e-03,  3.4161e-03,  3.3855e-03,  3.7766e-03,  3.2597e-03,
         3.5553e-03,  3.6087e-03,  3.3894e-03,  3.6678e-03,  3.5725e-03,
         3.2749e-03,  3.3245e-03,  3.5839e-03,  3.2978e-03,  3.3340e-03,
         3.4695e-03,  3.3245e-03,  3.5057e-03,  3.6850e-03, -7.8058e-04,
         3.4466e-03,  3.3169e-03,  2.9392e-03,  4.4441e-03,  3.5706e-03,
         3.3417e-03,  3.2711e-03,  3.2825e-03,  3.2730e-03,  3.7804e-03,
         4.1580e-03,  3.2749e-03,  3.2864e-03,  3.1185e-03,  3.2558e-03,
         1.3523e-03,  3.2444e-03,  3.4561e-03,  3.5553e-03,  3.9825e-03,
        -1.9646e-03,  3.4332e-03,  3.5629e-03,  3.3016e-03,  3.5343e-03,
         3.5839e-03,  3.2616e-03,  2.6989e-03,  3.5248e-03,  3.2654e-03,
         1.8215e-03,  3.3188e-03,  3.3646e-03,  3.2787e-03,  3.4256e-03,
         3.2749e-03,  3.4981e-03,  3.4218e-03,  3.2883e-03,  3.3092e-03,
         3.6068e-03,  3.5267e-03,  3.5877e-03,  3.2558e-03,  3.8338e-03,
         3.2997e-03,  3.2520e-03,  3.3436e-03,  3.4180e-03,  3.6736e-03,
         3.2806e-03,  3.5057e-03,  3.4294e-03,  3.3150e-03,  3.4046e-03,
         3.8834e-03,  3.3512e-03,  3.5591e-03,  3.2120e-03,  1.1644e-03,
         3.2806e-03,  3.4943e-03,  4.0092e-03,  2.1095e-03,  3.2616e-03,
         3.3607e-03,  3.5534e-03,  2.8553e-03,  3.2597e-03,  3.2654e-03,
         3.2654e-03,  3.2902e-03,  4.2534e-03,  3.3665e-03,  3.8395e-03,
         3.2864e-03,  3.3932e-03,  3.3245e-03,  4.3716e-03,  3.6087e-03,
         4.1542e-03,  3.2578e-03,  3.2291e-03,  3.6564e-03,  2.9659e-03,
         3.4504e-03,  3.4924e-03,  3.2616e-03,  3.2616e-03,  3.7041e-03,
         3.0727e-03,  3.5305e-03,  3.2730e-03,  3.1185e-03,  3.4809e-03,
         3.2597e-03,  3.2692e-03,  3.3379e-03,  3.2978e-03,  3.2959e-03,
         3.2616e-03,  3.2597e-03, -4.4584e-04,  3.2902e-03,  3.4809e-03,
         4.3449e-03,  3.3455e-03,  3.7766e-03,  3.2711e-03,  3.4695e-03,
         3.2597e-03,  3.2787e-03,  3.5152e-03,  3.5782e-03,  3.3245e-03,
         3.2616e-03,  3.2578e-03,  3.3474e-03,  6.2323e-04,  3.3226e-03,
         3.6545e-03, -1.1368e-03,  3.3455e-03,  3.1605e-03,  3.3894e-03,
         3.3016e-03,  3.5553e-03,  3.2730e-03,  3.4161e-03,  3.3722e-03,
         3.4618e-03,  3.4885e-03,  3.4657e-03,  3.7880e-03,  3.3588e-03,
         3.5496e-03,  3.2959e-03,  3.6755e-03,  3.3951e-03,  3.4580e-03,
         3.6411e-03,  4.2686e-03,  3.2558e-03,  3.2558e-03,  3.2883e-03,
         4.1885e-03,  3.4599e-03,  3.6278e-03,  3.2558e-03,  3.4599e-03,
         3.5305e-03,  3.2825e-03,  3.4752e-03,  3.2749e-03,  3.5934e-03,
         3.4370e-03,  3.5038e-03,  2.0008e-03,  3.2120e-03,  3.5496e-03,
         3.5095e-03,  3.4409e-03,  3.2825e-03,  3.2597e-03,  3.4122e-03,
         1.2903e-03,  1.2159e-05,  4.8027e-03,  4.3678e-03,  3.2597e-03,
         3.3112e-03,  3.2578e-03,  3.8624e-03,  3.2730e-03,  4.1046e-03,
         3.3340e-03,  3.3245e-03,  3.2692e-03,  3.2711e-03,  3.2539e-03,
         3.3169e-03,  3.4103e-03,  3.6125e-03,  3.4580e-03,  3.2749e-03,
         4.1008e-03,  3.2711e-03,  3.2749e-03,  3.2558e-03,  3.7823e-03,
         3.2959e-03,  3.2673e-03,  3.6125e-03,  3.2940e-03,  3.2578e-03,
         3.4847e-03,  3.3550e-03,  3.3169e-03,  3.2711e-03, -3.9196e-04,
         3.2616e-03,  4.3030e-03,  3.2654e-03,  3.2749e-03,  3.2597e-03,
         3.3073e-03,  3.4199e-03, -2.0714e-03,  3.7403e-03,  3.2673e-03,
         3.4618e-03,  3.2635e-03,  3.5172e-03,  3.2864e-03,  3.2730e-03,
         3.2711e-03,  3.4351e-03,  3.3417e-03,  3.2978e-03,  4.1809e-03,
         3.2711e-03,  3.2578e-03,  3.6488e-03,  3.4523e-03,  3.3207e-03,
         3.3131e-03,  3.2558e-03,  3.9940e-03,  2.8744e-03,  3.6774e-03,
         3.3646e-03,  3.3340e-03,  3.2787e-03,  3.2673e-03,  3.5686e-03,
         3.3855e-03,  3.2749e-03, -5.1498e-04,  3.3092e-03,  3.3398e-03,
         3.2692e-03,  3.2578e-03,  2.2087e-03,  3.4389e-03,  3.3073e-03,
         3.1376e-03,  3.2635e-03,  3.3226e-03,  3.7308e-03,  3.3627e-03,
         3.2864e-03,  1.6251e-03,  3.4256e-03,  2.5444e-03,  3.3035e-03,
         3.6774e-03,  3.2654e-03,  2.5444e-03,  4.0741e-03,  4.0359e-03,
         3.6659e-03,  3.3684e-03,  3.5954e-03,  3.2139e-03,  3.4771e-03,
         2.9831e-03,  4.1237e-03,  3.2711e-03,  4.2038e-03,  4.0894e-03,
         3.3512e-03,  3.2558e-03,  3.7842e-03,  3.2864e-03,  4.0283e-03,
         3.2787e-03,  3.3741e-03,  3.6373e-03,  3.5324e-03,  3.5229e-03,
         3.3951e-03,  3.5229e-03,  3.4294e-03,  3.2711e-03, -1.9729e-05,
         3.4637e-03,  9.0265e-04, -1.2674e-03,  3.2806e-03,  4.1924e-03,
         3.1052e-03,  4.5433e-03,  3.6106e-03,  3.2864e-03,  3.8757e-03,
         3.4065e-03,  3.5324e-03,  1.3275e-03,  3.2558e-03,  3.4065e-03,
         3.2883e-03, -5.9748e-04,  3.3188e-03,  3.2864e-03,  3.3493e-03,
         3.4885e-03,  3.1261e-03,  3.5572e-03,  3.3875e-03,  3.3760e-03,
         3.7594e-03,  3.3894e-03,  3.2635e-03,  3.2921e-03,  3.3264e-03,
         3.4504e-03,  3.3531e-03,  3.2597e-03,  3.5324e-03,  3.2864e-03,
         3.2673e-03,  3.2864e-03,  3.5877e-03,  3.3607e-03,  3.3550e-03,
         3.2635e-03,  3.8681e-03,  3.5915e-03,  3.9711e-03, -4.7851e-04,
         3.2635e-03,  3.2635e-03,  3.3588e-03,  3.2749e-03,  3.2864e-03,
         3.5114e-03,  3.0384e-03,  3.3894e-03,  3.2959e-03,  3.3188e-03,
         3.4351e-03,  3.2635e-03,  3.2673e-03,  3.3264e-03,  5.6314e-04,
         3.6411e-03,  3.2921e-03,  3.5934e-03,  3.4561e-03,  3.6259e-03,
         3.7975e-03,  3.3684e-03,  3.2921e-03,  3.2673e-03,  3.6983e-03,
         3.6869e-03,  3.3894e-03,  3.2730e-03,  2.0027e-03,  3.7804e-03,
         3.3970e-03,  3.2673e-03,  3.2597e-03,  3.2673e-03,  3.4256e-03,
         3.3722e-03,  3.3932e-03,  3.5782e-03,  3.2692e-03, -2.0294e-03,
         3.5000e-03,  3.2768e-03,  3.2730e-03,  3.2806e-03,  3.2749e-03,
         3.7785e-03,  3.2597e-03,  3.3283e-03,  2.9259e-03,  3.2711e-03,
         3.2635e-03,  3.2539e-03,  3.2597e-03,  3.2539e-03,  3.2711e-03,
         4.7340e-03,  3.9253e-03,  3.5534e-03,  2.3422e-03,  2.8095e-03,
        -1.4305e-03,  3.8834e-03,  3.2654e-03,  3.3302e-03,  3.6602e-03,
         3.3722e-03,  3.2883e-03,  3.6640e-03,  3.6736e-03,  3.3112e-03,
         3.3703e-03,  4.3640e-03,  3.3436e-03,  4.0092e-03,  3.2616e-03,
         3.4142e-03,  3.2654e-03,  4.3335e-03,  3.2597e-03,  3.5267e-03,
         3.2616e-03,  3.2845e-03,  4.0627e-03,  3.2692e-03,  3.3207e-03,
         3.6201e-03,  3.4275e-03,  3.4618e-03,  3.7956e-03,  3.8948e-03,
        -1.5936e-03,  3.2825e-03,  4.0092e-03,  3.2597e-03,  3.6068e-03,
         3.3150e-03,  4.4060e-03,  3.5763e-03,  3.5572e-03,  3.4046e-03,
         3.3188e-03,  3.2787e-03,  3.7384e-03,  3.4904e-03,  3.2578e-03,
         3.6316e-03,  3.9291e-03,  1.8501e-03,  3.3035e-03,  3.5896e-03,
         3.2654e-03,  4.2229e-03,  3.5877e-03,  3.2463e-03,  3.2616e-03,
         3.7956e-03,  3.4828e-03,  3.9024e-03,  3.3340e-03,  3.5057e-03,
         4.3182e-03,  3.7594e-03,  3.2711e-03,  3.3035e-03,  3.2921e-03,
         3.6583e-03,  3.3836e-03,  3.2768e-03,  4.2763e-03,  3.2578e-03],
       device='cuda:2', dtype=torch.float16, requires_grad=True)[2023-09-12 00:32:53,919][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
loss: tensor(10329.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5493, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8943.4512, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0327, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 00:39:23,993][train_inner][INFO] - {"epoch": 2, "update": 1.927, "loss": "6.093", "ntokens": "149637", "nsentences": "539.605", "prob_perplexity": "232.774", "code_perplexity": "228.237", "temp": "1.991", "loss_0": "5.98", "loss_1": "0.092", "loss_2": "0.022", "accuracy": "0.06807", "wps": "37423.3", "ups": "0.25", "wpb": "149637", "bsz": "539.6", "num_updates": "1000", "lr": "1.5625e-05", "gnorm": "0.597", "loss_scale": "32", "train_wall": "799", "gb_free": "12.7", "wall": "4030"}
loss: tensor(8169.8154, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2722, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10405.4805, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5571, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 00:41:50,467][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 00:41:50,468][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 00:41:50,599][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2023-09-12 00:42:14,320][valid][INFO] - {"epoch": 2, "valid_loss": "5.719", "valid_ntokens": "7892.36", "valid_nsentences": "55.2525", "valid_prob_perplexity": "151.918", "valid_code_perplexity": "145.967", "valid_temp": "1.99", "valid_loss_0": "5.59", "valid_loss_1": "0.11", "valid_loss_2": "0.02", "valid_accuracy": "0.12489", "valid_wps": "33085.4", "valid_wpb": "7892.4", "valid_bsz": "55.3", "valid_num_updates": "1038", "valid_best_loss": "5.719"}
[2023-09-12 00:42:14,322][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 1038 updates
[2023-09-12 00:42:14,323][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 00:42:16,760][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 00:42:18,032][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 1038 updates, score 5.719) (writing took 3.710052242036909 seconds)
[2023-09-12 00:42:18,032][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2023-09-12 00:42:18,032][train][INFO] - {"epoch": 2, "train_loss": "6.339", "train_ntokens": "149454", "train_nsentences": "538.334", "train_prob_perplexity": "379.14", "train_code_perplexity": "369.336", "train_temp": "1.992", "train_loss_0": "6.258", "train_loss_1": "0.059", "train_loss_2": "0.023", "train_accuracy": "0.04428", "train_wps": "37087.8", "train_ups": "0.25", "train_wpb": "149454", "train_bsz": "538.3", "train_num_updates": "1038", "train_lr": "1.62188e-05", "train_gnorm": "0.376", "train_loss_scale": "32", "train_train_wall": "2056", "train_gb_free": "13", "train_wall": "4205"}
[2023-09-12 00:42:18,034][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 00:42:18,117][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2023-09-12 00:42:18,354][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 00:42:18,358][fairseq.trainer][INFO] - begin training epoch 3
[2023-09-12 00:42:18,358][fairseq_cli.train][INFO] - Start iterating over samples

loss: tensor(10207.2285, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8125, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10486.7920, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3926, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7786.0835, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5977, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10206.4570, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0592, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9394.0635, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0884, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9382.5527, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0277, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5771.9814, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6670, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6760.7471, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0947, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 4.0092e-03,  4.0321e-03,  2.5196e-03,  4.1733e-03, -1.3685e-04,
         3.8490e-03,  3.8586e-03,  3.8757e-03,  4.3182e-03,  4.2305e-03,
         4.0359e-03,  4.1695e-03,  4.2725e-03,  3.8643e-03,  3.9520e-03,
         3.9330e-03,  4.1504e-03,  4.0932e-03, -2.1210e-03,  3.8605e-03,
         3.9005e-03,  4.1962e-03,  4.0283e-03,  4.3335e-03,  4.1656e-03,
         4.2648e-03,  4.1428e-03,  3.9368e-03,  3.8567e-03,  3.8528e-03,
         3.8948e-03,  4.2992e-03,  2.9240e-03,  3.8815e-03,  4.0665e-03,
         3.9673e-03,  3.5152e-03,  3.8586e-03,  4.1389e-03,  4.1504e-03,
         3.8605e-03,  4.3030e-03,  3.8929e-03,  3.6983e-03,  3.8872e-03,
         4.1046e-03,  3.8757e-03,  3.8853e-03,  4.0703e-03,  4.5433e-03,
         3.8815e-03,  4.0894e-03,  4.1199e-03,  3.8242e-03,  3.8948e-03,
         3.5744e-03,  4.0398e-03,  2.5201e-04,  3.9062e-03,  3.7384e-03,
         4.0359e-03,  3.8643e-03,  3.8643e-03,  3.8586e-03,  3.9978e-03,
         3.9978e-03,  3.8815e-03,  4.5662e-03,  3.8776e-03,  3.8605e-03,
         3.8528e-03,  3.9978e-03,  3.8738e-03,  3.9444e-03,  3.7880e-03,
         4.1733e-03,  3.9043e-03, -2.4776e-03,  3.9253e-03,  4.1237e-03,
         3.8605e-03,  4.0703e-03,  3.9444e-03,  4.0207e-03,  3.8548e-03,
         3.9444e-03,  3.8815e-03,  3.8643e-03,  4.0512e-03,  4.0703e-03,
         3.8376e-03,  3.8452e-03,  4.3030e-03,  3.9711e-03,  3.9406e-03,
         4.1962e-03,  2.0695e-03,  4.0550e-03,  3.9482e-03,  3.8834e-03,
         4.2076e-03,  4.0169e-03,  4.2610e-03,  4.2267e-03,  3.9787e-03,
         3.8490e-03,  3.9940e-03,  4.2953e-03,  4.4136e-03,  4.1618e-03,
         4.3793e-03,  3.9978e-03,  2.4629e-04,  3.8929e-03,  3.8891e-03,
         3.8891e-03,  3.8509e-03,  3.9215e-03,  3.5648e-03,  4.3221e-03,
         3.8548e-03,  3.8586e-03,  3.8586e-03,  3.8738e-03,  2.0065e-03,
         4.3068e-03,  3.9482e-03,  3.8700e-03,  3.9215e-03,  3.9215e-03,
         3.9253e-03,  3.8643e-03,  4.9114e-04,  3.9520e-03,  3.5839e-03,
         3.7708e-03,  4.0321e-03,  3.9215e-03,  3.3169e-03,  3.9444e-03,
         3.6449e-03, -1.2245e-03,  4.4022e-03,  4.0436e-03,  3.8204e-03,
         4.1695e-03,  3.8586e-03,  4.0321e-03,  4.2114e-03,  4.1733e-03,
         4.1924e-03,  4.1924e-03,  4.4594e-03,  3.6488e-03,  3.8605e-03,
         3.9520e-03,  3.9825e-03,  3.9215e-03,  3.8719e-03,  3.8548e-03,
         3.8795e-03,  4.0092e-03,  3.9787e-03,  4.2000e-03,  3.8548e-03,
         4.1466e-03,  4.1809e-03,  3.9825e-03,  4.2076e-03,  4.1580e-03,
         3.8700e-03,  3.9215e-03,  3.8261e-03,  3.8929e-03,  3.9253e-03,
         4.0131e-03,  3.9177e-03,  4.0970e-03,  4.1962e-03, -6.3419e-04,
         4.0398e-03,  3.9062e-03,  3.5019e-03,  4.5738e-03,  4.1542e-03,
         3.9291e-03,  3.8662e-03,  3.8776e-03,  3.8681e-03,  4.2725e-03,
         4.3144e-03,  3.8681e-03,  3.8815e-03,  3.6697e-03,  3.8509e-03,
         1.4715e-03,  3.8395e-03,  4.0474e-03,  4.1389e-03,  4.3449e-03,
        -1.7500e-03,  3.9902e-03,  4.1466e-03,  3.8948e-03,  4.1275e-03,
         4.0741e-03,  3.8567e-03,  3.2921e-03,  4.0932e-03,  3.8605e-03,
         2.3918e-03,  3.9139e-03,  3.9253e-03,  3.8738e-03,  4.0207e-03,
         3.8681e-03,  4.0092e-03,  4.0131e-03,  3.8738e-03,  3.8834e-03,
         4.1885e-03,  4.1084e-03,  4.1771e-03,  3.8528e-03,  4.0627e-03,
         3.8948e-03,  3.8471e-03,  3.9406e-03,  4.0092e-03,  4.0207e-03,
         3.8700e-03,  4.0932e-03,  4.0169e-03,  3.9101e-03,  3.9978e-03,
         4.2000e-03,  3.9444e-03,  3.9291e-03,  3.7365e-03,  1.3895e-03,
         3.8681e-03,  3.9597e-03,  4.3526e-03,  2.6760e-03,  3.8567e-03,
         3.9520e-03,  4.0588e-03,  3.4447e-03,  3.8548e-03,  3.8605e-03,
         3.8605e-03,  3.8853e-03,  4.3678e-03,  3.9482e-03,  4.0512e-03,
         3.8815e-03,  3.9864e-03,  3.9177e-03,  4.4937e-03,  4.1809e-03,
         4.2572e-03,  3.8528e-03,  3.8223e-03,  4.2191e-03,  3.2387e-03,
         3.9787e-03,  4.0741e-03,  3.8567e-03,  3.8567e-03,  4.1199e-03,
         3.1757e-03,  4.0283e-03,  3.8681e-03,  3.7136e-03,  3.6564e-03,
         3.8548e-03,  3.8643e-03,  3.9330e-03,  3.8929e-03,  3.8910e-03,
         3.8567e-03,  3.8548e-03, -2.8181e-04,  3.8853e-03,  3.9520e-03,
         4.5433e-03,  3.9215e-03,  4.1351e-03,  3.8662e-03,  4.0550e-03,
         3.8548e-03,  3.8643e-03,  3.7231e-03,  4.1618e-03,  3.9139e-03,
         3.8567e-03,  3.8528e-03,  3.9406e-03,  7.4148e-04,  3.8815e-03,
         4.2305e-03, -8.2874e-04,  3.9291e-03,  3.7479e-03,  3.9635e-03,
         3.8967e-03,  4.1389e-03,  3.8681e-03,  4.0054e-03,  3.9635e-03,
         4.0512e-03,  3.6774e-03,  4.0321e-03,  4.1618e-03,  3.9520e-03,
         4.1237e-03,  3.8910e-03,  4.2381e-03,  3.9825e-03,  3.9940e-03,
         3.8815e-03,  4.9591e-03,  3.8509e-03,  3.8509e-03,  3.8834e-03,
         4.3488e-03,  4.0512e-03,  4.2038e-03,  3.8509e-03,  4.0016e-03,
         4.1161e-03,  3.8776e-03,  3.9978e-03,  3.8700e-03,  4.1771e-03,
         4.0283e-03,  4.0207e-03,  2.4986e-03,  3.8071e-03,  4.0550e-03,
         4.0741e-03,  3.9902e-03,  3.8776e-03,  3.8548e-03,  3.9749e-03,
         1.4305e-03,  1.9884e-04,  2.6627e-03,  4.5891e-03,  3.8567e-03,
         3.8986e-03,  3.8528e-03,  4.3831e-03,  3.8681e-03,  4.2419e-03,
         3.9291e-03,  3.9177e-03,  3.8605e-03,  3.8681e-03,  3.8490e-03,
         3.9101e-03,  3.9864e-03,  3.8834e-03,  4.0436e-03,  3.8700e-03,
         4.4060e-03,  3.8662e-03,  3.8700e-03,  3.8509e-03,  3.9635e-03,
         3.8910e-03,  3.8643e-03,  4.1809e-03,  3.8891e-03,  3.8528e-03,
         3.6697e-03,  3.9482e-03,  3.9101e-03,  3.8662e-03, -3.2961e-05,
         3.8567e-03,  4.5090e-03,  3.8605e-03,  3.8700e-03,  3.8548e-03,
         3.9024e-03,  3.9597e-03, -1.7185e-03,  4.0779e-03,  3.8643e-03,
         4.0512e-03,  3.8586e-03,  4.1008e-03,  3.8815e-03,  3.8681e-03,
         3.8662e-03,  4.0283e-03,  3.9043e-03,  3.8929e-03,  4.3869e-03,
         3.8662e-03,  3.8528e-03,  4.2114e-03,  4.0321e-03,  3.9177e-03,
         3.8967e-03,  3.8509e-03,  4.1428e-03,  3.4676e-03,  4.2229e-03,
         3.9597e-03,  3.9215e-03,  3.8738e-03,  3.8624e-03,  4.1542e-03,
         3.9825e-03,  3.8700e-03, -7.6056e-05,  3.9043e-03,  3.9291e-03,
         3.8643e-03,  3.8528e-03,  2.5349e-03,  4.0321e-03,  3.8872e-03,
         3.5877e-03,  3.8586e-03,  3.9177e-03,  4.3564e-03,  3.9139e-03,
         3.8815e-03,  2.1820e-03,  4.0169e-03,  2.8515e-03,  3.8967e-03,
         4.2496e-03,  3.8586e-03,  3.0994e-03,  4.3106e-03,  4.4174e-03,
         4.1199e-03,  3.9597e-03,  4.1771e-03,  3.8090e-03,  4.0703e-03,
         3.4409e-03,  4.3831e-03,  3.8643e-03,  4.4632e-03,  4.2839e-03,
         3.9444e-03,  3.8490e-03,  4.1046e-03,  3.8815e-03,  4.2114e-03,
         3.8738e-03,  3.9673e-03,  4.2191e-03,  4.1199e-03,  4.1122e-03,
         3.9864e-03,  4.1084e-03,  4.0092e-03,  3.8643e-03,  3.1638e-04,
         4.0550e-03,  1.3895e-03, -1.0834e-03,  3.8776e-03,  4.3488e-03,
         3.6755e-03,  4.6463e-03,  4.1924e-03,  3.8815e-03,  4.2763e-03,
         4.0016e-03,  3.9978e-03,  1.6117e-03,  3.8509e-03,  3.9825e-03,
         3.8834e-03, -4.7994e-04,  3.9139e-03,  3.8815e-03,  3.9215e-03,
         4.0779e-03,  3.7231e-03,  4.1389e-03,  3.9825e-03,  3.9597e-03,
         4.1466e-03,  3.9673e-03,  3.8586e-03,  3.8853e-03,  3.9215e-03,
         4.0131e-03,  3.9444e-03,  3.8548e-03,  4.0512e-03,  3.8795e-03,
         3.8624e-03,  3.8795e-03,  4.1504e-03,  3.9444e-03,  3.9482e-03,
         3.8586e-03,  4.2801e-03,  4.1809e-03,  4.5547e-03, -2.2650e-04,
         3.8586e-03,  3.8586e-03,  3.9520e-03,  3.8662e-03,  3.8815e-03,
         4.0741e-03,  3.6335e-03,  3.9825e-03,  3.8910e-03,  3.9139e-03,
         3.9482e-03,  3.8586e-03,  3.8624e-03,  3.9215e-03,  1.1187e-03,
         4.0741e-03,  3.8872e-03,  4.0512e-03,  4.0436e-03,  4.2114e-03,
         4.5624e-03,  3.5439e-03,  3.8872e-03,  3.8624e-03,  4.2191e-03,
         4.2305e-03,  3.9825e-03,  3.8681e-03,  2.5864e-03,  4.3068e-03,
         4.6844e-03,  3.8605e-03,  3.8548e-03,  3.8624e-03,  3.6030e-03,
         3.9673e-03,  3.9864e-03,  4.1618e-03,  3.8643e-03, -1.9150e-03,
         4.0855e-03,  3.8700e-03,  3.8681e-03,  3.8757e-03,  3.8700e-03,
         4.2839e-03,  3.8548e-03,  3.9101e-03,  3.5210e-03,  3.8662e-03,
         3.8586e-03,  3.8490e-03,  3.8548e-03,  3.8490e-03,  3.8662e-03,
         4.9667e-03,  4.1809e-03,  4.1389e-03,  2.4471e-03,  4.4060e-03,
        -1.1854e-03,  4.2267e-03,  3.8605e-03,  3.9253e-03,  4.2343e-03,
         3.9558e-03,  3.8834e-03,  4.2038e-03,  4.0321e-03,  3.9062e-03,
         3.9635e-03,  4.6310e-03,  3.9368e-03,  4.1924e-03,  3.8548e-03,
         3.9597e-03,  3.8605e-03,  4.7150e-03,  3.8567e-03,  4.0894e-03,
         3.8567e-03,  3.8795e-03,  4.4518e-03,  3.8643e-03,  3.9139e-03,
         4.1389e-03,  3.9978e-03,  4.0512e-03,  4.2801e-03,  4.3259e-03,
        -3.5048e-04,  3.8776e-03,  4.3411e-03,  3.8548e-03,  4.1695e-03,
         3.9062e-03,  4.5433e-03,  4.1504e-03,  4.1389e-03,  3.9978e-03,
         3.9101e-03,  3.8738e-03,  4.2763e-03,  4.0817e-03,  3.8548e-03,
         4.1542e-03,  4.2763e-03,  2.4223e-03,  3.8967e-03,  4.1580e-03,
         3.8605e-03,  4.4899e-03,  4.0741e-03,  3.5667e-03,  3.8567e-03,
         4.1199e-03,  3.9749e-03,  4.2648e-03,  3.9291e-03,  4.0932e-03,
         4.4403e-03,  4.0970e-03,  3.8662e-03,  3.8986e-03,  3.8872e-03,
         4.2114e-03,  3.9787e-03,  3.8719e-03,  4.4785e-03,  3.8528e-03],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 4.0894e-03,  4.1161e-03,  2.6016e-03,  4.2534e-03, -6.4671e-05,
         3.9330e-03,  3.9406e-03,  3.9558e-03,  4.3907e-03,  4.3106e-03,
         4.1199e-03,  4.2534e-03,  4.3488e-03,  3.9482e-03,  4.0359e-03,
         4.0131e-03,  4.2305e-03,  4.1733e-03, -2.1095e-03,  3.9406e-03,
         3.9825e-03,  4.2534e-03,  4.1122e-03,  4.3945e-03,  4.2114e-03,
         4.3449e-03,  4.2267e-03,  4.0169e-03,  3.9368e-03,  3.9330e-03,
         3.9749e-03,  4.3678e-03,  2.9793e-03,  3.9635e-03,  4.1466e-03,
         4.0512e-03,  3.5820e-03,  3.9406e-03,  4.2191e-03,  4.2305e-03,
         3.9444e-03,  4.3793e-03,  3.9749e-03,  3.7804e-03,  3.9711e-03,
         4.1885e-03,  3.9597e-03,  3.9673e-03,  4.1504e-03,  4.5929e-03,
         3.9635e-03,  4.1733e-03,  4.1962e-03,  3.9062e-03,  3.9749e-03,
         3.6545e-03,  4.1237e-03,  2.7895e-04,  3.9864e-03,  3.8166e-03,
         4.1161e-03,  3.9482e-03,  3.9444e-03,  3.9406e-03,  4.0817e-03,
         4.0779e-03,  3.9635e-03,  4.6120e-03,  3.9597e-03,  3.9406e-03,
         3.9330e-03,  4.0779e-03,  3.9558e-03,  4.0245e-03,  3.8681e-03,
         4.2534e-03,  3.9864e-03, -2.4662e-03,  4.0092e-03,  4.2038e-03,
         3.9444e-03,  4.1504e-03,  4.0283e-03,  4.1008e-03,  3.9368e-03,
         4.0245e-03,  3.9635e-03,  3.9444e-03,  4.1161e-03,  4.1351e-03,
         3.9215e-03,  3.9062e-03,  4.3755e-03,  4.0512e-03,  4.0016e-03,
         4.2725e-03,  2.1496e-03,  4.1351e-03,  4.0283e-03,  3.9673e-03,
         4.2877e-03,  4.0970e-03,  4.3411e-03,  4.2839e-03,  4.0588e-03,
         3.9291e-03,  4.0741e-03,  4.3755e-03,  4.4861e-03,  4.2419e-03,
         4.4174e-03,  4.0817e-03,  2.8491e-04,  3.9749e-03,  3.9711e-03,
         3.9597e-03,  3.9330e-03,  4.0016e-03,  3.6354e-03,  4.3716e-03,
         3.9368e-03,  3.9406e-03,  3.9406e-03,  3.9558e-03,  2.0657e-03,
         4.3869e-03,  4.0283e-03,  3.9520e-03,  4.0054e-03,  4.0016e-03,
         4.0092e-03,  3.9444e-03,  5.6362e-04,  4.0131e-03,  3.6526e-03,
         3.8223e-03,  4.1122e-03,  3.9787e-03,  3.3989e-03,  4.0283e-03,
         3.7270e-03, -1.2074e-03,  4.4785e-03,  4.1275e-03,  3.8910e-03,
         4.2496e-03,  3.9406e-03,  4.1084e-03,  4.2915e-03,  4.2534e-03,
         4.2725e-03,  4.2725e-03,  4.5204e-03,  3.7289e-03,  3.9444e-03,
         4.0321e-03,  4.0627e-03,  4.0054e-03,  3.9215e-03,  3.9368e-03,
         3.9597e-03,  4.0932e-03,  4.0588e-03,  4.2725e-03,  3.9368e-03,
         4.2267e-03,  4.2610e-03,  4.0627e-03,  4.2877e-03,  4.2381e-03,
         3.9520e-03,  4.0016e-03,  3.8834e-03,  3.9749e-03,  4.0092e-03,
         4.0894e-03,  3.9978e-03,  4.1771e-03,  4.2763e-03, -5.9271e-04,
         4.1199e-03,  3.9864e-03,  3.5820e-03,  4.6005e-03,  4.2343e-03,
         4.0092e-03,  3.9482e-03,  3.9597e-03,  3.9482e-03,  4.3526e-03,
         4.3564e-03,  3.9520e-03,  3.9635e-03,  3.7498e-03,  3.9330e-03,
         1.5068e-03,  3.9215e-03,  4.1275e-03,  4.2191e-03,  4.4136e-03,
        -1.6966e-03,  4.0703e-03,  4.2267e-03,  3.9749e-03,  4.2076e-03,
         4.1504e-03,  3.9406e-03,  3.3741e-03,  4.1733e-03,  3.9406e-03,
         2.4738e-03,  3.9940e-03,  4.0054e-03,  3.9558e-03,  4.1008e-03,
         3.9482e-03,  4.0894e-03,  4.0970e-03,  3.9558e-03,  3.9635e-03,
         4.2686e-03,  4.1885e-03,  4.2572e-03,  3.9330e-03,  4.1122e-03,
         3.9749e-03,  3.9291e-03,  4.0207e-03,  4.0932e-03,  4.0894e-03,
         3.9520e-03,  4.1771e-03,  4.1008e-03,  3.9902e-03,  4.0779e-03,
         4.2686e-03,  4.0283e-03,  4.0016e-03,  3.8166e-03,  1.4381e-03,
         3.9482e-03,  4.0359e-03,  4.4212e-03,  2.7561e-03,  3.9406e-03,
         4.0359e-03,  4.1389e-03,  3.5267e-03,  3.9368e-03,  3.9406e-03,
         3.9406e-03,  3.9673e-03,  4.3907e-03,  4.0321e-03,  4.1046e-03,
         3.9635e-03,  4.0665e-03,  4.0016e-03,  4.5204e-03,  4.2610e-03,
         4.2763e-03,  3.9368e-03,  3.9043e-03,  4.2992e-03,  3.2997e-03,
         4.0550e-03,  4.1580e-03,  3.9406e-03,  3.9368e-03,  4.1962e-03,
         3.2024e-03,  4.1046e-03,  3.9482e-03,  3.7956e-03,  3.7022e-03,
         3.9368e-03,  3.9444e-03,  4.0131e-03,  3.9749e-03,  3.9711e-03,
         3.9368e-03,  3.9368e-03, -2.3746e-04,  3.9673e-03,  4.0283e-03,
         4.5853e-03,  4.0016e-03,  4.2038e-03,  3.9482e-03,  4.1351e-03,
         3.9368e-03,  3.9444e-03,  3.7766e-03,  4.2419e-03,  3.9940e-03,
         3.9406e-03,  3.9330e-03,  4.0207e-03,  7.7486e-04,  3.9635e-03,
         4.3106e-03, -7.6294e-04,  4.0092e-03,  3.8300e-03,  4.0474e-03,
         3.9787e-03,  4.2191e-03,  3.9482e-03,  4.0894e-03,  4.0436e-03,
         4.1351e-03,  3.7251e-03,  4.1122e-03,  4.2343e-03,  4.0321e-03,
         4.2076e-03,  3.9711e-03,  4.3221e-03,  4.0665e-03,  4.0741e-03,
         3.9368e-03,  4.9667e-03,  3.9330e-03,  3.9330e-03,  3.9635e-03,
         4.3907e-03,  4.1313e-03,  4.2877e-03,  3.9330e-03,  4.0779e-03,
         4.1962e-03,  3.9597e-03,  4.0779e-03,  3.9520e-03,  4.2572e-03,
         4.1084e-03,  4.1008e-03,  2.5768e-03,  3.8872e-03,  4.1351e-03,
         4.1542e-03,  4.0703e-03,  3.9597e-03,  3.9368e-03,  4.0550e-03,
         1.4486e-03,  2.4581e-04,  2.3422e-03,  4.6158e-03,  3.9368e-03,
         3.9787e-03,  3.9330e-03,  4.4594e-03,  3.9520e-03,  4.2763e-03,
         4.0092e-03,  3.9978e-03,  3.9406e-03,  3.9482e-03,  3.9291e-03,
         3.9940e-03,  4.0665e-03,  3.9444e-03,  4.1275e-03,  3.9520e-03,
         4.4708e-03,  3.9482e-03,  3.9520e-03,  3.9330e-03,  3.9902e-03,
         3.9711e-03,  3.9444e-03,  4.2610e-03,  3.9711e-03,  3.9330e-03,
         3.6888e-03,  4.0283e-03,  3.9940e-03,  3.9482e-03,  3.7134e-05,
         3.9368e-03,  4.5357e-03,  3.9444e-03,  3.9520e-03,  3.9368e-03,
         3.9825e-03,  4.0398e-03, -1.4610e-03,  4.1466e-03,  3.9444e-03,
         4.1313e-03,  3.9406e-03,  4.1809e-03,  3.9635e-03,  3.9482e-03,
         3.9482e-03,  4.1084e-03,  3.9864e-03,  3.9749e-03,  4.4289e-03,
         3.9482e-03,  3.9330e-03,  4.2915e-03,  4.1161e-03,  3.9978e-03,
         3.9787e-03,  3.9330e-03,  4.1809e-03,  3.5496e-03,  4.3030e-03,
         4.0398e-03,  4.0054e-03,  3.9558e-03,  3.9444e-03,  4.2343e-03,
         4.0627e-03,  3.9520e-03, -2.3842e-07,  3.9864e-03,  4.0131e-03,
         3.9444e-03,  3.9330e-03,  2.5997e-03,  4.1122e-03,  3.9673e-03,
         3.6640e-03,  3.9406e-03,  3.9978e-03,  4.6577e-03,  3.9940e-03,
         3.9635e-03,  2.2640e-03,  4.0970e-03,  2.9125e-03,  3.9787e-03,
         4.3297e-03,  3.9406e-03,  3.1796e-03,  4.3564e-03,  4.4899e-03,
         4.1962e-03,  4.0398e-03,  4.2572e-03,  3.8910e-03,  4.1504e-03,
         3.4485e-03,  4.4441e-03,  3.9482e-03,  4.5662e-03,  4.3068e-03,
         4.0283e-03,  3.9330e-03,  4.1695e-03,  3.9635e-03,  4.2496e-03,
         3.9558e-03,  4.0474e-03,  4.2992e-03,  4.2000e-03,  4.1924e-03,
         4.0665e-03,  4.1885e-03,  4.0894e-03,  3.9482e-03,  3.8385e-04,
         4.1389e-03,  1.4677e-03, -1.0347e-03,  3.9597e-03,  4.3907e-03,
         3.7556e-03,  4.6692e-03,  4.2763e-03,  3.9635e-03,  4.3488e-03,
         4.0817e-03,  4.0741e-03,  1.6737e-03,  3.9330e-03,  4.0627e-03,
         3.9635e-03, -4.7827e-04,  3.9940e-03,  3.9635e-03,  4.0016e-03,
         4.1580e-03,  3.8033e-03,  4.2229e-03,  4.0627e-03,  4.0398e-03,
         4.2191e-03,  4.0474e-03,  3.9406e-03,  3.9673e-03,  4.0016e-03,
         4.0932e-03,  4.0245e-03,  3.9368e-03,  4.1313e-03,  3.9635e-03,
         3.9444e-03,  3.9597e-03,  4.2305e-03,  4.0245e-03,  4.0321e-03,
         3.9406e-03,  4.3526e-03,  4.2610e-03,  4.5662e-03, -1.6737e-04,
         3.9406e-03,  3.9406e-03,  4.0321e-03,  3.9482e-03,  3.9635e-03,
         4.1542e-03,  3.7155e-03,  4.0665e-03,  3.9711e-03,  3.9978e-03,
         4.0283e-03,  3.9406e-03,  3.9444e-03,  4.0016e-03,  1.1988e-03,
         4.2000e-03,  3.9673e-03,  4.1275e-03,  4.1275e-03,  4.2915e-03,
         4.9400e-03,  3.5858e-03,  3.9711e-03,  3.9444e-03,  4.2992e-03,
         4.3106e-03,  4.0665e-03,  3.9482e-03,  2.6665e-03,  4.3869e-03,
         4.6883e-03,  3.9444e-03,  3.9368e-03,  3.9444e-03,  3.6469e-03,
         4.0474e-03,  4.0703e-03,  4.2419e-03,  3.9482e-03, -1.8806e-03,
         4.1656e-03,  3.9520e-03,  3.9482e-03,  3.9558e-03,  3.9520e-03,
         4.3602e-03,  3.9368e-03,  3.9940e-03,  3.6030e-03,  3.9482e-03,
         3.9406e-03,  3.9291e-03,  3.9368e-03,  3.9330e-03,  3.9482e-03,
         4.9858e-03,  4.2343e-03,  4.2191e-03,  2.4757e-03,  4.6539e-03,
        -1.1339e-03,  4.2953e-03,  3.9406e-03,  4.0054e-03,  4.3144e-03,
         4.0359e-03,  3.9635e-03,  4.2839e-03,  4.1046e-03,  3.9864e-03,
         4.0436e-03,  4.6501e-03,  4.0169e-03,  4.2267e-03,  3.9368e-03,
         4.0398e-03,  3.9406e-03,  4.7302e-03,  3.9368e-03,  4.1695e-03,
         3.9368e-03,  3.9635e-03,  4.5013e-03,  3.9482e-03,  3.9978e-03,
         4.2191e-03,  4.0779e-03,  4.1351e-03,  4.3564e-03,  4.9171e-03,
        -8.2493e-05,  3.9597e-03,  4.3983e-03,  3.9368e-03,  4.2496e-03,
         3.9864e-03,  4.5700e-03,  4.2305e-03,  4.2229e-03,  4.0779e-03,
         3.9902e-03,  3.9558e-03,  4.3564e-03,  4.1618e-03,  3.9368e-03,
         4.2343e-03,  4.3449e-03,  2.5043e-03,  3.9787e-03,  4.2381e-03,
         3.9406e-03,  4.5395e-03,  4.1542e-03,  3.6316e-03,  3.9368e-03,
         4.1809e-03,  4.0512e-03,  4.3335e-03,  4.0131e-03,  4.1733e-03,
         4.4594e-03,  4.1656e-03,  3.9482e-03,  3.9787e-03,  3.9673e-03,
         4.2915e-03,  4.0588e-03,  3.9558e-03,  4.5128e-03,  3.9330e-03],
       device='cuda:3', dtype=torch.float16, requires_grad=True)loss: tensor(8614.5479, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1049, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10595.7607, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0611, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10169.7256, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5830, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 00:51:01,969][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2023-09-12 00:53:07,314][train_inner][INFO] - {"epoch": 3, "update": 2.313, "loss": "5.802", "ntokens": "149211", "nsentences": "535.49", "prob_perplexity": "149.786", "code_perplexity": "146.905", "temp": "1.989", "loss_0": "5.671", "loss_1": "0.111", "loss_2": "0.02", "accuracy": "0.10159", "wps": "36246.2", "ups": "0.24", "wpb": "149211", "bsz": "535.5", "num_updates": "1200", "lr": "1.875e-05", "gnorm": "0.641", "loss_scale": "32", "train_wall": "794", "gb_free": "13", "wall": "4854"}
loss: tensor(9051.7715, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7100, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 01:06:19,024][train_inner][INFO] - {"epoch": 3, "update": 2.697, "loss": "5.571", "ntokens": "149656", "nsentences": "540.815", "prob_perplexity": "78.588", "code_perplexity": "77.512", "temp": "1.987", "loss_0": "5.426", "loss_1": "0.127", "loss_2": "0.019", "accuracy": "0.14491", "wps": "37805.8", "ups": "0.25", "wpb": "149656", "bsz": "540.8", "num_updates": "1400", "lr": "2.1875e-05", "gnorm": "0.64", "loss_scale": "32", "train_wall": "790", "gb_free": "12.7", "wall": "5646"}
loss: tensor(7767.4683, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1017, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9827.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4143, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9870.3291, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1367, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 01:11:46,868][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
loss: tensor(8329.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6279, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9714.2900, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8062, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 01:16:39,580][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 01:16:39,581][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 01:16:39,730][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2023-09-12 01:17:03,311][valid][INFO] - {"epoch": 3, "valid_loss": "5.073", "valid_ntokens": "7906.31", "valid_nsentences": "55.2525", "valid_prob_perplexity": "50.566", "valid_code_perplexity": "50.227", "valid_temp": "1.985", "valid_loss_0": "4.924", "valid_loss_1": "0.133", "valid_loss_2": "0.016", "valid_accuracy": "0.23322", "valid_wps": "33400.1", "valid_wpb": "7906.3", "valid_bsz": "55.3", "valid_num_updates": "1557", "valid_best_loss": "5.073"}
[2023-09-12 01:17:03,312][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 1557 updates
[2023-09-12 01:17:03,313][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 01:17:05,801][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 01:17:07,082][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 1557 updates, score 5.073) (writing took 3.76998378301505 seconds)
[2023-09-12 01:17:07,083][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2023-09-12 01:17:07,083][train][INFO] - {"epoch": 3, "train_loss": "5.581", "train_ntokens": "149470", "train_nsentences": "538.297", "train_prob_perplexity": "91.412", "train_code_perplexity": "90.024", "train_temp": "1.987", "train_loss_0": "5.439", "train_loss_1": "0.124", "train_loss_2": "0.019", "train_accuracy": "0.14246", "train_wps": "37134.1", "train_ups": "0.25", "train_wpb": "149470", "train_bsz": "538.3", "train_num_updates": "1557", "train_lr": "2.43281e-05", "train_gnorm": "0.651", "train_loss_scale": "32", "train_train_wall": "2058", "train_gb_free": "15.7", "train_wall": "6294"}
[2023-09-12 01:17:07,085][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 01:17:07,169][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2023-09-12 01:17:07,414][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 01:17:07,417][fairseq.trainer][INFO] - begin training epoch 4
[2023-09-12 01:17:07,417][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 01:17:22,269][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0068,  0.0069,  0.0054,  0.0071,  0.0027,  0.0067,  0.0067,  0.0067,
         0.0071,  0.0071,  0.0069,  0.0070,  0.0071,  0.0067,  0.0068,  0.0068,
         0.0070,  0.0069,  0.0001,  0.0067,  0.0067,  0.0070,  0.0069,  0.0077,
         0.0068,  0.0071,  0.0070,  0.0068,  0.0067,  0.0067,  0.0095,  0.0071,
         0.0057,  0.0067,  0.0069,  0.0068,  0.0063,  0.0067,  0.0070,  0.0070,
         0.0067,  0.0071,  0.0067,  0.0065,  0.0067,  0.0069,  0.0067,  0.0067,
         0.0069,  0.0072,  0.0067,  0.0069,  0.0069,  0.0067,  0.0067,  0.0064,
         0.0069,  0.0028,  0.0067,  0.0066,  0.0069,  0.0067,  0.0067,  0.0067,
         0.0069,  0.0068,  0.0067,  0.0072,  0.0067,  0.0067,  0.0067,  0.0068,
         0.0067,  0.0068,  0.0066,  0.0070,  0.0067, -0.0003,  0.0068,  0.0100,
         0.0067,  0.0069,  0.0070,  0.0069,  0.0072,  0.0069,  0.0067,  0.0067,
         0.0068,  0.0069,  0.0067,  0.0066,  0.0071,  0.0068,  0.0067,  0.0070,
         0.0049,  0.0069,  0.0072,  0.0067,  0.0071,  0.0069,  0.0071,  0.0070,
         0.0068,  0.0067,  0.0068,  0.0071,  0.0073,  0.0070,  0.0071,  0.0069,
         0.0029,  0.0067,  0.0067,  0.0067,  0.0067,  0.0072,  0.0064,  0.0125,
         0.0067,  0.0067,  0.0067,  0.0067,  0.0048,  0.0071,  0.0068,  0.0067,
         0.0068,  0.0068,  0.0068,  0.0067,  0.0033,  0.0067,  0.0064,  0.0065,
         0.0069,  0.0067,  0.0062,  0.0068,  0.0065,  0.0011,  0.0072,  0.0069,
         0.0066,  0.0070,  0.0067,  0.0069,  0.0070,  0.0070,  0.0070,  0.0070,
         0.0072,  0.0065,  0.0067,  0.0068,  0.0068,  0.0068,  0.0066,  0.0067,
         0.0067,  0.0068,  0.0068,  0.0070,  0.0067,  0.0070,  0.0072,  0.0068,
         0.0070,  0.0070,  0.0067,  0.0068,  0.0066,  0.0067,  0.0068,  0.0069,
         0.0068,  0.0069,  0.0072,  0.0020,  0.0069,  0.0067,  0.0063,  0.0073,
         0.0070,  0.0068,  0.0067,  0.0067,  0.0067,  0.0071,  0.0070,  0.0067,
         0.0067,  0.0065,  0.0067,  0.0041,  0.0067,  0.0069,  0.0070,  0.0071,
         0.0010,  0.0068,  0.0071,  0.0067,  0.0070,  0.0069,  0.0067,  0.0061,
         0.0069,  0.0067,  0.0103,  0.0068,  0.0068,  0.0067,  0.0069,  0.0067,
         0.0093,  0.0069,  0.0067,  0.0067,  0.0070,  0.0069,  0.0070,  0.0067,
         0.0068,  0.0067,  0.0067,  0.0068,  0.0068,  0.0068,  0.0067,  0.0069,
         0.0069,  0.0067,  0.0068,  0.0070,  0.0068,  0.0067,  0.0066,  0.0041,
         0.0067,  0.0068,  0.0071,  0.0055,  0.0067,  0.0068,  0.0069,  0.0063,
         0.0067,  0.0067,  0.0067,  0.0067,  0.0068,  0.0068,  0.0068,  0.0067,
         0.0068,  0.0068,  0.0070,  0.0108,  0.0066,  0.0067,  0.0067,  0.0077,
         0.0060,  0.0068,  0.0069,  0.0067,  0.0067,  0.0069,  0.0057,  0.0069,
         0.0067,  0.0066,  0.0063,  0.0067,  0.0067,  0.0068,  0.0067,  0.0077,
         0.0067,  0.0067,  0.0024,  0.0067,  0.0068,  0.0072,  0.0068,  0.0069,
         0.0067,  0.0069,  0.0067,  0.0067,  0.0064,  0.0070,  0.0068,  0.0067,
         0.0067,  0.0068,  0.0033,  0.0067,  0.0071,  0.0020,  0.0068,  0.0066,
         0.0068,  0.0067,  0.0070,  0.0067,  0.0069,  0.0068,  0.0069,  0.0064,
         0.0069,  0.0070,  0.0068,  0.0070,  0.0067,  0.0071,  0.0068,  0.0068,
         0.0066,  0.0083,  0.0067,  0.0067,  0.0067,  0.0070,  0.0069,  0.0078,
         0.0067,  0.0068,  0.0074,  0.0067,  0.0069,  0.0067,  0.0070,  0.0069,
         0.0069,  0.0053,  0.0066,  0.0069,  0.0069,  0.0068,  0.0067,  0.0067,
         0.0068,  0.0037,  0.0029, -0.0074,  0.0071,  0.0067,  0.0067,  0.0067,
         0.0072,  0.0067,  0.0068,  0.0068,  0.0068,  0.0067,  0.0067,  0.0067,
         0.0067,  0.0068,  0.0066,  0.0069,  0.0076,  0.0072,  0.0067,  0.0067,
         0.0067,  0.0093,  0.0067,  0.0067,  0.0070,  0.0067,  0.0068,  0.0060,
         0.0068,  0.0068,  0.0067,  0.0028,  0.0067,  0.0070,  0.0067,  0.0067,
         0.0067,  0.0067,  0.0068,  0.0044,  0.0069,  0.0067,  0.0069,  0.0067,
         0.0069,  0.0067,  0.0067,  0.0067,  0.0069,  0.0067,  0.0067,  0.0070,
         0.0067,  0.0067,  0.0070,  0.0069,  0.0068,  0.0067,  0.0067,  0.0067,
         0.0063,  0.0071,  0.0068,  0.0068,  0.0067,  0.0067,  0.0070,  0.0068,
         0.0067,  0.0027,  0.0067,  0.0068,  0.0067,  0.0067,  0.0054,  0.0069,
         0.0067,  0.0064,  0.0067,  0.0068,  0.0093,  0.0067,  0.0067,  0.0050,
         0.0069,  0.0056,  0.0067,  0.0071,  0.0067,  0.0059,  0.0073,  0.0072,
         0.0069,  0.0068,  0.0070,  0.0066,  0.0069,  0.0053,  0.0071,  0.0067,
         0.0133,  0.0070,  0.0068,  0.0067,  0.0069,  0.0067,  0.0068,  0.0067,
         0.0068,  0.0071,  0.0070,  0.0070,  0.0068,  0.0069,  0.0068,  0.0067,
         0.0031,  0.0069,  0.0042,  0.0016,  0.0067,  0.0070,  0.0065,  0.0070,
         0.0070,  0.0067,  0.0071,  0.0068,  0.0068,  0.0044,  0.0067,  0.0068,
         0.0067,  0.0007,  0.0068,  0.0067,  0.0068,  0.0069,  0.0066,  0.0085,
         0.0068,  0.0068,  0.0070,  0.0068,  0.0067,  0.0067,  0.0067,  0.0068,
         0.0068,  0.0067,  0.0099,  0.0067,  0.0067,  0.0067,  0.0070,  0.0068,
         0.0068,  0.0067,  0.0073,  0.0070,  0.0066,  0.0025,  0.0067,  0.0067,
         0.0068,  0.0067,  0.0067,  0.0069,  0.0065,  0.0068,  0.0067,  0.0068,
         0.0068,  0.0067,  0.0067,  0.0068,  0.0040,  0.0083,  0.0067,  0.0069,
         0.0069,  0.0070,  0.0089,  0.0062,  0.0067,  0.0067,  0.0071,  0.0073,
         0.0068,  0.0067,  0.0054,  0.0071,  0.0124,  0.0067,  0.0068,  0.0067,
         0.0063,  0.0068,  0.0068,  0.0070,  0.0067,  0.0007,  0.0069,  0.0067,
         0.0067,  0.0067,  0.0067,  0.0071,  0.0067,  0.0083,  0.0064,  0.0067,
         0.0067,  0.0067,  0.0067,  0.0067,  0.0067,  0.0073,  0.0069,  0.0070,
         0.0050,  0.0107,  0.0015,  0.0070,  0.0067,  0.0068,  0.0071,  0.0068,
         0.0067,  0.0070,  0.0068,  0.0067,  0.0068,  0.0075,  0.0068,  0.0114,
         0.0067,  0.0068,  0.0067,  0.0071,  0.0067,  0.0069,  0.0067,  0.0067,
         0.0071,  0.0067,  0.0068,  0.0070,  0.0068,  0.0069,  0.0071,  0.0133,
         0.0089,  0.0067,  0.0071,  0.0067,  0.0070,  0.0067,  0.0071,  0.0070,
         0.0070,  0.0068,  0.0067,  0.0067,  0.0080,  0.0070,  0.0067,  0.0076,
         0.0071,  0.0053,  0.0067,  0.0070,  0.0069,  0.0072,  0.0069,  0.0063,
         0.0067,  0.0069,  0.0068,  0.0071,  0.0068,  0.0069,  0.0068,  0.0069,
         0.0067,  0.0067,  0.0067,  0.0072,  0.0068,  0.0067,  0.0071,  0.0067],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 01:20:02,248][train_inner][INFO] - {"epoch": 4, "update": 3.084, "loss": "5.375", "ntokens": "148972", "nsentences": "535.675", "prob_perplexity": "54.799", "code_perplexity": "54.416", "temp": "1.985", "loss_0": "5.226", "loss_1": "0.132", "loss_2": "0.017", "accuracy": "0.17992", "wps": "36192.3", "ups": "0.24", "wpb": "148972", "bsz": "535.7", "num_updates": "1600", "lr": "2.5e-05", "gnorm": "0.684", "loss_scale": "16", "train_wall": "794", "gb_free": "12.7", "wall": "6469"}
loss: tensor(5818.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2488, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9845.9043, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0332, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 01:33:13,378][train_inner][INFO] - {"epoch": 4, "update": 3.468, "loss": "5.226", "ntokens": "149738", "nsentences": "540.115", "prob_perplexity": "52.55", "code_perplexity": "52.312", "temp": "1.983", "loss_0": "5.078", "loss_1": "0.132", "loss_2": "0.016", "accuracy": "0.19635", "wps": "37854.2", "ups": "0.25", "wpb": "149738", "bsz": "540.1", "num_updates": "1800", "lr": "2.8125e-05", "gnorm": "0.662", "loss_scale": "16", "train_wall": "790", "gb_free": "13", "wall": "7260"}
loss: tensor(6284.7866, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4180, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 01:39:07,625][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
loss: tensor(9326.7344, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1775, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8014.9478, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9878, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8295.4883, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0371, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 01:46:30,739][train_inner][INFO] - {"epoch": 4, "update": 3.854, "loss": "5.104", "ntokens": "149734", "nsentences": "540.825", "prob_perplexity": "52.245", "code_perplexity": "52.054", "temp": "1.981", "loss_0": "4.957", "loss_1": "0.132", "loss_2": "0.014", "accuracy": "0.21068", "wps": "37557.5", "ups": "0.25", "wpb": "149734", "bsz": "540.8", "num_updates": "2000", "lr": "3.125e-05", "gnorm": "0.649", "loss_scale": "16", "train_wall": "796", "gb_free": "12.7", "wall": "8057"}
[2023-09-12 01:51:31,406][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 01:51:31,407][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 01:51:31,594][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2023-09-12 01:51:55,452][valid][INFO] - {"epoch": 4, "valid_loss": "4.702", "valid_ntokens": "7896.52", "valid_nsentences": "55.2525", "valid_prob_perplexity": "50.737", "valid_code_perplexity": "50.522", "valid_temp": "1.979", "valid_loss_0": "4.556", "valid_loss_1": "0.133", "valid_loss_2": "0.014", "valid_accuracy": "0.27673", "valid_wps": "32919.2", "valid_wpb": "7896.5", "valid_bsz": "55.3", "valid_num_updates": "2076", "valid_best_loss": "4.702"}
[2023-09-12 01:51:55,454][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 2076 updates
[2023-09-12 01:51:55,455][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 01:51:57,937][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 01:51:59,373][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 2076 updates, score 4.702) (writing took 3.9191823210567236 seconds)
[2023-09-12 01:51:59,374][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2023-09-12 01:51:59,374][train][INFO] - {"epoch": 4, "train_loss": "5.159", "train_ntokens": "149467", "train_nsentences": "538.487", "train_prob_perplexity": "52.462", "train_code_perplexity": "52.245", "train_temp": "1.982", "train_loss_0": "5.012", "train_loss_1": "0.132", "train_loss_2": "0.015", "train_accuracy": "0.20465", "train_wps": "37075.9", "train_ups": "0.25", "train_wpb": "149467", "train_bsz": "538.5", "train_num_updates": "2076", "train_lr": "3.24375e-05", "train_gnorm": "0.672", "train_loss_scale": "16", "train_train_wall": "2060", "train_gb_free": "14", "train_wall": "8386"}
[2023-09-12 01:51:59,377][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 01:51:59,462][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2023-09-12 01:51:59,693][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 01:51:59,696][fairseq.trainer][INFO] - begin training epoch 5
[2023-09-12 01:51:59,696][fairseq_cli.train][INFO] - Start iterating over samples

loss: tensor(11438.9053, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8086, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11186.4141, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.1836, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11850.9180, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8047, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7864.0537, device='cuda:1')
loss_ent_max: tensor(-4.8906, device='cuda:1', dtype=torch.float16)
loss: tensor(12480.1475, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7500, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(13145.2988, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8320, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10448.1289, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.1289, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7594.8550, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6270, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9056.6992, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5781, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10483.9150, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1729, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6502.6206, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1055, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8682.6943, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5435, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10290.5654, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1409, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9202.7051, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2267, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6312.1895, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5571, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9479.6260, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4153, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9906.3760, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2217, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10433.4355, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6792, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8812.5322, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6885, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5183.7705, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0746, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8689.3887, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6538, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4742.1533, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6670, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9032.6367, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3621, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6553.9741, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4175, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7347.0234, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9390, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8981.1650, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4905, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7926.5186, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4426, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5657.7080, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5210, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6559.1812, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3887, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6831.3979, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5298, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8422.2998, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0244, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9378.1240, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7988, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9541.0039, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6084, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9363.6621, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6328, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8639.1221, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2133, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9232.7168, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6157, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8694.7842, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8804, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5615.5938, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8828, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8439.9023, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5654, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5907.1792, device='cuda:1')
loss_ent_max: tensor(-0.0512, device='cuda:1', dtype=torch.float16)
self.scaling_factor_for_vector: [2023-09-12 01:57:25,683][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
loss: tensor(4518.1553, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1436, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6666.9619, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2052, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8270.4512, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7925, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 02:00:12,306][train_inner][INFO] - {"epoch": 5, "update": 4.24, "loss": "5.008", "ntokens": "149084", "nsentences": "535.575", "prob_perplexity": "52.478", "code_perplexity": "52.302", "temp": "1.979", "loss_0": "4.863", "loss_1": "0.132", "loss_2": "0.013", "accuracy": "0.2253", "wps": "36292.6", "ups": "0.24", "wpb": "149084", "bsz": "535.6", "num_updates": "2200", "lr": "3.4375e-05", "gnorm": "0.73", "loss_scale": "16", "train_wall": "792", "gb_free": "12.6", "wall": "8879"}

loss: tensor(8629.5430, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6899, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10263.7412, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6924, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8533.6455, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6978, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8103.7583, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5400, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9632.7197, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4512, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8739.4131, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4590, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8048.9370, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6826, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4533.0249, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7898.7832, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7861, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7467.3701, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6924, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8428.3994, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9756, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0108,  0.0109,  0.0093,  0.0140,  0.0067,  0.0107,  0.0107,  0.0107,
         0.0111,  0.0110,  0.0109,  0.0110,  0.0111,  0.0107,  0.0108,  0.0107,
         0.0110,  0.0109,  0.0041,  0.0107,  0.0107,  0.0115,  0.0108,  0.0146,
         0.0108,  0.0111,  0.0110,  0.0107,  0.0107,  0.0107,  0.0129,  0.0111,
         0.0096,  0.0107,  0.0109,  0.0108,  0.0103,  0.0107,  0.0110,  0.0110,
         0.0107,  0.0111,  0.0107,  0.0105,  0.0107,  0.0109,  0.0107,  0.0107,
         0.0109,  0.0113,  0.0107,  0.0109,  0.0109,  0.0106,  0.0107,  0.0104,
         0.0109,  0.0068,  0.0107,  0.0105,  0.0108,  0.0107,  0.0107,  0.0107,
         0.0112,  0.0108,  0.0107,  0.0112,  0.0107,  0.0107,  0.0107,  0.0108,
         0.0107,  0.0108,  0.0106,  0.0110,  0.0107,  0.0037,  0.0107,  0.0198,
         0.0107,  0.0109,  0.0118,  0.0108,  0.0136,  0.0109,  0.0107,  0.0107,
         0.0108,  0.0109,  0.0107,  0.0106,  0.0111,  0.0108,  0.0107,  0.0110,
         0.0089,  0.0109,  0.0142,  0.0107,  0.0110,  0.0108,  0.0111,  0.0110,
         0.0108,  0.0107,  0.0108,  0.0111,  0.0112,  0.0110,  0.0110,  0.0109,
         0.0069,  0.0107,  0.0107,  0.0107,  0.0107,  0.0123,  0.0104,  0.0221,
         0.0107,  0.0107,  0.0107,  0.0107,  0.0087,  0.0111,  0.0108,  0.0107,
         0.0107,  0.0107,  0.0107,  0.0107,  0.0073,  0.0107,  0.0104,  0.0105,
         0.0108,  0.0106,  0.0101,  0.0108,  0.0105,  0.0050,  0.0112,  0.0109,
         0.0106,  0.0110,  0.0110,  0.0108,  0.0110,  0.0110,  0.0110,  0.0111,
         0.0112,  0.0115,  0.0107,  0.0108,  0.0108,  0.0107,  0.0105,  0.0107,
         0.0107,  0.0108,  0.0108,  0.0110,  0.0107,  0.0110,  0.0112,  0.0108,
         0.0110,  0.0110,  0.0107,  0.0107,  0.0106,  0.0107,  0.0107,  0.0108,
         0.0107,  0.0109,  0.0115,  0.0060,  0.0109,  0.0107,  0.0103,  0.0122,
         0.0110,  0.0108,  0.0107,  0.0107,  0.0107,  0.0111,  0.0109,  0.0107,
         0.0107,  0.0105,  0.0108,  0.0080,  0.0107,  0.0109,  0.0110,  0.0111,
         0.0050,  0.0108,  0.0111,  0.0107,  0.0109,  0.0109,  0.0107,  0.0101,
         0.0109,  0.0107,  0.0174,  0.0107,  0.0107,  0.0107,  0.0108,  0.0107,
         0.0121,  0.0108,  0.0107,  0.0107,  0.0110,  0.0109,  0.0110,  0.0107,
         0.0107,  0.0107,  0.0107,  0.0108,  0.0108,  0.0109,  0.0107,  0.0109,
         0.0108,  0.0107,  0.0108,  0.0110,  0.0108,  0.0107,  0.0105,  0.0081,
         0.0107,  0.0108,  0.0111,  0.0095,  0.0107,  0.0108,  0.0109,  0.0103,
         0.0107,  0.0107,  0.0107,  0.0107,  0.0110,  0.0108,  0.0112,  0.0107,
         0.0108,  0.0107,  0.0110,  0.0151,  0.0106,  0.0107,  0.0106,  0.0146,
         0.0100,  0.0108,  0.0109,  0.0107,  0.0107,  0.0109,  0.0096,  0.0108,
         0.0107,  0.0105,  0.0103,  0.0107,  0.0107,  0.0107,  0.0107,  0.0152,
         0.0107,  0.0107,  0.0064,  0.0107,  0.0108,  0.0112,  0.0107,  0.0109,
         0.0107,  0.0109,  0.0107,  0.0107,  0.0104,  0.0110,  0.0107,  0.0107,
         0.0107,  0.0108,  0.0073,  0.0107,  0.0111,  0.0059,  0.0107,  0.0106,
         0.0108,  0.0107,  0.0110,  0.0107,  0.0108,  0.0108,  0.0109,  0.0103,
         0.0108,  0.0109,  0.0108,  0.0109,  0.0107,  0.0111,  0.0108,  0.0108,
         0.0106,  0.0123,  0.0107,  0.0107,  0.0107,  0.0110,  0.0109,  0.0137,
         0.0107,  0.0108,  0.0120,  0.0107,  0.0109,  0.0107,  0.0110,  0.0108,
         0.0108,  0.0093,  0.0106,  0.0109,  0.0109,  0.0108,  0.0107,  0.0107,
         0.0108,  0.0077,  0.0069, -0.0135,  0.0110,  0.0107,  0.0107,  0.0107,
         0.0112,  0.0107,  0.0108,  0.0107,  0.0107,  0.0107,  0.0107,  0.0107,
         0.0107,  0.0108,  0.0106,  0.0109,  0.0157,  0.0112,  0.0107,  0.0107,
         0.0107,  0.0138,  0.0107,  0.0107,  0.0110,  0.0107,  0.0106,  0.0100,
         0.0108,  0.0107,  0.0107,  0.0067,  0.0107,  0.0110,  0.0107,  0.0107,
         0.0107,  0.0107,  0.0108,  0.0063,  0.0109,  0.0107,  0.0109,  0.0107,
         0.0109,  0.0107,  0.0107,  0.0107,  0.0109,  0.0107,  0.0107,  0.0110,
         0.0107,  0.0108,  0.0110,  0.0108,  0.0107,  0.0107,  0.0107,  0.0107,
         0.0103,  0.0110,  0.0108,  0.0107,  0.0107,  0.0107,  0.0110,  0.0108,
         0.0107,  0.0069,  0.0107,  0.0107,  0.0107,  0.0107,  0.0094,  0.0108,
         0.0108,  0.0104,  0.0107,  0.0107,  0.0125,  0.0107,  0.0107,  0.0090,
         0.0108,  0.0096,  0.0107,  0.0111,  0.0107,  0.0099,  0.0113,  0.0112,
         0.0109,  0.0108,  0.0110,  0.0106,  0.0109,  0.0093,  0.0111,  0.0107,
         0.0226,  0.0110,  0.0108,  0.0107,  0.0109,  0.0107,  0.0108,  0.0107,
         0.0108,  0.0110,  0.0109,  0.0109,  0.0108,  0.0109,  0.0108,  0.0107,
         0.0071,  0.0109,  0.0082,  0.0056,  0.0107,  0.0110,  0.0105,  0.0110,
         0.0110,  0.0107,  0.0111,  0.0108,  0.0109,  0.0084,  0.0107,  0.0108,
         0.0107,  0.0046,  0.0109,  0.0107,  0.0107,  0.0109,  0.0105,  0.0129,
         0.0108,  0.0108,  0.0109,  0.0108,  0.0107,  0.0107,  0.0091,  0.0108,
         0.0108,  0.0107,  0.0194,  0.0107,  0.0107,  0.0107,  0.0110,  0.0108,
         0.0108,  0.0107,  0.0141,  0.0110,  0.0107,  0.0065,  0.0107,  0.0107,
         0.0108,  0.0107,  0.0107,  0.0109,  0.0105,  0.0108,  0.0107,  0.0107,
         0.0108,  0.0107,  0.0107,  0.0107,  0.0079,  0.0137,  0.0107,  0.0109,
         0.0109,  0.0110,  0.0127,  0.0103,  0.0107,  0.0107,  0.0110,  0.0114,
         0.0108,  0.0107,  0.0094,  0.0111,  0.0205,  0.0107,  0.0134,  0.0107,
         0.0103,  0.0108,  0.0108,  0.0110,  0.0107,  0.0046,  0.0109,  0.0107,
         0.0107,  0.0107,  0.0107,  0.0111,  0.0107,  0.0145,  0.0104,  0.0107,
         0.0107,  0.0107,  0.0107,  0.0107,  0.0107,  0.0113,  0.0109,  0.0110,
         0.0089,  0.0033,  0.0055,  0.0110,  0.0107,  0.0107,  0.0110,  0.0108,
         0.0107,  0.0110,  0.0108,  0.0107,  0.0108,  0.0158,  0.0107,  0.0184,
         0.0107,  0.0108,  0.0107,  0.0111,  0.0107,  0.0109,  0.0107,  0.0107,
         0.0111,  0.0107,  0.0107,  0.0109,  0.0108,  0.0109,  0.0111,  0.0215,
         0.0101,  0.0107,  0.0111,  0.0107,  0.0110,  0.0107,  0.0110,  0.0110,
         0.0110,  0.0108,  0.0107,  0.0107,  0.0120,  0.0109,  0.0107,  0.0125,
         0.0110,  0.0092,  0.0107,  0.0110,  0.0109,  0.0112,  0.0109,  0.0103,
         0.0107,  0.0109,  0.0108,  0.0110,  0.0107,  0.0109,  0.0107,  0.0109,
         0.0107,  0.0107,  0.0107,  0.0114,  0.0108,  0.0107,  0.0111,  0.0107],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(9277.5244, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2159, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8316.5137, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4880, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8735.9463, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2139, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8775.9932, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7334, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7984.9473, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0205, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9147.1650, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6953, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(8564.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6387, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8984.9971, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2522, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3973.4268, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5347, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8602.4023, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7900, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 02:13:25,339][train_inner][INFO] - {"epoch": 5, "update": 4.624, "loss": "4.931", "ntokens": "149924", "nsentences": "536.875", "prob_perplexity": "53.064", "code_perplexity": "52.9", "temp": "1.977", "loss_0": "4.786", "loss_1": "0.132", "loss_2": "0.013", "accuracy": "0.23494", "wps": "37810.4", "ups": "0.25", "wpb": "149924", "bsz": "536.9", "num_updates": "2400", "lr": "3.75e-05", "gnorm": "0.693", "loss_scale": "16", "train_wall": "792", "gb_free": "13", "wall": "9672"}
[2023-09-12 02:15:01,100][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Parameter containing:
tensor([ 0.0163,  0.0163,  0.0148,  0.0231,  0.0121,  0.0161,  0.0161,  0.0162,
         0.0166,  0.0165,  0.0163,  0.0164,  0.0165,  0.0161,  0.0162,  0.0162,
         0.0164,  0.0164,  0.0096,  0.0161,  0.0162,  0.0174,  0.0163,  0.0224,
         0.0163,  0.0165,  0.0164,  0.0162,  0.0162,  0.0161,  0.0183,  0.0165,
         0.0151,  0.0162,  0.0163,  0.0163,  0.0157,  0.0161,  0.0164,  0.0164,
         0.0161,  0.0166,  0.0162,  0.0160,  0.0162,  0.0164,  0.0162,  0.0162,
         0.0163,  0.0168,  0.0162,  0.0165,  0.0164,  0.0161,  0.0162,  0.0158,
         0.0163,  0.0122,  0.0162,  0.0160,  0.0163,  0.0161,  0.0161,  0.0161,
         0.0191,  0.0163,  0.0162,  0.0167,  0.0162,  0.0161,  0.0161,  0.0163,
         0.0161,  0.0162,  0.0161,  0.0164,  0.0162,  0.0091,  0.0162,  0.0281,
         0.0161,  0.0163,  0.0178,  0.0163,  0.0229,  0.0164,  0.0162,  0.0161,
         0.0163,  0.0163,  0.0161,  0.0161,  0.0166,  0.0163,  0.0161,  0.0164,
         0.0143,  0.0163,  0.0241,  0.0162,  0.0165,  0.0163,  0.0165,  0.0164,
         0.0163,  0.0161,  0.0163,  0.0166,  0.0167,  0.0164,  0.0165,  0.0163,
         0.0123,  0.0162,  0.0162,  0.0161,  0.0161,  0.0182,  0.0158,  0.0318,
         0.0161,  0.0161,  0.0161,  0.0161,  0.0142,  0.0166,  0.0162,  0.0161,
         0.0162,  0.0162,  0.0162,  0.0161,  0.0127,  0.0162,  0.0158,  0.0159,
         0.0163,  0.0161,  0.0156,  0.0162,  0.0159,  0.0105,  0.0167,  0.0163,
         0.0161,  0.0164,  0.0177,  0.0163,  0.0165,  0.0164,  0.0165,  0.0167,
         0.0167,  0.0170,  0.0161,  0.0162,  0.0163,  0.0162,  0.0160,  0.0161,
         0.0162,  0.0163,  0.0163,  0.0165,  0.0161,  0.0164,  0.0167,  0.0163,
         0.0165,  0.0164,  0.0161,  0.0162,  0.0160,  0.0162,  0.0162,  0.0163,
         0.0162,  0.0164,  0.0171,  0.0115,  0.0163,  0.0162,  0.0158,  0.0191,
         0.0164,  0.0163,  0.0161,  0.0162,  0.0161,  0.0165,  0.0164,  0.0161,
         0.0162,  0.0159,  0.0162,  0.0135,  0.0162,  0.0163,  0.0164,  0.0166,
         0.0104,  0.0163,  0.0167,  0.0162,  0.0164,  0.0163,  0.0162,  0.0156,
         0.0164,  0.0161,  0.0228,  0.0162,  0.0162,  0.0162,  0.0163,  0.0161,
         0.0183,  0.0163,  0.0162,  0.0162,  0.0165,  0.0164,  0.0164,  0.0161,
         0.0162,  0.0162,  0.0161,  0.0162,  0.0163,  0.0173,  0.0161,  0.0164,
         0.0163,  0.0162,  0.0163,  0.0164,  0.0162,  0.0162,  0.0160,  0.0135,
         0.0162,  0.0162,  0.0167,  0.0150,  0.0161,  0.0162,  0.0164,  0.0157,
         0.0161,  0.0161,  0.0161,  0.0162,  0.0168,  0.0162,  0.0172,  0.0162,
         0.0164,  0.0162,  0.0164,  0.0192,  0.0161,  0.0161,  0.0161,  0.0226,
         0.0154,  0.0163,  0.0164,  0.0161,  0.0161,  0.0164,  0.0151,  0.0163,
         0.0161,  0.0160,  0.0158,  0.0161,  0.0161,  0.0162,  0.0162,  0.0253,
         0.0161,  0.0161,  0.0118,  0.0162,  0.0162,  0.0166,  0.0162,  0.0164,
         0.0161,  0.0164,  0.0161,  0.0161,  0.0159,  0.0164,  0.0162,  0.0163,
         0.0161,  0.0162,  0.0127,  0.0162,  0.0166,  0.0114,  0.0162,  0.0160,
         0.0162,  0.0162,  0.0164,  0.0161,  0.0163,  0.0163,  0.0163,  0.0158,
         0.0163,  0.0164,  0.0162,  0.0164,  0.0162,  0.0165,  0.0163,  0.0163,
         0.0161,  0.0178,  0.0161,  0.0161,  0.0162,  0.0164,  0.0163,  0.0219,
         0.0161,  0.0163,  0.0177,  0.0162,  0.0164,  0.0162,  0.0164,  0.0163,
         0.0163,  0.0148,  0.0161,  0.0163,  0.0163,  0.0163,  0.0162,  0.0163,
         0.0163,  0.0132,  0.0123, -0.0085,  0.0165,  0.0161,  0.0162,  0.0161,
         0.0166,  0.0161,  0.0162,  0.0162,  0.0162,  0.0161,  0.0161,  0.0161,
         0.0162,  0.0163,  0.0161,  0.0163,  0.0256,  0.0166,  0.0161,  0.0161,
         0.0161,  0.0194,  0.0162,  0.0161,  0.0165,  0.0162,  0.0161,  0.0155,
         0.0162,  0.0162,  0.0161,  0.0122,  0.0161,  0.0164,  0.0161,  0.0161,
         0.0161,  0.0162,  0.0162,  0.0116,  0.0164,  0.0161,  0.0163,  0.0161,
         0.0164,  0.0162,  0.0162,  0.0161,  0.0163,  0.0162,  0.0162,  0.0165,
         0.0161,  0.0168,  0.0165,  0.0163,  0.0162,  0.0162,  0.0161,  0.0162,
         0.0157,  0.0165,  0.0162,  0.0162,  0.0162,  0.0162,  0.0164,  0.0163,
         0.0161,  0.0132,  0.0162,  0.0162,  0.0161,  0.0161,  0.0148,  0.0163,
         0.0163,  0.0159,  0.0161,  0.0162,  0.0180,  0.0162,  0.0162,  0.0145,
         0.0163,  0.0151,  0.0162,  0.0165,  0.0161,  0.0154,  0.0167,  0.0168,
         0.0164,  0.0162,  0.0164,  0.0161,  0.0163,  0.0147,  0.0166,  0.0161,
         0.0316,  0.0165,  0.0162,  0.0167,  0.0163,  0.0162,  0.0163,  0.0161,
         0.0163,  0.0165,  0.0164,  0.0164,  0.0163,  0.0164,  0.0163,  0.0161,
         0.0126,  0.0163,  0.0137,  0.0111,  0.0162,  0.0165,  0.0159,  0.0165,
         0.0165,  0.0162,  0.0165,  0.0163,  0.0164,  0.0138,  0.0161,  0.0163,
         0.0162,  0.0101,  0.0163,  0.0162,  0.0162,  0.0164,  0.0160,  0.0184,
         0.0163,  0.0162,  0.0164,  0.0163,  0.0161,  0.0162,  0.0143,  0.0163,
         0.0162,  0.0161,  0.0308,  0.0162,  0.0161,  0.0162,  0.0164,  0.0162,
         0.0162,  0.0161,  0.0213,  0.0165,  0.0161,  0.0120,  0.0162,  0.0161,
         0.0162,  0.0161,  0.0162,  0.0164,  0.0159,  0.0163,  0.0162,  0.0162,
         0.0162,  0.0161,  0.0161,  0.0162,  0.0134,  0.0196,  0.0162,  0.0163,
         0.0163,  0.0165,  0.0182,  0.0164,  0.0162,  0.0161,  0.0165,  0.0170,
         0.0163,  0.0161,  0.0149,  0.0166,  0.0323,  0.0162,  0.0192,  0.0161,
         0.0157,  0.0163,  0.0163,  0.0164,  0.0161,  0.0101,  0.0164,  0.0162,
         0.0161,  0.0162,  0.0161,  0.0166,  0.0161,  0.0263,  0.0160,  0.0161,
         0.0161,  0.0161,  0.0161,  0.0161,  0.0161,  0.0168,  0.0164,  0.0164,
         0.0144, -0.0096,  0.0110,  0.0165,  0.0161,  0.0162,  0.0165,  0.0162,
         0.0162,  0.0165,  0.0163,  0.0162,  0.0162,  0.0186,  0.0162,  0.0258,
         0.0161,  0.0162,  0.0161,  0.0166,  0.0161,  0.0164,  0.0161,  0.0162,
         0.0166,  0.0161,  0.0162,  0.0164,  0.0163,  0.0163,  0.0165,  0.0287,
         0.0173,  0.0162,  0.0165,  0.0161,  0.0164,  0.0162,  0.0165,  0.0164,
         0.0164,  0.0163,  0.0162,  0.0161,  0.0175,  0.0164,  0.0161,  0.0215,
         0.0165,  0.0147,  0.0162,  0.0164,  0.0167,  0.0166,  0.0163,  0.0158,
         0.0176,  0.0163,  0.0162,  0.0165,  0.0162,  0.0164,  0.0162,  0.0163,
         0.0161,  0.0162,  0.0162,  0.0186,  0.0163,  0.0161,  0.0166,  0.0162],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(9177.4297, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6719, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7867.1016, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9834, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7117.6763, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9443, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5175.6934, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0959, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: 
loss: tensor(10913.6816, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3096, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10560.7930, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10525.5781, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7671, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8050.8359, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2379, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7970.8452, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6001, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10329.3906, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0245, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8849.5361, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2856, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9194.3760, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1370, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8858.8525, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0028, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9548.2549, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3704, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9714.7559, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5244, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10147.6660, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1768, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10538.4590, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6597, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9695.7129, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5337, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9614.3623, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2610, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9031.3174, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0579, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7868.7847, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0371, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9652.4463, device='cuda:2')
loss_ent_max: tensor(-0.7612, device='cuda:2', dtype=torch.float16)
loss: tensor(6389.1685, device='cuda:2')
loss_ent_max: tensor(-0.5190, device='cuda:2', dtype=torch.float16)
loss: tensor(6272.8667, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1176, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8308.8691, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5913, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7219.6372, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2803, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8589.7939, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5869, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7507.1548, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3291, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6639.4058, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1924, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0126,  0.0126,  0.0111,  0.0168,  0.0085,  0.0124,  0.0124,  0.0125,
         0.0129,  0.0128,  0.0126,  0.0128,  0.0128,  0.0125,  0.0125,  0.0125,
         0.0127,  0.0127,  0.0059,  0.0124,  0.0125,  0.0135,  0.0126,  0.0177,
         0.0126,  0.0128,  0.0127,  0.0125,  0.0125,  0.0124,  0.0146,  0.0128,
         0.0114,  0.0125,  0.0126,  0.0126,  0.0120,  0.0124,  0.0127,  0.0127,
         0.0124,  0.0129,  0.0125,  0.0123,  0.0125,  0.0127,  0.0125,  0.0125,
         0.0127,  0.0131,  0.0125,  0.0128,  0.0127,  0.0124,  0.0125,  0.0122,
         0.0126,  0.0085,  0.0125,  0.0123,  0.0126,  0.0125,  0.0125,  0.0124,
         0.0136,  0.0126,  0.0125,  0.0130,  0.0125,  0.0124,  0.0124,  0.0126,
         0.0125,  0.0125,  0.0124,  0.0128,  0.0125,  0.0054,  0.0125,  0.0226,
         0.0124,  0.0127,  0.0140,  0.0126,  0.0172,  0.0127,  0.0125,  0.0125,
         0.0126,  0.0126,  0.0124,  0.0124,  0.0129,  0.0126,  0.0124,  0.0128,
         0.0107,  0.0126,  0.0173,  0.0125,  0.0128,  0.0126,  0.0128,  0.0127,
         0.0126,  0.0124,  0.0126,  0.0129,  0.0130,  0.0127,  0.0128,  0.0126,
         0.0086,  0.0125,  0.0125,  0.0124,  0.0124,  0.0144,  0.0121,  0.0253,
         0.0124,  0.0124,  0.0124,  0.0125,  0.0105,  0.0129,  0.0125,  0.0125,
         0.0125,  0.0125,  0.0125,  0.0125,  0.0090,  0.0125,  0.0121,  0.0122,
         0.0126,  0.0124,  0.0119,  0.0125,  0.0122,  0.0068,  0.0130,  0.0126,
         0.0124,  0.0127,  0.0130,  0.0126,  0.0128,  0.0128,  0.0128,  0.0130,
         0.0130,  0.0133,  0.0124,  0.0125,  0.0126,  0.0125,  0.0123,  0.0124,
         0.0125,  0.0126,  0.0126,  0.0128,  0.0124,  0.0127,  0.0130,  0.0126,
         0.0128,  0.0127,  0.0125,  0.0125,  0.0123,  0.0125,  0.0125,  0.0126,
         0.0125,  0.0127,  0.0134,  0.0078,  0.0126,  0.0125,  0.0121,  0.0145,
         0.0127,  0.0126,  0.0125,  0.0125,  0.0125,  0.0128,  0.0127,  0.0125,
         0.0125,  0.0123,  0.0125,  0.0098,  0.0125,  0.0126,  0.0127,  0.0129,
         0.0067,  0.0126,  0.0129,  0.0125,  0.0127,  0.0126,  0.0125,  0.0119,
         0.0127,  0.0124,  0.0189,  0.0125,  0.0125,  0.0125,  0.0126,  0.0125,
         0.0139,  0.0126,  0.0125,  0.0125,  0.0128,  0.0127,  0.0128,  0.0124,
         0.0125,  0.0125,  0.0124,  0.0125,  0.0126,  0.0128,  0.0125,  0.0127,
         0.0126,  0.0125,  0.0126,  0.0127,  0.0125,  0.0125,  0.0123,  0.0098,
         0.0125,  0.0125,  0.0129,  0.0113,  0.0124,  0.0125,  0.0127,  0.0120,
         0.0124,  0.0124,  0.0124,  0.0125,  0.0130,  0.0125,  0.0131,  0.0125,
         0.0126,  0.0125,  0.0127,  0.0161,  0.0124,  0.0125,  0.0124,  0.0179,
         0.0118,  0.0126,  0.0127,  0.0124,  0.0124,  0.0127,  0.0114,  0.0126,
         0.0125,  0.0123,  0.0121,  0.0124,  0.0125,  0.0125,  0.0125,  0.0191,
         0.0125,  0.0124,  0.0081,  0.0125,  0.0125,  0.0129,  0.0125,  0.0127,
         0.0125,  0.0127,  0.0124,  0.0125,  0.0122,  0.0127,  0.0125,  0.0125,
         0.0124,  0.0125,  0.0091,  0.0125,  0.0129,  0.0077,  0.0125,  0.0123,
         0.0126,  0.0125,  0.0127,  0.0125,  0.0126,  0.0126,  0.0126,  0.0121,
         0.0126,  0.0127,  0.0125,  0.0127,  0.0125,  0.0128,  0.0126,  0.0126,
         0.0124,  0.0141,  0.0124,  0.0124,  0.0125,  0.0127,  0.0126,  0.0158,
         0.0124,  0.0126,  0.0139,  0.0125,  0.0127,  0.0125,  0.0128,  0.0126,
         0.0126,  0.0111,  0.0124,  0.0126,  0.0127,  0.0126,  0.0125,  0.0126,
         0.0126,  0.0095,  0.0086, -0.0139,  0.0128,  0.0124,  0.0125,  0.0124,
         0.0130,  0.0125,  0.0125,  0.0125,  0.0125,  0.0124,  0.0125,  0.0124,
         0.0125,  0.0126,  0.0124,  0.0126,  0.0189,  0.0129,  0.0125,  0.0125,
         0.0124,  0.0156,  0.0125,  0.0125,  0.0128,  0.0125,  0.0124,  0.0118,
         0.0125,  0.0125,  0.0125,  0.0085,  0.0124,  0.0127,  0.0124,  0.0125,
         0.0124,  0.0125,  0.0125,  0.0079,  0.0126,  0.0125,  0.0126,  0.0124,
         0.0127,  0.0125,  0.0125,  0.0125,  0.0126,  0.0125,  0.0125,  0.0128,
         0.0125,  0.0126,  0.0128,  0.0126,  0.0125,  0.0125,  0.0124,  0.0125,
         0.0121,  0.0128,  0.0125,  0.0125,  0.0125,  0.0125,  0.0127,  0.0126,
         0.0125,  0.0090,  0.0125,  0.0125,  0.0125,  0.0124,  0.0111,  0.0126,
         0.0126,  0.0122,  0.0125,  0.0125,  0.0143,  0.0125,  0.0125,  0.0108,
         0.0126,  0.0114,  0.0125,  0.0128,  0.0124,  0.0117,  0.0130,  0.0130,
         0.0127,  0.0125,  0.0128,  0.0124,  0.0127,  0.0110,  0.0129,  0.0125,
         0.0261,  0.0128,  0.0125,  0.0126,  0.0126,  0.0125,  0.0126,  0.0125,
         0.0126,  0.0128,  0.0127,  0.0127,  0.0126,  0.0127,  0.0126,  0.0125,
         0.0089,  0.0126,  0.0100,  0.0074,  0.0125,  0.0128,  0.0123,  0.0128,
         0.0128,  0.0125,  0.0128,  0.0126,  0.0127,  0.0101,  0.0124,  0.0126,
         0.0125,  0.0064,  0.0126,  0.0125,  0.0125,  0.0127,  0.0123,  0.0147,
         0.0126,  0.0125,  0.0127,  0.0126,  0.0124,  0.0125,  0.0107,  0.0126,
         0.0125,  0.0124,  0.0242,  0.0125,  0.0125,  0.0125,  0.0127,  0.0125,
         0.0125,  0.0125,  0.0164,  0.0128,  0.0124,  0.0083,  0.0125,  0.0124,
         0.0125,  0.0125,  0.0125,  0.0127,  0.0122,  0.0126,  0.0125,  0.0125,
         0.0125,  0.0124,  0.0125,  0.0125,  0.0097,  0.0158,  0.0125,  0.0126,
         0.0126,  0.0128,  0.0145,  0.0122,  0.0125,  0.0125,  0.0128,  0.0133,
         0.0126,  0.0125,  0.0112,  0.0129,  0.0243,  0.0125,  0.0152,  0.0125,
         0.0120,  0.0126,  0.0126,  0.0127,  0.0125,  0.0064,  0.0127,  0.0125,
         0.0125,  0.0125,  0.0125,  0.0129,  0.0124,  0.0182,  0.0121,  0.0125,
         0.0124,  0.0124,  0.0124,  0.0124,  0.0125,  0.0131,  0.0127,  0.0127,
         0.0107, -0.0008,  0.0073,  0.0128,  0.0124,  0.0125,  0.0128,  0.0126,
         0.0125,  0.0128,  0.0126,  0.0125,  0.0126,  0.0173,  0.0125,  0.0205,
         0.0124,  0.0125,  0.0124,  0.0129,  0.0124,  0.0127,  0.0124,  0.0125,
         0.0129,  0.0125,  0.0125,  0.0127,  0.0126,  0.0126,  0.0129,  0.0245,
         0.0115,  0.0125,  0.0128,  0.0124,  0.0128,  0.0125,  0.0128,  0.0127,
         0.0127,  0.0126,  0.0125,  0.0125,  0.0138,  0.0127,  0.0124,  0.0152,
         0.0128,  0.0110,  0.0125,  0.0127,  0.0128,  0.0129,  0.0126,  0.0121,
         0.0129,  0.0126,  0.0126,  0.0128,  0.0125,  0.0127,  0.0125,  0.0126,
         0.0125,  0.0125,  0.0125,  0.0133,  0.0126,  0.0125,  0.0129,  0.0125],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8677.0127, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9663, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6051.1230, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2991, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9449.6094, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6855, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8811.0781, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1447, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8343.0605, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8359, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7010.3940, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9351, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8626.1182, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9268, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9212.8955, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7124, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6533.1440, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8115, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 02:26:21,929][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 02:26:21,930][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 02:26:22,062][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2023-09-12 02:26:45,805][valid][INFO] - {"epoch": 5, "valid_loss": "4.583", "valid_ntokens": "7899.44", "valid_nsentences": "55.2525", "valid_prob_perplexity": "53.277", "valid_code_perplexity": "53.12", "valid_temp": "1.974", "valid_loss_0": "4.439", "valid_loss_1": "0.132", "valid_loss_2": "0.012", "valid_accuracy": "0.28642", "valid_wps": "33143.4", "valid_wpb": "7899.4", "valid_bsz": "55.3", "valid_num_updates": "2595", "valid_best_loss": "4.583"}
[2023-09-12 02:26:45,806][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 2595 updates
[2023-09-12 02:26:45,807][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 02:26:48,319][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 02:26:49,682][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 2595 updates, score 4.583) (writing took 3.876225365907885 seconds)
[2023-09-12 02:26:49,683][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2023-09-12 02:26:49,684][train][INFO] - {"epoch": 5, "train_loss": "4.921", "train_ntokens": "149449", "train_nsentences": "538.341", "train_prob_perplexity": "53.338", "train_code_perplexity": "53.176", "train_temp": "1.977", "train_loss_0": "4.777", "train_loss_1": "0.132", "train_loss_2": "0.013", "train_accuracy": "0.23565", "train_wps": "37106.4", "train_ups": "0.25", "train_wpb": "149449", "train_bsz": "538.3", "train_num_updates": "2595", "train_lr": "4.05469e-05", "train_gnorm": "0.7", "train_loss_scale": "16", "train_train_wall": "2059", "train_gb_free": "14", "train_wall": "10476"}
[2023-09-12 02:26:49,686][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 02:26:49,768][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2023-09-12 02:26:50,014][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 02:26:50,017][fairseq.trainer][INFO] - begin training epoch 6
[2023-09-12 02:26:50,017][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 02:27:09,154][train_inner][INFO] - {"epoch": 6, "update": 5.01, "loss": "4.869", "ntokens": "148817", "nsentences": "539.095", "prob_perplexity": "54.14", "code_perplexity": "53.985", "temp": "1.975", "loss_0": "4.725", "loss_1": "0.132", "loss_2": "0.012", "accuracy": "0.24108", "wps": "36128.7", "ups": "0.24", "wpb": "148817", "bsz": "539.1", "num_updates": "2600", "lr": "4.0625e-05", "gnorm": "0.707", "loss_scale": "16", "train_wall": "794", "gb_free": "12.6", "wall": "10496"}
loss: tensor(7974.7104, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1064, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9081.2676, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8008, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4562.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5122, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 02:33:56,129][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Parameter containing:
tensor([ 0.0137,  0.0137,  0.0122,  0.0189,  0.0096,  0.0135,  0.0136,  0.0136,
         0.0140,  0.0139,  0.0137,  0.0139,  0.0140,  0.0136,  0.0136,  0.0136,
         0.0138,  0.0138,  0.0070,  0.0136,  0.0136,  0.0147,  0.0137,  0.0192,
         0.0137,  0.0140,  0.0138,  0.0136,  0.0136,  0.0135,  0.0157,  0.0140,
         0.0125,  0.0136,  0.0138,  0.0137,  0.0132,  0.0136,  0.0138,  0.0138,
         0.0136,  0.0140,  0.0136,  0.0134,  0.0136,  0.0138,  0.0136,  0.0136,
         0.0138,  0.0142,  0.0136,  0.0139,  0.0138,  0.0135,  0.0136,  0.0133,
         0.0137,  0.0097,  0.0136,  0.0134,  0.0137,  0.0136,  0.0136,  0.0136,
         0.0151,  0.0137,  0.0136,  0.0141,  0.0136,  0.0136,  0.0135,  0.0137,
         0.0136,  0.0136,  0.0135,  0.0139,  0.0136,  0.0066,  0.0136,  0.0243,
         0.0136,  0.0138,  0.0151,  0.0137,  0.0188,  0.0138,  0.0136,  0.0136,
         0.0137,  0.0138,  0.0135,  0.0135,  0.0140,  0.0137,  0.0136,  0.0139,
         0.0118,  0.0138,  0.0196,  0.0136,  0.0139,  0.0137,  0.0140,  0.0138,
         0.0137,  0.0135,  0.0137,  0.0140,  0.0141,  0.0139,  0.0139,  0.0137,
         0.0097,  0.0136,  0.0136,  0.0136,  0.0135,  0.0155,  0.0132,  0.0272,
         0.0136,  0.0136,  0.0136,  0.0136,  0.0116,  0.0140,  0.0136,  0.0136,
         0.0136,  0.0136,  0.0136,  0.0136,  0.0102,  0.0136,  0.0132,  0.0133,
         0.0137,  0.0135,  0.0130,  0.0136,  0.0133,  0.0079,  0.0141,  0.0137,
         0.0135,  0.0139,  0.0145,  0.0137,  0.0139,  0.0139,  0.0139,  0.0141,
         0.0141,  0.0144,  0.0136,  0.0136,  0.0137,  0.0136,  0.0134,  0.0136,
         0.0136,  0.0137,  0.0137,  0.0139,  0.0136,  0.0138,  0.0141,  0.0137,
         0.0139,  0.0139,  0.0136,  0.0136,  0.0134,  0.0136,  0.0136,  0.0137,
         0.0136,  0.0138,  0.0145,  0.0089,  0.0137,  0.0136,  0.0132,  0.0161,
         0.0139,  0.0137,  0.0136,  0.0136,  0.0136,  0.0140,  0.0138,  0.0136,
         0.0136,  0.0134,  0.0136,  0.0109,  0.0136,  0.0137,  0.0138,  0.0140,
         0.0079,  0.0137,  0.0141,  0.0136,  0.0138,  0.0138,  0.0136,  0.0130,
         0.0138,  0.0136,  0.0190,  0.0136,  0.0136,  0.0136,  0.0137,  0.0136,
         0.0151,  0.0137,  0.0136,  0.0136,  0.0139,  0.0138,  0.0139,  0.0135,
         0.0136,  0.0136,  0.0135,  0.0136,  0.0137,  0.0140,  0.0136,  0.0138,
         0.0137,  0.0136,  0.0137,  0.0138,  0.0136,  0.0136,  0.0134,  0.0109,
         0.0136,  0.0136,  0.0141,  0.0124,  0.0136,  0.0136,  0.0138,  0.0132,
         0.0136,  0.0136,  0.0136,  0.0136,  0.0142,  0.0137,  0.0143,  0.0136,
         0.0138,  0.0136,  0.0138,  0.0170,  0.0135,  0.0136,  0.0135,  0.0198,
         0.0129,  0.0137,  0.0138,  0.0136,  0.0136,  0.0138,  0.0125,  0.0137,
         0.0136,  0.0134,  0.0132,  0.0135,  0.0136,  0.0136,  0.0136,  0.0210,
         0.0136,  0.0135,  0.0092,  0.0136,  0.0136,  0.0141,  0.0136,  0.0138,
         0.0136,  0.0138,  0.0135,  0.0136,  0.0133,  0.0139,  0.0136,  0.0136,
         0.0135,  0.0136,  0.0102,  0.0136,  0.0140,  0.0088,  0.0136,  0.0135,
         0.0137,  0.0136,  0.0138,  0.0136,  0.0137,  0.0137,  0.0137,  0.0132,
         0.0137,  0.0138,  0.0136,  0.0138,  0.0136,  0.0139,  0.0137,  0.0137,
         0.0135,  0.0152,  0.0135,  0.0135,  0.0136,  0.0138,  0.0137,  0.0177,
         0.0136,  0.0137,  0.0151,  0.0136,  0.0138,  0.0136,  0.0139,  0.0137,
         0.0137,  0.0122,  0.0135,  0.0137,  0.0138,  0.0137,  0.0136,  0.0137,
         0.0137,  0.0106,  0.0097, -0.0134,  0.0139,  0.0136,  0.0136,  0.0135,
         0.0141,  0.0136,  0.0136,  0.0136,  0.0136,  0.0136,  0.0136,  0.0135,
         0.0136,  0.0137,  0.0135,  0.0137,  0.0210,  0.0141,  0.0136,  0.0136,
         0.0135,  0.0167,  0.0136,  0.0136,  0.0139,  0.0136,  0.0135,  0.0129,
         0.0136,  0.0136,  0.0136,  0.0096,  0.0136,  0.0138,  0.0136,  0.0136,
         0.0136,  0.0136,  0.0137,  0.0091,  0.0138,  0.0136,  0.0137,  0.0136,
         0.0138,  0.0136,  0.0136,  0.0136,  0.0137,  0.0136,  0.0136,  0.0139,
         0.0136,  0.0139,  0.0139,  0.0137,  0.0136,  0.0136,  0.0135,  0.0136,
         0.0132,  0.0139,  0.0137,  0.0136,  0.0136,  0.0136,  0.0139,  0.0137,
         0.0136,  0.0104,  0.0136,  0.0136,  0.0136,  0.0135,  0.0122,  0.0137,
         0.0137,  0.0133,  0.0136,  0.0136,  0.0154,  0.0136,  0.0136,  0.0119,
         0.0137,  0.0125,  0.0136,  0.0139,  0.0136,  0.0128,  0.0142,  0.0141,
         0.0138,  0.0137,  0.0139,  0.0135,  0.0138,  0.0121,  0.0140,  0.0136,
         0.0282,  0.0139,  0.0136,  0.0138,  0.0137,  0.0136,  0.0137,  0.0136,
         0.0137,  0.0139,  0.0138,  0.0138,  0.0137,  0.0138,  0.0137,  0.0136,
         0.0100,  0.0138,  0.0111,  0.0085,  0.0136,  0.0139,  0.0134,  0.0139,
         0.0139,  0.0136,  0.0140,  0.0137,  0.0139,  0.0112,  0.0135,  0.0137,
         0.0136,  0.0075,  0.0137,  0.0136,  0.0136,  0.0138,  0.0134,  0.0158,
         0.0137,  0.0137,  0.0138,  0.0137,  0.0136,  0.0136,  0.0118,  0.0137,
         0.0136,  0.0136,  0.0257,  0.0136,  0.0136,  0.0136,  0.0138,  0.0136,
         0.0136,  0.0136,  0.0184,  0.0139,  0.0136,  0.0094,  0.0136,  0.0136,
         0.0136,  0.0136,  0.0136,  0.0138,  0.0133,  0.0137,  0.0136,  0.0136,
         0.0136,  0.0136,  0.0136,  0.0136,  0.0108,  0.0170,  0.0136,  0.0137,
         0.0137,  0.0139,  0.0156,  0.0136,  0.0136,  0.0136,  0.0139,  0.0144,
         0.0137,  0.0136,  0.0123,  0.0140,  0.0267,  0.0136,  0.0164,  0.0136,
         0.0131,  0.0137,  0.0137,  0.0139,  0.0136,  0.0075,  0.0138,  0.0136,
         0.0136,  0.0136,  0.0136,  0.0140,  0.0135,  0.0205,  0.0133,  0.0136,
         0.0136,  0.0135,  0.0135,  0.0135,  0.0136,  0.0142,  0.0138,  0.0138,
         0.0118, -0.0033,  0.0084,  0.0139,  0.0136,  0.0136,  0.0139,  0.0137,
         0.0136,  0.0139,  0.0137,  0.0136,  0.0137,  0.0176,  0.0136,  0.0220,
         0.0136,  0.0137,  0.0136,  0.0140,  0.0136,  0.0138,  0.0136,  0.0136,
         0.0140,  0.0136,  0.0136,  0.0138,  0.0137,  0.0137,  0.0140,  0.0258,
         0.0132,  0.0136,  0.0140,  0.0136,  0.0139,  0.0136,  0.0139,  0.0138,
         0.0138,  0.0137,  0.0136,  0.0136,  0.0149,  0.0138,  0.0135,  0.0173,
         0.0139,  0.0121,  0.0136,  0.0139,  0.0140,  0.0140,  0.0138,  0.0132,
         0.0150,  0.0137,  0.0137,  0.0139,  0.0136,  0.0138,  0.0136,  0.0137,
         0.0136,  0.0136,  0.0136,  0.0147,  0.0137,  0.0136,  0.0140,  0.0136],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(8001.4800, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3340, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6883.5142, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7568, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9883.4600, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1466, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8509.2324, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2126, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5938.6904, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4626, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4686.5503, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1473, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4826.5288, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7944, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7982.5967, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7930, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4950.3940, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2223, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8216.2168, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2974, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8166.1958, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(9324.2480, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3560, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 02:40:28,923][train_inner][INFO] - {"epoch": 6, "update": 5.395, "loss": "4.816", "ntokens": "149668", "nsentences": "543.165", "prob_perplexity": "55.244", "code_perplexity": "55.099", "temp": "1.973", "loss_0": "4.673", "loss_1": "0.132", "loss_2": "0.012", "accuracy": "0.24627", "wps": "37427.9", "ups": "0.25", "wpb": "149668", "bsz": "543.2", "num_updates": "2800", "lr": "4.375e-05", "gnorm": "0.657", "loss_scale": "16", "train_wall": "799", "gb_free": "12.9", "wall": "11295"}
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0239,  0.0240,  0.0224,  0.0385,  0.0197,  0.0237,  0.0237,  0.0237,
         0.0242,  0.0241,  0.0239,  0.0240,  0.0241,  0.0237,  0.0238,  0.0238,
         0.0240,  0.0240,  0.0165,  0.0237,  0.0238,  0.0253,  0.0239,  0.0306,
         0.0239,  0.0241,  0.0240,  0.0238,  0.0238,  0.0237,  0.0259,  0.0241,
         0.0227,  0.0238,  0.0239,  0.0238,  0.0233,  0.0237,  0.0240,  0.0240,
         0.0237,  0.0242,  0.0238,  0.0236,  0.0238,  0.0240,  0.0237,  0.0238,
         0.0239,  0.0244,  0.0238,  0.0248,  0.0240,  0.0237,  0.0238,  0.0234,
         0.0239,  0.0198,  0.0238,  0.0236,  0.0239,  0.0237,  0.0237,  0.0237,
         0.0311,  0.0239,  0.0238,  0.0243,  0.0237,  0.0237,  0.0237,  0.0239,
         0.0237,  0.0238,  0.0237,  0.0240,  0.0238,  0.0167,  0.0238,  0.0384,
         0.0237,  0.0239,  0.0262,  0.0239,  0.0368,  0.0240,  0.0238,  0.0237,
         0.0240,  0.0239,  0.0237,  0.0242,  0.0242,  0.0238,  0.0237,  0.0240,
         0.0219,  0.0239,  0.0347,  0.0238,  0.0241,  0.0239,  0.0241,  0.0240,
         0.0238,  0.0237,  0.0239,  0.0242,  0.0243,  0.0240,  0.0241,  0.0239,
         0.0199,  0.0238,  0.0238,  0.0237,  0.0237,  0.0260,  0.0234,  0.0462,
         0.0237,  0.0237,  0.0237,  0.0237,  0.0218,  0.0242,  0.0238,  0.0237,
         0.0238,  0.0238,  0.0238,  0.0237,  0.0203,  0.0237,  0.0234,  0.0235,
         0.0239,  0.0237,  0.0232,  0.0238,  0.0235,  0.0181,  0.0243,  0.0239,
         0.0237,  0.0240,  0.0261,  0.0239,  0.0241,  0.0240,  0.0241,  0.0243,
         0.0243,  0.0246,  0.0237,  0.0238,  0.0238,  0.0238,  0.0236,  0.0237,
         0.0238,  0.0239,  0.0238,  0.0240,  0.0237,  0.0240,  0.0243,  0.0239,
         0.0241,  0.0240,  0.0237,  0.0238,  0.0236,  0.0238,  0.0238,  0.0239,
         0.0238,  0.0240,  0.0247,  0.0190,  0.0239,  0.0238,  0.0234,  0.0282,
         0.0240,  0.0243,  0.0237,  0.0237,  0.0237,  0.0241,  0.0240,  0.0237,
         0.0238,  0.0235,  0.0238,  0.0211,  0.0238,  0.0239,  0.0240,  0.0242,
         0.0180,  0.0239,  0.0243,  0.0238,  0.0240,  0.0239,  0.0239,  0.0232,
         0.0240,  0.0237,  0.0365,  0.0238,  0.0238,  0.0237,  0.0239,  0.0237,
         0.0262,  0.0239,  0.0238,  0.0238,  0.0241,  0.0240,  0.0240,  0.0237,
         0.0238,  0.0238,  0.0237,  0.0238,  0.0239,  0.0260,  0.0237,  0.0240,
         0.0239,  0.0238,  0.0239,  0.0240,  0.0238,  0.0238,  0.0236,  0.0211,
         0.0238,  0.0238,  0.0245,  0.0225,  0.0237,  0.0238,  0.0240,  0.0233,
         0.0237,  0.0237,  0.0237,  0.0238,  0.0251,  0.0238,  0.0273,  0.0238,
         0.0241,  0.0238,  0.0240,  0.0268,  0.0237,  0.0237,  0.0237,  0.0296,
         0.0230,  0.0238,  0.0239,  0.0237,  0.0237,  0.0240,  0.0227,  0.0239,
         0.0237,  0.0236,  0.0234,  0.0237,  0.0237,  0.0238,  0.0238,  0.0425,
         0.0237,  0.0237,  0.0194,  0.0238,  0.0238,  0.0242,  0.0238,  0.0240,
         0.0237,  0.0239,  0.0237,  0.0237,  0.0235,  0.0240,  0.0238,  0.0238,
         0.0237,  0.0238,  0.0203,  0.0237,  0.0242,  0.0190,  0.0238,  0.0236,
         0.0238,  0.0238,  0.0240,  0.0237,  0.0239,  0.0239,  0.0239,  0.0234,
         0.0239,  0.0240,  0.0238,  0.0240,  0.0242,  0.0241,  0.0239,  0.0239,
         0.0237,  0.0254,  0.0237,  0.0237,  0.0238,  0.0240,  0.0239,  0.0312,
         0.0237,  0.0239,  0.0252,  0.0237,  0.0240,  0.0238,  0.0240,  0.0239,
         0.0239,  0.0224,  0.0237,  0.0239,  0.0239,  0.0239,  0.0237,  0.0239,
         0.0238,  0.0208,  0.0199,  0.0067,  0.0241,  0.0237,  0.0238,  0.0237,
         0.0242,  0.0237,  0.0238,  0.0238,  0.0238,  0.0237,  0.0237,  0.0237,
         0.0238,  0.0238,  0.0237,  0.0239,  0.0375,  0.0242,  0.0237,  0.0237,
         0.0237,  0.0270,  0.0238,  0.0237,  0.0241,  0.0238,  0.0237,  0.0231,
         0.0238,  0.0238,  0.0237,  0.0198,  0.0237,  0.0240,  0.0237,  0.0237,
         0.0237,  0.0238,  0.0238,  0.0192,  0.0245,  0.0237,  0.0239,  0.0237,
         0.0240,  0.0238,  0.0237,  0.0237,  0.0239,  0.0238,  0.0238,  0.0241,
         0.0237,  0.0249,  0.0241,  0.0239,  0.0238,  0.0238,  0.0237,  0.0238,
         0.0233,  0.0241,  0.0238,  0.0238,  0.0238,  0.0238,  0.0240,  0.0238,
         0.0237,  0.0220,  0.0238,  0.0238,  0.0237,  0.0237,  0.0224,  0.0239,
         0.0239,  0.0234,  0.0237,  0.0238,  0.0256,  0.0238,  0.0238,  0.0220,
         0.0239,  0.0226,  0.0238,  0.0241,  0.0237,  0.0230,  0.0243,  0.0251,
         0.0240,  0.0238,  0.0240,  0.0237,  0.0239,  0.0223,  0.0242,  0.0237,
         0.0441,  0.0241,  0.0238,  0.0253,  0.0239,  0.0238,  0.0239,  0.0237,
         0.0238,  0.0241,  0.0240,  0.0240,  0.0239,  0.0240,  0.0239,  0.0237,
         0.0201,  0.0239,  0.0213,  0.0186,  0.0237,  0.0241,  0.0235,  0.0240,
         0.0241,  0.0238,  0.0241,  0.0239,  0.0240,  0.0214,  0.0237,  0.0238,
         0.0238,  0.0177,  0.0239,  0.0238,  0.0238,  0.0239,  0.0236,  0.0260,
         0.0238,  0.0238,  0.0240,  0.0238,  0.0237,  0.0238,  0.0219,  0.0239,
         0.0238,  0.0237,  0.0235,  0.0238,  0.0237,  0.0238,  0.0240,  0.0238,
         0.0238,  0.0237,  0.0294,  0.0240,  0.0237,  0.0196,  0.0238,  0.0237,
         0.0238,  0.0237,  0.0238,  0.0239,  0.0235,  0.0238,  0.0238,  0.0238,
         0.0238,  0.0237,  0.0237,  0.0238,  0.0210,  0.0274,  0.0238,  0.0239,
         0.0239,  0.0241,  0.0257,  0.0242,  0.0238,  0.0237,  0.0241,  0.0246,
         0.0239,  0.0237,  0.0225,  0.0242,  0.0469,  0.0237,  0.0265,  0.0237,
         0.0233,  0.0238,  0.0239,  0.0240,  0.0237,  0.0177,  0.0240,  0.0238,
         0.0237,  0.0237,  0.0237,  0.0241,  0.0237,  0.0425,  0.0245,  0.0237,
         0.0237,  0.0237,  0.0237,  0.0237,  0.0237,  0.0244,  0.0239,  0.0240,
         0.0220, -0.0263,  0.0186,  0.0240,  0.0237,  0.0238,  0.0241,  0.0238,
         0.0238,  0.0241,  0.0239,  0.0238,  0.0238,  0.0257,  0.0238,  0.0341,
         0.0237,  0.0238,  0.0237,  0.0242,  0.0237,  0.0240,  0.0237,  0.0238,
         0.0242,  0.0237,  0.0238,  0.0240,  0.0239,  0.0239,  0.0241,  0.0346,
         0.0245,  0.0237,  0.0241,  0.0237,  0.0240,  0.0238,  0.0241,  0.0240,
         0.0240,  0.0239,  0.0238,  0.0237,  0.0251,  0.0240,  0.0237,  0.0339,
         0.0241,  0.0223,  0.0238,  0.0240,  0.0244,  0.0242,  0.0239,  0.0234,
         0.0263,  0.0239,  0.0238,  0.0241,  0.0238,  0.0240,  0.0238,  0.0239,
         0.0237,  0.0238,  0.0238,  0.0288,  0.0238,  0.0237,  0.0242,  0.0237],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7901.8447, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1902, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8734.7656, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0254, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7509.1963, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5371, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0255,  0.0257,  0.0240,  0.0421,  0.0214,  0.0254,  0.0254,  0.0254,
         0.0258,  0.0258,  0.0256,  0.0257,  0.0258,  0.0254,  0.0255,  0.0255,
         0.0257,  0.0256,  0.0182,  0.0254,  0.0254,  0.0270,  0.0256,  0.0328,
         0.0255,  0.0258,  0.0257,  0.0255,  0.0254,  0.0254,  0.0276,  0.0258,
         0.0243,  0.0254,  0.0256,  0.0255,  0.0250,  0.0254,  0.0257,  0.0257,
         0.0254,  0.0258,  0.0254,  0.0252,  0.0254,  0.0256,  0.0254,  0.0254,
         0.0256,  0.0260,  0.0254,  0.0265,  0.0256,  0.0253,  0.0254,  0.0251,
         0.0256,  0.0215,  0.0254,  0.0253,  0.0256,  0.0254,  0.0254,  0.0254,
         0.0341,  0.0255,  0.0254,  0.0259,  0.0254,  0.0254,  0.0254,  0.0255,
         0.0254,  0.0255,  0.0253,  0.0257,  0.0254,  0.0184,  0.0255,  0.0408,
         0.0254,  0.0256,  0.0283,  0.0255,  0.0403,  0.0256,  0.0254,  0.0254,
         0.0258,  0.0256,  0.0254,  0.0260,  0.0258,  0.0255,  0.0254,  0.0257,
         0.0236,  0.0256,  0.0370,  0.0254,  0.0257,  0.0255,  0.0258,  0.0257,
         0.0255,  0.0254,  0.0255,  0.0258,  0.0259,  0.0257,  0.0258,  0.0256,
         0.0216,  0.0254,  0.0254,  0.0254,  0.0254,  0.0277,  0.0251,  0.0490,
         0.0254,  0.0254,  0.0254,  0.0254,  0.0235,  0.0258,  0.0255,  0.0254,
         0.0255,  0.0255,  0.0255,  0.0254,  0.0220,  0.0254,  0.0251,  0.0252,
         0.0255,  0.0253,  0.0248,  0.0255,  0.0252,  0.0197,  0.0259,  0.0256,
         0.0253,  0.0257,  0.0279,  0.0255,  0.0257,  0.0257,  0.0257,  0.0259,
         0.0259,  0.0262,  0.0254,  0.0255,  0.0255,  0.0255,  0.0253,  0.0254,
         0.0254,  0.0255,  0.0255,  0.0257,  0.0254,  0.0257,  0.0259,  0.0255,
         0.0257,  0.0257,  0.0254,  0.0255,  0.0253,  0.0254,  0.0255,  0.0256,
         0.0254,  0.0256,  0.0264,  0.0207,  0.0256,  0.0254,  0.0250,  0.0303,
         0.0257,  0.0261,  0.0254,  0.0254,  0.0254,  0.0258,  0.0257,  0.0254,
         0.0254,  0.0252,  0.0255,  0.0228,  0.0255,  0.0256,  0.0257,  0.0258,
         0.0197,  0.0255,  0.0260,  0.0254,  0.0257,  0.0256,  0.0256,  0.0248,
         0.0256,  0.0254,  0.0384,  0.0255,  0.0255,  0.0254,  0.0255,  0.0254,
         0.0280,  0.0255,  0.0254,  0.0254,  0.0257,  0.0256,  0.0257,  0.0254,
         0.0255,  0.0254,  0.0254,  0.0255,  0.0255,  0.0277,  0.0254,  0.0257,
         0.0255,  0.0254,  0.0255,  0.0257,  0.0255,  0.0254,  0.0253,  0.0228,
         0.0254,  0.0255,  0.0262,  0.0242,  0.0254,  0.0255,  0.0257,  0.0250,
         0.0254,  0.0254,  0.0254,  0.0254,  0.0276,  0.0255,  0.0301,  0.0254,
         0.0258,  0.0254,  0.0257,  0.0285,  0.0253,  0.0254,  0.0253,  0.0313,
         0.0247,  0.0255,  0.0256,  0.0254,  0.0254,  0.0256,  0.0243,  0.0255,
         0.0254,  0.0252,  0.0250,  0.0254,  0.0254,  0.0255,  0.0255,  0.0466,
         0.0254,  0.0254,  0.0211,  0.0254,  0.0255,  0.0259,  0.0254,  0.0256,
         0.0254,  0.0256,  0.0254,  0.0254,  0.0251,  0.0257,  0.0254,  0.0255,
         0.0254,  0.0255,  0.0220,  0.0254,  0.0258,  0.0206,  0.0255,  0.0253,
         0.0255,  0.0254,  0.0257,  0.0254,  0.0256,  0.0256,  0.0256,  0.0251,
         0.0256,  0.0257,  0.0255,  0.0257,  0.0262,  0.0258,  0.0255,  0.0255,
         0.0253,  0.0270,  0.0254,  0.0254,  0.0254,  0.0257,  0.0256,  0.0333,
         0.0254,  0.0255,  0.0269,  0.0254,  0.0256,  0.0254,  0.0257,  0.0256,
         0.0255,  0.0240,  0.0253,  0.0256,  0.0256,  0.0255,  0.0254,  0.0256,
         0.0255,  0.0224,  0.0216,  0.0098,  0.0258,  0.0254,  0.0254,  0.0254,
         0.0259,  0.0254,  0.0255,  0.0255,  0.0254,  0.0254,  0.0254,  0.0254,
         0.0254,  0.0255,  0.0253,  0.0256,  0.0401,  0.0259,  0.0254,  0.0254,
         0.0254,  0.0286,  0.0254,  0.0254,  0.0258,  0.0254,  0.0253,  0.0247,
         0.0255,  0.0254,  0.0254,  0.0215,  0.0254,  0.0257,  0.0254,  0.0254,
         0.0254,  0.0254,  0.0255,  0.0209,  0.0267,  0.0254,  0.0256,  0.0254,
         0.0256,  0.0254,  0.0254,  0.0254,  0.0256,  0.0254,  0.0254,  0.0257,
         0.0254,  0.0268,  0.0257,  0.0256,  0.0254,  0.0254,  0.0254,  0.0254,
         0.0250,  0.0257,  0.0255,  0.0255,  0.0254,  0.0254,  0.0257,  0.0255,
         0.0254,  0.0237,  0.0254,  0.0255,  0.0254,  0.0254,  0.0241,  0.0256,
         0.0255,  0.0251,  0.0254,  0.0254,  0.0272,  0.0254,  0.0254,  0.0237,
         0.0255,  0.0243,  0.0254,  0.0258,  0.0254,  0.0246,  0.0260,  0.0270,
         0.0256,  0.0255,  0.0257,  0.0253,  0.0256,  0.0240,  0.0258,  0.0254,
         0.0425,  0.0257,  0.0255,  0.0271,  0.0256,  0.0254,  0.0255,  0.0254,
         0.0255,  0.0257,  0.0257,  0.0257,  0.0256,  0.0256,  0.0255,  0.0254,
         0.0218,  0.0256,  0.0229,  0.0203,  0.0254,  0.0257,  0.0252,  0.0257,
         0.0257,  0.0254,  0.0258,  0.0255,  0.0257,  0.0231,  0.0254,  0.0255,
         0.0254,  0.0193,  0.0256,  0.0254,  0.0255,  0.0256,  0.0253,  0.0276,
         0.0255,  0.0255,  0.0256,  0.0255,  0.0254,  0.0254,  0.0235,  0.0255,
         0.0255,  0.0254,  0.0233,  0.0254,  0.0254,  0.0254,  0.0257,  0.0255,
         0.0255,  0.0254,  0.0313,  0.0257,  0.0254,  0.0212,  0.0254,  0.0254,
         0.0255,  0.0254,  0.0254,  0.0256,  0.0252,  0.0255,  0.0254,  0.0254,
         0.0255,  0.0254,  0.0254,  0.0255,  0.0226,  0.0293,  0.0254,  0.0256,
         0.0256,  0.0257,  0.0274,  0.0260,  0.0254,  0.0254,  0.0257,  0.0263,
         0.0255,  0.0254,  0.0241,  0.0258,  0.0496,  0.0254,  0.0281,  0.0254,
         0.0250,  0.0255,  0.0255,  0.0257,  0.0254,  0.0193,  0.0256,  0.0254,
         0.0254,  0.0254,  0.0254,  0.0258,  0.0254,  0.0464,  0.0262,  0.0254,
         0.0254,  0.0254,  0.0254,  0.0254,  0.0254,  0.0260,  0.0256,  0.0257,
         0.0237, -0.0298,  0.0202,  0.0257,  0.0254,  0.0255,  0.0258,  0.0255,
         0.0254,  0.0257,  0.0255,  0.0254,  0.0255,  0.0274,  0.0255,  0.0363,
         0.0254,  0.0255,  0.0254,  0.0258,  0.0254,  0.0256,  0.0254,  0.0254,
         0.0258,  0.0254,  0.0254,  0.0257,  0.0255,  0.0256,  0.0258,  0.0366,
         0.0256,  0.0254,  0.0258,  0.0254,  0.0257,  0.0254,  0.0257,  0.0257,
         0.0257,  0.0255,  0.0254,  0.0254,  0.0267,  0.0257,  0.0254,  0.0367,
         0.0258,  0.0239,  0.0254,  0.0257,  0.0261,  0.0259,  0.0256,  0.0250,
         0.0282,  0.0256,  0.0255,  0.0258,  0.0255,  0.0256,  0.0255,  0.0256,
         0.0254,  0.0254,  0.0254,  0.0308,  0.0255,  0.0254,  0.0258,  0.0254],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 02:49:31,223][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Parameter containing:
tensor([ 0.0202,  0.0203,  0.0187,  0.0310,  0.0161,  0.0200,  0.0200,  0.0201,
         0.0205,  0.0204,  0.0202,  0.0204,  0.0204,  0.0201,  0.0201,  0.0201,
         0.0203,  0.0203,  0.0137,  0.0201,  0.0201,  0.0215,  0.0202,  0.0262,
         0.0202,  0.0204,  0.0203,  0.0201,  0.0201,  0.0200,  0.0222,  0.0204,
         0.0190,  0.0201,  0.0202,  0.0202,  0.0197,  0.0201,  0.0203,  0.0203,
         0.0201,  0.0205,  0.0201,  0.0199,  0.0201,  0.0203,  0.0201,  0.0201,
         0.0202,  0.0207,  0.0201,  0.0205,  0.0203,  0.0200,  0.0201,  0.0198,
         0.0202,  0.0161,  0.0201,  0.0199,  0.0202,  0.0201,  0.0201,  0.0201,
         0.0257,  0.0202,  0.0201,  0.0206,  0.0201,  0.0201,  0.0200,  0.0202,
         0.0201,  0.0201,  0.0200,  0.0204,  0.0201,  0.0130,  0.0201,  0.0332,
         0.0201,  0.0202,  0.0219,  0.0202,  0.0291,  0.0203,  0.0201,  0.0201,
         0.0202,  0.0202,  0.0200,  0.0202,  0.0205,  0.0202,  0.0201,  0.0204,
         0.0182,  0.0202,  0.0293,  0.0201,  0.0204,  0.0202,  0.0204,  0.0203,
         0.0202,  0.0200,  0.0202,  0.0205,  0.0206,  0.0203,  0.0204,  0.0202,
         0.0162,  0.0201,  0.0201,  0.0201,  0.0200,  0.0222,  0.0197,  0.0394,
         0.0200,  0.0200,  0.0201,  0.0201,  0.0181,  0.0205,  0.0201,  0.0201,
         0.0201,  0.0201,  0.0201,  0.0201,  0.0166,  0.0201,  0.0197,  0.0198,
         0.0202,  0.0200,  0.0195,  0.0201,  0.0198,  0.0144,  0.0206,  0.0202,
         0.0200,  0.0204,  0.0222,  0.0202,  0.0204,  0.0204,  0.0204,  0.0206,
         0.0206,  0.0209,  0.0201,  0.0201,  0.0202,  0.0201,  0.0199,  0.0200,
         0.0201,  0.0202,  0.0202,  0.0204,  0.0200,  0.0203,  0.0206,  0.0202,
         0.0204,  0.0203,  0.0201,  0.0201,  0.0199,  0.0201,  0.0201,  0.0202,
         0.0201,  0.0203,  0.0210,  0.0154,  0.0202,  0.0201,  0.0197,  0.0238,
         0.0203,  0.0204,  0.0201,  0.0201,  0.0201,  0.0204,  0.0203,  0.0201,
         0.0201,  0.0199,  0.0201,  0.0174,  0.0201,  0.0202,  0.0203,  0.0205,
         0.0143,  0.0202,  0.0206,  0.0201,  0.0203,  0.0202,  0.0201,  0.0195,
         0.0203,  0.0201,  0.0306,  0.0201,  0.0201,  0.0201,  0.0202,  0.0201,
         0.0224,  0.0202,  0.0201,  0.0201,  0.0204,  0.0203,  0.0204,  0.0200,
         0.0201,  0.0201,  0.0200,  0.0201,  0.0202,  0.0220,  0.0201,  0.0204,
         0.0202,  0.0201,  0.0202,  0.0203,  0.0201,  0.0201,  0.0199,  0.0174,
         0.0201,  0.0201,  0.0207,  0.0189,  0.0200,  0.0201,  0.0203,  0.0196,
         0.0200,  0.0201,  0.0201,  0.0201,  0.0210,  0.0201,  0.0223,  0.0201,
         0.0203,  0.0201,  0.0203,  0.0231,  0.0200,  0.0201,  0.0200,  0.0264,
         0.0193,  0.0202,  0.0203,  0.0200,  0.0200,  0.0203,  0.0190,  0.0202,
         0.0201,  0.0199,  0.0197,  0.0200,  0.0201,  0.0201,  0.0202,  0.0348,
         0.0201,  0.0200,  0.0157,  0.0201,  0.0201,  0.0205,  0.0201,  0.0203,
         0.0201,  0.0203,  0.0200,  0.0201,  0.0198,  0.0203,  0.0201,  0.0202,
         0.0200,  0.0201,  0.0166,  0.0201,  0.0205,  0.0153,  0.0201,  0.0199,
         0.0202,  0.0201,  0.0203,  0.0201,  0.0202,  0.0202,  0.0202,  0.0197,
         0.0202,  0.0203,  0.0201,  0.0203,  0.0202,  0.0204,  0.0202,  0.0202,
         0.0200,  0.0217,  0.0200,  0.0200,  0.0201,  0.0203,  0.0202,  0.0261,
         0.0201,  0.0202,  0.0216,  0.0201,  0.0203,  0.0201,  0.0204,  0.0202,
         0.0202,  0.0187,  0.0200,  0.0202,  0.0203,  0.0202,  0.0201,  0.0202,
         0.0202,  0.0171,  0.0162, -0.0006,  0.0204,  0.0200,  0.0201,  0.0200,
         0.0206,  0.0201,  0.0201,  0.0201,  0.0201,  0.0201,  0.0201,  0.0200,
         0.0201,  0.0202,  0.0200,  0.0202,  0.0328,  0.0205,  0.0201,  0.0201,
         0.0200,  0.0233,  0.0201,  0.0201,  0.0204,  0.0201,  0.0200,  0.0194,
         0.0201,  0.0201,  0.0201,  0.0161,  0.0200,  0.0203,  0.0201,  0.0201,
         0.0200,  0.0201,  0.0201,  0.0155,  0.0204,  0.0201,  0.0202,  0.0200,
         0.0203,  0.0201,  0.0201,  0.0201,  0.0202,  0.0201,  0.0201,  0.0204,
         0.0201,  0.0210,  0.0204,  0.0202,  0.0201,  0.0201,  0.0200,  0.0201,
         0.0197,  0.0204,  0.0201,  0.0201,  0.0201,  0.0201,  0.0203,  0.0202,
         0.0201,  0.0174,  0.0201,  0.0201,  0.0201,  0.0200,  0.0187,  0.0202,
         0.0202,  0.0198,  0.0201,  0.0201,  0.0219,  0.0201,  0.0201,  0.0184,
         0.0202,  0.0190,  0.0201,  0.0204,  0.0201,  0.0193,  0.0206,  0.0208,
         0.0203,  0.0201,  0.0204,  0.0200,  0.0202,  0.0186,  0.0205,  0.0201,
         0.0416,  0.0204,  0.0201,  0.0209,  0.0202,  0.0201,  0.0202,  0.0201,
         0.0202,  0.0204,  0.0203,  0.0203,  0.0202,  0.0203,  0.0202,  0.0201,
         0.0164,  0.0202,  0.0176,  0.0150,  0.0201,  0.0204,  0.0199,  0.0204,
         0.0204,  0.0201,  0.0204,  0.0202,  0.0204,  0.0177,  0.0200,  0.0202,
         0.0201,  0.0140,  0.0202,  0.0201,  0.0201,  0.0203,  0.0199,  0.0223,
         0.0202,  0.0201,  0.0203,  0.0202,  0.0201,  0.0201,  0.0182,  0.0202,
         0.0201,  0.0200,  0.0265,  0.0201,  0.0201,  0.0201,  0.0203,  0.0201,
         0.0201,  0.0201,  0.0254,  0.0204,  0.0201,  0.0159,  0.0201,  0.0201,
         0.0201,  0.0201,  0.0201,  0.0203,  0.0198,  0.0202,  0.0201,  0.0201,
         0.0201,  0.0200,  0.0201,  0.0201,  0.0173,  0.0233,  0.0201,  0.0202,
         0.0202,  0.0204,  0.0221,  0.0204,  0.0201,  0.0201,  0.0204,  0.0209,
         0.0202,  0.0201,  0.0188,  0.0205,  0.0396,  0.0201,  0.0230,  0.0201,
         0.0196,  0.0202,  0.0202,  0.0203,  0.0201,  0.0140,  0.0203,  0.0201,
         0.0201,  0.0201,  0.0201,  0.0205,  0.0200,  0.0340,  0.0201,  0.0201,
         0.0201,  0.0200,  0.0200,  0.0200,  0.0201,  0.0207,  0.0203,  0.0203,
         0.0183, -0.0182,  0.0149,  0.0204,  0.0201,  0.0201,  0.0204,  0.0201,
         0.0201,  0.0204,  0.0202,  0.0201,  0.0201,  0.0220,  0.0201,  0.0297,
         0.0200,  0.0201,  0.0201,  0.0205,  0.0200,  0.0203,  0.0200,  0.0201,
         0.0205,  0.0201,  0.0201,  0.0203,  0.0202,  0.0202,  0.0204,  0.0325,
         0.0223,  0.0201,  0.0204,  0.0200,  0.0204,  0.0201,  0.0204,  0.0203,
         0.0203,  0.0202,  0.0201,  0.0201,  0.0214,  0.0203,  0.0200,  0.0273,
         0.0204,  0.0186,  0.0201,  0.0203,  0.0206,  0.0205,  0.0202,  0.0197,
         0.0224,  0.0202,  0.0201,  0.0204,  0.0201,  0.0203,  0.0201,  0.0202,
         0.0201,  0.0201,  0.0201,  0.0254,  0.0202,  0.0201,  0.0205,  0.0201],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(9232.6201, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6182, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7849.5293, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4792, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8393.1787, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1182, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7358.3550, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6938, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8801.4961, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6519, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7904.2109, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5171, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: [2023-09-12 02:53:44,471][train_inner][INFO] - {"epoch": 6, "update": 5.781, "loss": "4.779", "ntokens": "149906", "nsentences": "537.14", "prob_perplexity": "56.552", "code_perplexity": "56.41", "temp": "1.971", "loss_0": "4.637", "loss_1": "0.132", "loss_2": "0.011", "accuracy": "0.24888", "wps": "37686.3", "ups": "0.25", "wpb": "149906", "bsz": "537.1", "num_updates": "3000", "lr": "4.6875e-05", "gnorm": "0.677", "loss_scale": "8", "train_wall": "794", "gb_free": "13", "wall": "12091"}
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0278,  0.0281,  0.0263,  0.0467,  0.0236,  0.0276,  0.0276,  0.0276,
         0.0280,  0.0280,  0.0278,  0.0279,  0.0280,  0.0276,  0.0277,  0.0277,
         0.0279,  0.0278,  0.0206,  0.0276,  0.0276,  0.0293,  0.0278,  0.0353,
         0.0277,  0.0280,  0.0279,  0.0277,  0.0276,  0.0276,  0.0298,  0.0280,
         0.0266,  0.0276,  0.0278,  0.0277,  0.0272,  0.0276,  0.0279,  0.0279,
         0.0276,  0.0280,  0.0276,  0.0274,  0.0276,  0.0278,  0.0276,  0.0276,
         0.0278,  0.0282,  0.0276,  0.0295,  0.0278,  0.0276,  0.0276,  0.0273,
         0.0278,  0.0237,  0.0276,  0.0275,  0.0278,  0.0276,  0.0276,  0.0276,
         0.0394,  0.0277,  0.0276,  0.0281,  0.0276,  0.0276,  0.0276,  0.0277,
         0.0276,  0.0277,  0.0275,  0.0279,  0.0276,  0.0206,  0.0277,  0.0444,
         0.0276,  0.0278,  0.0312,  0.0278,  0.0435,  0.0278,  0.0276,  0.0276,
         0.0286,  0.0278,  0.0276,  0.0285,  0.0280,  0.0277,  0.0276,  0.0279,
         0.0258,  0.0278,  0.0402,  0.0276,  0.0280,  0.0278,  0.0280,  0.0279,
         0.0277,  0.0276,  0.0277,  0.0280,  0.0282,  0.0279,  0.0280,  0.0278,
         0.0238,  0.0276,  0.0276,  0.0276,  0.0276,  0.0300,  0.0273,  0.0521,
         0.0276,  0.0276,  0.0276,  0.0276,  0.0257,  0.0280,  0.0277,  0.0276,
         0.0276,  0.0276,  0.0277,  0.0276,  0.0242,  0.0276,  0.0273,  0.0274,
         0.0278,  0.0276,  0.0271,  0.0277,  0.0274,  0.0220,  0.0281,  0.0278,
         0.0275,  0.0279,  0.0304,  0.0278,  0.0279,  0.0279,  0.0280,  0.0281,
         0.0282,  0.0284,  0.0276,  0.0277,  0.0277,  0.0276,  0.0275,  0.0276,
         0.0276,  0.0277,  0.0277,  0.0279,  0.0276,  0.0279,  0.0281,  0.0277,
         0.0279,  0.0279,  0.0276,  0.0276,  0.0275,  0.0276,  0.0277,  0.0278,
         0.0276,  0.0278,  0.0286,  0.0229,  0.0278,  0.0276,  0.0272,  0.0330,
         0.0279,  0.0288,  0.0276,  0.0276,  0.0276,  0.0280,  0.0279,  0.0276,
         0.0276,  0.0274,  0.0277,  0.0249,  0.0277,  0.0278,  0.0279,  0.0280,
         0.0219,  0.0277,  0.0282,  0.0276,  0.0279,  0.0278,  0.0279,  0.0270,
         0.0278,  0.0276,  0.0407,  0.0277,  0.0277,  0.0276,  0.0278,  0.0276,
         0.0303,  0.0277,  0.0276,  0.0276,  0.0279,  0.0278,  0.0279,  0.0276,
         0.0277,  0.0276,  0.0276,  0.0277,  0.0277,  0.0302,  0.0276,  0.0280,
         0.0278,  0.0276,  0.0277,  0.0279,  0.0277,  0.0276,  0.0275,  0.0250,
         0.0276,  0.0277,  0.0286,  0.0264,  0.0276,  0.0277,  0.0279,  0.0272,
         0.0276,  0.0276,  0.0276,  0.0276,  0.0312,  0.0277,  0.0332,  0.0276,
         0.0281,  0.0276,  0.0279,  0.0307,  0.0275,  0.0276,  0.0276,  0.0337,
         0.0269,  0.0277,  0.0278,  0.0276,  0.0276,  0.0278,  0.0266,  0.0278,
         0.0276,  0.0275,  0.0272,  0.0276,  0.0276,  0.0277,  0.0277,  0.0512,
         0.0276,  0.0276,  0.0233,  0.0276,  0.0277,  0.0281,  0.0276,  0.0278,
         0.0276,  0.0278,  0.0276,  0.0276,  0.0273,  0.0279,  0.0276,  0.0277,
         0.0276,  0.0277,  0.0242,  0.0276,  0.0280,  0.0229,  0.0277,  0.0275,
         0.0277,  0.0276,  0.0279,  0.0276,  0.0278,  0.0278,  0.0278,  0.0273,
         0.0278,  0.0279,  0.0277,  0.0279,  0.0286,  0.0280,  0.0277,  0.0277,
         0.0275,  0.0293,  0.0276,  0.0276,  0.0276,  0.0279,  0.0278,  0.0368,
         0.0276,  0.0277,  0.0291,  0.0276,  0.0278,  0.0276,  0.0279,  0.0278,
         0.0277,  0.0262,  0.0275,  0.0278,  0.0278,  0.0277,  0.0276,  0.0278,
         0.0277,  0.0246,  0.0238,  0.0140,  0.0280,  0.0276,  0.0276,  0.0276,
         0.0281,  0.0276,  0.0277,  0.0277,  0.0276,  0.0276,  0.0276,  0.0276,
         0.0276,  0.0277,  0.0275,  0.0278,  0.0448,  0.0281,  0.0276,  0.0276,
         0.0276,  0.0308,  0.0276,  0.0276,  0.0280,  0.0276,  0.0276,  0.0269,
         0.0277,  0.0276,  0.0276,  0.0237,  0.0276,  0.0279,  0.0276,  0.0276,
         0.0276,  0.0276,  0.0277,  0.0231,  0.0310,  0.0276,  0.0278,  0.0276,
         0.0278,  0.0276,  0.0276,  0.0276,  0.0278,  0.0276,  0.0276,  0.0279,
         0.0276,  0.0291,  0.0279,  0.0278,  0.0276,  0.0276,  0.0276,  0.0276,
         0.0272,  0.0280,  0.0277,  0.0276,  0.0276,  0.0276,  0.0279,  0.0277,
         0.0276,  0.0261,  0.0276,  0.0277,  0.0276,  0.0276,  0.0263,  0.0278,
         0.0277,  0.0273,  0.0276,  0.0276,  0.0294,  0.0276,  0.0276,  0.0259,
         0.0278,  0.0265,  0.0276,  0.0280,  0.0276,  0.0268,  0.0282,  0.0295,
         0.0278,  0.0277,  0.0279,  0.0275,  0.0278,  0.0262,  0.0280,  0.0276,
         0.0401,  0.0279,  0.0277,  0.0304,  0.0278,  0.0276,  0.0277,  0.0276,
         0.0277,  0.0280,  0.0278,  0.0280,  0.0282,  0.0278,  0.0277,  0.0276,
         0.0240,  0.0278,  0.0251,  0.0225,  0.0276,  0.0279,  0.0274,  0.0279,
         0.0279,  0.0276,  0.0280,  0.0277,  0.0279,  0.0253,  0.0276,  0.0277,
         0.0276,  0.0215,  0.0278,  0.0276,  0.0276,  0.0278,  0.0275,  0.0299,
         0.0277,  0.0277,  0.0278,  0.0277,  0.0276,  0.0276,  0.0257,  0.0277,
         0.0277,  0.0276,  0.0222,  0.0276,  0.0276,  0.0276,  0.0279,  0.0277,
         0.0277,  0.0276,  0.0336,  0.0279,  0.0276,  0.0234,  0.0276,  0.0276,
         0.0277,  0.0276,  0.0276,  0.0278,  0.0274,  0.0277,  0.0276,  0.0276,
         0.0277,  0.0276,  0.0276,  0.0276,  0.0249,  0.0316,  0.0276,  0.0278,
         0.0278,  0.0279,  0.0296,  0.0283,  0.0276,  0.0276,  0.0280,  0.0285,
         0.0277,  0.0276,  0.0263,  0.0280,  0.0538,  0.0276,  0.0303,  0.0276,
         0.0272,  0.0277,  0.0277,  0.0279,  0.0276,  0.0216,  0.0278,  0.0276,
         0.0276,  0.0276,  0.0276,  0.0280,  0.0276,  0.0509,  0.0298,  0.0276,
         0.0276,  0.0276,  0.0276,  0.0276,  0.0276,  0.0282,  0.0278,  0.0279,
         0.0259, -0.0339,  0.0224,  0.0279,  0.0276,  0.0277,  0.0280,  0.0277,
         0.0276,  0.0279,  0.0277,  0.0276,  0.0277,  0.0296,  0.0277,  0.0397,
         0.0276,  0.0277,  0.0276,  0.0280,  0.0276,  0.0278,  0.0276,  0.0276,
         0.0280,  0.0276,  0.0276,  0.0279,  0.0277,  0.0278,  0.0280,  0.0397,
         0.0274,  0.0276,  0.0280,  0.0276,  0.0279,  0.0276,  0.0280,  0.0279,
         0.0279,  0.0277,  0.0276,  0.0276,  0.0289,  0.0278,  0.0276,  0.0396,
         0.0280,  0.0262,  0.0276,  0.0279,  0.0286,  0.0281,  0.0278,  0.0272,
         0.0311,  0.0278,  0.0277,  0.0280,  0.0277,  0.0278,  0.0277,  0.0278,
         0.0276,  0.0276,  0.0276,  0.0327,  0.0277,  0.0276,  0.0280,  0.0276],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(9108.9824, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0420, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 03:01:09,115][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 03:01:09,116][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 03:01:09,287][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2023-09-12 03:01:32,982][valid][INFO] - {"epoch": 6, "valid_loss": "4.481", "valid_ntokens": "7922.99", "valid_nsentences": "55.2525", "valid_prob_perplexity": "56.557", "valid_code_perplexity": "56.386", "valid_temp": "1.969", "valid_loss_0": "4.339", "valid_loss_1": "0.132", "valid_loss_2": "0.011", "valid_accuracy": "0.30193", "valid_wps": "33225.8", "valid_wpb": "7923", "valid_bsz": "55.3", "valid_num_updates": "3114", "valid_best_loss": "4.481"}
[2023-09-12 03:01:32,983][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 3114 updates
[2023-09-12 03:01:32,985][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 03:01:35,462][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 03:01:36,800][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 3114 updates, score 4.481) (writing took 3.8166963619878516 seconds)
[2023-09-12 03:01:36,801][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2023-09-12 03:01:36,801][train][INFO] - {"epoch": 6, "train_loss": "4.786", "train_ntokens": "149455", "train_nsentences": "538.362", "train_prob_perplexity": "56.237", "train_code_perplexity": "56.094", "train_temp": "1.972", "train_loss_0": "4.643", "train_loss_1": "0.132", "train_loss_2": "0.011", "train_accuracy": "0.24886", "train_wps": "37164.8", "train_ups": "0.25", "train_wpb": "149455", "train_bsz": "538.4", "train_num_updates": "3114", "train_lr": "4.86563e-05", "train_gnorm": "0.676", "train_loss_scale": "8", "train_train_wall": "2056", "train_gb_free": "16", "train_wall": "12563"}
[2023-09-12 03:01:36,803][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 03:01:36,885][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2023-09-12 03:01:37,125][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 03:01:37,128][fairseq.trainer][INFO] - begin training epoch 7
[2023-09-12 03:01:37,128][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(7196.0166, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2666, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7491.2134, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1475, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8412.2119, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3845, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 03:07:12,831][train_inner][INFO] - {"epoch": 7, "update": 6.165, "loss": "4.728", "ntokens": "148717", "nsentences": "537.05", "prob_perplexity": "57.709", "code_perplexity": "57.564", "temp": "1.969", "loss_0": "4.586", "loss_1": "0.131", "loss_2": "0.011", "accuracy": "0.25609", "wps": "36794.6", "ups": "0.25", "wpb": "148716", "bsz": "537", "num_updates": "3200", "lr": "5e-05", "gnorm": "0.681", "loss_scale": "16", "train_wall": "779", "gb_free": "12.8", "wall": "12899"}
tensor(-1.3340, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7296.1572, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0574, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8384.5723, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5049, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8798.9736, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5029, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7236.4482, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4053, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5601.0786, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0820, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5439.1323, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8843, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6834.4248, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8643, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6416.4819, device='cuda:1')
loss_ent_max: tensor(-0.5200, device='cuda:1', dtype=torch.float16)
loss: tensor(8075.5381, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3347, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0309,  0.0314,  0.0293,  0.0542,  0.0267,  0.0306,  0.0306,  0.0307,
         0.0311,  0.0310,  0.0308,  0.0310,  0.0311,  0.0307,  0.0307,  0.0307,
         0.0309,  0.0309,  0.0236,  0.0306,  0.0307,  0.0323,  0.0308,  0.0387,
         0.0308,  0.0311,  0.0309,  0.0307,  0.0307,  0.0306,  0.0329,  0.0311,
         0.0296,  0.0307,  0.0309,  0.0307,  0.0302,  0.0306,  0.0309,  0.0309,
         0.0306,  0.0311,  0.0307,  0.0305,  0.0307,  0.0309,  0.0307,  0.0307,
         0.0309,  0.0313,  0.0307,  0.0334,  0.0309,  0.0306,  0.0307,  0.0303,
         0.0308,  0.0267,  0.0307,  0.0305,  0.0308,  0.0307,  0.0307,  0.0306,
         0.0454,  0.0308,  0.0307,  0.0312,  0.0307,  0.0306,  0.0306,  0.0308,
         0.0307,  0.0307,  0.0306,  0.0310,  0.0307,  0.0237,  0.0307,  0.0492,
         0.0306,  0.0309,  0.0347,  0.0308,  0.0481,  0.0309,  0.0307,  0.0307,
         0.0316,  0.0309,  0.0306,  0.0316,  0.0311,  0.0308,  0.0306,  0.0310,
         0.0289,  0.0308,  0.0436,  0.0307,  0.0310,  0.0308,  0.0310,  0.0309,
         0.0308,  0.0306,  0.0308,  0.0311,  0.0312,  0.0309,  0.0310,  0.0308,
         0.0268,  0.0307,  0.0307,  0.0306,  0.0306,  0.0331,  0.0303,  0.0555,
         0.0306,  0.0306,  0.0306,  0.0307,  0.0287,  0.0311,  0.0307,  0.0307,
         0.0307,  0.0307,  0.0307,  0.0307,  0.0273,  0.0307,  0.0303,  0.0304,
         0.0308,  0.0306,  0.0301,  0.0307,  0.0304,  0.0250,  0.0312,  0.0308,
         0.0306,  0.0309,  0.0338,  0.0308,  0.0310,  0.0310,  0.0315,  0.0312,
         0.0312,  0.0315,  0.0306,  0.0307,  0.0308,  0.0307,  0.0305,  0.0306,
         0.0307,  0.0308,  0.0308,  0.0310,  0.0306,  0.0309,  0.0312,  0.0308,
         0.0310,  0.0309,  0.0307,  0.0307,  0.0305,  0.0307,  0.0307,  0.0308,
         0.0307,  0.0309,  0.0316,  0.0260,  0.0308,  0.0307,  0.0303,  0.0363,
         0.0309,  0.0320,  0.0307,  0.0307,  0.0307,  0.0311,  0.0309,  0.0307,
         0.0307,  0.0305,  0.0307,  0.0280,  0.0308,  0.0308,  0.0309,  0.0311,
         0.0249,  0.0308,  0.0312,  0.0307,  0.0309,  0.0309,  0.0310,  0.0301,
         0.0309,  0.0306,  0.0437,  0.0307,  0.0307,  0.0307,  0.0308,  0.0307,
         0.0334,  0.0308,  0.0307,  0.0307,  0.0310,  0.0309,  0.0310,  0.0306,
         0.0307,  0.0307,  0.0306,  0.0307,  0.0308,  0.0335,  0.0307,  0.0310,
         0.0308,  0.0307,  0.0308,  0.0309,  0.0307,  0.0307,  0.0305,  0.0280,
         0.0307,  0.0307,  0.0317,  0.0295,  0.0306,  0.0307,  0.0310,  0.0302,
         0.0306,  0.0306,  0.0306,  0.0307,  0.0353,  0.0307,  0.0380,  0.0307,
         0.0312,  0.0307,  0.0309,  0.0338,  0.0306,  0.0307,  0.0306,  0.0367,
         0.0300,  0.0308,  0.0309,  0.0306,  0.0306,  0.0309,  0.0296,  0.0308,
         0.0307,  0.0305,  0.0303,  0.0306,  0.0307,  0.0307,  0.0308,  0.0581,
         0.0308,  0.0306,  0.0264,  0.0307,  0.0307,  0.0311,  0.0307,  0.0309,
         0.0307,  0.0309,  0.0306,  0.0307,  0.0304,  0.0309,  0.0307,  0.0308,
         0.0306,  0.0307,  0.0273,  0.0307,  0.0311,  0.0259,  0.0307,  0.0305,
         0.0307,  0.0307,  0.0309,  0.0307,  0.0308,  0.0309,  0.0308,  0.0303,
         0.0308,  0.0309,  0.0307,  0.0309,  0.0321,  0.0310,  0.0308,  0.0308,
         0.0306,  0.0323,  0.0306,  0.0306,  0.0307,  0.0309,  0.0308,  0.0405,
         0.0307,  0.0308,  0.0322,  0.0307,  0.0309,  0.0307,  0.0310,  0.0308,
         0.0308,  0.0293,  0.0306,  0.0308,  0.0309,  0.0308,  0.0307,  0.0308,
         0.0308,  0.0277,  0.0268,  0.0195,  0.0310,  0.0306,  0.0307,  0.0306,
         0.0312,  0.0307,  0.0307,  0.0307,  0.0307,  0.0306,  0.0307,  0.0306,
         0.0307,  0.0308,  0.0306,  0.0308,  0.0496,  0.0312,  0.0307,  0.0307,
         0.0306,  0.0339,  0.0307,  0.0307,  0.0310,  0.0307,  0.0307,  0.0300,
         0.0307,  0.0307,  0.0307,  0.0267,  0.0306,  0.0309,  0.0306,  0.0307,
         0.0306,  0.0307,  0.0307,  0.0261,  0.0352,  0.0307,  0.0309,  0.0306,
         0.0309,  0.0307,  0.0307,  0.0307,  0.0308,  0.0307,  0.0307,  0.0310,
         0.0307,  0.0321,  0.0310,  0.0308,  0.0307,  0.0307,  0.0306,  0.0307,
         0.0303,  0.0310,  0.0307,  0.0307,  0.0307,  0.0307,  0.0309,  0.0308,
         0.0307,  0.0292,  0.0307,  0.0307,  0.0307,  0.0306,  0.0293,  0.0308,
         0.0308,  0.0303,  0.0307,  0.0307,  0.0325,  0.0307,  0.0307,  0.0290,
         0.0308,  0.0296,  0.0307,  0.0310,  0.0307,  0.0299,  0.0312,  0.0327,
         0.0309,  0.0307,  0.0310,  0.0306,  0.0309,  0.0292,  0.0311,  0.0307,
         0.0378,  0.0310,  0.0307,  0.0337,  0.0308,  0.0307,  0.0308,  0.0307,
         0.0307,  0.0310,  0.0309,  0.0310,  0.0328,  0.0309,  0.0308,  0.0307,
         0.0271,  0.0308,  0.0282,  0.0256,  0.0307,  0.0310,  0.0305,  0.0310,
         0.0310,  0.0307,  0.0310,  0.0308,  0.0310,  0.0283,  0.0306,  0.0308,
         0.0307,  0.0246,  0.0308,  0.0307,  0.0307,  0.0309,  0.0305,  0.0330,
         0.0308,  0.0307,  0.0309,  0.0307,  0.0306,  0.0307,  0.0288,  0.0308,
         0.0307,  0.0307,  0.0200,  0.0307,  0.0307,  0.0307,  0.0309,  0.0307,
         0.0307,  0.0306,  0.0367,  0.0310,  0.0306,  0.0265,  0.0307,  0.0306,
         0.0307,  0.0307,  0.0307,  0.0309,  0.0304,  0.0308,  0.0307,  0.0307,
         0.0307,  0.0306,  0.0306,  0.0307,  0.0279,  0.0342,  0.0307,  0.0308,
         0.0308,  0.0310,  0.0327,  0.0316,  0.0307,  0.0306,  0.0310,  0.0315,
         0.0308,  0.0307,  0.0294,  0.0311,  0.0599,  0.0307,  0.0334,  0.0306,
         0.0302,  0.0307,  0.0308,  0.0309,  0.0307,  0.0246,  0.0309,  0.0307,
         0.0307,  0.0307,  0.0307,  0.0311,  0.0306,  0.0544,  0.0343,  0.0307,
         0.0306,  0.0306,  0.0306,  0.0306,  0.0307,  0.0313,  0.0309,  0.0309,
         0.0289, -0.0373,  0.0255,  0.0310,  0.0306,  0.0307,  0.0310,  0.0307,
         0.0307,  0.0310,  0.0308,  0.0307,  0.0307,  0.0326,  0.0307,  0.0439,
         0.0306,  0.0307,  0.0307,  0.0311,  0.0307,  0.0309,  0.0306,  0.0307,
         0.0311,  0.0307,  0.0307,  0.0309,  0.0308,  0.0308,  0.0311,  0.0424,
         0.0302,  0.0307,  0.0310,  0.0306,  0.0309,  0.0307,  0.0310,  0.0309,
         0.0309,  0.0308,  0.0307,  0.0307,  0.0320,  0.0309,  0.0306,  0.0434,
         0.0311,  0.0292,  0.0307,  0.0309,  0.0316,  0.0311,  0.0309,  0.0303,
         0.0348,  0.0308,  0.0307,  0.0310,  0.0307,  0.0309,  0.0307,  0.0308,
         0.0307,  0.0307,  0.0307,  0.0353,  0.0308,  0.0307,  0.0311,  0.0307],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7557.0210, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9727, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: loss: tensor(8054.7212, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0684, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7845.8018, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9058, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6239.3804, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2766, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7869.3687, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1289, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8169.2803, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0322, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6641.9058, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8584, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7282.9917, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9253, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 03:20:29,984][train_inner][INFO] - {"epoch": 7, "update": 6.549, "loss": "4.611", "ntokens": "149896", "nsentences": "539.67", "prob_perplexity": "58.144", "code_perplexity": "58.002", "temp": "1.967", "loss_0": "4.469", "loss_1": "0.131", "loss_2": "0.01", "accuracy": "0.27369", "wps": "37607.8", "ups": "0.25", "wpb": "149896", "bsz": "539.7", "num_updates": "3400", "lr": "5.3125e-05", "gnorm": "0.692", "loss_scale": "16", "train_wall": "796", "gb_free": "12.4", "wall": "13696"}
loss: tensor(8098.0991, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4355, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3150.7993, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0117, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7607.2036, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3728, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 03:23:51,182][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Parameter containing:
tensor([ 0.0275,  0.0278,  0.0260,  0.0462,  0.0233,  0.0273,  0.0273,  0.0273,
         0.0278,  0.0277,  0.0275,  0.0276,  0.0277,  0.0273,  0.0274,  0.0274,
         0.0276,  0.0276,  0.0203,  0.0273,  0.0274,  0.0290,  0.0275,  0.0350,
         0.0275,  0.0277,  0.0276,  0.0274,  0.0274,  0.0273,  0.0296,  0.0277,
         0.0263,  0.0274,  0.0275,  0.0274,  0.0269,  0.0273,  0.0276,  0.0276,
         0.0273,  0.0278,  0.0274,  0.0272,  0.0274,  0.0276,  0.0273,  0.0274,
         0.0275,  0.0280,  0.0273,  0.0293,  0.0276,  0.0273,  0.0274,  0.0270,
         0.0275,  0.0234,  0.0274,  0.0272,  0.0275,  0.0273,  0.0273,  0.0273,
         0.0388,  0.0275,  0.0274,  0.0279,  0.0273,  0.0273,  0.0273,  0.0275,
         0.0273,  0.0274,  0.0273,  0.0276,  0.0274,  0.0203,  0.0274,  0.0441,
         0.0273,  0.0275,  0.0309,  0.0275,  0.0433,  0.0276,  0.0274,  0.0273,
         0.0283,  0.0275,  0.0273,  0.0282,  0.0278,  0.0274,  0.0273,  0.0276,
         0.0256,  0.0275,  0.0400,  0.0274,  0.0277,  0.0275,  0.0277,  0.0276,
         0.0275,  0.0273,  0.0275,  0.0278,  0.0279,  0.0276,  0.0277,  0.0275,
         0.0235,  0.0274,  0.0274,  0.0273,  0.0273,  0.0297,  0.0270,  0.0519,
         0.0273,  0.0273,  0.0273,  0.0273,  0.0254,  0.0278,  0.0274,  0.0273,
         0.0274,  0.0274,  0.0274,  0.0273,  0.0239,  0.0273,  0.0270,  0.0271,
         0.0275,  0.0273,  0.0268,  0.0274,  0.0271,  0.0217,  0.0279,  0.0275,
         0.0273,  0.0276,  0.0301,  0.0275,  0.0277,  0.0276,  0.0278,  0.0279,
         0.0279,  0.0282,  0.0273,  0.0274,  0.0275,  0.0274,  0.0272,  0.0273,
         0.0274,  0.0275,  0.0275,  0.0276,  0.0273,  0.0276,  0.0279,  0.0275,
         0.0277,  0.0276,  0.0273,  0.0274,  0.0272,  0.0274,  0.0274,  0.0275,
         0.0274,  0.0276,  0.0283,  0.0226,  0.0275,  0.0274,  0.0270,  0.0327,
         0.0276,  0.0285,  0.0273,  0.0273,  0.0273,  0.0277,  0.0276,  0.0273,
         0.0274,  0.0271,  0.0274,  0.0247,  0.0275,  0.0275,  0.0276,  0.0278,
         0.0216,  0.0275,  0.0279,  0.0274,  0.0276,  0.0275,  0.0276,  0.0268,
         0.0276,  0.0273,  0.0404,  0.0274,  0.0274,  0.0273,  0.0275,  0.0273,
         0.0300,  0.0275,  0.0274,  0.0274,  0.0276,  0.0276,  0.0276,  0.0273,
         0.0274,  0.0274,  0.0273,  0.0274,  0.0275,  0.0299,  0.0273,  0.0277,
         0.0275,  0.0274,  0.0275,  0.0276,  0.0274,  0.0274,  0.0272,  0.0247,
         0.0274,  0.0274,  0.0283,  0.0261,  0.0273,  0.0274,  0.0276,  0.0269,
         0.0273,  0.0273,  0.0273,  0.0274,  0.0310,  0.0274,  0.0330,  0.0274,
         0.0278,  0.0274,  0.0276,  0.0304,  0.0273,  0.0273,  0.0273,  0.0334,
         0.0266,  0.0274,  0.0275,  0.0273,  0.0273,  0.0276,  0.0263,  0.0275,
         0.0273,  0.0272,  0.0270,  0.0273,  0.0273,  0.0274,  0.0275,  0.0507,
         0.0273,  0.0273,  0.0230,  0.0274,  0.0274,  0.0278,  0.0274,  0.0276,
         0.0273,  0.0275,  0.0273,  0.0273,  0.0271,  0.0276,  0.0274,  0.0275,
         0.0273,  0.0274,  0.0239,  0.0273,  0.0278,  0.0226,  0.0274,  0.0272,
         0.0274,  0.0274,  0.0276,  0.0273,  0.0275,  0.0275,  0.0275,  0.0270,
         0.0275,  0.0276,  0.0274,  0.0276,  0.0283,  0.0277,  0.0275,  0.0275,
         0.0273,  0.0290,  0.0273,  0.0273,  0.0274,  0.0276,  0.0275,  0.0365,
         0.0273,  0.0275,  0.0288,  0.0273,  0.0276,  0.0274,  0.0276,  0.0275,
         0.0275,  0.0260,  0.0273,  0.0275,  0.0275,  0.0275,  0.0273,  0.0275,
         0.0274,  0.0244,  0.0235,  0.0135,  0.0277,  0.0273,  0.0274,  0.0273,
         0.0278,  0.0273,  0.0274,  0.0274,  0.0274,  0.0273,  0.0273,  0.0273,
         0.0274,  0.0275,  0.0273,  0.0275,  0.0442,  0.0279,  0.0273,  0.0273,
         0.0273,  0.0306,  0.0274,  0.0273,  0.0277,  0.0274,  0.0273,  0.0267,
         0.0274,  0.0274,  0.0273,  0.0234,  0.0273,  0.0276,  0.0273,  0.0273,
         0.0273,  0.0274,  0.0274,  0.0228,  0.0304,  0.0273,  0.0275,  0.0273,
         0.0276,  0.0273,  0.0273,  0.0273,  0.0275,  0.0274,  0.0274,  0.0277,
         0.0273,  0.0289,  0.0277,  0.0275,  0.0274,  0.0274,  0.0273,  0.0274,
         0.0269,  0.0277,  0.0274,  0.0274,  0.0274,  0.0274,  0.0276,  0.0275,
         0.0273,  0.0258,  0.0274,  0.0274,  0.0273,  0.0273,  0.0260,  0.0275,
         0.0275,  0.0270,  0.0274,  0.0274,  0.0291,  0.0274,  0.0274,  0.0257,
         0.0275,  0.0262,  0.0274,  0.0277,  0.0273,  0.0266,  0.0279,  0.0292,
         0.0276,  0.0274,  0.0276,  0.0273,  0.0275,  0.0259,  0.0278,  0.0273,
         0.0403,  0.0277,  0.0274,  0.0301,  0.0275,  0.0274,  0.0275,  0.0273,
         0.0274,  0.0277,  0.0276,  0.0277,  0.0279,  0.0276,  0.0275,  0.0273,
         0.0237,  0.0275,  0.0249,  0.0222,  0.0273,  0.0277,  0.0271,  0.0276,
         0.0277,  0.0273,  0.0277,  0.0275,  0.0276,  0.0250,  0.0273,  0.0275,
         0.0274,  0.0213,  0.0275,  0.0273,  0.0274,  0.0275,  0.0272,  0.0296,
         0.0275,  0.0274,  0.0276,  0.0274,  0.0273,  0.0274,  0.0255,  0.0275,
         0.0274,  0.0273,  0.0223,  0.0274,  0.0273,  0.0273,  0.0276,  0.0274,
         0.0274,  0.0273,  0.0333,  0.0276,  0.0273,  0.0232,  0.0274,  0.0273,
         0.0274,  0.0273,  0.0274,  0.0275,  0.0271,  0.0275,  0.0274,  0.0274,
         0.0274,  0.0273,  0.0273,  0.0274,  0.0246,  0.0312,  0.0274,  0.0275,
         0.0275,  0.0277,  0.0293,  0.0280,  0.0274,  0.0273,  0.0277,  0.0282,
         0.0275,  0.0273,  0.0261,  0.0278,  0.0531,  0.0273,  0.0300,  0.0273,
         0.0269,  0.0274,  0.0275,  0.0276,  0.0273,  0.0213,  0.0276,  0.0274,
         0.0273,  0.0274,  0.0273,  0.0277,  0.0273,  0.0506,  0.0294,  0.0273,
         0.0273,  0.0273,  0.0273,  0.0273,  0.0273,  0.0280,  0.0275,  0.0276,
         0.0256, -0.0333,  0.0222,  0.0276,  0.0273,  0.0274,  0.0277,  0.0274,
         0.0274,  0.0277,  0.0275,  0.0274,  0.0274,  0.0293,  0.0274,  0.0392,
         0.0273,  0.0274,  0.0273,  0.0278,  0.0273,  0.0276,  0.0273,  0.0273,
         0.0278,  0.0273,  0.0274,  0.0276,  0.0275,  0.0275,  0.0277,  0.0395,
         0.0272,  0.0273,  0.0277,  0.0273,  0.0276,  0.0274,  0.0277,  0.0276,
         0.0276,  0.0275,  0.0274,  0.0273,  0.0287,  0.0276,  0.0273,  0.0394,
         0.0277,  0.0259,  0.0274,  0.0276,  0.0283,  0.0278,  0.0275,  0.0270,
         0.0307,  0.0275,  0.0274,  0.0277,  0.0274,  0.0276,  0.0274,  0.0275,
         0.0273,  0.0274,  0.0274,  0.0325,  0.0275,  0.0273,  0.0278,  0.0273],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8233.0928, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4734, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6964.8091, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5063, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4509.3022, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2764, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6303.3618, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7275, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8208.2490, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2803, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0376,  0.0375,  0.0353,  0.0695,  0.0327,  0.0366,  0.0367,  0.0367,
         0.0371,  0.0370,  0.0368,  0.0370,  0.0373,  0.0367,  0.0367,  0.0367,
         0.0370,  0.0369,  0.0298,  0.0367,  0.0367,  0.0383,  0.0368,  0.0459,
         0.0368,  0.0370,  0.0369,  0.0367,  0.0367,  0.0367,  0.0389,  0.0370,
         0.0356,  0.0367,  0.0368,  0.0367,  0.0363,  0.0367,  0.0369,  0.0369,
         0.0367,  0.0371,  0.0367,  0.0365,  0.0367,  0.0369,  0.0367,  0.0367,
         0.0369,  0.0373,  0.0367,  0.0419,  0.0369,  0.0366,  0.0367,  0.0363,
         0.0368,  0.0327,  0.0367,  0.0365,  0.0368,  0.0367,  0.0368,  0.0367,
         0.0570,  0.0368,  0.0367,  0.0372,  0.0367,  0.0367,  0.0367,  0.0368,
         0.0367,  0.0367,  0.0366,  0.0370,  0.0367,  0.0296,  0.0367,  0.0572,
         0.0367,  0.0369,  0.0417,  0.0368,  0.0574,  0.0369,  0.0367,  0.0367,
         0.0382,  0.0368,  0.0366,  0.0384,  0.0371,  0.0367,  0.0367,  0.0370,
         0.0349,  0.0368,  0.0524,  0.0367,  0.0370,  0.0368,  0.0370,  0.0369,
         0.0368,  0.0366,  0.0368,  0.0371,  0.0372,  0.0370,  0.0370,  0.0368,
         0.0328,  0.0367,  0.0367,  0.0367,  0.0366,  0.0392,  0.0363,  0.0656,
         0.0367,  0.0367,  0.0367,  0.0367,  0.0347,  0.0371,  0.0367,  0.0367,
         0.0367,  0.0367,  0.0367,  0.0367,  0.0333,  0.0367,  0.0363,  0.0365,
         0.0368,  0.0366,  0.0361,  0.0367,  0.0364,  0.0310,  0.0372,  0.0368,
         0.0366,  0.0370,  0.0400,  0.0368,  0.0370,  0.0370,  0.0395,  0.0372,
         0.0372,  0.0375,  0.0367,  0.0367,  0.0368,  0.0367,  0.0365,  0.0367,
         0.0367,  0.0368,  0.0368,  0.0370,  0.0367,  0.0369,  0.0372,  0.0368,
         0.0370,  0.0370,  0.0367,  0.0367,  0.0365,  0.0367,  0.0367,  0.0368,
         0.0367,  0.0369,  0.0377,  0.0320,  0.0368,  0.0367,  0.0363,  0.0427,
         0.0369,  0.0387,  0.0367,  0.0367,  0.0367,  0.0370,  0.0369,  0.0367,
         0.0367,  0.0364,  0.0367,  0.0340,  0.0368,  0.0368,  0.0369,  0.0371,
         0.0309,  0.0368,  0.0373,  0.0367,  0.0369,  0.0368,  0.0372,  0.0361,
         0.0369,  0.0367,  0.0488,  0.0367,  0.0367,  0.0367,  0.0368,  0.0367,
         0.0396,  0.0368,  0.0367,  0.0367,  0.0370,  0.0369,  0.0370,  0.0367,
         0.0367,  0.0367,  0.0366,  0.0367,  0.0368,  0.0399,  0.0367,  0.0370,
         0.0368,  0.0367,  0.0368,  0.0369,  0.0367,  0.0367,  0.0365,  0.0340,
         0.0367,  0.0368,  0.0378,  0.0355,  0.0367,  0.0367,  0.0370,  0.0363,
         0.0367,  0.0367,  0.0367,  0.0367,  0.0479,  0.0367,  0.0477,  0.0367,
         0.0378,  0.0367,  0.0369,  0.0398,  0.0366,  0.0367,  0.0366,  0.0427,
         0.0359,  0.0367,  0.0369,  0.0367,  0.0367,  0.0369,  0.0356,  0.0368,
         0.0367,  0.0365,  0.0363,  0.0367,  0.0367,  0.0367,  0.0368,  0.0730,
         0.0376,  0.0367,  0.0324,  0.0367,  0.0367,  0.0371,  0.0367,  0.0369,
         0.0367,  0.0369,  0.0367,  0.0367,  0.0364,  0.0370,  0.0367,  0.0368,
         0.0367,  0.0367,  0.0333,  0.0367,  0.0370,  0.0319,  0.0367,  0.0365,
         0.0367,  0.0367,  0.0369,  0.0367,  0.0368,  0.0369,  0.0368,  0.0363,
         0.0368,  0.0370,  0.0367,  0.0369,  0.0399,  0.0370,  0.0368,  0.0368,
         0.0366,  0.0383,  0.0366,  0.0366,  0.0367,  0.0369,  0.0368,  0.0475,
         0.0367,  0.0368,  0.0381,  0.0367,  0.0369,  0.0367,  0.0370,  0.0368,
         0.0368,  0.0353,  0.0366,  0.0368,  0.0369,  0.0368,  0.0367,  0.0368,
         0.0367,  0.0337,  0.0328,  0.0315,  0.0370,  0.0367,  0.0367,  0.0367,
         0.0371,  0.0367,  0.0367,  0.0367,  0.0367,  0.0367,  0.0367,  0.0366,
         0.0367,  0.0368,  0.0366,  0.0368,  0.0583,  0.0374,  0.0367,  0.0367,
         0.0366,  0.0399,  0.0367,  0.0367,  0.0370,  0.0367,  0.0379,  0.0360,
         0.0367,  0.0367,  0.0367,  0.0327,  0.0367,  0.0369,  0.0367,  0.0367,
         0.0367,  0.0367,  0.0367,  0.0321,  0.0480,  0.0367,  0.0369,  0.0367,
         0.0369,  0.0367,  0.0367,  0.0367,  0.0368,  0.0367,  0.0367,  0.0370,
         0.0367,  0.0367,  0.0370,  0.0368,  0.0367,  0.0367,  0.0366,  0.0367,
         0.0363,  0.0370,  0.0367,  0.0367,  0.0367,  0.0367,  0.0369,  0.0368,
         0.0367,  0.0359,  0.0367,  0.0367,  0.0367,  0.0367,  0.0353,  0.0368,
         0.0368,  0.0363,  0.0370,  0.0367,  0.0385,  0.0367,  0.0367,  0.0350,
         0.0368,  0.0356,  0.0367,  0.0370,  0.0367,  0.0359,  0.0372,  0.0392,
         0.0369,  0.0367,  0.0370,  0.0366,  0.0369,  0.0352,  0.0371,  0.0367,
         0.0274,  0.0370,  0.0367,  0.0395,  0.0368,  0.0367,  0.0368,  0.0367,
         0.0367,  0.0370,  0.0369,  0.0372,  0.0433,  0.0369,  0.0368,  0.0367,
         0.0331,  0.0368,  0.0342,  0.0316,  0.0367,  0.0370,  0.0365,  0.0370,
         0.0370,  0.0367,  0.0370,  0.0368,  0.0370,  0.0343,  0.0366,  0.0368,
         0.0367,  0.0306,  0.0368,  0.0367,  0.0367,  0.0369,  0.0365,  0.0389,
         0.0368,  0.0367,  0.0369,  0.0367,  0.0367,  0.0367,  0.0348,  0.0368,
         0.0367,  0.0368,  0.0190,  0.0367,  0.0367,  0.0367,  0.0369,  0.0367,
         0.0367,  0.0367,  0.0426,  0.0370,  0.0367,  0.0325,  0.0367,  0.0367,
         0.0367,  0.0367,  0.0367,  0.0369,  0.0364,  0.0368,  0.0367,  0.0367,
         0.0367,  0.0367,  0.0367,  0.0367,  0.0339,  0.0408,  0.0367,  0.0368,
         0.0368,  0.0370,  0.0387,  0.0383,  0.0367,  0.0367,  0.0370,  0.0375,
         0.0368,  0.0367,  0.0354,  0.0371,  0.0730,  0.0367,  0.0394,  0.0367,
         0.0362,  0.0367,  0.0368,  0.0370,  0.0367,  0.0306,  0.0369,  0.0367,
         0.0367,  0.0367,  0.0367,  0.0370,  0.0367,  0.0571,  0.0419,  0.0367,
         0.0367,  0.0366,  0.0367,  0.0366,  0.0367,  0.0373,  0.0369,  0.0369,
         0.0349, -0.0360,  0.0315,  0.0370,  0.0367,  0.0367,  0.0370,  0.0367,
         0.0367,  0.0370,  0.0368,  0.0367,  0.0367,  0.0386,  0.0367,  0.0535,
         0.0367,  0.0367,  0.0367,  0.0371,  0.0370,  0.0369,  0.0367,  0.0367,
         0.0371,  0.0367,  0.0367,  0.0369,  0.0368,  0.0368,  0.0370,  0.0480,
         0.0359,  0.0367,  0.0370,  0.0367,  0.0370,  0.0367,  0.0370,  0.0369,
         0.0369,  0.0368,  0.0367,  0.0367,  0.0380,  0.0369,  0.0367,  0.0510,
         0.0371,  0.0352,  0.0367,  0.0369,  0.0378,  0.0371,  0.0368,  0.0363,
         0.0422,  0.0368,  0.0367,  0.0370,  0.0367,  0.0369,  0.0367,  0.0368,
         0.0367,  0.0367,  0.0367,  0.0413,  0.0368,  0.0367,  0.0371,  0.0367],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0382,  0.0380,  0.0358,  0.0707,  0.0332,  0.0371,  0.0372,  0.0372,
         0.0376,  0.0375,  0.0373,  0.0375,  0.0378,  0.0372,  0.0373,  0.0372,
         0.0375,  0.0374,  0.0304,  0.0372,  0.0372,  0.0388,  0.0373,  0.0466,
         0.0373,  0.0376,  0.0374,  0.0372,  0.0372,  0.0371,  0.0394,  0.0376,
         0.0361,  0.0372,  0.0374,  0.0373,  0.0368,  0.0372,  0.0374,  0.0374,
         0.0372,  0.0376,  0.0372,  0.0370,  0.0372,  0.0374,  0.0372,  0.0372,
         0.0374,  0.0378,  0.0372,  0.0425,  0.0374,  0.0371,  0.0372,  0.0369,
         0.0374,  0.0333,  0.0372,  0.0370,  0.0373,  0.0372,  0.0374,  0.0372,
         0.0581,  0.0373,  0.0372,  0.0377,  0.0372,  0.0372,  0.0371,  0.0373,
         0.0372,  0.0372,  0.0371,  0.0375,  0.0372,  0.0302,  0.0372,  0.0579,
         0.0372,  0.0374,  0.0424,  0.0373,  0.0580,  0.0374,  0.0372,  0.0372,
         0.0388,  0.0374,  0.0371,  0.0390,  0.0376,  0.0373,  0.0372,  0.0375,
         0.0354,  0.0374,  0.0532,  0.0372,  0.0375,  0.0373,  0.0376,  0.0374,
         0.0373,  0.0371,  0.0373,  0.0376,  0.0377,  0.0374,  0.0375,  0.0374,
         0.0333,  0.0372,  0.0372,  0.0372,  0.0371,  0.0397,  0.0368,  0.0665,
         0.0371,  0.0372,  0.0372,  0.0372,  0.0352,  0.0376,  0.0372,  0.0372,
         0.0372,  0.0372,  0.0372,  0.0372,  0.0338,  0.0372,  0.0368,  0.0370,
         0.0373,  0.0371,  0.0366,  0.0372,  0.0370,  0.0315,  0.0377,  0.0374,
         0.0371,  0.0375,  0.0406,  0.0373,  0.0375,  0.0375,  0.0403,  0.0377,
         0.0377,  0.0380,  0.0372,  0.0372,  0.0373,  0.0372,  0.0370,  0.0371,
         0.0372,  0.0373,  0.0373,  0.0375,  0.0371,  0.0374,  0.0377,  0.0373,
         0.0375,  0.0374,  0.0372,  0.0372,  0.0370,  0.0372,  0.0372,  0.0373,
         0.0372,  0.0374,  0.0381,  0.0325,  0.0373,  0.0372,  0.0368,  0.0432,
         0.0374,  0.0392,  0.0372,  0.0372,  0.0372,  0.0376,  0.0374,  0.0372,
         0.0372,  0.0370,  0.0372,  0.0345,  0.0374,  0.0374,  0.0374,  0.0376,
         0.0315,  0.0373,  0.0378,  0.0372,  0.0374,  0.0374,  0.0378,  0.0366,
         0.0374,  0.0372,  0.0493,  0.0372,  0.0372,  0.0372,  0.0373,  0.0372,
         0.0401,  0.0373,  0.0372,  0.0372,  0.0375,  0.0374,  0.0375,  0.0371,
         0.0372,  0.0372,  0.0371,  0.0372,  0.0373,  0.0406,  0.0372,  0.0375,
         0.0373,  0.0372,  0.0373,  0.0374,  0.0373,  0.0372,  0.0370,  0.0345,
         0.0372,  0.0374,  0.0384,  0.0360,  0.0372,  0.0373,  0.0376,  0.0368,
         0.0371,  0.0372,  0.0372,  0.0372,  0.0485,  0.0373,  0.0484,  0.0372,
         0.0383,  0.0372,  0.0374,  0.0403,  0.0371,  0.0372,  0.0371,  0.0432,
         0.0365,  0.0373,  0.0374,  0.0372,  0.0371,  0.0374,  0.0361,  0.0373,
         0.0372,  0.0370,  0.0368,  0.0371,  0.0372,  0.0372,  0.0373,  0.0741,
         0.0381,  0.0371,  0.0330,  0.0372,  0.0372,  0.0377,  0.0372,  0.0374,
         0.0372,  0.0374,  0.0371,  0.0372,  0.0369,  0.0374,  0.0372,  0.0373,
         0.0371,  0.0372,  0.0338,  0.0372,  0.0376,  0.0324,  0.0372,  0.0370,
         0.0373,  0.0372,  0.0374,  0.0372,  0.0373,  0.0374,  0.0374,  0.0368,
         0.0373,  0.0375,  0.0373,  0.0374,  0.0407,  0.0375,  0.0373,  0.0373,
         0.0371,  0.0388,  0.0371,  0.0371,  0.0372,  0.0374,  0.0374,  0.0480,
         0.0372,  0.0373,  0.0387,  0.0372,  0.0374,  0.0372,  0.0375,  0.0373,
         0.0373,  0.0358,  0.0371,  0.0374,  0.0374,  0.0373,  0.0372,  0.0373,
         0.0373,  0.0342,  0.0334,  0.0326,  0.0375,  0.0371,  0.0372,  0.0371,
         0.0377,  0.0372,  0.0372,  0.0372,  0.0372,  0.0372,  0.0372,  0.0371,
         0.0372,  0.0373,  0.0371,  0.0374,  0.0589,  0.0379,  0.0372,  0.0372,
         0.0371,  0.0404,  0.0372,  0.0372,  0.0375,  0.0372,  0.0384,  0.0365,
         0.0372,  0.0372,  0.0372,  0.0332,  0.0371,  0.0374,  0.0372,  0.0372,
         0.0371,  0.0372,  0.0373,  0.0327,  0.0487,  0.0372,  0.0374,  0.0372,
         0.0374,  0.0372,  0.0372,  0.0372,  0.0373,  0.0372,  0.0372,  0.0375,
         0.0372,  0.0373,  0.0375,  0.0373,  0.0372,  0.0372,  0.0371,  0.0372,
         0.0368,  0.0375,  0.0373,  0.0372,  0.0372,  0.0372,  0.0374,  0.0373,
         0.0372,  0.0366,  0.0372,  0.0372,  0.0372,  0.0371,  0.0359,  0.0373,
         0.0373,  0.0369,  0.0376,  0.0372,  0.0390,  0.0372,  0.0372,  0.0355,
         0.0373,  0.0361,  0.0372,  0.0375,  0.0372,  0.0364,  0.0378,  0.0398,
         0.0374,  0.0373,  0.0375,  0.0371,  0.0374,  0.0357,  0.0376,  0.0372,
         0.0260,  0.0375,  0.0372,  0.0400,  0.0374,  0.0372,  0.0373,  0.0372,
         0.0373,  0.0375,  0.0374,  0.0377,  0.0440,  0.0374,  0.0373,  0.0372,
         0.0336,  0.0374,  0.0347,  0.0321,  0.0372,  0.0375,  0.0370,  0.0375,
         0.0375,  0.0372,  0.0375,  0.0373,  0.0375,  0.0349,  0.0371,  0.0373,
         0.0372,  0.0311,  0.0374,  0.0372,  0.0372,  0.0374,  0.0370,  0.0395,
         0.0373,  0.0373,  0.0374,  0.0373,  0.0372,  0.0372,  0.0353,  0.0373,
         0.0372,  0.0373,  0.0193,  0.0372,  0.0372,  0.0372,  0.0374,  0.0372,
         0.0373,  0.0372,  0.0431,  0.0375,  0.0372,  0.0330,  0.0372,  0.0372,
         0.0373,  0.0372,  0.0372,  0.0374,  0.0369,  0.0373,  0.0372,  0.0372,
         0.0373,  0.0372,  0.0372,  0.0372,  0.0344,  0.0413,  0.0372,  0.0373,
         0.0374,  0.0375,  0.0392,  0.0390,  0.0372,  0.0372,  0.0375,  0.0380,
         0.0373,  0.0372,  0.0359,  0.0376,  0.0742,  0.0372,  0.0399,  0.0372,
         0.0367,  0.0373,  0.0373,  0.0374,  0.0372,  0.0311,  0.0374,  0.0372,
         0.0372,  0.0372,  0.0372,  0.0376,  0.0371,  0.0576,  0.0422,  0.0372,
         0.0372,  0.0371,  0.0371,  0.0371,  0.0372,  0.0378,  0.0374,  0.0374,
         0.0354, -0.0354,  0.0320,  0.0375,  0.0372,  0.0372,  0.0375,  0.0373,
         0.0372,  0.0375,  0.0373,  0.0372,  0.0373,  0.0392,  0.0372,  0.0547,
         0.0371,  0.0373,  0.0372,  0.0376,  0.0377,  0.0374,  0.0372,  0.0372,
         0.0376,  0.0372,  0.0372,  0.0374,  0.0373,  0.0374,  0.0376,  0.0489,
         0.0364,  0.0372,  0.0376,  0.0371,  0.0375,  0.0372,  0.0375,  0.0374,
         0.0374,  0.0373,  0.0372,  0.0372,  0.0385,  0.0374,  0.0371,  0.0518,
         0.0376,  0.0357,  0.0372,  0.0374,  0.0383,  0.0376,  0.0374,  0.0368,
         0.0430,  0.0374,  0.0373,  0.0375,  0.0372,  0.0374,  0.0372,  0.0374,
         0.0372,  0.0372,  0.0372,  0.0418,  0.0373,  0.0372,  0.0376,  0.0372],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(5891.4634, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9126, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 03:33:50,203][train_inner][INFO] - {"epoch": 7, "update": 6.935, "loss": "4.569", "ntokens": "149635", "nsentences": "538.215", "prob_perplexity": "59.186", "code_perplexity": "59.055", "temp": "1.965", "loss_0": "4.428", "loss_1": "0.131", "loss_2": "0.01", "accuracy": "0.27793", "wps": "37398.6", "ups": "0.25", "wpb": "149635", "bsz": "538.2", "num_updates": "3600", "lr": "5.625e-05", "gnorm": "0.666", "loss_scale": "16", "train_wall": "799", "gb_free": "12.4", "wall": "14497"}
[2023-09-12 03:36:02,371][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 03:36:02,372][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 03:36:02,549][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2023-09-12 03:36:26,289][valid][INFO] - {"epoch": 7, "valid_loss": "4.29", "valid_ntokens": "7909.68", "valid_nsentences": "55.2525", "valid_prob_perplexity": "58.606", "valid_code_perplexity": "58.506", "valid_temp": "1.964", "valid_loss_0": "4.149", "valid_loss_1": "0.131", "valid_loss_2": "0.01", "valid_accuracy": "0.32716", "valid_wps": "33033.2", "valid_wpb": "7909.7", "valid_bsz": "55.3", "valid_num_updates": "3634", "valid_best_loss": "4.29"}
[2023-09-12 03:36:26,290][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 3634 updates
[2023-09-12 03:36:26,299][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 03:36:28,690][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 03:36:30,029][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 3634 updates, score 4.29) (writing took 3.738426580093801 seconds)
[2023-09-12 03:36:30,029][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2023-09-12 03:36:30,030][train][INFO] - {"epoch": 7, "train_loss": "4.608", "train_ntokens": "149476", "train_nsentences": "538.3", "train_prob_perplexity": "58.636", "train_code_perplexity": "58.498", "train_temp": "1.967", "train_loss_0": "4.467", "train_loss_1": "0.131", "train_loss_2": "0.01", "train_accuracy": "0.27308", "train_wps": "37132.9", "train_ups": "0.25", "train_wpb": "149476", "train_bsz": "538.3", "train_num_updates": "3634", "train_lr": "5.67813e-05", "train_gnorm": "0.68", "train_loss_scale": "16", "train_train_wall": "2062", "train_gb_free": "13.3", "train_wall": "14657"}
[2023-09-12 03:36:30,031][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 03:36:30,119][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2023-09-12 03:36:30,418][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 03:36:30,423][fairseq.trainer][INFO] - begin training epoch 8
[2023-09-12 03:36:30,424][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([ 0.0189,  0.0190,  0.0175,  0.0283,  0.0148,  0.0188,  0.0188,  0.0188,
         0.0192,  0.0192,  0.0190,  0.0191,  0.0192,  0.0188,  0.0189,  0.0189,
         0.0191,  0.0190,  0.0123,  0.0188,  0.0188,  0.0202,  0.0190,  0.0249,
         0.0189,  0.0192,  0.0191,  0.0189,  0.0188,  0.0188,  0.0210,  0.0192,
         0.0177,  0.0188,  0.0190,  0.0189,  0.0184,  0.0188,  0.0191,  0.0191,
         0.0188,  0.0192,  0.0188,  0.0186,  0.0188,  0.0190,  0.0188,  0.0188,
         0.0190,  0.0194,  0.0188,  0.0192,  0.0190,  0.0188,  0.0188,  0.0185,
         0.0190,  0.0149,  0.0188,  0.0187,  0.0190,  0.0188,  0.0188,  0.0188,
         0.0231,  0.0189,  0.0188,  0.0193,  0.0188,  0.0188,  0.0188,  0.0189,
         0.0188,  0.0189,  0.0187,  0.0191,  0.0188,  0.0118,  0.0189,  0.0312,
         0.0188,  0.0190,  0.0206,  0.0190,  0.0267,  0.0190,  0.0188,  0.0188,
         0.0189,  0.0190,  0.0188,  0.0189,  0.0192,  0.0189,  0.0188,  0.0191,
         0.0170,  0.0190,  0.0270,  0.0188,  0.0191,  0.0190,  0.0192,  0.0191,
         0.0189,  0.0188,  0.0189,  0.0192,  0.0193,  0.0191,  0.0192,  0.0190,
         0.0150,  0.0188,  0.0188,  0.0188,  0.0188,  0.0210,  0.0185,  0.0373,
         0.0188,  0.0188,  0.0188,  0.0188,  0.0169,  0.0192,  0.0189,  0.0188,
         0.0189,  0.0189,  0.0189,  0.0188,  0.0154,  0.0188,  0.0185,  0.0186,
         0.0190,  0.0188,  0.0182,  0.0189,  0.0186,  0.0132,  0.0193,  0.0190,
         0.0187,  0.0191,  0.0206,  0.0190,  0.0191,  0.0191,  0.0191,  0.0193,
         0.0193,  0.0197,  0.0188,  0.0189,  0.0189,  0.0189,  0.0187,  0.0188,
         0.0188,  0.0189,  0.0189,  0.0191,  0.0188,  0.0191,  0.0193,  0.0189,
         0.0191,  0.0191,  0.0188,  0.0189,  0.0187,  0.0188,  0.0189,  0.0190,
         0.0189,  0.0190,  0.0198,  0.0141,  0.0190,  0.0188,  0.0184,  0.0223,
         0.0191,  0.0190,  0.0188,  0.0188,  0.0188,  0.0192,  0.0191,  0.0188,
         0.0188,  0.0186,  0.0189,  0.0162,  0.0189,  0.0190,  0.0191,  0.0192,
         0.0131,  0.0189,  0.0193,  0.0188,  0.0191,  0.0190,  0.0189,  0.0182,
         0.0190,  0.0188,  0.0284,  0.0189,  0.0189,  0.0188,  0.0190,  0.0188,
         0.0211,  0.0190,  0.0188,  0.0188,  0.0191,  0.0190,  0.0191,  0.0188,
         0.0189,  0.0188,  0.0188,  0.0189,  0.0189,  0.0207,  0.0188,  0.0191,
         0.0190,  0.0188,  0.0189,  0.0191,  0.0189,  0.0188,  0.0187,  0.0162,
         0.0188,  0.0189,  0.0194,  0.0176,  0.0188,  0.0189,  0.0191,  0.0184,
         0.0188,  0.0188,  0.0188,  0.0188,  0.0196,  0.0189,  0.0204,  0.0188,
         0.0190,  0.0188,  0.0191,  0.0218,  0.0187,  0.0188,  0.0188,  0.0251,
         0.0181,  0.0189,  0.0190,  0.0188,  0.0188,  0.0190,  0.0177,  0.0190,
         0.0188,  0.0186,  0.0184,  0.0188,  0.0188,  0.0189,  0.0189,  0.0317,
         0.0188,  0.0188,  0.0145,  0.0188,  0.0189,  0.0193,  0.0189,  0.0190,
         0.0188,  0.0190,  0.0188,  0.0188,  0.0185,  0.0191,  0.0188,  0.0189,
         0.0188,  0.0189,  0.0154,  0.0188,  0.0192,  0.0141,  0.0189,  0.0187,
         0.0189,  0.0188,  0.0191,  0.0188,  0.0190,  0.0190,  0.0190,  0.0185,
         0.0190,  0.0191,  0.0189,  0.0191,  0.0189,  0.0192,  0.0189,  0.0189,
         0.0187,  0.0204,  0.0188,  0.0188,  0.0188,  0.0191,  0.0190,  0.0247,
         0.0188,  0.0189,  0.0203,  0.0188,  0.0190,  0.0188,  0.0191,  0.0190,
         0.0190,  0.0174,  0.0187,  0.0190,  0.0190,  0.0189,  0.0188,  0.0190,
         0.0189,  0.0158,  0.0150, -0.0032,  0.0192,  0.0188,  0.0188,  0.0188,
         0.0193,  0.0188,  0.0189,  0.0189,  0.0188,  0.0188,  0.0188,  0.0188,
         0.0188,  0.0189,  0.0187,  0.0190,  0.0299,  0.0193,  0.0188,  0.0188,
         0.0188,  0.0220,  0.0188,  0.0188,  0.0191,  0.0188,  0.0187,  0.0181,
         0.0189,  0.0188,  0.0188,  0.0149,  0.0188,  0.0191,  0.0188,  0.0188,
         0.0188,  0.0188,  0.0189,  0.0143,  0.0191,  0.0188,  0.0190,  0.0188,
         0.0190,  0.0188,  0.0188,  0.0188,  0.0190,  0.0188,  0.0188,  0.0191,
         0.0188,  0.0197,  0.0191,  0.0190,  0.0188,  0.0188,  0.0188,  0.0188,
         0.0184,  0.0191,  0.0189,  0.0189,  0.0188,  0.0188,  0.0191,  0.0189,
         0.0188,  0.0161,  0.0188,  0.0189,  0.0188,  0.0188,  0.0175,  0.0190,
         0.0189,  0.0185,  0.0188,  0.0188,  0.0206,  0.0188,  0.0188,  0.0171,
         0.0190,  0.0177,  0.0188,  0.0192,  0.0188,  0.0180,  0.0194,  0.0195,
         0.0190,  0.0189,  0.0191,  0.0187,  0.0190,  0.0174,  0.0192,  0.0188,
         0.0383,  0.0191,  0.0189,  0.0194,  0.0190,  0.0188,  0.0189,  0.0188,
         0.0189,  0.0191,  0.0191,  0.0191,  0.0189,  0.0190,  0.0189,  0.0188,
         0.0152,  0.0190,  0.0163,  0.0137,  0.0188,  0.0191,  0.0186,  0.0191,
         0.0191,  0.0188,  0.0192,  0.0189,  0.0191,  0.0165,  0.0188,  0.0189,
         0.0188,  0.0127,  0.0190,  0.0188,  0.0189,  0.0190,  0.0187,  0.0211,
         0.0189,  0.0189,  0.0190,  0.0189,  0.0188,  0.0188,  0.0169,  0.0189,
         0.0189,  0.0188,  0.0292,  0.0188,  0.0188,  0.0188,  0.0191,  0.0189,
         0.0189,  0.0188,  0.0241,  0.0191,  0.0188,  0.0146,  0.0188,  0.0188,
         0.0189,  0.0188,  0.0188,  0.0190,  0.0186,  0.0189,  0.0188,  0.0188,
         0.0189,  0.0188,  0.0188,  0.0189,  0.0161,  0.0220,  0.0188,  0.0190,
         0.0190,  0.0191,  0.0208,  0.0191,  0.0188,  0.0188,  0.0191,  0.0197,
         0.0189,  0.0188,  0.0175,  0.0192,  0.0371,  0.0188,  0.0218,  0.0188,
         0.0184,  0.0189,  0.0189,  0.0191,  0.0188,  0.0128,  0.0190,  0.0188,
         0.0188,  0.0188,  0.0188,  0.0192,  0.0188,  0.0316,  0.0187,  0.0188,
         0.0188,  0.0188,  0.0188,  0.0188,  0.0188,  0.0194,  0.0190,  0.0191,
         0.0171, -0.0156,  0.0136,  0.0191,  0.0188,  0.0189,  0.0192,  0.0189,
         0.0188,  0.0191,  0.0189,  0.0188,  0.0189,  0.0208,  0.0189,  0.0284,
         0.0188,  0.0189,  0.0188,  0.0192,  0.0188,  0.0190,  0.0188,  0.0188,
         0.0192,  0.0188,  0.0188,  0.0191,  0.0189,  0.0190,  0.0192,  0.0315,
         0.0206,  0.0188,  0.0192,  0.0188,  0.0191,  0.0188,  0.0191,  0.0191,
         0.0191,  0.0189,  0.0188,  0.0188,  0.0201,  0.0191,  0.0188,  0.0259,
         0.0192,  0.0173,  0.0188,  0.0191,  0.0194,  0.0193,  0.0190,  0.0184,
         0.0208,  0.0190,  0.0189,  0.0192,  0.0189,  0.0190,  0.0189,  0.0190,
         0.0188,  0.0188,  0.0188,  0.0235,  0.0189,  0.0188,  0.0192,  0.0188],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7911.0352, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6597, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7297.9077, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0518, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7476.0244, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3301, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7658.8916, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9580, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8363.5469, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1045, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7745.5288, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9463, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7573.4268, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0107, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8039.4121, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0684, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5655.0918, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1631, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6268.2139, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9380, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9057.8857, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(6442.3643, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5972, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 03:41:16,191][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2023-09-12 03:41:44,391][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2023-09-12 03:47:35,391][train_inner][INFO] - {"epoch": 8, "update": 7.322, "loss": "4.547", "ntokens": "149077", "nsentences": "537.27", "prob_perplexity": "60.302", "code_perplexity": "60.175", "temp": "1.963", "loss_0": "4.406", "loss_1": "0.131", "loss_2": "0.01", "accuracy": "0.27951", "wps": "36131.8", "ups": "0.24", "wpb": "149078", "bsz": "537.3", "num_updates": "3800", "lr": "5.9375e-05", "gnorm": "0.65", "loss_scale": "8", "train_wall": "796", "gb_free": "12.7", "wall": "15322"}
Parameter containing:
tensor([ 0.0382,  0.0381,  0.0359,  0.0709,  0.0332,  0.0372,  0.0372,  0.0372,
         0.0377,  0.0376,  0.0374,  0.0375,  0.0379,  0.0372,  0.0373,  0.0373,
         0.0375,  0.0374,  0.0304,  0.0372,  0.0373,  0.0389,  0.0374,  0.0467,
         0.0374,  0.0376,  0.0375,  0.0373,  0.0373,  0.0372,  0.0395,  0.0376,
         0.0362,  0.0372,  0.0374,  0.0373,  0.0368,  0.0372,  0.0375,  0.0375,
         0.0372,  0.0377,  0.0373,  0.0370,  0.0373,  0.0375,  0.0372,  0.0373,
         0.0374,  0.0379,  0.0372,  0.0426,  0.0375,  0.0372,  0.0373,  0.0369,
         0.0374,  0.0333,  0.0373,  0.0371,  0.0374,  0.0372,  0.0375,  0.0372,
         0.0583,  0.0374,  0.0372,  0.0378,  0.0372,  0.0372,  0.0372,  0.0374,
         0.0372,  0.0373,  0.0371,  0.0375,  0.0373,  0.0302,  0.0373,  0.0580,
         0.0372,  0.0374,  0.0425,  0.0374,  0.0581,  0.0375,  0.0373,  0.0372,
         0.0388,  0.0374,  0.0372,  0.0391,  0.0377,  0.0373,  0.0372,  0.0375,
         0.0355,  0.0374,  0.0533,  0.0373,  0.0376,  0.0374,  0.0376,  0.0375,
         0.0374,  0.0372,  0.0374,  0.0377,  0.0378,  0.0375,  0.0376,  0.0374,
         0.0334,  0.0373,  0.0373,  0.0372,  0.0372,  0.0398,  0.0369,  0.0666,
         0.0372,  0.0372,  0.0372,  0.0372,  0.0353,  0.0377,  0.0373,  0.0372,
         0.0373,  0.0373,  0.0373,  0.0372,  0.0338,  0.0372,  0.0369,  0.0371,
         0.0374,  0.0372,  0.0367,  0.0373,  0.0370,  0.0316,  0.0378,  0.0374,
         0.0371,  0.0375,  0.0407,  0.0374,  0.0376,  0.0375,  0.0405,  0.0378,
         0.0378,  0.0381,  0.0372,  0.0373,  0.0374,  0.0373,  0.0371,  0.0372,
         0.0372,  0.0374,  0.0374,  0.0375,  0.0372,  0.0375,  0.0378,  0.0374,
         0.0376,  0.0375,  0.0372,  0.0373,  0.0371,  0.0373,  0.0373,  0.0374,
         0.0373,  0.0375,  0.0382,  0.0325,  0.0374,  0.0373,  0.0369,  0.0433,
         0.0375,  0.0393,  0.0372,  0.0372,  0.0372,  0.0376,  0.0375,  0.0372,
         0.0372,  0.0370,  0.0373,  0.0346,  0.0374,  0.0374,  0.0375,  0.0377,
         0.0315,  0.0374,  0.0378,  0.0373,  0.0375,  0.0374,  0.0379,  0.0367,
         0.0374,  0.0372,  0.0494,  0.0373,  0.0373,  0.0372,  0.0374,  0.0372,
         0.0402,  0.0374,  0.0373,  0.0373,  0.0375,  0.0375,  0.0375,  0.0372,
         0.0373,  0.0373,  0.0372,  0.0373,  0.0374,  0.0406,  0.0372,  0.0376,
         0.0374,  0.0373,  0.0374,  0.0375,  0.0373,  0.0373,  0.0371,  0.0346,
         0.0373,  0.0374,  0.0384,  0.0360,  0.0372,  0.0373,  0.0377,  0.0368,
         0.0372,  0.0372,  0.0372,  0.0373,  0.0486,  0.0373,  0.0486,  0.0372,
         0.0384,  0.0373,  0.0375,  0.0404,  0.0371,  0.0372,  0.0372,  0.0433,
         0.0366,  0.0373,  0.0374,  0.0372,  0.0372,  0.0375,  0.0362,  0.0374,
         0.0372,  0.0371,  0.0369,  0.0372,  0.0372,  0.0373,  0.0374,  0.0741,
         0.0382,  0.0372,  0.0331,  0.0373,  0.0373,  0.0377,  0.0373,  0.0374,
         0.0372,  0.0374,  0.0372,  0.0372,  0.0370,  0.0375,  0.0373,  0.0373,
         0.0372,  0.0373,  0.0338,  0.0372,  0.0376,  0.0325,  0.0373,  0.0371,
         0.0373,  0.0373,  0.0375,  0.0372,  0.0374,  0.0374,  0.0374,  0.0369,
         0.0374,  0.0376,  0.0373,  0.0375,  0.0408,  0.0376,  0.0374,  0.0374,
         0.0372,  0.0389,  0.0372,  0.0372,  0.0372,  0.0375,  0.0374,  0.0481,
         0.0373,  0.0374,  0.0387,  0.0372,  0.0375,  0.0373,  0.0375,  0.0374,
         0.0374,  0.0359,  0.0372,  0.0374,  0.0374,  0.0374,  0.0372,  0.0374,
         0.0373,  0.0343,  0.0334,  0.0328,  0.0376,  0.0372,  0.0373,  0.0372,
         0.0377,  0.0372,  0.0373,  0.0373,  0.0373,  0.0372,  0.0372,  0.0372,
         0.0373,  0.0374,  0.0372,  0.0374,  0.0590,  0.0380,  0.0372,  0.0372,
         0.0372,  0.0405,  0.0373,  0.0372,  0.0376,  0.0373,  0.0385,  0.0366,
         0.0373,  0.0373,  0.0372,  0.0333,  0.0372,  0.0375,  0.0372,  0.0372,
         0.0372,  0.0373,  0.0373,  0.0327,  0.0488,  0.0372,  0.0374,  0.0372,
         0.0375,  0.0372,  0.0372,  0.0372,  0.0374,  0.0373,  0.0373,  0.0376,
         0.0372,  0.0374,  0.0376,  0.0374,  0.0373,  0.0373,  0.0372,  0.0373,
         0.0368,  0.0376,  0.0373,  0.0373,  0.0373,  0.0373,  0.0375,  0.0374,
         0.0372,  0.0366,  0.0373,  0.0373,  0.0372,  0.0372,  0.0359,  0.0374,
         0.0374,  0.0369,  0.0377,  0.0373,  0.0390,  0.0373,  0.0372,  0.0356,
         0.0374,  0.0361,  0.0373,  0.0376,  0.0372,  0.0365,  0.0378,  0.0399,
         0.0375,  0.0373,  0.0376,  0.0372,  0.0374,  0.0358,  0.0377,  0.0372,
         0.0258,  0.0376,  0.0373,  0.0401,  0.0374,  0.0372,  0.0374,  0.0372,
         0.0373,  0.0376,  0.0375,  0.0378,  0.0441,  0.0375,  0.0374,  0.0372,
         0.0336,  0.0374,  0.0348,  0.0321,  0.0372,  0.0376,  0.0370,  0.0375,
         0.0376,  0.0372,  0.0376,  0.0374,  0.0376,  0.0349,  0.0372,  0.0374,
         0.0372,  0.0312,  0.0374,  0.0372,  0.0373,  0.0374,  0.0371,  0.0395,
         0.0374,  0.0373,  0.0375,  0.0373,  0.0372,  0.0373,  0.0354,  0.0374,
         0.0373,  0.0374,  0.0193,  0.0373,  0.0372,  0.0372,  0.0375,  0.0373,
         0.0373,  0.0372,  0.0432,  0.0375,  0.0372,  0.0331,  0.0373,  0.0372,
         0.0373,  0.0372,  0.0372,  0.0374,  0.0370,  0.0374,  0.0373,  0.0373,
         0.0373,  0.0372,  0.0372,  0.0373,  0.0345,  0.0414,  0.0373,  0.0374,
         0.0374,  0.0376,  0.0392,  0.0391,  0.0373,  0.0372,  0.0376,  0.0381,
         0.0374,  0.0372,  0.0359,  0.0377,  0.0742,  0.0373,  0.0399,  0.0372,
         0.0368,  0.0373,  0.0374,  0.0375,  0.0372,  0.0312,  0.0374,  0.0373,
         0.0372,  0.0373,  0.0372,  0.0376,  0.0372,  0.0577,  0.0423,  0.0372,
         0.0372,  0.0372,  0.0372,  0.0372,  0.0372,  0.0378,  0.0374,  0.0375,
         0.0355, -0.0354,  0.0321,  0.0375,  0.0372,  0.0373,  0.0376,  0.0373,
         0.0372,  0.0376,  0.0374,  0.0373,  0.0373,  0.0392,  0.0373,  0.0549,
         0.0372,  0.0373,  0.0372,  0.0377,  0.0378,  0.0374,  0.0372,  0.0372,
         0.0377,  0.0372,  0.0373,  0.0375,  0.0374,  0.0374,  0.0376,  0.0490,
         0.0365,  0.0372,  0.0376,  0.0372,  0.0375,  0.0373,  0.0376,  0.0375,
         0.0375,  0.0374,  0.0373,  0.0372,  0.0385,  0.0375,  0.0372,  0.0519,
         0.0377,  0.0358,  0.0373,  0.0375,  0.0384,  0.0377,  0.0374,  0.0369,
         0.0432,  0.0374,  0.0373,  0.0376,  0.0373,  0.0374,  0.0373,  0.0374,
         0.0372,  0.0373,  0.0373,  0.0419,  0.0374,  0.0372,  0.0377,  0.0372],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8367.6289, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6934, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5243.1729, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1279, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: [2023-09-12 03:58:50,006][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
loss: tensor(8012.8047, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 04:00:57,299][train_inner][INFO] - {"epoch": 8, "update": 7.708, "loss": "4.514", "ntokens": "149728", "nsentences": "538.93", "prob_perplexity": "61.528", "code_perplexity": "61.404", "temp": "1.961", "loss_0": "4.374", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.28205", "wps": "37342.9", "ups": "0.25", "wpb": "149728", "bsz": "538.9", "num_updates": "4000", "lr": "6.25e-05", "gnorm": "0.626", "loss_scale": "8", "train_wall": "801", "gb_free": "12.7", "wall": "16124"}
loss: tensor(7209.4180, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7325.9448, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5938, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6530.5166, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6777, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7237.9058, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6724, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 04:10:57,811][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 04:10:57,812][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 04:10:57,899][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2023-09-12 04:11:21,422][valid][INFO] - {"epoch": 8, "valid_loss": "4.247", "valid_ntokens": "7883.93", "valid_nsentences": "55.2525", "valid_prob_perplexity": "62.348", "valid_code_perplexity": "62.228", "valid_temp": "1.959", "valid_loss_0": "4.106", "valid_loss_1": "0.13", "valid_loss_2": "0.01", "valid_accuracy": "0.32948", "valid_wps": "33474.4", "valid_wpb": "7883.9", "valid_bsz": "55.3", "valid_num_updates": "4152", "valid_best_loss": "4.247"}
[2023-09-12 04:11:21,424][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 4152 updates
[2023-09-12 04:11:21,425][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 04:11:23,908][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 04:11:25,373][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 4152 updates, score 4.247) (writing took 3.948944839066826 seconds)
[2023-09-12 04:11:25,374][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2023-09-12 04:11:25,375][train][INFO] - {"epoch": 8, "train_loss": "4.519", "train_ntokens": "149481", "train_nsentences": "538.338", "train_prob_perplexity": "61.463", "train_code_perplexity": "61.339", "train_temp": "1.961", "train_loss_0": "4.379", "train_loss_1": "0.13", "train_loss_2": "0.01", "train_accuracy": "0.28173", "train_wps": "36953.9", "train_ups": "0.25", "train_wpb": "149481", "train_bsz": "538.3", "train_num_updates": "4152", "train_lr": "6.4875e-05", "train_gnorm": "0.632", "train_loss_scale": "8", "train_train_wall": "2064", "train_gb_free": "15.9", "train_wall": "16752"}
[2023-09-12 04:11:25,377][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 04:11:25,466][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2023-09-12 04:11:25,713][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 04:11:25,716][fairseq.trainer][INFO] - begin training epoch 9
[2023-09-12 04:11:25,717][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(6096.8633, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2991, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6057.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9170, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 04:14:33,832][train_inner][INFO] - {"epoch": 9, "update": 8.092, "loss": "4.5", "ntokens": "149178", "nsentences": "535.5", "prob_perplexity": "62.668", "code_perplexity": "62.546", "temp": "1.959", "loss_0": "4.36", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.28276", "wps": "36539.3", "ups": "0.24", "wpb": "149178", "bsz": "535.5", "num_updates": "4200", "lr": "6.5625e-05", "gnorm": "0.628", "loss_scale": "8", "train_wall": "787", "gb_free": "12.5", "wall": "16940"}
[2023-09-12 04:16:17,709][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
tensor(8422.3320, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0137, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6971.4585, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0283, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7201.2173, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1865, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7586.0879, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9458, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7889.7676, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2891, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5735.0796, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0791, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8025.2158, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3540, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5251.9082, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5942, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7603.1489, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6509, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8139.0825, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8438, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5526.5444, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0635, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0493,  0.0455,  0.0431,  0.0762,  0.0404,  0.0444,  0.0444,  0.0444,
         0.0448,  0.0448,  0.0446,  0.0447,  0.0452,  0.0444,  0.0445,  0.0446,
         0.0447,  0.0446,  0.0381,  0.0444,  0.0445,  0.0461,  0.0446,  0.0555,
         0.0446,  0.0448,  0.0447,  0.0445,  0.0445,  0.0444,  0.0467,  0.0448,
         0.0434,  0.0444,  0.0446,  0.0445,  0.0440,  0.0444,  0.0447,  0.0447,
         0.0444,  0.0448,  0.0444,  0.0443,  0.0444,  0.0449,  0.0444,  0.0444,
         0.0446,  0.0451,  0.0444,  0.0527,  0.0446,  0.0444,  0.0445,  0.0441,
         0.0446,  0.0405,  0.0445,  0.0443,  0.0446,  0.0444,  0.0475,  0.0446,
         0.0670,  0.0446,  0.0444,  0.0450,  0.0444,  0.0444,  0.0444,  0.0446,
         0.0444,  0.0445,  0.0443,  0.0447,  0.0445,  0.0374,  0.0445,  0.0688,
         0.0444,  0.0446,  0.0516,  0.0446,  0.0703,  0.0446,  0.0444,  0.0444,
         0.0471,  0.0446,  0.0444,  0.0472,  0.0448,  0.0445,  0.0444,  0.0447,
         0.0427,  0.0446,  0.0638,  0.0444,  0.0448,  0.0446,  0.0448,  0.0447,
         0.0445,  0.0444,  0.0446,  0.0448,  0.0450,  0.0447,  0.0448,  0.0446,
         0.0406,  0.0445,  0.0444,  0.0444,  0.0444,  0.0471,  0.0441,  0.0786,
         0.0444,  0.0444,  0.0450,  0.0444,  0.0425,  0.0449,  0.0445,  0.0444,
         0.0445,  0.0445,  0.0445,  0.0444,  0.0410,  0.0444,  0.0441,  0.0457,
         0.0446,  0.0444,  0.0439,  0.0445,  0.0442,  0.0388,  0.0450,  0.0446,
         0.0443,  0.0447,  0.0483,  0.0446,  0.0447,  0.0447,  0.0528,  0.0450,
         0.0450,  0.0453,  0.0445,  0.0445,  0.0445,  0.0445,  0.0443,  0.0444,
         0.0444,  0.0446,  0.0445,  0.0447,  0.0444,  0.0447,  0.0450,  0.0445,
         0.0448,  0.0447,  0.0444,  0.0445,  0.0443,  0.0444,  0.0445,  0.0446,
         0.0445,  0.0446,  0.0454,  0.0397,  0.0446,  0.0445,  0.0440,  0.0508,
         0.0447,  0.0469,  0.0444,  0.0444,  0.0444,  0.0448,  0.0447,  0.0444,
         0.0444,  0.0442,  0.0445,  0.0418,  0.0450,  0.0446,  0.0447,  0.0449,
         0.0387,  0.0445,  0.0450,  0.0444,  0.0447,  0.0446,  0.0462,  0.0439,
         0.0446,  0.0444,  0.0585,  0.0445,  0.0445,  0.0444,  0.0446,  0.0444,
         0.0475,  0.0446,  0.0445,  0.0444,  0.0447,  0.0446,  0.0447,  0.0444,
         0.0445,  0.0444,  0.0444,  0.0445,  0.0446,  0.0489,  0.0444,  0.0448,
         0.0446,  0.0445,  0.0446,  0.0447,  0.0445,  0.0444,  0.0443,  0.0418,
         0.0444,  0.0450,  0.0457,  0.0432,  0.0444,  0.0445,  0.0453,  0.0440,
         0.0444,  0.0444,  0.0444,  0.0444,  0.0623,  0.0445,  0.0591,  0.0444,
         0.0463,  0.0445,  0.0447,  0.0476,  0.0443,  0.0444,  0.0444,  0.0505,
         0.0437,  0.0445,  0.0446,  0.0444,  0.0444,  0.0446,  0.0434,  0.0446,
         0.0444,  0.0443,  0.0440,  0.0444,  0.0444,  0.0445,  0.0445,  0.0885,
         0.0470,  0.0444,  0.0416,  0.0444,  0.0445,  0.0449,  0.0445,  0.0446,
         0.0444,  0.0446,  0.0444,  0.0444,  0.0442,  0.0447,  0.0445,  0.0445,
         0.0444,  0.0445,  0.0410,  0.0444,  0.0448,  0.0397,  0.0445,  0.0443,
         0.0445,  0.0444,  0.0447,  0.0444,  0.0446,  0.0446,  0.0446,  0.0441,
         0.0446,  0.0450,  0.0445,  0.0447,  0.0505,  0.0448,  0.0446,  0.0446,
         0.0443,  0.0461,  0.0444,  0.0444,  0.0444,  0.0447,  0.0446,  0.0559,
         0.0446,  0.0446,  0.0459,  0.0444,  0.0447,  0.0444,  0.0447,  0.0446,
         0.0446,  0.0433,  0.0443,  0.0446,  0.0447,  0.0445,  0.0444,  0.0446,
         0.0445,  0.0415,  0.0406,  0.0472,  0.0448,  0.0444,  0.0444,  0.0444,
         0.0449,  0.0444,  0.0445,  0.0445,  0.0445,  0.0444,  0.0444,  0.0444,
         0.0445,  0.0445,  0.0443,  0.0446,  0.0667,  0.0461,  0.0444,  0.0444,
         0.0444,  0.0477,  0.0444,  0.0444,  0.0448,  0.0444,  0.0503,  0.0437,
         0.0445,  0.0445,  0.0444,  0.0405,  0.0444,  0.0447,  0.0444,  0.0444,
         0.0444,  0.0445,  0.0445,  0.0399,  0.0623,  0.0444,  0.0447,  0.0444,
         0.0461,  0.0444,  0.0444,  0.0444,  0.0446,  0.0445,  0.0444,  0.0447,
         0.0444,  0.0467,  0.0448,  0.0446,  0.0445,  0.0444,  0.0444,  0.0445,
         0.0440,  0.0448,  0.0445,  0.0445,  0.0445,  0.0445,  0.0447,  0.0445,
         0.0444,  0.0454,  0.0445,  0.0445,  0.0444,  0.0444,  0.0431,  0.0446,
         0.0446,  0.0441,  0.0464,  0.0445,  0.0462,  0.0445,  0.0444,  0.0427,
         0.0446,  0.0433,  0.0444,  0.0448,  0.0445,  0.0436,  0.0450,  0.0484,
         0.0446,  0.0445,  0.0448,  0.0444,  0.0446,  0.0430,  0.0449,  0.0444,
         0.0147,  0.0448,  0.0445,  0.0473,  0.0446,  0.0444,  0.0446,  0.0444,
         0.0445,  0.0448,  0.0447,  0.0456,  0.0582,  0.0446,  0.0446,  0.0444,
         0.0408,  0.0446,  0.0419,  0.0393,  0.0444,  0.0448,  0.0442,  0.0447,
         0.0447,  0.0444,  0.0448,  0.0446,  0.0455,  0.0421,  0.0444,  0.0445,
         0.0444,  0.0383,  0.0446,  0.0444,  0.0445,  0.0446,  0.0443,  0.0467,
         0.0445,  0.0445,  0.0447,  0.0445,  0.0444,  0.0444,  0.0425,  0.0446,
         0.0445,  0.0446,  0.0280,  0.0445,  0.0444,  0.0444,  0.0447,  0.0445,
         0.0445,  0.0444,  0.0504,  0.0447,  0.0444,  0.0403,  0.0444,  0.0444,
         0.0445,  0.0444,  0.0444,  0.0446,  0.0442,  0.0445,  0.0444,  0.0445,
         0.0445,  0.0444,  0.0444,  0.0445,  0.0417,  0.0492,  0.0444,  0.0446,
         0.0446,  0.0448,  0.0464,  0.0485,  0.0444,  0.0444,  0.0448,  0.0453,
         0.0445,  0.0444,  0.0432,  0.0449,  0.0878,  0.0445,  0.0471,  0.0444,
         0.0440,  0.0445,  0.0445,  0.0447,  0.0444,  0.0384,  0.0446,  0.0444,
         0.0444,  0.0445,  0.0444,  0.0448,  0.0444,  0.0536,  0.0519,  0.0444,
         0.0444,  0.0444,  0.0444,  0.0444,  0.0444,  0.0450,  0.0446,  0.0447,
         0.0427, -0.0274,  0.0392,  0.0447,  0.0444,  0.0445,  0.0448,  0.0445,
         0.0444,  0.0447,  0.0446,  0.0445,  0.0445,  0.0464,  0.0445,  0.0642,
         0.0444,  0.0445,  0.0444,  0.0448,  0.0466,  0.0446,  0.0444,  0.0444,
         0.0449,  0.0444,  0.0445,  0.0447,  0.0446,  0.0446,  0.0448,  0.0584,
         0.0440,  0.0444,  0.0448,  0.0444,  0.0447,  0.0445,  0.0448,  0.0447,
         0.0447,  0.0446,  0.0445,  0.0444,  0.0457,  0.0447,  0.0444,  0.0622,
         0.0460,  0.0430,  0.0444,  0.0447,  0.0464,  0.0449,  0.0446,  0.0441,
         0.0533,  0.0446,  0.0445,  0.0448,  0.0445,  0.0446,  0.0445,  0.0446,
         0.0444,  0.0444,  0.0444,  0.0491,  0.0445,  0.0444,  0.0448,  0.0444],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(8291.0645, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1299, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7280.6997, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0986, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7357.5806, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7148, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7778.2407, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0068, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5873.6221, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4150, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7922.6328, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7075, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7070.7178, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1565, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7662.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3774, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7654.1587, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6177, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6436.7241, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3992, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7873.5200, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3735, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 04:26:41,317][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2023-09-12 04:27:56,850][train_inner][INFO] - {"epoch": 9, "update": 8.48, "loss": "4.469", "ntokens": "149909", "nsentences": "539.16", "prob_perplexity": "63.721", "code_perplexity": "63.599", "temp": "1.957", "loss_0": "4.33", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.28545", "wps": "37336.4", "ups": "0.25", "wpb": "149909", "bsz": "539.2", "num_updates": "4400", "lr": "6.875e-05", "gnorm": "0.586", "loss_scale": "4", "train_wall": "802", "gb_free": "13", "wall": "17743"}
Parameter containing:
tensor([ 0.0476,  0.0440,  0.0416,  0.0766,  0.0390,  0.0430,  0.0430,  0.0430,
         0.0434,  0.0433,  0.0432,  0.0433,  0.0437,  0.0430,  0.0431,  0.0431,
         0.0433,  0.0432,  0.0365,  0.0430,  0.0430,  0.0447,  0.0431,  0.0539,
         0.0431,  0.0434,  0.0432,  0.0430,  0.0430,  0.0430,  0.0453,  0.0434,
         0.0419,  0.0430,  0.0432,  0.0431,  0.0426,  0.0430,  0.0432,  0.0432,
         0.0430,  0.0434,  0.0430,  0.0428,  0.0430,  0.0434,  0.0430,  0.0430,
         0.0432,  0.0436,  0.0430,  0.0508,  0.0432,  0.0429,  0.0430,  0.0427,
         0.0432,  0.0391,  0.0430,  0.0428,  0.0432,  0.0430,  0.0451,  0.0431,
         0.0648,  0.0431,  0.0430,  0.0435,  0.0430,  0.0430,  0.0430,  0.0431,
         0.0430,  0.0431,  0.0429,  0.0433,  0.0430,  0.0360,  0.0430,  0.0665,
         0.0430,  0.0432,  0.0504,  0.0431,  0.0677,  0.0432,  0.0430,  0.0430,
         0.0456,  0.0432,  0.0429,  0.0454,  0.0434,  0.0431,  0.0430,  0.0433,
         0.0413,  0.0432,  0.0613,  0.0430,  0.0433,  0.0431,  0.0434,  0.0433,
         0.0431,  0.0430,  0.0431,  0.0434,  0.0435,  0.0433,  0.0433,  0.0432,
         0.0392,  0.0430,  0.0430,  0.0430,  0.0430,  0.0457,  0.0426,  0.0765,
         0.0430,  0.0430,  0.0433,  0.0430,  0.0410,  0.0434,  0.0431,  0.0430,
         0.0430,  0.0430,  0.0430,  0.0430,  0.0396,  0.0430,  0.0427,  0.0440,
         0.0431,  0.0429,  0.0424,  0.0431,  0.0428,  0.0373,  0.0435,  0.0432,
         0.0429,  0.0433,  0.0468,  0.0431,  0.0433,  0.0433,  0.0509,  0.0435,
         0.0435,  0.0438,  0.0430,  0.0431,  0.0431,  0.0430,  0.0428,  0.0430,
         0.0430,  0.0431,  0.0431,  0.0433,  0.0430,  0.0432,  0.0435,  0.0431,
         0.0433,  0.0433,  0.0430,  0.0430,  0.0428,  0.0430,  0.0430,  0.0432,
         0.0430,  0.0432,  0.0440,  0.0383,  0.0432,  0.0430,  0.0426,  0.0493,
         0.0433,  0.0454,  0.0430,  0.0430,  0.0430,  0.0434,  0.0432,  0.0430,
         0.0430,  0.0428,  0.0431,  0.0403,  0.0435,  0.0432,  0.0432,  0.0434,
         0.0373,  0.0431,  0.0436,  0.0430,  0.0432,  0.0432,  0.0446,  0.0424,
         0.0432,  0.0430,  0.0572,  0.0431,  0.0430,  0.0430,  0.0431,  0.0430,
         0.0460,  0.0431,  0.0430,  0.0430,  0.0433,  0.0432,  0.0433,  0.0430,
         0.0430,  0.0430,  0.0430,  0.0431,  0.0431,  0.0472,  0.0430,  0.0433,
         0.0431,  0.0430,  0.0431,  0.0432,  0.0431,  0.0430,  0.0428,  0.0404,
         0.0430,  0.0435,  0.0442,  0.0418,  0.0430,  0.0431,  0.0438,  0.0426,
         0.0430,  0.0430,  0.0430,  0.0430,  0.0610,  0.0431,  0.0570,  0.0430,
         0.0447,  0.0430,  0.0432,  0.0461,  0.0429,  0.0430,  0.0429,  0.0490,
         0.0423,  0.0431,  0.0432,  0.0430,  0.0430,  0.0432,  0.0419,  0.0431,
         0.0430,  0.0428,  0.0426,  0.0430,  0.0430,  0.0430,  0.0431,  0.0859,
         0.0455,  0.0430,  0.0398,  0.0430,  0.0431,  0.0435,  0.0430,  0.0432,
         0.0430,  0.0432,  0.0430,  0.0430,  0.0427,  0.0433,  0.0430,  0.0431,
         0.0430,  0.0431,  0.0396,  0.0430,  0.0434,  0.0382,  0.0430,  0.0428,
         0.0431,  0.0430,  0.0432,  0.0430,  0.0431,  0.0432,  0.0432,  0.0426,
         0.0432,  0.0435,  0.0431,  0.0432,  0.0478,  0.0434,  0.0431,  0.0431,
         0.0429,  0.0447,  0.0430,  0.0430,  0.0430,  0.0432,  0.0432,  0.0544,
         0.0432,  0.0431,  0.0445,  0.0430,  0.0432,  0.0430,  0.0433,  0.0431,
         0.0431,  0.0418,  0.0429,  0.0432,  0.0432,  0.0431,  0.0430,  0.0432,
         0.0431,  0.0400,  0.0392,  0.0443,  0.0433,  0.0430,  0.0430,  0.0430,
         0.0435,  0.0430,  0.0431,  0.0430,  0.0430,  0.0430,  0.0430,  0.0430,
         0.0430,  0.0431,  0.0429,  0.0432,  0.0652,  0.0444,  0.0430,  0.0430,
         0.0430,  0.0462,  0.0430,  0.0430,  0.0434,  0.0430,  0.0479,  0.0423,
         0.0431,  0.0430,  0.0430,  0.0390,  0.0430,  0.0432,  0.0430,  0.0430,
         0.0430,  0.0430,  0.0431,  0.0385,  0.0592,  0.0430,  0.0432,  0.0430,
         0.0442,  0.0430,  0.0430,  0.0430,  0.0432,  0.0430,  0.0430,  0.0433,
         0.0430,  0.0448,  0.0433,  0.0432,  0.0430,  0.0430,  0.0430,  0.0430,
         0.0426,  0.0433,  0.0431,  0.0430,  0.0430,  0.0430,  0.0433,  0.0431,
         0.0430,  0.0435,  0.0430,  0.0430,  0.0430,  0.0430,  0.0417,  0.0432,
         0.0431,  0.0427,  0.0445,  0.0430,  0.0448,  0.0430,  0.0430,  0.0413,
         0.0432,  0.0419,  0.0430,  0.0434,  0.0430,  0.0422,  0.0436,  0.0468,
         0.0432,  0.0431,  0.0433,  0.0429,  0.0432,  0.0415,  0.0434,  0.0430,
         0.0159,  0.0433,  0.0431,  0.0459,  0.0432,  0.0430,  0.0431,  0.0430,
         0.0431,  0.0433,  0.0432,  0.0440,  0.0544,  0.0432,  0.0431,  0.0430,
         0.0394,  0.0432,  0.0405,  0.0379,  0.0430,  0.0433,  0.0428,  0.0433,
         0.0433,  0.0430,  0.0434,  0.0431,  0.0435,  0.0406,  0.0430,  0.0431,
         0.0430,  0.0369,  0.0432,  0.0430,  0.0430,  0.0432,  0.0428,  0.0453,
         0.0431,  0.0431,  0.0432,  0.0431,  0.0430,  0.0430,  0.0411,  0.0431,
         0.0431,  0.0432,  0.0273,  0.0430,  0.0430,  0.0430,  0.0432,  0.0431,
         0.0431,  0.0430,  0.0489,  0.0433,  0.0430,  0.0388,  0.0430,  0.0430,
         0.0431,  0.0430,  0.0430,  0.0432,  0.0428,  0.0431,  0.0430,  0.0430,
         0.0431,  0.0430,  0.0430,  0.0430,  0.0402,  0.0476,  0.0430,  0.0432,
         0.0432,  0.0433,  0.0450,  0.0463,  0.0430,  0.0430,  0.0433,  0.0439,
         0.0431,  0.0430,  0.0417,  0.0434,  0.0852,  0.0430,  0.0457,  0.0430,
         0.0425,  0.0431,  0.0431,  0.0433,  0.0430,  0.0370,  0.0432,  0.0430,
         0.0430,  0.0430,  0.0430,  0.0434,  0.0430,  0.0551,  0.0496,  0.0430,
         0.0430,  0.0430,  0.0430,  0.0430,  0.0430,  0.0436,  0.0432,  0.0432,
         0.0412, -0.0301,  0.0378,  0.0433,  0.0430,  0.0430,  0.0433,  0.0431,
         0.0430,  0.0433,  0.0431,  0.0430,  0.0431,  0.0450,  0.0430,  0.0629,
         0.0430,  0.0431,  0.0430,  0.0434,  0.0448,  0.0432,  0.0430,  0.0430,
         0.0434,  0.0430,  0.0430,  0.0432,  0.0431,  0.0432,  0.0434,  0.0563,
         0.0425,  0.0430,  0.0434,  0.0430,  0.0433,  0.0430,  0.0433,  0.0432,
         0.0432,  0.0431,  0.0430,  0.0430,  0.0443,  0.0432,  0.0430,  0.0603,
         0.0443,  0.0415,  0.0430,  0.0433,  0.0447,  0.0435,  0.0432,  0.0426,
         0.0501,  0.0432,  0.0431,  0.0433,  0.0430,  0.0432,  0.0430,  0.0432,
         0.0430,  0.0430,  0.0430,  0.0476,  0.0431,  0.0430,  0.0434,  0.0430],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(5829.9648, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3040, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3491.8069, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3667, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6726.0972, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0713, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7648.6406, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0127, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4174.3340, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8540, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6983.1006, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7422, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7249.4053, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9126, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5833.3257, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3291, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7426.0479, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8721, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8290.9365, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4355, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7157.3472, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(5350.0107, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0977, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7625.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5439, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 04:41:09,580][train_inner][INFO] - {"epoch": 9, "update": 8.864, "loss": "4.448", "ntokens": "149776", "nsentences": "541.34", "prob_perplexity": "64.661", "code_perplexity": "64.534", "temp": "1.956", "loss_0": "4.308", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.2876", "wps": "37787.5", "ups": "0.25", "wpb": "149776", "bsz": "541.3", "num_updates": "4600", "lr": "7.1875e-05", "gnorm": "0.603", "loss_scale": "4", "train_wall": "791", "gb_free": "13", "wall": "18536"}
[2023-09-12 04:45:12,481][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2023-09-12 04:45:44,359][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 04:45:44,360][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 04:45:44,474][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2023-09-12 04:46:08,326][valid][INFO] - {"epoch": 9, "valid_loss": "4.215", "valid_ntokens": "7865.9", "valid_nsentences": "55.2525", "valid_prob_perplexity": "63.804", "valid_code_perplexity": "63.703", "valid_temp": "1.954", "valid_loss_0": "4.076", "valid_loss_1": "0.13", "valid_loss_2": "0.01", "valid_accuracy": "0.33394", "valid_wps": "32832.3", "valid_wpb": "7865.9", "valid_bsz": "55.3", "valid_num_updates": "4670", "valid_best_loss": "4.215"}
[2023-09-12 04:46:08,328][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 4670 updates
[2023-09-12 04:46:08,329][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 04:46:10,851][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
tensor(-0.5635, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7184.5029, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9829, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4768.9141, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8867, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5101.7583, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2510, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7934.4692, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3125, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7097.5303, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2512, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4572.3320, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5122, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6969.5303, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9185, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7857.2466, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2100, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7012.3369, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2202, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7913.6509, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0469, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8226.0244, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8018, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6407.6987, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3381, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0665,  0.0550,  0.0524,  0.0752,  0.0502,  0.0537,  0.0537,  0.0538,
         0.0542,  0.0541,  0.0539,  0.0540,  0.0556,  0.0537,  0.0538,  0.0548,
         0.0541,  0.0540,  0.0476,  0.0537,  0.0538,  0.0556,  0.0539,  0.0671,
         0.0539,  0.0541,  0.0540,  0.0538,  0.0538,  0.0537,  0.0561,  0.0541,
         0.0527,  0.0538,  0.0540,  0.0539,  0.0533,  0.0537,  0.0540,  0.0540,
         0.0537,  0.0542,  0.0538,  0.0536,  0.0538,  0.0543,  0.0538,  0.0538,
         0.0540,  0.0544,  0.0538,  0.0642,  0.0540,  0.0537,  0.0538,  0.0534,
         0.0539,  0.0498,  0.0538,  0.0536,  0.0539,  0.0537,  0.0608,  0.0549,
         0.0822,  0.0539,  0.0538,  0.0543,  0.0538,  0.0537,  0.0537,  0.0539,
         0.0537,  0.0538,  0.0537,  0.0540,  0.0538,  0.0468,  0.0538,  0.0792,
         0.0537,  0.0540,  0.0615,  0.0539,  0.0818,  0.0540,  0.0538,  0.0537,
         0.0590,  0.0540,  0.0537,  0.0574,  0.0542,  0.0539,  0.0537,  0.0540,
         0.0526,  0.0540,  0.0778,  0.0538,  0.0541,  0.0539,  0.0541,  0.0540,
         0.0539,  0.0537,  0.0539,  0.0542,  0.0543,  0.0541,  0.0541,  0.0539,
         0.0499,  0.0539,  0.0538,  0.0537,  0.0537,  0.0566,  0.0534,  0.0919,
         0.0537,  0.0537,  0.0553,  0.0537,  0.0518,  0.0542,  0.0538,  0.0537,
         0.0538,  0.0538,  0.0538,  0.0537,  0.0504,  0.0538,  0.0534,  0.0588,
         0.0539,  0.0537,  0.0532,  0.0538,  0.0535,  0.0481,  0.0543,  0.0539,
         0.0536,  0.0540,  0.0581,  0.0539,  0.0541,  0.0540,  0.0671,  0.0544,
         0.0543,  0.0546,  0.0542,  0.0538,  0.0539,  0.0538,  0.0536,  0.0537,
         0.0538,  0.0539,  0.0539,  0.0541,  0.0537,  0.0540,  0.0543,  0.0539,
         0.0541,  0.0540,  0.0537,  0.0538,  0.0536,  0.0538,  0.0538,  0.0539,
         0.0538,  0.0540,  0.0547,  0.0491,  0.0539,  0.0538,  0.0534,  0.0606,
         0.0540,  0.0566,  0.0537,  0.0538,  0.0537,  0.0541,  0.0540,  0.0537,
         0.0538,  0.0536,  0.0538,  0.0511,  0.0554,  0.0539,  0.0540,  0.0542,
         0.0480,  0.0539,  0.0545,  0.0538,  0.0540,  0.0540,  0.0573,  0.0532,
         0.0540,  0.0537,  0.0679,  0.0539,  0.0538,  0.0538,  0.0539,  0.0537,
         0.0569,  0.0539,  0.0564,  0.0538,  0.0541,  0.0540,  0.0540,  0.0537,
         0.0538,  0.0538,  0.0537,  0.0538,  0.0539,  0.0594,  0.0538,  0.0541,
         0.0539,  0.0538,  0.0539,  0.0540,  0.0539,  0.0538,  0.0536,  0.0511,
         0.0538,  0.0550,  0.0551,  0.0526,  0.0537,  0.0538,  0.0547,  0.0533,
         0.0537,  0.0537,  0.0537,  0.0538,  0.0811,  0.0538,  0.0720,  0.0538,
         0.0558,  0.0538,  0.0540,  0.0569,  0.0536,  0.0537,  0.0537,  0.0598,
         0.0531,  0.0539,  0.0540,  0.0537,  0.0537,  0.0540,  0.0527,  0.0539,
         0.0537,  0.0536,  0.0534,  0.0537,  0.0537,  0.0538,  0.0539,  0.0875,
         0.0590,  0.0537,  0.0557,  0.0538,  0.0538,  0.0542,  0.0538,  0.0540,
         0.0537,  0.0540,  0.0537,  0.0537,  0.0535,  0.0540,  0.0538,  0.0539,
         0.0544,  0.0538,  0.0504,  0.0538,  0.0542,  0.0490,  0.0538,  0.0536,
         0.0538,  0.0538,  0.0540,  0.0537,  0.0539,  0.0540,  0.0539,  0.0534,
         0.0539,  0.0548,  0.0538,  0.0540,  0.0645,  0.0541,  0.0539,  0.0539,
         0.0537,  0.0555,  0.0537,  0.0537,  0.0538,  0.0540,  0.0539,  0.0654,
         0.0544,  0.0539,  0.0552,  0.0538,  0.0542,  0.0538,  0.0540,  0.0539,
         0.0539,  0.0542,  0.0537,  0.0539,  0.0542,  0.0539,  0.0538,  0.0542,
         0.0539,  0.0508,  0.0499,  0.0638,  0.0541,  0.0537,  0.0538,  0.0537,
         0.0543,  0.0537,  0.0538,  0.0538,  0.0538,  0.0537,  0.0537,  0.0537,
         0.0538,  0.0539,  0.0537,  0.0539,  0.0781,  0.0590,  0.0538,  0.0537,
         0.0537,  0.0570,  0.0538,  0.0537,  0.0547,  0.0538,  0.0673,  0.0531,
         0.0538,  0.0538,  0.0537,  0.0498,  0.0537,  0.0540,  0.0537,  0.0537,
         0.0537,  0.0538,  0.0538,  0.0493,  0.0811,  0.0537,  0.0540,  0.0537,
         0.0599,  0.0538,  0.0538,  0.0537,  0.0539,  0.0538,  0.0538,  0.0543,
         0.0537,  0.0588,  0.0541,  0.0539,  0.0538,  0.0538,  0.0537,  0.0538,
         0.0533,  0.0541,  0.0538,  0.0538,  0.0538,  0.0538,  0.0540,  0.0539,
         0.0537,  0.0580,  0.0538,  0.0538,  0.0537,  0.0537,  0.0524,  0.0539,
         0.0539,  0.0535,  0.0609,  0.0538,  0.0556,  0.0538,  0.0538,  0.0521,
         0.0539,  0.0527,  0.0538,  0.0541,  0.0539,  0.0530,  0.0544,  0.0598,
         0.0540,  0.0538,  0.0541,  0.0537,  0.0540,  0.0523,  0.0542,  0.0537,
         0.0050,  0.0541,  0.0538,  0.0568,  0.0539,  0.0538,  0.0539,  0.0537,
         0.0538,  0.0541,  0.0540,  0.0570,  0.0802,  0.0540,  0.0539,  0.0537,
         0.0502,  0.0539,  0.0513,  0.0487,  0.0538,  0.0541,  0.0536,  0.0541,
         0.0541,  0.0538,  0.0541,  0.0539,  0.0572,  0.0514,  0.0537,  0.0539,
         0.0538,  0.0477,  0.0539,  0.0538,  0.0538,  0.0540,  0.0537,  0.0560,
         0.0539,  0.0538,  0.0540,  0.0538,  0.0537,  0.0538,  0.0519,  0.0539,
         0.0538,  0.0542,  0.0385,  0.0538,  0.0537,  0.0538,  0.0540,  0.0538,
         0.0538,  0.0537,  0.0597,  0.0540,  0.0537,  0.0496,  0.0538,  0.0538,
         0.0538,  0.0537,  0.0538,  0.0540,  0.0535,  0.0539,  0.0538,  0.0538,
         0.0538,  0.0537,  0.0537,  0.0538,  0.0510,  0.0594,  0.0538,  0.0539,
         0.0539,  0.0541,  0.0558,  0.0640,  0.0538,  0.0538,  0.0541,  0.0546,
         0.0539,  0.0537,  0.0525,  0.0542,  0.0987,  0.0538,  0.0565,  0.0537,
         0.0533,  0.0538,  0.0539,  0.0540,  0.0538,  0.0477,  0.0540,  0.0538,
         0.0537,  0.0542,  0.0537,  0.0542,  0.0537,  0.0455,  0.0663,  0.0537,
         0.0537,  0.0537,  0.0537,  0.0537,  0.0537,  0.0544,  0.0540,  0.0540,
         0.0520, -0.0111,  0.0486,  0.0541,  0.0538,  0.0538,  0.0541,  0.0538,
         0.0538,  0.0541,  0.0539,  0.0538,  0.0539,  0.0557,  0.0538,  0.0752,
         0.0537,  0.0538,  0.0537,  0.0542,  0.0607,  0.0540,  0.0537,  0.0538,
         0.0542,  0.0537,  0.0538,  0.0540,  0.0540,  0.0539,  0.0541,  0.0698,
         0.0535,  0.0538,  0.0543,  0.0537,  0.0540,  0.0538,  0.0541,  0.0540,
         0.0540,  0.0539,  0.0538,  0.0537,  0.0551,  0.0540,  0.0537,  0.0756,
         0.0578,  0.0523,  0.0538,  0.0540,  0.0637,  0.0542,  0.0546,  0.0534,
         0.0682,  0.0539,  0.0538,  0.0541,  0.0538,  0.0540,  0.0538,  0.0539,
         0.0537,  0.0538,  0.0538,  0.0584,  0.0539,  0.0537,  0.0542,  0.0538],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(8568.6064, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9189, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8287.0156, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0645, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7837.7178, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2825, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8319.0703, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2238, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8112.8506, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6191, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8034.6265, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1582, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7893.2046, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0248, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6603.7412, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0317, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7085.9136, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0225, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7453.7988, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0923, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7071.0303, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: [2023-09-12 04:46:12,219][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 4670 updates, score 4.215) (writing took 3.8914694340201095 seconds)
[2023-09-12 04:46:12,220][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2023-09-12 04:46:12,220][train][INFO] - {"epoch": 9, "train_loss": "4.461", "train_ntokens": "149531", "train_nsentences": "538.375", "train_prob_perplexity": "64.244", "train_code_perplexity": "64.119", "train_temp": "1.956", "train_loss_0": "4.321", "train_loss_1": "0.13", "train_loss_2": "0.01", "train_accuracy": "0.28619", "train_wps": "37116.8", "train_ups": "0.25", "train_wpb": "149531", "train_bsz": "538.4", "train_num_updates": "4670", "train_lr": "7.29687e-05", "train_gnorm": "0.602", "train_loss_scale": "4", "train_train_wall": "2055", "train_gb_free": "13.4", "train_wall": "18839"}
[2023-09-12 04:46:12,223][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 04:46:12,312][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2023-09-12 04:46:12,558][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 04:46:12,561][fairseq.trainer][INFO] - begin training epoch 10
[2023-09-12 04:46:12,562][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(5950.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5630, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 04:54:49,724][train_inner][INFO] - {"epoch": 10, "update": 9.25, "loss": "4.432", "ntokens": "148960", "nsentences": "538.005", "prob_perplexity": "65.519", "code_perplexity": "65.388", "temp": "1.954", "loss_0": "4.293", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.28923", "wps": "36325.4", "ups": "0.24", "wpb": "148960", "bsz": "538", "num_updates": "4800", "lr": "7.5e-05", "gnorm": "0.607", "loss_scale": "4", "train_wall": "790", "gb_free": "13.3", "wall": "19356"}
Parameter containing:
tensor([ 0.0690,  0.0558,  0.0532,  0.0751,  0.0511,  0.0545,  0.0545,  0.0545,
         0.0550,  0.0549,  0.0547,  0.0548,  0.0565,  0.0545,  0.0546,  0.0557,
         0.0549,  0.0547,  0.0484,  0.0545,  0.0546,  0.0564,  0.0547,  0.0681,
         0.0547,  0.0549,  0.0548,  0.0546,  0.0546,  0.0545,  0.0569,  0.0549,
         0.0535,  0.0545,  0.0547,  0.0546,  0.0541,  0.0545,  0.0548,  0.0548,
         0.0545,  0.0550,  0.0546,  0.0544,  0.0545,  0.0551,  0.0545,  0.0545,
         0.0547,  0.0552,  0.0545,  0.0649,  0.0547,  0.0545,  0.0546,  0.0542,
         0.0547,  0.0506,  0.0546,  0.0544,  0.0547,  0.0545,  0.0616,  0.0561,
         0.0842,  0.0547,  0.0545,  0.0551,  0.0545,  0.0545,  0.0545,  0.0547,
         0.0545,  0.0546,  0.0544,  0.0548,  0.0546,  0.0475,  0.0546,  0.0799,
         0.0545,  0.0547,  0.0623,  0.0547,  0.0826,  0.0547,  0.0546,  0.0545,
         0.0600,  0.0547,  0.0545,  0.0583,  0.0549,  0.0546,  0.0545,  0.0548,
         0.0534,  0.0548,  0.0791,  0.0545,  0.0549,  0.0547,  0.0549,  0.0548,
         0.0546,  0.0545,  0.0547,  0.0549,  0.0551,  0.0549,  0.0549,  0.0547,
         0.0507,  0.0546,  0.0545,  0.0545,  0.0545,  0.0574,  0.0542,  0.0933,
         0.0545,  0.0545,  0.0561,  0.0545,  0.0526,  0.0550,  0.0546,  0.0545,
         0.0546,  0.0546,  0.0546,  0.0545,  0.0511,  0.0545,  0.0542,  0.0598,
         0.0547,  0.0545,  0.0540,  0.0546,  0.0543,  0.0489,  0.0551,  0.0547,
         0.0544,  0.0548,  0.0590,  0.0547,  0.0549,  0.0548,  0.0693,  0.0551,
         0.0551,  0.0554,  0.0550,  0.0546,  0.0546,  0.0546,  0.0544,  0.0545,
         0.0545,  0.0547,  0.0546,  0.0548,  0.0545,  0.0548,  0.0551,  0.0547,
         0.0549,  0.0548,  0.0545,  0.0546,  0.0544,  0.0546,  0.0546,  0.0547,
         0.0546,  0.0547,  0.0555,  0.0498,  0.0547,  0.0546,  0.0542,  0.0614,
         0.0548,  0.0574,  0.0545,  0.0545,  0.0545,  0.0549,  0.0548,  0.0545,
         0.0545,  0.0544,  0.0546,  0.0519,  0.0562,  0.0547,  0.0548,  0.0550,
         0.0488,  0.0547,  0.0553,  0.0546,  0.0548,  0.0547,  0.0581,  0.0540,
         0.0547,  0.0545,  0.0688,  0.0547,  0.0546,  0.0545,  0.0547,  0.0545,
         0.0577,  0.0547,  0.0576,  0.0545,  0.0548,  0.0548,  0.0548,  0.0545,
         0.0546,  0.0546,  0.0545,  0.0546,  0.0547,  0.0602,  0.0546,  0.0549,
         0.0547,  0.0546,  0.0547,  0.0548,  0.0547,  0.0546,  0.0544,  0.0519,
         0.0546,  0.0558,  0.0558,  0.0533,  0.0545,  0.0546,  0.0555,  0.0541,
         0.0545,  0.0545,  0.0545,  0.0545,  0.0829,  0.0546,  0.0737,  0.0545,
         0.0566,  0.0546,  0.0548,  0.0577,  0.0544,  0.0545,  0.0545,  0.0606,
         0.0539,  0.0546,  0.0547,  0.0545,  0.0545,  0.0547,  0.0535,  0.0547,
         0.0545,  0.0544,  0.0541,  0.0545,  0.0545,  0.0546,  0.0546,  0.0861,
         0.0601,  0.0545,  0.0574,  0.0545,  0.0546,  0.0550,  0.0546,  0.0547,
         0.0545,  0.0547,  0.0545,  0.0545,  0.0543,  0.0548,  0.0546,  0.0546,
         0.0553,  0.0546,  0.0511,  0.0545,  0.0549,  0.0498,  0.0546,  0.0544,
         0.0546,  0.0546,  0.0548,  0.0545,  0.0547,  0.0547,  0.0547,  0.0542,
         0.0547,  0.0559,  0.0546,  0.0548,  0.0660,  0.0549,  0.0547,  0.0547,
         0.0544,  0.0563,  0.0545,  0.0545,  0.0545,  0.0548,  0.0547,  0.0662,
         0.0552,  0.0547,  0.0560,  0.0545,  0.0550,  0.0546,  0.0548,  0.0547,
         0.0547,  0.0552,  0.0545,  0.0547,  0.0550,  0.0547,  0.0545,  0.0550,
         0.0546,  0.0516,  0.0507,  0.0648,  0.0549,  0.0545,  0.0546,  0.0545,
         0.0550,  0.0545,  0.0546,  0.0546,  0.0546,  0.0545,  0.0545,  0.0545,
         0.0546,  0.0546,  0.0544,  0.0547,  0.0793,  0.0599,  0.0546,  0.0545,
         0.0545,  0.0578,  0.0545,  0.0545,  0.0555,  0.0545,  0.0686,  0.0539,
         0.0546,  0.0546,  0.0545,  0.0506,  0.0545,  0.0548,  0.0545,  0.0545,
         0.0545,  0.0546,  0.0546,  0.0500,  0.0823,  0.0545,  0.0548,  0.0545,
         0.0608,  0.0545,  0.0545,  0.0545,  0.0547,  0.0546,  0.0546,  0.0551,
         0.0545,  0.0598,  0.0549,  0.0547,  0.0546,  0.0546,  0.0545,  0.0546,
         0.0541,  0.0549,  0.0546,  0.0546,  0.0546,  0.0546,  0.0548,  0.0546,
         0.0545,  0.0589,  0.0546,  0.0546,  0.0545,  0.0545,  0.0532,  0.0547,
         0.0547,  0.0542,  0.0620,  0.0546,  0.0563,  0.0546,  0.0545,  0.0528,
         0.0547,  0.0534,  0.0546,  0.0549,  0.0546,  0.0537,  0.0552,  0.0611,
         0.0547,  0.0546,  0.0549,  0.0545,  0.0547,  0.0531,  0.0550,  0.0545,
         0.0050,  0.0549,  0.0546,  0.0576,  0.0547,  0.0545,  0.0547,  0.0545,
         0.0546,  0.0549,  0.0548,  0.0582,  0.0825,  0.0548,  0.0547,  0.0545,
         0.0509,  0.0547,  0.0520,  0.0494,  0.0545,  0.0549,  0.0544,  0.0548,
         0.0548,  0.0545,  0.0549,  0.0547,  0.0581,  0.0522,  0.0545,  0.0546,
         0.0545,  0.0485,  0.0547,  0.0545,  0.0546,  0.0547,  0.0545,  0.0568,
         0.0546,  0.0546,  0.0548,  0.0546,  0.0545,  0.0545,  0.0526,  0.0547,
         0.0546,  0.0550,  0.0399,  0.0546,  0.0545,  0.0545,  0.0548,  0.0546,
         0.0546,  0.0545,  0.0605,  0.0548,  0.0545,  0.0504,  0.0546,  0.0547,
         0.0546,  0.0545,  0.0545,  0.0547,  0.0543,  0.0546,  0.0545,  0.0546,
         0.0546,  0.0545,  0.0545,  0.0546,  0.0518,  0.0602,  0.0545,  0.0547,
         0.0547,  0.0549,  0.0565,  0.0652,  0.0546,  0.0545,  0.0549,  0.0554,
         0.0547,  0.0545,  0.0533,  0.0550,  0.0987,  0.0546,  0.0573,  0.0545,
         0.0541,  0.0546,  0.0547,  0.0548,  0.0545,  0.0485,  0.0547,  0.0545,
         0.0545,  0.0550,  0.0545,  0.0549,  0.0545,  0.0449,  0.0676,  0.0545,
         0.0545,  0.0545,  0.0545,  0.0545,  0.0545,  0.0551,  0.0547,  0.0548,
         0.0528, -0.0095,  0.0493,  0.0548,  0.0546,  0.0546,  0.0549,  0.0546,
         0.0545,  0.0549,  0.0547,  0.0546,  0.0546,  0.0565,  0.0546,  0.0759,
         0.0545,  0.0546,  0.0545,  0.0549,  0.0622,  0.0547,  0.0545,  0.0545,
         0.0550,  0.0545,  0.0546,  0.0548,  0.0547,  0.0547,  0.0549,  0.0709,
         0.0543,  0.0545,  0.0551,  0.0545,  0.0548,  0.0546,  0.0549,  0.0548,
         0.0548,  0.0547,  0.0546,  0.0545,  0.0558,  0.0548,  0.0545,  0.0769,
         0.0586,  0.0531,  0.0546,  0.0548,  0.0646,  0.0550,  0.0560,  0.0542,
         0.0693,  0.0547,  0.0546,  0.0549,  0.0546,  0.0547,  0.0546,  0.0547,
         0.0545,  0.0546,  0.0545,  0.0592,  0.0546,  0.0545,  0.0549,  0.0545],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7783.2705, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6929, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7453.1826, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6621, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9184.8164, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4336, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7175.9233, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9175, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3313.3650, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1816, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7159.0874, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0321, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5778.2261, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5908, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7006.7520, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6099, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4457.5132, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0604, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8594.7109, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0779, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7423.2217, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(6930.6826, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2957, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5645.9263, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1257, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 05:00:07,162][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
loss: tensor(6482.1470, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6460, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5214.7529, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7676, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7701.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9585, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 05:08:05,350][train_inner][INFO] - {"epoch": 10, "update": 9.635, "loss": "4.404", "ntokens": "149756", "nsentences": "537.59", "prob_perplexity": "67.154", "code_perplexity": "67.005", "temp": "1.952", "loss_0": "4.265", "loss_1": "0.129", "loss_2": "0.01", "accuracy": "0.29138", "wps": "37644.9", "ups": "0.25", "wpb": "149756", "bsz": "537.6", "num_updates": "5000", "lr": "7.8125e-05", "gnorm": "0.589", "loss_scale": "2", "train_wall": "794", "gb_free": "12.9", "wall": "20152"}
tensor(-0.8906, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7902.7729, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9590, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7728.4717, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9360, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7836.1846, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0724, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7010.4565, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5581, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7372.1025, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3079, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6475.9878, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8105, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1108, 0.0789, 0.0784, 0.0914, 0.0800, 0.0774, 0.0774, 0.0774, 0.0778,
        0.0778, 0.0776, 0.0777, 0.0809, 0.0774, 0.0775, 0.0774, 0.0778, 0.0776,
        0.0713, 0.0774, 0.0775, 0.0803, 0.0776, 0.0953, 0.0779, 0.0778, 0.0777,
        0.0775, 0.0775, 0.0775, 0.0821, 0.0778, 0.0764, 0.0774, 0.0776, 0.0775,
        0.0770, 0.0775, 0.0777, 0.0777, 0.0774, 0.0778, 0.0774, 0.0772, 0.0774,
        0.0795, 0.0774, 0.0774, 0.0776, 0.0781, 0.0774, 0.0897, 0.0776, 0.0773,
        0.0775, 0.0771, 0.0781, 0.0735, 0.0775, 0.0773, 0.0776, 0.0774, 0.0975,
        0.0822, 0.1206, 0.0776, 0.0774, 0.0779, 0.0774, 0.0774, 0.0774, 0.0775,
        0.0774, 0.0775, 0.0773, 0.0777, 0.0775, 0.0704, 0.0775, 0.1057, 0.0774,
        0.0776, 0.0873, 0.0776, 0.1106, 0.0776, 0.0774, 0.0774, 0.0890, 0.0776,
        0.0774, 0.0830, 0.0778, 0.0775, 0.0774, 0.0777, 0.0844, 0.0780, 0.1170,
        0.0774, 0.0778, 0.0775, 0.0778, 0.0777, 0.0776, 0.0774, 0.0775, 0.0778,
        0.0780, 0.0808, 0.0778, 0.0776, 0.0735, 0.0792, 0.0774, 0.0774, 0.0774,
        0.0808, 0.0771, 0.1223, 0.0774, 0.0774, 0.0818, 0.0774, 0.0754, 0.0778,
        0.0779, 0.0774, 0.0779, 0.0775, 0.0775, 0.0774, 0.0740, 0.0774, 0.0771,
        0.0859, 0.0776, 0.0774, 0.0768, 0.0775, 0.0771, 0.0718, 0.0779, 0.0776,
        0.0773, 0.0777, 0.0916, 0.0776, 0.0777, 0.0777, 0.1088, 0.0781, 0.0779,
        0.0782, 0.0814, 0.0775, 0.0775, 0.0775, 0.0773, 0.0774, 0.0774, 0.0775,
        0.0775, 0.0777, 0.0775, 0.0777, 0.0779, 0.0775, 0.0778, 0.0777, 0.0774,
        0.0775, 0.0773, 0.0776, 0.0775, 0.0776, 0.0775, 0.0776, 0.0784, 0.0727,
        0.0776, 0.0775, 0.0770, 0.0856, 0.0777, 0.0806, 0.0799, 0.0774, 0.0774,
        0.0778, 0.0776, 0.0774, 0.0774, 0.0776, 0.0775, 0.0748, 0.0798, 0.0776,
        0.0777, 0.0779, 0.0717, 0.0775, 0.0782, 0.0774, 0.0776, 0.0776, 0.0819,
        0.0768, 0.0776, 0.0774, 0.0924, 0.0781, 0.0775, 0.0776, 0.0776, 0.0774,
        0.0812, 0.0775, 0.0927, 0.0774, 0.0777, 0.0776, 0.0777, 0.0774, 0.0775,
        0.0774, 0.0783, 0.0775, 0.0775, 0.0842, 0.0811, 0.0778, 0.0775, 0.0775,
        0.0775, 0.0777, 0.0781, 0.0775, 0.0773, 0.0748, 0.0774, 0.0801, 0.0787,
        0.0762, 0.0774, 0.0775, 0.0784, 0.0770, 0.0774, 0.0774, 0.0774, 0.0774,
        0.1257, 0.0775, 0.1039, 0.0774, 0.0795, 0.0775, 0.0776, 0.0806, 0.0773,
        0.0774, 0.0773, 0.0836, 0.0770, 0.0775, 0.0776, 0.0774, 0.0774, 0.0776,
        0.0764, 0.0775, 0.0774, 0.0773, 0.0770, 0.0774, 0.0774, 0.0775, 0.0775,
        0.0367, 0.0848, 0.0774, 0.0977, 0.0774, 0.0775, 0.0779, 0.0775, 0.0776,
        0.0774, 0.0776, 0.0774, 0.0774, 0.0771, 0.0777, 0.0775, 0.0775, 0.0842,
        0.0775, 0.0740, 0.0774, 0.0778, 0.0726, 0.0775, 0.0773, 0.0775, 0.0774,
        0.0776, 0.0774, 0.0776, 0.0776, 0.0776, 0.0771, 0.0776, 0.0833, 0.0775,
        0.0776, 0.1013, 0.0778, 0.0775, 0.0775, 0.0773, 0.0792, 0.0774, 0.0774,
        0.0774, 0.0777, 0.0776, 0.0891, 0.0823, 0.0775, 0.0789, 0.0774, 0.0844,
        0.0803, 0.0777, 0.0776, 0.0775, 0.0839, 0.0773, 0.0776, 0.0803, 0.0775,
        0.0774, 0.0782, 0.0775, 0.0745, 0.0737, 0.0947, 0.0778, 0.0774, 0.0775,
        0.0774, 0.0779, 0.0774, 0.0775, 0.0775, 0.0775, 0.0774, 0.0774, 0.0774,
        0.0775, 0.0775, 0.0773, 0.0776, 0.1041, 0.0858, 0.0774, 0.0774, 0.0774,
        0.0806, 0.0774, 0.0774, 0.0840, 0.0774, 0.1088, 0.0767, 0.0775, 0.0775,
        0.0774, 0.0735, 0.0774, 0.0776, 0.0774, 0.0774, 0.0774, 0.0775, 0.0775,
        0.0729, 0.1146, 0.0774, 0.0777, 0.0774, 0.0880, 0.0774, 0.0774, 0.0774,
        0.0776, 0.0775, 0.0775, 0.0815, 0.0774, 0.0845, 0.0778, 0.0779, 0.0775,
        0.0823, 0.0774, 0.0775, 0.0770, 0.0778, 0.0775, 0.0775, 0.0775, 0.0775,
        0.0777, 0.0775, 0.0774, 0.0865, 0.0775, 0.0775, 0.0774, 0.0774, 0.0760,
        0.0776, 0.0776, 0.0771, 0.0912, 0.0775, 0.0792, 0.0775, 0.0774, 0.0757,
        0.0776, 0.0767, 0.0775, 0.0778, 0.0780, 0.0766, 0.0794, 0.0860, 0.0776,
        0.0775, 0.0781, 0.0773, 0.0795, 0.0760, 0.0778, 0.0774, 0.0405, 0.0778,
        0.0775, 0.0809, 0.0776, 0.0774, 0.0775, 0.0774, 0.0775, 0.0778, 0.0776,
        0.0864, 0.1235, 0.0776, 0.0775, 0.0774, 0.0738, 0.0776, 0.0749, 0.0723,
        0.0774, 0.0779, 0.0773, 0.0777, 0.0777, 0.0774, 0.0778, 0.0775, 0.0908,
        0.0751, 0.0774, 0.0776, 0.0774, 0.0714, 0.0776, 0.0774, 0.0775, 0.0776,
        0.0795, 0.0797, 0.0775, 0.0775, 0.0776, 0.0775, 0.0774, 0.0774, 0.0755,
        0.0775, 0.0775, 0.0781, 0.0806, 0.0775, 0.0774, 0.0774, 0.0777, 0.0775,
        0.0775, 0.0774, 0.0834, 0.0777, 0.0774, 0.0732, 0.0774, 0.0815, 0.0844,
        0.0774, 0.0774, 0.0776, 0.0771, 0.0775, 0.0774, 0.0775, 0.0775, 0.0774,
        0.0774, 0.0775, 0.0746, 0.0839, 0.0774, 0.0776, 0.0776, 0.0778, 0.0797,
        0.0981, 0.0778, 0.0787, 0.0778, 0.0782, 0.0775, 0.0774, 0.0765, 0.0778,
        0.1189, 0.0775, 0.0806, 0.0774, 0.0770, 0.0775, 0.0775, 0.0777, 0.0774,
        0.0714, 0.0776, 0.0774, 0.0774, 0.0797, 0.0774, 0.0778, 0.0774, 0.0530,
        0.0986, 0.0774, 0.0774, 0.0774, 0.0774, 0.0774, 0.0774, 0.0780, 0.0776,
        0.0776, 0.0757, 0.0236, 0.0723, 0.0777, 0.0797, 0.0775, 0.0778, 0.0775,
        0.0774, 0.0778, 0.0775, 0.0775, 0.0788, 0.0793, 0.0775, 0.0997, 0.0774,
        0.0775, 0.0774, 0.0778, 0.1004, 0.0776, 0.0774, 0.0774, 0.0778, 0.0774,
        0.0775, 0.0776, 0.0782, 0.0776, 0.0778, 0.0956, 0.0772, 0.0774, 0.0784,
        0.0774, 0.0777, 0.0775, 0.0778, 0.0777, 0.0777, 0.0775, 0.0775, 0.0774,
        0.0814, 0.0776, 0.0774, 0.1038, 0.0855, 0.0759, 0.0790, 0.0777, 0.0971,
        0.0779, 0.0848, 0.0770, 0.1077, 0.0776, 0.0775, 0.0778, 0.0775, 0.0776,
        0.0775, 0.0776, 0.0774, 0.0775, 0.0774, 0.0820, 0.0775, 0.0774, 0.0778,
        0.0774], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(8370.2256, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2185, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8223.1592, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4883, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7897.8711, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5312, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7157.8096, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2251, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6031.8936, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0955, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7380.6670, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5156, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 05:20:31,709][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 05:20:31,710][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 05:20:31,821][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
Parameter containing:
tensor([0.1170, 0.0839, 0.0844, 0.0920, 0.0864, 0.0824, 0.0824, 0.0826, 0.0828,
        0.0828, 0.0826, 0.0828, 0.0861, 0.0824, 0.0825, 0.0831, 0.0828, 0.0826,
        0.0763, 0.0824, 0.0825, 0.0854, 0.0826, 0.1020, 0.0830, 0.0828, 0.0827,
        0.0825, 0.0825, 0.0828, 0.0883, 0.0828, 0.0814, 0.0824, 0.0826, 0.0825,
        0.0820, 0.0825, 0.0827, 0.0827, 0.0824, 0.0828, 0.0824, 0.0822, 0.0824,
        0.0850, 0.0824, 0.0824, 0.0826, 0.0831, 0.0824, 0.0948, 0.0826, 0.0823,
        0.0825, 0.0821, 0.0837, 0.0785, 0.0825, 0.0823, 0.0826, 0.0824, 0.1029,
        0.0873, 0.1294, 0.0828, 0.0824, 0.0829, 0.0824, 0.0824, 0.0824, 0.0825,
        0.0824, 0.0825, 0.0823, 0.0827, 0.0825, 0.0754, 0.0825, 0.1113, 0.0824,
        0.0826, 0.0925, 0.0826, 0.1153, 0.0826, 0.0824, 0.0824, 0.0942, 0.0826,
        0.0824, 0.0881, 0.0828, 0.0825, 0.0824, 0.0827, 0.0912, 0.0830, 0.1241,
        0.0824, 0.0828, 0.0825, 0.0828, 0.0827, 0.0826, 0.0824, 0.0825, 0.0828,
        0.0832, 0.0874, 0.0828, 0.0826, 0.0786, 0.0851, 0.0824, 0.0824, 0.0824,
        0.0859, 0.0821, 0.1268, 0.0824, 0.0824, 0.0877, 0.0824, 0.0804, 0.0828,
        0.0839, 0.0824, 0.0831, 0.0825, 0.0825, 0.0824, 0.0790, 0.0824, 0.0821,
        0.0930, 0.0826, 0.0824, 0.0818, 0.0825, 0.0822, 0.0768, 0.0829, 0.0826,
        0.0823, 0.0827, 0.0972, 0.0826, 0.0827, 0.0827, 0.1179, 0.0832, 0.0829,
        0.0833, 0.0872, 0.0825, 0.0825, 0.0825, 0.0823, 0.0824, 0.0824, 0.0825,
        0.0825, 0.0827, 0.0825, 0.0827, 0.0829, 0.0825, 0.0828, 0.0827, 0.0824,
        0.0825, 0.0823, 0.0826, 0.0825, 0.0826, 0.0825, 0.0826, 0.0834, 0.0777,
        0.0826, 0.0825, 0.0820, 0.0908, 0.0827, 0.0856, 0.0861, 0.0824, 0.0824,
        0.0828, 0.0826, 0.0824, 0.0824, 0.0826, 0.0825, 0.0799, 0.0849, 0.0826,
        0.0827, 0.0829, 0.0767, 0.0825, 0.0833, 0.0824, 0.0826, 0.0826, 0.0869,
        0.0818, 0.0826, 0.0824, 0.0975, 0.0833, 0.0825, 0.0828, 0.0825, 0.0824,
        0.0864, 0.0825, 0.1007, 0.0824, 0.0827, 0.0826, 0.0827, 0.0824, 0.0825,
        0.0824, 0.0833, 0.0825, 0.0825, 0.0894, 0.0865, 0.0828, 0.0825, 0.0825,
        0.0825, 0.0827, 0.0834, 0.0825, 0.0823, 0.0798, 0.0824, 0.0853, 0.0837,
        0.0812, 0.0824, 0.0825, 0.0834, 0.0820, 0.0824, 0.0824, 0.0824, 0.0824,
        0.1324, 0.0825, 0.1095, 0.0824, 0.0845, 0.0825, 0.0826, 0.0856, 0.0823,
        0.0824, 0.0823, 0.0886, 0.0822, 0.0825, 0.0826, 0.0824, 0.0824, 0.0826,
        0.0814, 0.0825, 0.0824, 0.0823, 0.0820, 0.0824, 0.0824, 0.0825, 0.0825,
        0.0277, 0.0905, 0.0824, 0.1059, 0.0824, 0.0825, 0.0829, 0.0825, 0.0826,
        0.0824, 0.0826, 0.0824, 0.0824, 0.0822, 0.0827, 0.0825, 0.0825, 0.0914,
        0.0825, 0.0790, 0.0824, 0.0828, 0.0776, 0.0825, 0.0823, 0.0825, 0.0824,
        0.0826, 0.0824, 0.0826, 0.0826, 0.0826, 0.0820, 0.0826, 0.0892, 0.0825,
        0.0826, 0.1058, 0.0828, 0.0825, 0.0825, 0.0823, 0.0842, 0.0824, 0.0824,
        0.0824, 0.0827, 0.0826, 0.0941, 0.0891, 0.0826, 0.0839, 0.0824, 0.0905,
        0.0879, 0.0828, 0.0826, 0.0825, 0.0904, 0.0823, 0.0826, 0.0856, 0.0825,
        0.0824, 0.0832, 0.0825, 0.0795, 0.0787, 0.1015, 0.0828, 0.0824, 0.0826,
        0.0824, 0.0829, 0.0824, 0.0825, 0.0825, 0.0825, 0.0824, 0.0824, 0.0824,
        0.0825, 0.0825, 0.0823, 0.0826, 0.1100, 0.0908, 0.0824, 0.0824, 0.0824,
        0.0856, 0.0824, 0.0824, 0.0914, 0.0824, 0.1165, 0.0817, 0.0825, 0.0825,
        0.0824, 0.0785, 0.0824, 0.0826, 0.0824, 0.0824, 0.0824, 0.0825, 0.0825,
        0.0779, 0.1207, 0.0824, 0.0828, 0.0824, 0.0928, 0.0824, 0.0824, 0.0824,
        0.0826, 0.0825, 0.0825, 0.0878, 0.0824, 0.0897, 0.0828, 0.0834, 0.0825,
        0.0886, 0.0824, 0.0825, 0.0820, 0.0828, 0.0825, 0.0825, 0.0825, 0.0825,
        0.0827, 0.0825, 0.0824, 0.0918, 0.0825, 0.0825, 0.0824, 0.0824, 0.0811,
        0.0826, 0.0826, 0.0821, 0.0970, 0.0825, 0.0842, 0.0825, 0.0824, 0.0807,
        0.0826, 0.0820, 0.0824, 0.0828, 0.0831, 0.0816, 0.0844, 0.0909, 0.0826,
        0.0825, 0.0832, 0.0823, 0.0866, 0.0810, 0.0828, 0.0824, 0.0477, 0.0828,
        0.0825, 0.0862, 0.0826, 0.0824, 0.0825, 0.0824, 0.0825, 0.0828, 0.0826,
        0.0921, 0.1311, 0.0826, 0.0825, 0.0824, 0.0788, 0.0826, 0.0799, 0.0773,
        0.0824, 0.0830, 0.0823, 0.0827, 0.0827, 0.0824, 0.0828, 0.0825, 0.0999,
        0.0801, 0.0824, 0.0830, 0.0824, 0.0764, 0.0826, 0.0824, 0.0825, 0.0826,
        0.0853, 0.0847, 0.0825, 0.0825, 0.0826, 0.0825, 0.0824, 0.0824, 0.0805,
        0.0825, 0.0825, 0.0831, 0.0875, 0.0825, 0.0824, 0.0824, 0.0827, 0.0825,
        0.0825, 0.0824, 0.0884, 0.0827, 0.0824, 0.0782, 0.0824, 0.0876, 0.0917,
        0.0824, 0.0824, 0.0826, 0.0822, 0.0825, 0.0824, 0.0825, 0.0825, 0.0824,
        0.0824, 0.0825, 0.0797, 0.0889, 0.0824, 0.0826, 0.0826, 0.0828, 0.0852,
        0.1042, 0.0828, 0.0841, 0.0828, 0.0833, 0.0825, 0.0824, 0.0815, 0.0828,
        0.1235, 0.0825, 0.0862, 0.0824, 0.0820, 0.0825, 0.0825, 0.0827, 0.0824,
        0.0764, 0.0826, 0.0824, 0.0824, 0.0839, 0.0824, 0.0828, 0.0824, 0.0599,
        0.1043, 0.0824, 0.0824, 0.0824, 0.0824, 0.0824, 0.0824, 0.0830, 0.0826,
        0.0826, 0.0807, 0.0284, 0.0773, 0.0827, 0.0863, 0.0825, 0.0828, 0.0825,
        0.0824, 0.0827, 0.0825, 0.0825, 0.0849, 0.0844, 0.0825, 0.1047, 0.0824,
        0.0825, 0.0824, 0.0828, 0.1078, 0.0826, 0.0824, 0.0824, 0.0828, 0.0824,
        0.0825, 0.0826, 0.0836, 0.0826, 0.0828, 0.1002, 0.0822, 0.0824, 0.0834,
        0.0824, 0.0827, 0.0825, 0.0828, 0.0827, 0.0827, 0.0825, 0.0825, 0.0824,
        0.0885, 0.0826, 0.0824, 0.1103, 0.0906, 0.0809, 0.0850, 0.0827, 0.1025,
        0.0829, 0.0901, 0.0820, 0.1097, 0.0826, 0.0825, 0.0828, 0.0825, 0.0826,
        0.0825, 0.0826, 0.0824, 0.0825, 0.0824, 0.0870, 0.0825, 0.0824, 0.0828,
        0.0824], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7976.4570, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4983, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: [2023-09-12 05:20:55,397][valid][INFO] - {"epoch": 10, "valid_loss": "4.219", "valid_ntokens": "7884.04", "valid_nsentences": "55.2525", "valid_prob_perplexity": "67.881", "valid_code_perplexity": "67.735", "valid_temp": "1.949", "valid_loss_0": "4.08", "valid_loss_1": "0.129", "valid_loss_2": "0.01", "valid_accuracy": "0.33424", "valid_wps": "33308.4", "valid_wpb": "7884", "valid_bsz": "55.3", "valid_num_updates": "5190", "valid_best_loss": "4.215"}
[2023-09-12 05:20:55,400][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 5190 updates
[2023-09-12 05:20:55,402][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_last.pt
[2023-09-12 05:20:57,867][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_last.pt
[2023-09-12 05:20:57,915][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 10 @ 5190 updates, score 4.219) (writing took 2.5157657959498465 seconds)
[2023-09-12 05:20:57,916][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2023-09-12 05:20:57,917][train][INFO] - {"epoch": 10, "train_loss": "4.403", "train_ntokens": "149458", "train_nsentences": "538.35", "train_prob_perplexity": "67.476", "train_code_perplexity": "67.328", "train_temp": "1.951", "train_loss_0": "4.264", "train_loss_1": "0.129", "train_loss_2": "0.01", "train_accuracy": "0.29147", "train_wps": "37262.4", "train_ups": "0.25", "train_wpb": "149458", "train_bsz": "538.4", "train_num_updates": "5190", "train_lr": "8.10938e-05", "train_gnorm": "0.591", "train_loss_scale": "4", "train_train_wall": "2056", "train_gb_free": "13.4", "train_wall": "20924"}
[2023-09-12 05:20:57,919][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 05:20:58,021][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2023-09-12 05:20:58,280][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 05:20:58,283][fairseq.trainer][INFO] - begin training epoch 11
[2023-09-12 05:20:58,284][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 05:21:37,668][train_inner][INFO] - {"epoch": 11, "update": 10.019, "loss": "4.39", "ntokens": "149012", "nsentences": "535.89", "prob_perplexity": "69.071", "code_perplexity": "68.91", "temp": "1.95", "loss_0": "4.252", "loss_1": "0.129", "loss_2": "0.01", "accuracy": "0.29193", "wps": "36688.1", "ups": "0.25", "wpb": "149012", "bsz": "535.9", "num_updates": "5200", "lr": "8.125e-05", "gnorm": "0.613", "loss_scale": "4", "train_wall": "784", "gb_free": "12.9", "wall": "20964"}
loss: tensor(6959.2695, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1359, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8195.7529, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1768, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 05:34:52,451][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2023-09-12 05:34:56,104][train_inner][INFO] - {"epoch": 11, "update": 10.405, "loss": "4.412", "ntokens": "149815", "nsentences": "540.8", "prob_perplexity": "70.564", "code_perplexity": "70.377", "temp": "1.948", "loss_0": "4.273", "loss_1": "0.128", "loss_2": "0.01", "accuracy": "0.28605", "wps": "37527", "ups": "0.25", "wpb": "149815", "bsz": "540.8", "num_updates": "5400", "lr": "8.4375e-05", "gnorm": "0.627", "loss_scale": "4", "train_wall": "797", "gb_free": "12.9", "wall": "21763"}
Parameter containing:
tensor([0.1175, 0.0842, 0.0848, 0.0918, 0.0865, 0.0826, 0.0826, 0.0829, 0.0831,
        0.0831, 0.0828, 0.0831, 0.0864, 0.0827, 0.0828, 0.0834, 0.0830, 0.0829,
        0.0766, 0.0826, 0.0827, 0.0858, 0.0828, 0.1024, 0.0833, 0.0831, 0.0829,
        0.0828, 0.0827, 0.0831, 0.0886, 0.0831, 0.0816, 0.0827, 0.0829, 0.0828,
        0.0823, 0.0828, 0.0829, 0.0829, 0.0826, 0.0831, 0.0827, 0.0825, 0.0827,
        0.0853, 0.0827, 0.0827, 0.0829, 0.0833, 0.0827, 0.0951, 0.0829, 0.0826,
        0.0827, 0.0824, 0.0840, 0.0788, 0.0827, 0.0825, 0.0828, 0.0827, 0.1033,
        0.0876, 0.1298, 0.0830, 0.0827, 0.0832, 0.0827, 0.0826, 0.0826, 0.0828,
        0.0827, 0.0828, 0.0826, 0.0830, 0.0827, 0.0757, 0.0828, 0.1116, 0.0826,
        0.0829, 0.0928, 0.0828, 0.1156, 0.0829, 0.0827, 0.0827, 0.0945, 0.0829,
        0.0826, 0.0884, 0.0831, 0.0828, 0.0826, 0.0830, 0.0916, 0.0833, 0.1246,
        0.0827, 0.0830, 0.0828, 0.0831, 0.0830, 0.0829, 0.0826, 0.0828, 0.0831,
        0.0834, 0.0878, 0.0831, 0.0829, 0.0789, 0.0854, 0.0827, 0.0826, 0.0826,
        0.0862, 0.0823, 0.1272, 0.0826, 0.0826, 0.0880, 0.0827, 0.0807, 0.0831,
        0.0842, 0.0827, 0.0834, 0.0827, 0.0828, 0.0827, 0.0793, 0.0827, 0.0823,
        0.0936, 0.0828, 0.0826, 0.0822, 0.0828, 0.0825, 0.0770, 0.0832, 0.0828,
        0.0826, 0.0829, 0.0974, 0.0829, 0.0830, 0.0830, 0.1185, 0.0835, 0.0833,
        0.0835, 0.0876, 0.0828, 0.0828, 0.0827, 0.0825, 0.0826, 0.0827, 0.0828,
        0.0828, 0.0830, 0.0828, 0.0829, 0.0832, 0.0828, 0.0830, 0.0829, 0.0827,
        0.0827, 0.0825, 0.0829, 0.0828, 0.0828, 0.0827, 0.0829, 0.0837, 0.0780,
        0.0828, 0.0827, 0.0823, 0.0911, 0.0829, 0.0859, 0.0865, 0.0827, 0.0827,
        0.0831, 0.0829, 0.0827, 0.0827, 0.0829, 0.0828, 0.0802, 0.0851, 0.0829,
        0.0829, 0.0831, 0.0770, 0.0828, 0.0836, 0.0827, 0.0829, 0.0829, 0.0872,
        0.0821, 0.0829, 0.0827, 0.0978, 0.0836, 0.0828, 0.0831, 0.0828, 0.0827,
        0.0867, 0.0828, 0.1012, 0.0827, 0.0830, 0.0829, 0.0830, 0.0826, 0.0828,
        0.0827, 0.0836, 0.0828, 0.0828, 0.0897, 0.0869, 0.0831, 0.0828, 0.0827,
        0.0828, 0.0829, 0.0837, 0.0827, 0.0825, 0.0801, 0.0827, 0.0856, 0.0840,
        0.0815, 0.0826, 0.0828, 0.0837, 0.0823, 0.0826, 0.0826, 0.0826, 0.0827,
        0.1328, 0.0828, 0.1098, 0.0827, 0.0848, 0.0827, 0.0829, 0.0859, 0.0826,
        0.0827, 0.0826, 0.0889, 0.0825, 0.0828, 0.0829, 0.0826, 0.0826, 0.0829,
        0.0816, 0.0828, 0.0827, 0.0825, 0.0823, 0.0826, 0.0827, 0.0828, 0.0828,
        0.0273, 0.0908, 0.0826, 0.1065, 0.0827, 0.0828, 0.0832, 0.0827, 0.0829,
        0.0827, 0.0829, 0.0826, 0.0827, 0.0824, 0.0829, 0.0827, 0.0828, 0.0917,
        0.0828, 0.0793, 0.0827, 0.0831, 0.0779, 0.0828, 0.0826, 0.0828, 0.0827,
        0.0829, 0.0827, 0.0828, 0.0829, 0.0829, 0.0823, 0.0828, 0.0895, 0.0828,
        0.0829, 0.1061, 0.0831, 0.0828, 0.0828, 0.0826, 0.0845, 0.0826, 0.0826,
        0.0827, 0.0829, 0.0829, 0.0944, 0.0897, 0.0829, 0.0842, 0.0827, 0.0910,
        0.0884, 0.0830, 0.0828, 0.0828, 0.0908, 0.0826, 0.0829, 0.0859, 0.0828,
        0.0827, 0.0834, 0.0828, 0.0797, 0.0790, 0.1018, 0.0830, 0.0826, 0.0829,
        0.0827, 0.0832, 0.0827, 0.0828, 0.0828, 0.0827, 0.0826, 0.0827, 0.0826,
        0.0827, 0.0828, 0.0826, 0.0828, 0.1102, 0.0912, 0.0827, 0.0827, 0.0826,
        0.0859, 0.0827, 0.0827, 0.0917, 0.0827, 0.1168, 0.0820, 0.0828, 0.0827,
        0.0827, 0.0787, 0.0826, 0.0829, 0.0826, 0.0827, 0.0826, 0.0827, 0.0828,
        0.0782, 0.1210, 0.0827, 0.0830, 0.0826, 0.0931, 0.0827, 0.0827, 0.0827,
        0.0828, 0.0827, 0.0827, 0.0880, 0.0827, 0.0900, 0.0830, 0.0837, 0.0827,
        0.0888, 0.0826, 0.0827, 0.0823, 0.0830, 0.0828, 0.0828, 0.0827, 0.0827,
        0.0829, 0.0828, 0.0827, 0.0921, 0.0827, 0.0828, 0.0827, 0.0826, 0.0814,
        0.0828, 0.0829, 0.0824, 0.0973, 0.0827, 0.0845, 0.0827, 0.0827, 0.0810,
        0.0828, 0.0824, 0.0827, 0.0831, 0.0833, 0.0819, 0.0847, 0.0912, 0.0829,
        0.0828, 0.0834, 0.0826, 0.0872, 0.0813, 0.0831, 0.0827, 0.0480, 0.0830,
        0.0828, 0.0865, 0.0829, 0.0827, 0.0828, 0.0827, 0.0828, 0.0830, 0.0829,
        0.0927, 0.1315, 0.0829, 0.0828, 0.0827, 0.0791, 0.0829, 0.0802, 0.0776,
        0.0827, 0.0833, 0.0826, 0.0830, 0.0830, 0.0827, 0.0831, 0.0828, 0.1000,
        0.0804, 0.0826, 0.0833, 0.0827, 0.0766, 0.0829, 0.0827, 0.0827, 0.0829,
        0.0858, 0.0850, 0.0828, 0.0828, 0.0829, 0.0828, 0.0826, 0.0827, 0.0808,
        0.0828, 0.0828, 0.0833, 0.0877, 0.0828, 0.0827, 0.0827, 0.0829, 0.0828,
        0.0828, 0.0827, 0.0886, 0.0830, 0.0826, 0.0785, 0.0827, 0.0881, 0.0922,
        0.0827, 0.0827, 0.0829, 0.0825, 0.0828, 0.0827, 0.0827, 0.0828, 0.0826,
        0.0827, 0.0827, 0.0800, 0.0892, 0.0827, 0.0828, 0.0828, 0.0830, 0.0854,
        0.1044, 0.0831, 0.0844, 0.0830, 0.0836, 0.0828, 0.0827, 0.0818, 0.0831,
        0.1239, 0.0827, 0.0865, 0.0827, 0.0823, 0.0828, 0.0828, 0.0829, 0.0827,
        0.0767, 0.0829, 0.0827, 0.0827, 0.0842, 0.0827, 0.0831, 0.0826, 0.0603,
        0.1046, 0.0827, 0.0826, 0.0826, 0.0826, 0.0826, 0.0827, 0.0833, 0.0829,
        0.0829, 0.0809, 0.0286, 0.0776, 0.0830, 0.0866, 0.0828, 0.0831, 0.0828,
        0.0827, 0.0830, 0.0828, 0.0827, 0.0853, 0.0846, 0.0828, 0.1050, 0.0826,
        0.0828, 0.0827, 0.0831, 0.1083, 0.0829, 0.0827, 0.0827, 0.0831, 0.0827,
        0.0827, 0.0829, 0.0839, 0.0829, 0.0831, 0.1003, 0.0825, 0.0827, 0.0837,
        0.0826, 0.0830, 0.0827, 0.0830, 0.0829, 0.0829, 0.0828, 0.0827, 0.0827,
        0.0887, 0.0829, 0.0826, 0.1107, 0.0909, 0.0812, 0.0853, 0.0829, 0.1030,
        0.0831, 0.0905, 0.0823, 0.1095, 0.0829, 0.0828, 0.0830, 0.0828, 0.0829,
        0.0828, 0.0829, 0.0827, 0.0827, 0.0827, 0.0873, 0.0828, 0.0827, 0.0831,
        0.0827], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6549.4004, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7148, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7764.4062, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1320, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7635.4746, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1094, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5543.0176, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8013, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7812.6343, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8125, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0.1378, 0.0983, 0.1049, 0.0875, 0.1034, 0.0967, 0.0968, 0.0975, 0.0972,
        0.0972, 0.0969, 0.0976, 0.1011, 0.0968, 0.0969, 0.1025, 0.0971, 0.0970,
        0.0908, 0.0968, 0.0968, 0.0999, 0.0969, 0.1136, 0.0977, 0.0972, 0.0970,
        0.0969, 0.0969, 0.0992, 0.1015, 0.0972, 0.0957, 0.0968, 0.0970, 0.0969,
        0.0964, 0.0970, 0.0970, 0.0970, 0.0968, 0.0972, 0.0968, 0.0966, 0.0968,
        0.1028, 0.0968, 0.0968, 0.0970, 0.0974, 0.0968, 0.1099, 0.0970, 0.0967,
        0.0969, 0.0965, 0.0991, 0.0929, 0.0968, 0.0966, 0.0969, 0.0968, 0.1183,
        0.1021, 0.1498, 0.0978, 0.0968, 0.0974, 0.0968, 0.0968, 0.0968, 0.0969,
        0.0968, 0.0969, 0.0967, 0.0971, 0.0968, 0.0899, 0.0969, 0.1271, 0.0968,
        0.0970, 0.1071, 0.0969, 0.1284, 0.0972, 0.0968, 0.0968, 0.1098, 0.0970,
        0.0967, 0.1041, 0.0972, 0.0969, 0.0968, 0.0971, 0.1083, 0.0975, 0.1412,
        0.0968, 0.0971, 0.0969, 0.0972, 0.0971, 0.0970, 0.0967, 0.0969, 0.0972,
        0.0977, 0.1045, 0.0972, 0.0970, 0.0930, 0.1016, 0.0968, 0.0968, 0.0967,
        0.1003, 0.0964, 0.1368, 0.0968, 0.0968, 0.1038, 0.0968, 0.0949, 0.0972,
        0.1000, 0.0968, 0.0986, 0.0969, 0.0969, 0.0968, 0.0934, 0.0968, 0.0964,
        0.1104, 0.0969, 0.0969, 0.0963, 0.0969, 0.0966, 0.0911, 0.0973, 0.0970,
        0.0967, 0.0971, 0.1148, 0.0972, 0.0971, 0.0971, 0.1399, 0.0975, 0.0974,
        0.0977, 0.1058, 0.0969, 0.0970, 0.0969, 0.0966, 0.0967, 0.0968, 0.0969,
        0.0969, 0.0971, 0.0966, 0.0971, 0.0973, 0.0969, 0.0971, 0.0970, 0.0968,
        0.0969, 0.0967, 0.0972, 0.0969, 0.0969, 0.0969, 0.0970, 0.0978, 0.0921,
        0.0970, 0.0968, 0.0964, 0.1051, 0.0970, 0.1002, 0.1050, 0.0968, 0.0968,
        0.0972, 0.0970, 0.0968, 0.0968, 0.0980, 0.0969, 0.0952, 0.0993, 0.0970,
        0.0971, 0.0972, 0.0911, 0.0969, 0.0977, 0.0968, 0.0970, 0.0970, 0.1014,
        0.0962, 0.0970, 0.0970, 0.1119, 0.0983, 0.0969, 0.0989, 0.0969, 0.0968,
        0.1012, 0.0969, 0.1223, 0.0968, 0.0971, 0.0970, 0.0971, 0.0968, 0.0969,
        0.0968, 0.0988, 0.0969, 0.0969, 0.1042, 0.1005, 0.0972, 0.0969, 0.0968,
        0.0969, 0.0970, 0.0986, 0.0968, 0.0967, 0.0942, 0.0968, 0.0998, 0.0981,
        0.0956, 0.0968, 0.0969, 0.0978, 0.0964, 0.0968, 0.0968, 0.0968, 0.0968,
        0.1471, 0.0969, 0.1234, 0.0968, 0.0989, 0.0969, 0.0970, 0.1001, 0.0967,
        0.0968, 0.0968, 0.1031, 0.0968, 0.0969, 0.0970, 0.0968, 0.0968, 0.0970,
        0.0958, 0.0969, 0.0968, 0.0966, 0.0964, 0.0967, 0.0968, 0.0969, 0.0969,
        0.0044, 0.1027, 0.0967, 0.1274, 0.0968, 0.0969, 0.0973, 0.0969, 0.0970,
        0.0968, 0.0970, 0.0967, 0.0968, 0.0965, 0.0971, 0.0968, 0.0969, 0.1086,
        0.0969, 0.0934, 0.0968, 0.0972, 0.0920, 0.0969, 0.0967, 0.0969, 0.0968,
        0.0970, 0.0968, 0.0969, 0.0970, 0.0972, 0.0964, 0.0969, 0.0999, 0.0969,
        0.0970, 0.1246, 0.0972, 0.0969, 0.0969, 0.0967, 0.0987, 0.0967, 0.0967,
        0.0968, 0.0970, 0.0970, 0.1085, 0.1080, 0.0970, 0.0983, 0.0968, 0.1072,
        0.1091, 0.0972, 0.0969, 0.0969, 0.1077, 0.0967, 0.0970, 0.1004, 0.0969,
        0.0968, 0.0978, 0.0969, 0.0938, 0.0937, 0.1171, 0.0972, 0.0968, 0.0979,
        0.0968, 0.0973, 0.0968, 0.0969, 0.0969, 0.0969, 0.0968, 0.0968, 0.0967,
        0.0969, 0.0969, 0.0967, 0.0970, 0.1249, 0.1057, 0.0968, 0.0968, 0.0967,
        0.1000, 0.0968, 0.0968, 0.1118, 0.0968, 0.1339, 0.0961, 0.0969, 0.0969,
        0.0968, 0.0928, 0.0968, 0.0970, 0.0968, 0.0968, 0.0968, 0.0968, 0.0969,
        0.0923, 0.1388, 0.0968, 0.0971, 0.0968, 0.1079, 0.0968, 0.0968, 0.0968,
        0.0970, 0.0968, 0.0968, 0.1028, 0.0968, 0.1047, 0.0972, 0.0999, 0.0969,
        0.1052, 0.0967, 0.0968, 0.0964, 0.0971, 0.0969, 0.0969, 0.0968, 0.0968,
        0.0970, 0.0969, 0.0968, 0.1071, 0.0968, 0.0969, 0.0972, 0.0967, 0.0955,
        0.0969, 0.0970, 0.0965, 0.1124, 0.0969, 0.0986, 0.0968, 0.0968, 0.0951,
        0.0969, 0.0982, 0.0968, 0.0972, 0.0981, 0.0960, 0.0994, 0.1054, 0.0970,
        0.0969, 0.0979, 0.0967, 0.1028, 0.0956, 0.0972, 0.0968, 0.0668, 0.0971,
        0.0969, 0.1008, 0.0970, 0.0968, 0.0969, 0.0968, 0.0969, 0.0971, 0.0970,
        0.1100, 0.1523, 0.0970, 0.0969, 0.0968, 0.0932, 0.0970, 0.0944, 0.0918,
        0.0968, 0.0978, 0.0967, 0.0971, 0.0971, 0.0968, 0.0972, 0.0969, 0.1193,
        0.0945, 0.0967, 0.0994, 0.0968, 0.0907, 0.0970, 0.0968, 0.0969, 0.0970,
        0.1024, 0.0991, 0.0969, 0.0969, 0.0970, 0.0969, 0.0968, 0.0968, 0.0949,
        0.0969, 0.0969, 0.0976, 0.1038, 0.0969, 0.0968, 0.0968, 0.0970, 0.0969,
        0.0969, 0.0968, 0.1028, 0.0971, 0.0968, 0.0926, 0.0968, 0.1041, 0.1064,
        0.0968, 0.0968, 0.0970, 0.0966, 0.0969, 0.0968, 0.0968, 0.0969, 0.0968,
        0.0968, 0.0969, 0.0941, 0.1033, 0.0968, 0.0969, 0.0970, 0.0971, 0.1008,
        0.1190, 0.0974, 0.0995, 0.0971, 0.0977, 0.0969, 0.0968, 0.0964, 0.0972,
        0.1370, 0.0968, 0.0988, 0.0968, 0.0964, 0.0969, 0.0969, 0.0970, 0.0968,
        0.0908, 0.0970, 0.0968, 0.0968, 0.0988, 0.0968, 0.0972, 0.0967, 0.0754,
        0.1212, 0.0968, 0.0968, 0.0967, 0.0967, 0.0967, 0.0968, 0.0974, 0.0970,
        0.0970, 0.0950, 0.0465, 0.0917, 0.0971, 0.1046, 0.0969, 0.0972, 0.0969,
        0.0968, 0.0971, 0.0969, 0.0968, 0.1024, 0.0987, 0.0969, 0.1191, 0.0968,
        0.0969, 0.0968, 0.0972, 0.1196, 0.0970, 0.0968, 0.0968, 0.0972, 0.0968,
        0.0968, 0.0970, 0.0984, 0.0970, 0.0972, 0.1116, 0.0966, 0.0968, 0.0984,
        0.0968, 0.0971, 0.0968, 0.0971, 0.0970, 0.0970, 0.0969, 0.0968, 0.0968,
        0.1077, 0.0970, 0.0968, 0.1248, 0.1059, 0.0953, 0.1002, 0.0970, 0.1151,
        0.0972, 0.1027, 0.0964, 0.1103, 0.0970, 0.0969, 0.0972, 0.0969, 0.0970,
        0.0969, 0.0970, 0.0968, 0.0968, 0.0968, 0.1014, 0.0969, 0.0968, 0.0972,
        0.0968], device='cuda:0', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([1.4282e-01, 1.0175e-01, 1.0950e-01, 8.5999e-02, 1.0809e-01, 1.0022e-01,
        1.0022e-01, 1.0095e-01, 1.0071e-01, 1.0065e-01, 1.0040e-01, 1.0132e-01,
        1.0455e-01, 1.0028e-01, 1.0034e-01, 1.0718e-01, 1.0059e-01, 1.0046e-01,
        9.4299e-02, 1.0028e-01, 1.0028e-01, 1.0339e-01, 1.0040e-01, 1.1670e-01,
        1.0114e-01, 1.0065e-01, 1.0052e-01, 1.0034e-01, 1.0034e-01, 1.0480e-01,
        1.0498e-01, 1.0065e-01, 9.9182e-02, 1.0028e-01, 1.0046e-01, 1.0034e-01,
        9.9854e-02, 1.0040e-01, 1.0052e-01, 1.0052e-01, 1.0028e-01, 1.0071e-01,
        1.0028e-01, 1.0010e-01, 1.0028e-01, 1.0504e-01, 1.0028e-01, 1.0028e-01,
        1.0046e-01, 1.0089e-01, 1.0028e-01, 1.1334e-01, 1.0052e-01, 1.0022e-01,
        1.0028e-01, 9.9976e-02, 1.0291e-01, 9.6375e-02, 1.0028e-01, 1.0010e-01,
        1.0040e-01, 1.0028e-01, 1.2152e-01, 1.0553e-01, 1.5466e-01, 1.0138e-01,
        1.0028e-01, 1.0077e-01, 1.0028e-01, 1.0028e-01, 1.0028e-01, 1.0040e-01,
        1.0028e-01, 1.0034e-01, 1.0016e-01, 1.0059e-01, 1.0028e-01, 9.3384e-02,
        1.0034e-01, 1.3037e-01, 1.0028e-01, 1.0046e-01, 1.1072e-01, 1.0040e-01,
        1.3196e-01, 1.0077e-01, 1.0028e-01, 1.0028e-01, 1.1383e-01, 1.0046e-01,
        1.0022e-01, 1.0797e-01, 1.0065e-01, 1.0034e-01, 1.0028e-01, 1.0059e-01,
        1.1139e-01, 1.0095e-01, 1.4600e-01, 1.0028e-01, 1.0059e-01, 1.0040e-01,
        1.0065e-01, 1.0059e-01, 1.0046e-01, 1.0022e-01, 1.0040e-01, 1.0071e-01,
        1.0126e-01, 1.0876e-01, 1.0065e-01, 1.0046e-01, 9.6436e-02, 1.0687e-01,
        1.0028e-01, 1.0028e-01, 1.0022e-01, 1.0382e-01, 9.9915e-02, 1.4026e-01,
        1.0022e-01, 1.0022e-01, 1.0687e-01, 1.0028e-01, 9.8389e-02, 1.0071e-01,
        1.0431e-01, 1.0028e-01, 1.0223e-01, 1.0034e-01, 1.0034e-01, 1.0028e-01,
        9.6863e-02, 1.0028e-01, 9.9915e-02, 1.1310e-01, 1.0040e-01, 1.0034e-01,
        9.9731e-02, 1.0046e-01, 1.0004e-01, 9.4604e-02, 1.0077e-01, 1.0046e-01,
        1.0016e-01, 1.0059e-01, 1.2048e-01, 1.0065e-01, 1.0059e-01, 1.0059e-01,
        1.4539e-01, 1.0101e-01, 1.0083e-01, 1.0114e-01, 1.1047e-01, 1.0034e-01,
        1.0046e-01, 1.0034e-01, 1.0010e-01, 1.0022e-01, 1.0028e-01, 1.0040e-01,
        1.0034e-01, 1.0059e-01, 1.0065e-01, 1.0059e-01, 1.0077e-01, 1.0040e-01,
        1.0059e-01, 1.0052e-01, 1.0028e-01, 1.0028e-01, 1.0010e-01, 1.0065e-01,
        1.0034e-01, 1.0040e-01, 1.0028e-01, 1.0046e-01, 1.0126e-01, 9.5581e-02,
        1.0040e-01, 1.0028e-01, 9.9915e-02, 1.0858e-01, 1.0052e-01, 1.0370e-01,
        1.1029e-01, 1.0028e-01, 1.0028e-01, 1.0065e-01, 1.0052e-01, 1.0028e-01,
        1.0028e-01, 1.0162e-01, 1.0034e-01, 9.8755e-02, 1.0278e-01, 1.0046e-01,
        1.0052e-01, 1.0071e-01, 9.4543e-02, 1.0040e-01, 1.0114e-01, 1.0028e-01,
        1.0052e-01, 1.0046e-01, 1.0486e-01, 9.9670e-02, 1.0046e-01, 1.0059e-01,
        1.1548e-01, 1.0272e-01, 1.0040e-01, 1.0303e-01, 1.0040e-01, 1.0028e-01,
        1.0474e-01, 1.0040e-01, 1.2817e-01, 1.0028e-01, 1.0059e-01, 1.0052e-01,
        1.0059e-01, 1.0028e-01, 1.0034e-01, 1.0028e-01, 1.0278e-01, 1.0034e-01,
        1.0040e-01, 1.0773e-01, 1.0388e-01, 1.0065e-01, 1.0040e-01, 1.0028e-01,
        1.0040e-01, 1.0052e-01, 1.0229e-01, 1.0028e-01, 1.0016e-01, 9.7656e-02,
        1.0028e-01, 1.0327e-01, 1.0156e-01, 9.9060e-02, 1.0022e-01, 1.0034e-01,
        1.0132e-01, 9.9854e-02, 1.0022e-01, 1.0028e-01, 1.0028e-01, 1.0028e-01,
        1.5088e-01, 1.0034e-01, 1.2646e-01, 1.0028e-01, 1.0242e-01, 1.0028e-01,
        1.0052e-01, 1.0358e-01, 1.0016e-01, 1.0028e-01, 1.0028e-01, 1.0657e-01,
        1.0034e-01, 1.0034e-01, 1.0052e-01, 1.0022e-01, 1.0022e-01, 1.0046e-01,
        9.9243e-02, 1.0040e-01, 1.0028e-01, 1.0010e-01, 9.9854e-02, 1.0022e-01,
        1.0028e-01, 1.0034e-01, 1.0040e-01, 2.9862e-05, 1.0651e-01, 1.0022e-01,
        1.3306e-01, 1.0028e-01, 1.0034e-01, 1.0077e-01, 1.0028e-01, 1.0046e-01,
        1.0028e-01, 1.0046e-01, 1.0022e-01, 1.0028e-01, 9.9976e-02, 1.0052e-01,
        1.0028e-01, 1.0034e-01, 1.1218e-01, 1.0034e-01, 9.6863e-02, 1.0028e-01,
        1.0065e-01, 9.5520e-02, 1.0034e-01, 1.0016e-01, 1.0034e-01, 1.0028e-01,
        1.0052e-01, 1.0028e-01, 1.0040e-01, 1.0046e-01, 1.0071e-01, 9.9915e-02,
        1.0040e-01, 1.0388e-01, 1.0034e-01, 1.0052e-01, 1.2878e-01, 1.0065e-01,
        1.0040e-01, 1.0040e-01, 1.0022e-01, 1.0236e-01, 1.0022e-01, 1.0022e-01,
        1.0028e-01, 1.0052e-01, 1.0046e-01, 1.1194e-01, 1.1414e-01, 1.0046e-01,
        1.0175e-01, 1.0028e-01, 1.0986e-01, 1.1505e-01, 1.0083e-01, 1.0040e-01,
        1.0040e-01, 1.1292e-01, 1.0022e-01, 1.0046e-01, 1.0425e-01, 1.0040e-01,
        1.0028e-01, 1.0132e-01, 1.0034e-01, 9.7290e-02, 9.7229e-02, 1.2054e-01,
        1.0065e-01, 1.0022e-01, 1.0120e-01, 1.0028e-01, 1.0077e-01, 1.0028e-01,
        1.0034e-01, 1.0034e-01, 1.0028e-01, 1.0028e-01, 1.0028e-01, 1.0022e-01,
        1.0065e-01, 1.0040e-01, 1.0022e-01, 1.0046e-01, 1.2891e-01, 1.0944e-01,
        1.0028e-01, 1.0028e-01, 1.0022e-01, 1.0352e-01, 1.0028e-01, 1.0028e-01,
        1.1572e-01, 1.0028e-01, 1.3330e-01, 9.9609e-02, 1.0040e-01, 1.0028e-01,
        1.0028e-01, 9.6313e-02, 1.0022e-01, 1.0052e-01, 1.0028e-01, 1.0028e-01,
        1.0022e-01, 1.0028e-01, 1.0034e-01, 9.5764e-02, 1.4343e-01, 1.0028e-01,
        1.0059e-01, 1.0022e-01, 1.1267e-01, 1.0028e-01, 1.0028e-01, 1.0028e-01,
        1.0040e-01, 1.0028e-01, 1.0028e-01, 1.0565e-01, 1.0028e-01, 1.0815e-01,
        1.0065e-01, 1.0358e-01, 1.0028e-01, 1.0956e-01, 1.0022e-01, 1.0028e-01,
        9.9854e-02, 1.0059e-01, 1.0034e-01, 1.0034e-01, 1.0028e-01, 1.0028e-01,
        1.0052e-01, 1.0034e-01, 1.0028e-01, 1.1108e-01, 1.0028e-01, 1.0034e-01,
        1.0071e-01, 1.0022e-01, 9.8938e-02, 1.0040e-01, 1.0046e-01, 9.9976e-02,
        1.1591e-01, 1.0028e-01, 1.0205e-01, 1.0028e-01, 1.0028e-01, 9.8572e-02,
        1.0040e-01, 1.0345e-01, 1.0028e-01, 1.0065e-01, 1.0175e-01, 9.9487e-02,
        1.0297e-01, 1.0889e-01, 1.0052e-01, 1.0034e-01, 1.0162e-01, 1.0022e-01,
        1.0785e-01, 9.9304e-02, 1.0071e-01, 1.0028e-01, 7.1045e-02, 1.0059e-01,
        1.0034e-01, 1.0437e-01, 1.0046e-01, 1.0028e-01, 1.0040e-01, 1.0028e-01,
        1.0034e-01, 1.0059e-01, 1.0052e-01, 1.1426e-01, 1.5857e-01, 1.0052e-01,
        1.0040e-01, 1.0028e-01, 9.6680e-02, 1.0046e-01, 9.7839e-02, 9.5276e-02,
        1.0028e-01, 1.0156e-01, 1.0016e-01, 1.0059e-01, 1.0059e-01, 1.0028e-01,
        1.0065e-01, 1.0040e-01, 1.2225e-01, 9.7961e-02, 1.0022e-01, 1.0455e-01,
        1.0028e-01, 9.4177e-02, 1.0046e-01, 1.0028e-01, 1.0034e-01, 1.0046e-01,
        1.0529e-01, 1.0254e-01, 1.0040e-01, 1.0034e-01, 1.0052e-01, 1.0034e-01,
        1.0028e-01, 1.0028e-01, 9.8389e-02, 1.0040e-01, 1.0034e-01, 1.0107e-01,
        1.0858e-01, 1.0034e-01, 1.0028e-01, 1.0028e-01, 1.0052e-01, 1.0034e-01,
        1.0034e-01, 1.0028e-01, 1.0626e-01, 1.0059e-01, 1.0022e-01, 9.6069e-02,
        1.0028e-01, 1.0760e-01, 1.1041e-01, 1.0028e-01, 1.0028e-01, 1.0046e-01,
        1.0004e-01, 1.0040e-01, 1.0028e-01, 1.0028e-01, 1.0034e-01, 1.0022e-01,
        1.0028e-01, 1.0034e-01, 9.7534e-02, 1.0681e-01, 1.0028e-01, 1.0040e-01,
        1.0046e-01, 1.0059e-01, 1.0449e-01, 1.2262e-01, 1.0114e-01, 1.0327e-01,
        1.0059e-01, 1.0114e-01, 1.0040e-01, 1.0028e-01, 1.0004e-01, 1.0071e-01,
        1.4087e-01, 1.0028e-01, 1.0260e-01, 1.0028e-01, 9.9854e-02, 1.0034e-01,
        1.0040e-01, 1.0052e-01, 1.0028e-01, 9.4299e-02, 1.0046e-01, 1.0028e-01,
        1.0028e-01, 1.0217e-01, 1.0028e-01, 1.0065e-01, 1.0022e-01, 8.0139e-02,
        1.2360e-01, 1.0028e-01, 1.0028e-01, 1.0022e-01, 1.0022e-01, 1.0022e-01,
        1.0028e-01, 1.0089e-01, 1.0046e-01, 1.0052e-01, 9.8511e-02, 5.0751e-02,
        9.5215e-02, 1.0059e-01, 1.0889e-01, 1.0034e-01, 1.0065e-01, 1.0034e-01,
        1.0028e-01, 1.0059e-01, 1.0040e-01, 1.0028e-01, 1.0663e-01, 1.0217e-01,
        1.0034e-01, 1.2262e-01, 1.0022e-01, 1.0034e-01, 1.0028e-01, 1.0065e-01,
        1.2408e-01, 1.0046e-01, 1.0028e-01, 1.0028e-01, 1.0071e-01, 1.0028e-01,
        1.0028e-01, 1.0052e-01, 1.0254e-01, 1.0046e-01, 1.0065e-01, 1.1517e-01,
        1.0004e-01, 1.0028e-01, 1.0211e-01, 1.0022e-01, 1.0059e-01, 1.0028e-01,
        1.0059e-01, 1.0052e-01, 1.0052e-01, 1.0040e-01, 1.0028e-01, 1.0028e-01,
        1.1200e-01, 1.0052e-01, 1.0022e-01, 1.2915e-01, 1.0974e-01, 9.8816e-02,
        1.0431e-01, 1.0052e-01, 1.1932e-01, 1.0071e-01, 1.0657e-01, 9.9915e-02,
        1.0632e-01, 1.0046e-01, 1.0034e-01, 1.0065e-01, 1.0034e-01, 1.0046e-01,
        1.0034e-01, 1.0046e-01, 1.0028e-01, 1.0028e-01, 1.0028e-01, 1.0492e-01,
        1.0034e-01, 1.0028e-01, 1.0065e-01, 1.0028e-01], device='cuda:0',
       dtype=torch.float16, requires_grad=True)
[2023-09-12 05:48:09,512][train_inner][INFO] - {"epoch": 11, "update": 10.789, "loss": "4.391", "ntokens": "149744", "nsentences": "538.755", "prob_perplexity": "71.714", "code_perplexity": "71.488", "temp": "1.946", "loss_0": "4.253", "loss_1": "0.128", "loss_2": "0.011", "accuracy": "0.28675", "wps": "37747.2", "ups": "0.25", "wpb": "149744", "bsz": "538.8", "num_updates": "5600", "lr": "8.75e-05", "gnorm": "0.573", "loss_scale": "4", "train_wall": "792", "gb_free": "12.5", "wall": "22556"}
tensor(-0.1531, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7327.7427, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1190, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7420.2964, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3472, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6818.2090, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6636, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1138, 0.0808, 0.0812, 0.0922, 0.0823, 0.0793, 0.0793, 0.0794, 0.0798,
        0.0797, 0.0795, 0.0797, 0.0829, 0.0793, 0.0794, 0.0799, 0.0797, 0.0795,
        0.0732, 0.0793, 0.0793, 0.0823, 0.0795, 0.0977, 0.0798, 0.0797, 0.0796,
        0.0794, 0.0793, 0.0794, 0.0847, 0.0797, 0.0782, 0.0793, 0.0795, 0.0794,
        0.0789, 0.0793, 0.0796, 0.0796, 0.0793, 0.0798, 0.0793, 0.0792, 0.0793,
        0.0817, 0.0793, 0.0793, 0.0795, 0.0800, 0.0793, 0.0917, 0.0796, 0.0793,
        0.0793, 0.0790, 0.0804, 0.0754, 0.0793, 0.0792, 0.0795, 0.0793, 0.0998,
        0.0841, 0.1245, 0.0795, 0.0793, 0.0798, 0.0793, 0.0793, 0.0793, 0.0795,
        0.0793, 0.0794, 0.0792, 0.0797, 0.0793, 0.0723, 0.0794, 0.1077, 0.0793,
        0.0795, 0.0892, 0.0795, 0.1120, 0.0796, 0.0793, 0.0793, 0.0909, 0.0795,
        0.0793, 0.0851, 0.0797, 0.0794, 0.0793, 0.0797, 0.0874, 0.0799, 0.1194,
        0.0793, 0.0797, 0.0795, 0.0797, 0.0796, 0.0795, 0.0793, 0.0795, 0.0798,
        0.0801, 0.0836, 0.0797, 0.0795, 0.0755, 0.0815, 0.0793, 0.0793, 0.0793,
        0.0828, 0.0790, 0.1241, 0.0793, 0.0793, 0.0839, 0.0793, 0.0774, 0.0798,
        0.0805, 0.0793, 0.0799, 0.0793, 0.0794, 0.0793, 0.0759, 0.0793, 0.0790,
        0.0886, 0.0795, 0.0793, 0.0788, 0.0794, 0.0791, 0.0737, 0.0798, 0.0795,
        0.0792, 0.0797, 0.0941, 0.0796, 0.0797, 0.0797, 0.1124, 0.0801, 0.0799,
        0.0802, 0.0837, 0.0794, 0.0795, 0.0794, 0.0792, 0.0793, 0.0793, 0.0795,
        0.0794, 0.0797, 0.0793, 0.0796, 0.0798, 0.0795, 0.0797, 0.0796, 0.0793,
        0.0793, 0.0792, 0.0795, 0.0794, 0.0795, 0.0793, 0.0795, 0.0803, 0.0746,
        0.0795, 0.0793, 0.0790, 0.0876, 0.0796, 0.0825, 0.0822, 0.0793, 0.0793,
        0.0797, 0.0796, 0.0793, 0.0793, 0.0795, 0.0794, 0.0768, 0.0818, 0.0795,
        0.0796, 0.0798, 0.0736, 0.0795, 0.0802, 0.0793, 0.0796, 0.0795, 0.0839,
        0.0787, 0.0795, 0.0793, 0.0944, 0.0802, 0.0794, 0.0795, 0.0795, 0.0793,
        0.0832, 0.0795, 0.0966, 0.0793, 0.0797, 0.0796, 0.0797, 0.0793, 0.0794,
        0.0793, 0.0802, 0.0794, 0.0795, 0.0862, 0.0832, 0.0797, 0.0795, 0.0793,
        0.0795, 0.0796, 0.0801, 0.0793, 0.0792, 0.0767, 0.0793, 0.0821, 0.0806,
        0.0781, 0.0793, 0.0794, 0.0803, 0.0789, 0.0793, 0.0793, 0.0793, 0.0793,
        0.1284, 0.0794, 0.1061, 0.0793, 0.0815, 0.0793, 0.0796, 0.0826, 0.0792,
        0.0793, 0.0793, 0.0855, 0.0790, 0.0794, 0.0795, 0.0793, 0.0793, 0.0795,
        0.0782, 0.0795, 0.0793, 0.0792, 0.0789, 0.0793, 0.0793, 0.0794, 0.0795,
        0.0330, 0.0867, 0.0793, 0.1003, 0.0793, 0.0794, 0.0798, 0.0793, 0.0795,
        0.0793, 0.0795, 0.0793, 0.0793, 0.0790, 0.0796, 0.0793, 0.0794, 0.0872,
        0.0794, 0.0759, 0.0793, 0.0797, 0.0746, 0.0794, 0.0792, 0.0794, 0.0793,
        0.0796, 0.0793, 0.0795, 0.0795, 0.0795, 0.0790, 0.0795, 0.0853, 0.0794,
        0.0796, 0.1031, 0.0797, 0.0795, 0.0795, 0.0793, 0.0811, 0.0793, 0.0793,
        0.0793, 0.0796, 0.0795, 0.0910, 0.0856, 0.0795, 0.0808, 0.0793, 0.0876,
        0.0845, 0.0797, 0.0795, 0.0795, 0.0861, 0.0793, 0.0795, 0.0823, 0.0795,
        0.0793, 0.0801, 0.0794, 0.0764, 0.0756, 0.0976, 0.0797, 0.0793, 0.0795,
        0.0793, 0.0798, 0.0793, 0.0794, 0.0794, 0.0793, 0.0793, 0.0793, 0.0793,
        0.0793, 0.0795, 0.0793, 0.0795, 0.1063, 0.0876, 0.0793, 0.0793, 0.0793,
        0.0826, 0.0793, 0.0793, 0.0867, 0.0793, 0.1120, 0.0787, 0.0794, 0.0793,
        0.0793, 0.0754, 0.0793, 0.0796, 0.0793, 0.0793, 0.0793, 0.0793, 0.0794,
        0.0748, 0.1168, 0.0793, 0.0797, 0.0793, 0.0901, 0.0793, 0.0793, 0.0793,
        0.0795, 0.0793, 0.0793, 0.0836, 0.0793, 0.0865, 0.0797, 0.0800, 0.0793,
        0.0840, 0.0793, 0.0793, 0.0789, 0.0797, 0.0794, 0.0794, 0.0793, 0.0793,
        0.0796, 0.0794, 0.0793, 0.0885, 0.0793, 0.0794, 0.0793, 0.0793, 0.0780,
        0.0795, 0.0795, 0.0790, 0.0933, 0.0793, 0.0811, 0.0793, 0.0793, 0.0776,
        0.0795, 0.0787, 0.0793, 0.0797, 0.0800, 0.0786, 0.0813, 0.0879, 0.0796,
        0.0794, 0.0801, 0.0793, 0.0820, 0.0779, 0.0798, 0.0793, 0.0436, 0.0797,
        0.0794, 0.0829, 0.0795, 0.0793, 0.0795, 0.0793, 0.0794, 0.0797, 0.0796,
        0.0881, 0.1260, 0.0796, 0.0795, 0.0793, 0.0757, 0.0795, 0.0768, 0.0742,
        0.0793, 0.0798, 0.0792, 0.0797, 0.0797, 0.0793, 0.0797, 0.0795, 0.0947,
        0.0770, 0.0793, 0.0797, 0.0793, 0.0732, 0.0795, 0.0793, 0.0793, 0.0795,
        0.0817, 0.0816, 0.0795, 0.0794, 0.0796, 0.0794, 0.0793, 0.0793, 0.0775,
        0.0795, 0.0794, 0.0800, 0.0833, 0.0794, 0.0793, 0.0793, 0.0796, 0.0794,
        0.0794, 0.0793, 0.0853, 0.0797, 0.0793, 0.0751, 0.0793, 0.0842, 0.0871,
        0.0793, 0.0793, 0.0795, 0.0791, 0.0795, 0.0793, 0.0793, 0.0794, 0.0793,
        0.0793, 0.0793, 0.0766, 0.0859, 0.0793, 0.0795, 0.0795, 0.0797, 0.0817,
        0.1007, 0.0797, 0.0809, 0.0797, 0.0802, 0.0795, 0.0793, 0.0784, 0.0798,
        0.1206, 0.0793, 0.0830, 0.0793, 0.0789, 0.0794, 0.0795, 0.0796, 0.0793,
        0.0734, 0.0795, 0.0793, 0.0793, 0.0811, 0.0793, 0.0797, 0.0793, 0.0553,
        0.1011, 0.0793, 0.0793, 0.0793, 0.0793, 0.0793, 0.0793, 0.0800, 0.0795,
        0.0796, 0.0776, 0.0256, 0.0742, 0.0797, 0.0820, 0.0794, 0.0797, 0.0794,
        0.0793, 0.0797, 0.0795, 0.0793, 0.0810, 0.0812, 0.0794, 0.1017, 0.0793,
        0.0794, 0.0793, 0.0797, 0.1035, 0.0795, 0.0793, 0.0793, 0.0798, 0.0793,
        0.0793, 0.0796, 0.0806, 0.0795, 0.0797, 0.0978, 0.0791, 0.0793, 0.0803,
        0.0793, 0.0797, 0.0793, 0.0797, 0.0796, 0.0796, 0.0795, 0.0793, 0.0793,
        0.0850, 0.0796, 0.0793, 0.1058, 0.0875, 0.0779, 0.0812, 0.0796, 0.0984,
        0.0798, 0.0867, 0.0790, 0.1095, 0.0795, 0.0794, 0.0797, 0.0794, 0.0795,
        0.0794, 0.0795, 0.0793, 0.0793, 0.0793, 0.0840, 0.0794, 0.0793, 0.0797,
        0.0793], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(4223.0889, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8208, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8031.4624, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1454, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7255.7261, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4143, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7974.5513, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0098, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5854.5015, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1719, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4267.7290, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4229, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5435.9458, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1426, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6358.0586, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4746, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1389, 0.0989, 0.1054, 0.0876, 0.1038, 0.0974, 0.0974, 0.0980, 0.0978,
        0.0977, 0.0975, 0.0982, 0.1016, 0.0974, 0.0975, 0.1032, 0.0977, 0.0976,
        0.0914, 0.0974, 0.0974, 0.1005, 0.0975, 0.1141, 0.0982, 0.0977, 0.0977,
        0.0974, 0.0974, 0.1005, 0.1021, 0.0977, 0.0963, 0.0974, 0.0975, 0.0975,
        0.0969, 0.0975, 0.0977, 0.0977, 0.0974, 0.0978, 0.0974, 0.0972, 0.0974,
        0.1031, 0.0974, 0.0974, 0.0975, 0.0980, 0.0974, 0.1105, 0.0976, 0.0973,
        0.0974, 0.0970, 0.0998, 0.0934, 0.0974, 0.0972, 0.0975, 0.0974, 0.1186,
        0.1027, 0.1509, 0.0984, 0.0974, 0.0979, 0.0974, 0.0974, 0.0974, 0.0975,
        0.0974, 0.0974, 0.0973, 0.0977, 0.0974, 0.0905, 0.0974, 0.1277, 0.0974,
        0.0975, 0.1077, 0.0975, 0.1290, 0.0978, 0.0974, 0.0974, 0.1104, 0.0975,
        0.0974, 0.1047, 0.0978, 0.0975, 0.0974, 0.0977, 0.1088, 0.0980, 0.1422,
        0.0974, 0.0977, 0.0975, 0.0977, 0.0977, 0.0976, 0.0974, 0.0975, 0.0978,
        0.0983, 0.1054, 0.0977, 0.0976, 0.0935, 0.1023, 0.0974, 0.0974, 0.0974,
        0.1009, 0.0970, 0.1376, 0.0974, 0.0974, 0.1042, 0.0974, 0.0955, 0.0978,
        0.1006, 0.0974, 0.0991, 0.0974, 0.0974, 0.0974, 0.0939, 0.0974, 0.0970,
        0.1108, 0.0975, 0.0974, 0.0968, 0.0975, 0.0971, 0.0917, 0.0979, 0.0975,
        0.0973, 0.0977, 0.1157, 0.0977, 0.0977, 0.0977, 0.1410, 0.0981, 0.0979,
        0.0982, 0.1068, 0.0974, 0.0976, 0.0974, 0.0972, 0.0974, 0.0974, 0.0975,
        0.0975, 0.0977, 0.0974, 0.0977, 0.0979, 0.0975, 0.0977, 0.0977, 0.0974,
        0.0974, 0.0972, 0.0978, 0.0974, 0.0975, 0.0974, 0.0976, 0.0983, 0.0927,
        0.0975, 0.0974, 0.0970, 0.1057, 0.0977, 0.1008, 0.1059, 0.0974, 0.0974,
        0.0978, 0.0976, 0.0974, 0.0974, 0.0986, 0.0974, 0.0958, 0.0999, 0.0975,
        0.0977, 0.0978, 0.0916, 0.0975, 0.0982, 0.0974, 0.0976, 0.0975, 0.1019,
        0.0968, 0.0976, 0.0977, 0.1125, 0.0989, 0.0975, 0.0995, 0.0975, 0.0974,
        0.1017, 0.0975, 0.1232, 0.0974, 0.0977, 0.0976, 0.0977, 0.0974, 0.0974,
        0.0974, 0.0993, 0.0974, 0.0975, 0.1048, 0.1010, 0.0977, 0.0975, 0.0974,
        0.0975, 0.0977, 0.0992, 0.0974, 0.0972, 0.0947, 0.0974, 0.1003, 0.0986,
        0.0962, 0.0974, 0.0974, 0.0984, 0.0969, 0.0974, 0.0974, 0.0974, 0.0974,
        0.1478, 0.0974, 0.1238, 0.0974, 0.0995, 0.0974, 0.0976, 0.1007, 0.0973,
        0.0974, 0.0974, 0.1036, 0.0974, 0.0975, 0.0976, 0.0974, 0.0974, 0.0976,
        0.0963, 0.0975, 0.0974, 0.0972, 0.0970, 0.0974, 0.0974, 0.0974, 0.0975,
        0.0039, 0.1032, 0.0974, 0.1284, 0.0974, 0.0974, 0.0978, 0.0974, 0.0976,
        0.0974, 0.0975, 0.0974, 0.0974, 0.0971, 0.0977, 0.0974, 0.0975, 0.1094,
        0.0974, 0.0939, 0.0974, 0.0978, 0.0926, 0.0974, 0.0972, 0.0975, 0.0974,
        0.0976, 0.0974, 0.0975, 0.0975, 0.0978, 0.0970, 0.0975, 0.1005, 0.0974,
        0.0976, 0.1251, 0.0977, 0.0975, 0.0975, 0.0973, 0.0993, 0.0974, 0.0974,
        0.0974, 0.0977, 0.0975, 0.1091, 0.1088, 0.0975, 0.0989, 0.0974, 0.1076,
        0.1100, 0.0978, 0.0975, 0.0975, 0.1089, 0.0973, 0.0975, 0.1010, 0.0975,
        0.0974, 0.0984, 0.0975, 0.0944, 0.0942, 0.1177, 0.0977, 0.0974, 0.0984,
        0.0974, 0.0978, 0.0974, 0.0974, 0.0974, 0.0974, 0.0974, 0.0974, 0.0974,
        0.0975, 0.0975, 0.0973, 0.0975, 0.1256, 0.1063, 0.0974, 0.0974, 0.0974,
        0.1006, 0.0974, 0.0974, 0.1120, 0.0974, 0.1342, 0.0967, 0.0975, 0.0974,
        0.0974, 0.0934, 0.0974, 0.0976, 0.0974, 0.0974, 0.0974, 0.0974, 0.0974,
        0.0928, 0.1395, 0.0974, 0.0977, 0.0974, 0.1086, 0.0974, 0.0974, 0.0974,
        0.0975, 0.0974, 0.0974, 0.1033, 0.0974, 0.1053, 0.0977, 0.1006, 0.0974,
        0.1057, 0.0974, 0.0974, 0.0970, 0.0977, 0.0974, 0.0974, 0.0974, 0.0974,
        0.0977, 0.0975, 0.0974, 0.1077, 0.0974, 0.0974, 0.0977, 0.0974, 0.0960,
        0.0975, 0.0976, 0.0970, 0.1129, 0.0974, 0.0992, 0.0974, 0.0974, 0.0956,
        0.0975, 0.0991, 0.0974, 0.0977, 0.0986, 0.0966, 0.1000, 0.1060, 0.0976,
        0.0975, 0.0985, 0.0973, 0.1035, 0.0962, 0.0978, 0.0974, 0.0676, 0.0977,
        0.0974, 0.1014, 0.0975, 0.0974, 0.0975, 0.0974, 0.0975, 0.0977, 0.0976,
        0.1108, 0.1534, 0.0976, 0.0975, 0.0974, 0.0938, 0.0975, 0.0949, 0.0923,
        0.0974, 0.0984, 0.0973, 0.0977, 0.0977, 0.0974, 0.0977, 0.0975, 0.1199,
        0.0950, 0.0974, 0.1000, 0.0974, 0.0913, 0.0975, 0.0974, 0.0974, 0.0975,
        0.1029, 0.0997, 0.0975, 0.0974, 0.0976, 0.0975, 0.0974, 0.0974, 0.0955,
        0.0975, 0.0975, 0.0981, 0.1045, 0.0974, 0.0974, 0.0974, 0.0977, 0.0974,
        0.0974, 0.0974, 0.1033, 0.0977, 0.0974, 0.0932, 0.0974, 0.1046, 0.1072,
        0.0974, 0.0974, 0.0975, 0.0971, 0.0975, 0.0974, 0.0974, 0.0974, 0.0974,
        0.0974, 0.0974, 0.0946, 0.1039, 0.0974, 0.0975, 0.0975, 0.0977, 0.1016,
        0.1195, 0.0980, 0.1002, 0.0977, 0.0982, 0.0975, 0.0974, 0.0969, 0.0978,
        0.1377, 0.0974, 0.0995, 0.0974, 0.0969, 0.0975, 0.0975, 0.0977, 0.0974,
        0.0914, 0.0976, 0.0974, 0.0974, 0.0995, 0.0974, 0.0978, 0.0974, 0.0762,
        0.1219, 0.0974, 0.0974, 0.0974, 0.0974, 0.0974, 0.0974, 0.0980, 0.0975,
        0.0976, 0.0956, 0.0471, 0.0923, 0.0977, 0.1049, 0.0974, 0.0977, 0.0975,
        0.0974, 0.0977, 0.0975, 0.0974, 0.1030, 0.0993, 0.0974, 0.1198, 0.0974,
        0.0975, 0.0974, 0.0978, 0.1205, 0.0976, 0.0974, 0.0974, 0.0978, 0.0974,
        0.0974, 0.0976, 0.0991, 0.0975, 0.0978, 0.1121, 0.0972, 0.0974, 0.0990,
        0.0974, 0.0977, 0.0974, 0.0977, 0.0977, 0.0977, 0.0975, 0.0974, 0.0974,
        0.1084, 0.0976, 0.0974, 0.1254, 0.1065, 0.0959, 0.1007, 0.0977, 0.1156,
        0.0978, 0.1033, 0.0970, 0.1089, 0.0975, 0.0975, 0.0977, 0.0974, 0.0976,
        0.0974, 0.0975, 0.0974, 0.0974, 0.0974, 0.1020, 0.0975, 0.0974, 0.0978,
        0.0974], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(8025.6753, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3418, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6479.2983, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9238, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6118.9907, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4502, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7183.5161, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8882, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: tensor(-0.4968, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6372.8188, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4626, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5411.3525, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1393, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7513.1826, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9375, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8081.3428, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0878, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4953.2739, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2334, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4889.2578, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0801, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4335.8960, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0571, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8338.8232, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7227, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5232.1089, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2097, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8084.1582, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2234, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7863.4023, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7612, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8013.9023, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2546, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1333, 0.0956, 0.1016, 0.0887, 0.1006, 0.0941, 0.0941, 0.0947, 0.0945,
        0.0945, 0.0943, 0.0948, 0.0983, 0.0941, 0.0942, 0.0986, 0.0945, 0.0944,
        0.0881, 0.0941, 0.0941, 0.0973, 0.0942, 0.1110, 0.0948, 0.0945, 0.0944,
        0.0942, 0.0942, 0.0957, 0.0988, 0.0945, 0.0931, 0.0941, 0.0943, 0.0942,
        0.0937, 0.0942, 0.0944, 0.0944, 0.0941, 0.0945, 0.0941, 0.0939, 0.0941,
        0.0991, 0.0941, 0.0941, 0.0943, 0.0947, 0.0941, 0.1072, 0.0944, 0.0941,
        0.0942, 0.0938, 0.0962, 0.0902, 0.0941, 0.0940, 0.0943, 0.0941, 0.1154,
        0.0993, 0.1460, 0.0947, 0.0941, 0.0947, 0.0941, 0.0941, 0.0941, 0.0942,
        0.0941, 0.0942, 0.0941, 0.0944, 0.0941, 0.0872, 0.0942, 0.1243, 0.0941,
        0.0943, 0.1044, 0.0942, 0.1259, 0.0945, 0.0941, 0.0941, 0.1077, 0.0943,
        0.0941, 0.1013, 0.0945, 0.0942, 0.0941, 0.0944, 0.1047, 0.0948, 0.1377,
        0.0941, 0.0945, 0.0942, 0.0945, 0.0944, 0.0943, 0.0941, 0.0942, 0.0945,
        0.0950, 0.1006, 0.0945, 0.0943, 0.0903, 0.0989, 0.0941, 0.0941, 0.0941,
        0.0977, 0.0938, 0.1338, 0.0941, 0.0941, 0.1014, 0.0941, 0.0922, 0.0945,
        0.0970, 0.0941, 0.0957, 0.0942, 0.0942, 0.0941, 0.0907, 0.0941, 0.0938,
        0.1072, 0.0942, 0.0941, 0.0936, 0.0942, 0.0939, 0.0885, 0.0946, 0.0943,
        0.0940, 0.0944, 0.1109, 0.0945, 0.0944, 0.0944, 0.1366, 0.0948, 0.0947,
        0.0950, 0.1022, 0.0942, 0.0943, 0.0942, 0.0940, 0.0941, 0.0941, 0.0942,
        0.0942, 0.0944, 0.0937, 0.0945, 0.0947, 0.0942, 0.0944, 0.0944, 0.0941,
        0.0942, 0.0940, 0.0945, 0.0942, 0.0943, 0.0942, 0.0944, 0.0951, 0.0894,
        0.0943, 0.0941, 0.0938, 0.1024, 0.0944, 0.0975, 0.1016, 0.0941, 0.0941,
        0.0945, 0.0944, 0.0941, 0.0941, 0.0948, 0.0942, 0.0921, 0.0966, 0.0943,
        0.0944, 0.0946, 0.0884, 0.0942, 0.0950, 0.0941, 0.0944, 0.0943, 0.0987,
        0.0935, 0.0943, 0.0943, 0.1093, 0.0955, 0.0942, 0.0961, 0.0942, 0.0941,
        0.0983, 0.0942, 0.1191, 0.0941, 0.0944, 0.0944, 0.0944, 0.0941, 0.0942,
        0.0941, 0.0958, 0.0942, 0.0942, 0.1014, 0.0978, 0.0945, 0.0942, 0.0942,
        0.0942, 0.0944, 0.0957, 0.0941, 0.0940, 0.0915, 0.0941, 0.0970, 0.0954,
        0.0929, 0.0941, 0.0942, 0.0952, 0.0937, 0.0941, 0.0941, 0.0941, 0.0941,
        0.1443, 0.0942, 0.1213, 0.0941, 0.0963, 0.0942, 0.0944, 0.0974, 0.0940,
        0.0941, 0.0941, 0.1004, 0.0941, 0.0942, 0.0944, 0.0941, 0.0941, 0.0944,
        0.0931, 0.0942, 0.0941, 0.0939, 0.0938, 0.0941, 0.0941, 0.0942, 0.0942,
        0.0083, 0.0997, 0.0941, 0.1226, 0.0941, 0.0942, 0.0946, 0.0942, 0.0944,
        0.0941, 0.0943, 0.0941, 0.0941, 0.0939, 0.0944, 0.0942, 0.0942, 0.1056,
        0.0942, 0.0907, 0.0941, 0.0945, 0.0894, 0.0942, 0.0940, 0.0942, 0.0941,
        0.0944, 0.0941, 0.0942, 0.0943, 0.0945, 0.0938, 0.0943, 0.0966, 0.0942,
        0.0944, 0.1204, 0.0945, 0.0942, 0.0942, 0.0941, 0.0959, 0.0941, 0.0941,
        0.0941, 0.0944, 0.0943, 0.1058, 0.1041, 0.0943, 0.0956, 0.0941, 0.1057,
        0.1052, 0.0945, 0.0942, 0.0942, 0.1045, 0.0941, 0.0943, 0.0978, 0.0942,
        0.0941, 0.0952, 0.0942, 0.0911, 0.0908, 0.1141, 0.0945, 0.0941, 0.0951,
        0.0941, 0.0946, 0.0941, 0.0942, 0.0942, 0.0942, 0.0941, 0.0941, 0.0941,
        0.0942, 0.0942, 0.0941, 0.0943, 0.1222, 0.1030, 0.0941, 0.0941, 0.0941,
        0.0974, 0.0941, 0.0941, 0.1093, 0.0941, 0.1317, 0.0934, 0.0942, 0.0942,
        0.0941, 0.0902, 0.0941, 0.0944, 0.0941, 0.0941, 0.0941, 0.0941, 0.0942,
        0.0896, 0.1353, 0.0941, 0.0945, 0.0941, 0.1051, 0.0941, 0.0941, 0.0941,
        0.0943, 0.0941, 0.0941, 0.1004, 0.0941, 0.1019, 0.0945, 0.0965, 0.0942,
        0.1019, 0.0941, 0.0942, 0.0937, 0.0945, 0.0942, 0.0942, 0.0942, 0.0942,
        0.0944, 0.0942, 0.0941, 0.1036, 0.0941, 0.0942, 0.0944, 0.0941, 0.0928,
        0.0943, 0.0944, 0.0938, 0.1093, 0.0942, 0.0959, 0.0942, 0.0941, 0.0924,
        0.0943, 0.0949, 0.0941, 0.0945, 0.0953, 0.0933, 0.0967, 0.1027, 0.0944,
        0.0942, 0.0952, 0.0941, 0.1001, 0.0929, 0.0945, 0.0941, 0.0634, 0.0944,
        0.0942, 0.0980, 0.0943, 0.0941, 0.0942, 0.0941, 0.0942, 0.0945, 0.0944,
        0.1064, 0.1482, 0.0944, 0.0942, 0.0941, 0.0905, 0.0943, 0.0917, 0.0891,
        0.0941, 0.0950, 0.0940, 0.0944, 0.0944, 0.0941, 0.0945, 0.0942, 0.1160,
        0.0918, 0.0941, 0.0959, 0.0941, 0.0881, 0.0943, 0.0941, 0.0942, 0.0943,
        0.0999, 0.0964, 0.0942, 0.0942, 0.0944, 0.0942, 0.0941, 0.0941, 0.0922,
        0.0942, 0.0942, 0.0949, 0.1008, 0.0942, 0.0941, 0.0941, 0.0944, 0.0942,
        0.0942, 0.0941, 0.1001, 0.0944, 0.0941, 0.0900, 0.0941, 0.1010, 0.1034,
        0.0941, 0.0941, 0.0943, 0.0939, 0.0942, 0.0941, 0.0942, 0.0942, 0.0941,
        0.0941, 0.0942, 0.0914, 0.1006, 0.0941, 0.0943, 0.0943, 0.0944, 0.0975,
        0.1158, 0.0947, 0.0967, 0.0945, 0.0950, 0.0942, 0.0941, 0.0935, 0.0945,
        0.1345, 0.0941, 0.0952, 0.0941, 0.0937, 0.0942, 0.0942, 0.0944, 0.0941,
        0.0881, 0.0943, 0.0941, 0.0941, 0.0958, 0.0941, 0.0945, 0.0941, 0.0728,
        0.1171, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0941, 0.0947, 0.0943,
        0.0944, 0.0923, 0.0428, 0.0891, 0.0944, 0.1006, 0.0942, 0.0945, 0.0942,
        0.0941, 0.0944, 0.0942, 0.0941, 0.0984, 0.0961, 0.0942, 0.1165, 0.0941,
        0.0942, 0.0941, 0.0945, 0.1163, 0.0943, 0.0941, 0.0941, 0.0945, 0.0941,
        0.0942, 0.0944, 0.0955, 0.0943, 0.0945, 0.1089, 0.0939, 0.0941, 0.0955,
        0.0941, 0.0944, 0.0941, 0.0945, 0.0944, 0.0944, 0.0942, 0.0942, 0.0941,
        0.1039, 0.0944, 0.0941, 0.1213, 0.1030, 0.0927, 0.0970, 0.0944, 0.1122,
        0.0946, 0.0993, 0.0938, 0.1116, 0.0943, 0.0942, 0.0945, 0.0942, 0.0944,
        0.0942, 0.0943, 0.0941, 0.0941, 0.0941, 0.0988, 0.0942, 0.0941, 0.0945,
        0.0941], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7201.7827, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9131, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5472.4683, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8496, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.1477,  0.1052,  0.1146,  0.0844,  0.1118,  0.1036,  0.1036,  0.1043,
         0.1041,  0.1040,  0.1038,  0.1048,  0.1080,  0.1036,  0.1038,  0.1111,
         0.1040,  0.1039,  0.0978,  0.1036,  0.1037,  0.1069,  0.1038,  0.1204,
         0.1046,  0.1041,  0.1039,  0.1037,  0.1038,  0.1097,  0.1086,  0.1041,
         0.1026,  0.1036,  0.1038,  0.1038,  0.1033,  0.1039,  0.1039,  0.1039,
         0.1036,  0.1041,  0.1037,  0.1035,  0.1037,  0.1087,  0.1036,  0.1036,
         0.1038,  0.1043,  0.1036,  0.1168,  0.1039,  0.1036,  0.1037,  0.1033,
         0.1067,  0.0997,  0.1037,  0.1035,  0.1038,  0.1036,  0.1261,  0.1089,
         0.1593,  0.1050,  0.1036,  0.1042,  0.1036,  0.1036,  0.1036,  0.1038,
         0.1036,  0.1037,  0.1036,  0.1039,  0.1037,  0.0968,  0.1037,  0.1339,
         0.1036,  0.1038,  0.1141,  0.1038,  0.1351,  0.1043,  0.1037,  0.1036,
         0.1174,  0.1038,  0.1036,  0.1119,  0.1041,  0.1038,  0.1036,  0.1039,
         0.1150,  0.1044,  0.1506,  0.1036,  0.1040,  0.1038,  0.1041,  0.1040,
         0.1039,  0.1036,  0.1038,  0.1041,  0.1047,  0.1119,  0.1040,  0.1039,
         0.0999,  0.1112,  0.1037,  0.1036,  0.1036,  0.1072,  0.1033,  0.1434,
         0.1036,  0.1036,  0.1102,  0.1037,  0.1017,  0.1041,  0.1089,  0.1036,
         0.1057,  0.1037,  0.1037,  0.1036,  0.1003,  0.1036,  0.1033,  0.1182,
         0.1038,  0.1038,  0.1031,  0.1039,  0.1035,  0.0980,  0.1042,  0.1038,
         0.1036,  0.1039,  0.1252,  0.1041,  0.1040,  0.1039,  0.1505,  0.1044,
         0.1042,  0.1045,  0.1140,  0.1037,  0.1039,  0.1037,  0.1035,  0.1036,
         0.1036,  0.1038,  0.1038,  0.1039,  0.1042,  0.1041,  0.1042,  0.1038,
         0.1040,  0.1039,  0.1036,  0.1037,  0.1035,  0.1042,  0.1037,  0.1038,
         0.1037,  0.1039,  0.1046,  0.0989,  0.1038,  0.1037,  0.1033,  0.1120,
         0.1039,  0.1071,  0.1155,  0.1036,  0.1037,  0.1041,  0.1039,  0.1036,
         0.1036,  0.1051,  0.1037,  0.1023,  0.1061,  0.1038,  0.1039,  0.1041,
         0.0980,  0.1038,  0.1046,  0.1037,  0.1039,  0.1038,  0.1082,  0.1031,
         0.1039,  0.1040,  0.1190,  0.1059,  0.1038,  0.1068,  0.1038,  0.1036,
         0.1083,  0.1038,  0.1337,  0.1036,  0.1039,  0.1039,  0.1039,  0.1036,
         0.1037,  0.1037,  0.1069,  0.1037,  0.1038,  0.1112,  0.1073,  0.1040,
         0.1038,  0.1037,  0.1038,  0.1039,  0.1059,  0.1037,  0.1035,  0.1010,
         0.1037,  0.1066,  0.1050,  0.1025,  0.1036,  0.1038,  0.1047,  0.1033,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1537,  0.1038,  0.1296,  0.1037,
         0.1058,  0.1037,  0.1039,  0.1070,  0.1036,  0.1036,  0.1037,  0.1100,
         0.1038,  0.1038,  0.1039,  0.1036,  0.1036,  0.1039,  0.1026,  0.1038,
         0.1036,  0.1035,  0.1033,  0.1036,  0.1036,  0.1037,  0.1038, -0.0047,
         0.1099,  0.1036,  0.1388,  0.1036,  0.1037,  0.1041,  0.1037,  0.1039,
         0.1036,  0.1039,  0.1036,  0.1036,  0.1034,  0.1039,  0.1037,  0.1038,
         0.1170,  0.1037,  0.1003,  0.1036,  0.1041,  0.0989,  0.1037,  0.1035,
         0.1038,  0.1037,  0.1039,  0.1036,  0.1038,  0.1039,  0.1042,  0.1033,
         0.1038,  0.1074,  0.1038,  0.1039,  0.1326,  0.1040,  0.1038,  0.1038,
         0.1036,  0.1057,  0.1036,  0.1036,  0.1036,  0.1039,  0.1038,  0.1154,
         0.1190,  0.1038,  0.1052,  0.1036,  0.1141,  0.1204,  0.1044,  0.1038,
         0.1038,  0.1151,  0.1036,  0.1038,  0.1085,  0.1038,  0.1037,  0.1047,
         0.1038,  0.1007,  0.1010,  0.1242,  0.1040,  0.1036,  0.1047,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1037,  0.1037,  0.1036,  0.1036,  0.1036,
         0.1041,  0.1038,  0.1036,  0.1038,  0.1317,  0.1130,  0.1037,  0.1036,
         0.1036,  0.1069,  0.1037,  0.1036,  0.1196,  0.1037,  0.1331,  0.1030,
         0.1038,  0.1037,  0.1036,  0.0997,  0.1036,  0.1039,  0.1036,  0.1036,
         0.1036,  0.1037,  0.1038,  0.0992,  0.1473,  0.1036,  0.1040,  0.1036,
         0.1167,  0.1036,  0.1036,  0.1036,  0.1038,  0.1037,  0.1037,  0.1086,
         0.1036,  0.1116,  0.1040,  0.1076,  0.1037,  0.1135,  0.1036,  0.1037,
         0.1033,  0.1040,  0.1038,  0.1037,  0.1037,  0.1037,  0.1039,  0.1038,
         0.1036,  0.1141,  0.1037,  0.1037,  0.1041,  0.1036,  0.1024,  0.1038,
         0.1039,  0.1033,  0.1194,  0.1037,  0.1055,  0.1037,  0.1036,  0.1020,
         0.1038,  0.1068,  0.1037,  0.1040,  0.1054,  0.1029,  0.1068,  0.1122,
         0.1039,  0.1038,  0.1050,  0.1036,  0.1118,  0.1028,  0.1041,  0.1036,
         0.0750,  0.1040,  0.1037,  0.1078,  0.1038,  0.1036,  0.1038,  0.1036,
         0.1038,  0.1040,  0.1039,  0.1183,  0.1614,  0.1039,  0.1038,  0.1037,
         0.1000,  0.1038,  0.1012,  0.0987,  0.1036,  0.1051,  0.1036,  0.1039,
         0.1039,  0.1036,  0.1040,  0.1038,  0.1256,  0.1013,  0.1036,  0.1084,
         0.1036,  0.0976,  0.1038,  0.1036,  0.1037,  0.1039,  0.1082,  0.1060,
         0.1038,  0.1038,  0.1039,  0.1038,  0.1036,  0.1036,  0.1018,  0.1038,
         0.1038,  0.1045,  0.1122,  0.1037,  0.1036,  0.1036,  0.1039,  0.1037,
         0.1038,  0.1036,  0.1097,  0.1039,  0.1036,  0.0995,  0.1037,  0.1118,
         0.1146,  0.1036,  0.1036,  0.1039,  0.1034,  0.1038,  0.1037,  0.1037,
         0.1038,  0.1036,  0.1036,  0.1037,  0.1009,  0.1102,  0.1037,  0.1038,
         0.1038,  0.1040,  0.1083,  0.1257,  0.1045,  0.1071,  0.1040,  0.1046,
         0.1038,  0.1036,  0.1036,  0.1041,  0.1444,  0.1037,  0.1063,  0.1036,
         0.1032,  0.1038,  0.1038,  0.1039,  0.1036,  0.0977,  0.1039,  0.1036,
         0.1036,  0.1055,  0.1037,  0.1041,  0.1036,  0.0839,  0.1267,  0.1036,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1036,  0.1042,  0.1039,  0.1039,
         0.1019,  0.0544,  0.0986,  0.1040,  0.1133,  0.1037,  0.1040,  0.1038,
         0.1036,  0.1040,  0.1038,  0.1037,  0.1115,  0.1056,  0.1037,  0.1260,
         0.1036,  0.1038,  0.1036,  0.1041,  0.1287,  0.1039,  0.1036,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1039,  0.1061,  0.1038,  0.1041,  0.1186,
         0.1035,  0.1036,  0.1057,  0.1036,  0.1039,  0.1037,  0.1040,  0.1039,
         0.1039,  0.1038,  0.1037,  0.1036,  0.1174,  0.1039,  0.1036,  0.1326,
         0.1133,  0.1022,  0.1085,  0.1039,  0.1249,  0.1041,  0.1110,  0.1033,
         0.1027,  0.1038,  0.1038,  0.1040,  0.1037,  0.1039,  0.1037,  0.1038,
         0.1036,  0.1037,  0.1037,  0.1083,  0.1038,  0.1036,  0.1041,  0.1036],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(4199.8530, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9209, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 05:51:58,615][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
loss: tensor(4116.8735, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6885, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 05:55:20,647][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 05:55:20,648][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 05:55:20,859][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2023-09-12 05:55:44,504][valid][INFO] - {"epoch": 11, "valid_loss": "4.125", "valid_ntokens": "7907.72", "valid_nsentences": "55.2525", "valid_prob_perplexity": "69.256", "valid_code_perplexity": "69.053", "valid_temp": "1.944", "valid_loss_0": "3.985", "valid_loss_1": "0.129", "valid_loss_2": "0.011", "valid_accuracy": "0.34153", "valid_wps": "33224.3", "valid_wpb": "7907.7", "valid_bsz": "55.3", "valid_num_updates": "5709", "valid_best_loss": "4.125"}
[2023-09-12 05:55:44,505][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 5709 updates
[2023-09-12 05:55:44,506][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 05:55:47,033][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 05:55:48,407][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 5709 updates, score 4.125) (writing took 3.902117060031742 seconds)
[2023-09-12 05:55:48,408][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2023-09-12 05:55:48,408][train][INFO] - {"epoch": 11, "train_loss": "4.396", "train_ntokens": "149463", "train_nsentences": "538.514", "train_prob_perplexity": "71.359", "train_code_perplexity": "71.141", "train_temp": "1.946", "train_loss_0": "4.257", "train_loss_1": "0.128", "train_loss_2": "0.011", "train_accuracy": "0.287", "train_wps": "37106.8", "train_ups": "0.25", "train_wpb": "149463", "train_bsz": "538.5", "train_num_updates": "5709", "train_lr": "8.92031e-05", "train_gnorm": "0.608", "train_loss_scale": "4", "train_train_wall": "2059", "train_gb_free": "13.2", "train_wall": "23015"}
[2023-09-12 05:55:48,410][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 05:55:48,496][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2023-09-12 05:55:48,743][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 05:55:48,747][fairseq.trainer][INFO] - begin training epoch 12
[2023-09-12 05:55:48,747][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(5370.7935, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6226, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 06:00:30,617][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2023-09-12 06:01:55,353][train_inner][INFO] - {"epoch": 12, "update": 11.177, "loss": "4.372", "ntokens": "148990", "nsentences": "536.365", "prob_perplexity": "72.52", "code_perplexity": "72.249", "temp": "1.944", "loss_0": "4.233", "loss_1": "0.128", "loss_2": "0.011", "accuracy": "0.28942", "wps": "36082", "ups": "0.24", "wpb": "148990", "bsz": "536.4", "num_updates": "5800", "lr": "9.0625e-05", "gnorm": "0.588", "loss_scale": "2", "train_wall": "796", "gb_free": "12.6", "wall": "23382"}
Parameter containing:
tensor([ 0.1477,  0.1052,  0.1146,  0.0844,  0.1118,  0.1036,  0.1036,  0.1043,
         0.1041,  0.1040,  0.1038,  0.1048,  0.1080,  0.1036,  0.1038,  0.1111,
         0.1040,  0.1039,  0.0978,  0.1036,  0.1037,  0.1069,  0.1038,  0.1204,
         0.1046,  0.1041,  0.1039,  0.1037,  0.1038,  0.1097,  0.1086,  0.1041,
         0.1026,  0.1036,  0.1038,  0.1038,  0.1033,  0.1039,  0.1039,  0.1039,
         0.1036,  0.1041,  0.1037,  0.1035,  0.1037,  0.1087,  0.1036,  0.1036,
         0.1038,  0.1043,  0.1036,  0.1168,  0.1039,  0.1036,  0.1037,  0.1033,
         0.1067,  0.0997,  0.1037,  0.1035,  0.1038,  0.1036,  0.1261,  0.1089,
         0.1593,  0.1050,  0.1036,  0.1042,  0.1036,  0.1036,  0.1036,  0.1038,
         0.1036,  0.1037,  0.1036,  0.1039,  0.1037,  0.0968,  0.1037,  0.1339,
         0.1036,  0.1038,  0.1141,  0.1038,  0.1351,  0.1043,  0.1037,  0.1036,
         0.1174,  0.1038,  0.1036,  0.1119,  0.1041,  0.1038,  0.1036,  0.1039,
         0.1150,  0.1044,  0.1506,  0.1036,  0.1040,  0.1038,  0.1041,  0.1040,
         0.1039,  0.1036,  0.1038,  0.1041,  0.1047,  0.1119,  0.1040,  0.1039,
         0.0999,  0.1112,  0.1037,  0.1036,  0.1036,  0.1072,  0.1033,  0.1434,
         0.1036,  0.1036,  0.1102,  0.1037,  0.1017,  0.1041,  0.1089,  0.1036,
         0.1057,  0.1037,  0.1037,  0.1036,  0.1003,  0.1036,  0.1033,  0.1182,
         0.1038,  0.1038,  0.1031,  0.1039,  0.1035,  0.0980,  0.1042,  0.1038,
         0.1036,  0.1039,  0.1252,  0.1041,  0.1040,  0.1039,  0.1505,  0.1044,
         0.1042,  0.1045,  0.1140,  0.1037,  0.1039,  0.1037,  0.1035,  0.1036,
         0.1036,  0.1038,  0.1038,  0.1039,  0.1042,  0.1041,  0.1042,  0.1038,
         0.1040,  0.1039,  0.1036,  0.1037,  0.1035,  0.1042,  0.1037,  0.1038,
         0.1037,  0.1039,  0.1046,  0.0989,  0.1038,  0.1037,  0.1033,  0.1120,
         0.1039,  0.1071,  0.1155,  0.1036,  0.1037,  0.1041,  0.1039,  0.1036,
         0.1036,  0.1051,  0.1037,  0.1023,  0.1061,  0.1038,  0.1039,  0.1041,
         0.0980,  0.1038,  0.1046,  0.1037,  0.1039,  0.1038,  0.1082,  0.1031,
         0.1039,  0.1040,  0.1190,  0.1059,  0.1038,  0.1068,  0.1038,  0.1036,
         0.1083,  0.1038,  0.1337,  0.1036,  0.1039,  0.1039,  0.1039,  0.1036,
         0.1037,  0.1037,  0.1069,  0.1037,  0.1038,  0.1112,  0.1073,  0.1040,
         0.1038,  0.1037,  0.1038,  0.1039,  0.1059,  0.1037,  0.1035,  0.1010,
         0.1037,  0.1066,  0.1050,  0.1025,  0.1036,  0.1038,  0.1047,  0.1033,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1537,  0.1038,  0.1296,  0.1037,
         0.1058,  0.1037,  0.1039,  0.1070,  0.1036,  0.1036,  0.1037,  0.1100,
         0.1038,  0.1038,  0.1039,  0.1036,  0.1036,  0.1039,  0.1026,  0.1038,
         0.1036,  0.1035,  0.1033,  0.1036,  0.1036,  0.1037,  0.1038, -0.0047,
         0.1099,  0.1036,  0.1388,  0.1036,  0.1037,  0.1041,  0.1037,  0.1039,
         0.1036,  0.1039,  0.1036,  0.1036,  0.1034,  0.1039,  0.1037,  0.1038,
         0.1170,  0.1037,  0.1003,  0.1036,  0.1041,  0.0989,  0.1037,  0.1035,
         0.1038,  0.1037,  0.1039,  0.1036,  0.1038,  0.1039,  0.1042,  0.1033,
         0.1038,  0.1074,  0.1038,  0.1039,  0.1326,  0.1040,  0.1038,  0.1038,
         0.1036,  0.1057,  0.1036,  0.1036,  0.1036,  0.1039,  0.1038,  0.1154,
         0.1190,  0.1038,  0.1052,  0.1036,  0.1141,  0.1204,  0.1044,  0.1038,
         0.1038,  0.1151,  0.1036,  0.1038,  0.1085,  0.1038,  0.1037,  0.1047,
         0.1038,  0.1007,  0.1010,  0.1242,  0.1040,  0.1036,  0.1047,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1037,  0.1037,  0.1036,  0.1036,  0.1036,
         0.1041,  0.1038,  0.1036,  0.1038,  0.1317,  0.1130,  0.1037,  0.1036,
         0.1036,  0.1069,  0.1037,  0.1036,  0.1196,  0.1037,  0.1331,  0.1030,
         0.1038,  0.1037,  0.1036,  0.0997,  0.1036,  0.1039,  0.1036,  0.1036,
         0.1036,  0.1037,  0.1038,  0.0992,  0.1473,  0.1036,  0.1040,  0.1036,
         0.1167,  0.1036,  0.1036,  0.1036,  0.1038,  0.1037,  0.1037,  0.1086,
         0.1036,  0.1116,  0.1040,  0.1076,  0.1037,  0.1135,  0.1036,  0.1037,
         0.1033,  0.1040,  0.1038,  0.1037,  0.1037,  0.1037,  0.1039,  0.1038,
         0.1036,  0.1141,  0.1037,  0.1037,  0.1041,  0.1036,  0.1024,  0.1038,
         0.1039,  0.1033,  0.1194,  0.1037,  0.1055,  0.1037,  0.1036,  0.1020,
         0.1038,  0.1068,  0.1037,  0.1040,  0.1054,  0.1029,  0.1068,  0.1122,
         0.1039,  0.1038,  0.1050,  0.1036,  0.1118,  0.1028,  0.1041,  0.1036,
         0.0750,  0.1040,  0.1037,  0.1078,  0.1038,  0.1036,  0.1038,  0.1036,
         0.1038,  0.1040,  0.1039,  0.1183,  0.1614,  0.1039,  0.1038,  0.1037,
         0.1000,  0.1038,  0.1012,  0.0987,  0.1036,  0.1051,  0.1036,  0.1039,
         0.1039,  0.1036,  0.1040,  0.1038,  0.1256,  0.1013,  0.1036,  0.1084,
         0.1036,  0.0976,  0.1038,  0.1036,  0.1037,  0.1039,  0.1082,  0.1060,
         0.1038,  0.1038,  0.1039,  0.1038,  0.1036,  0.1036,  0.1018,  0.1038,
         0.1038,  0.1045,  0.1122,  0.1037,  0.1036,  0.1036,  0.1039,  0.1037,
         0.1038,  0.1036,  0.1097,  0.1039,  0.1036,  0.0995,  0.1037,  0.1118,
         0.1146,  0.1036,  0.1036,  0.1039,  0.1034,  0.1038,  0.1037,  0.1037,
         0.1038,  0.1036,  0.1036,  0.1037,  0.1009,  0.1102,  0.1037,  0.1038,
         0.1038,  0.1040,  0.1083,  0.1257,  0.1045,  0.1071,  0.1040,  0.1046,
         0.1038,  0.1036,  0.1036,  0.1041,  0.1444,  0.1037,  0.1063,  0.1036,
         0.1032,  0.1038,  0.1038,  0.1039,  0.1036,  0.0977,  0.1039,  0.1036,
         0.1036,  0.1055,  0.1037,  0.1041,  0.1036,  0.0839,  0.1267,  0.1036,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1036,  0.1042,  0.1039,  0.1039,
         0.1019,  0.0544,  0.0986,  0.1040,  0.1133,  0.1037,  0.1040,  0.1038,
         0.1036,  0.1040,  0.1038,  0.1037,  0.1115,  0.1056,  0.1037,  0.1260,
         0.1036,  0.1038,  0.1036,  0.1041,  0.1287,  0.1039,  0.1036,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1039,  0.1061,  0.1038,  0.1041,  0.1186,
         0.1035,  0.1036,  0.1057,  0.1036,  0.1039,  0.1037,  0.1040,  0.1039,
         0.1039,  0.1038,  0.1037,  0.1036,  0.1174,  0.1039,  0.1036,  0.1326,
         0.1133,  0.1022,  0.1085,  0.1039,  0.1249,  0.1041,  0.1110,  0.1033,
         0.1027,  0.1038,  0.1038,  0.1040,  0.1037,  0.1039,  0.1037,  0.1038,
         0.1036,  0.1037,  0.1037,  0.1083,  0.1038,  0.1036,  0.1041,  0.1036],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6938.7881, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9141, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7297., device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8779, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7083.7412, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7754, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7949.6577, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1309, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6632.6665, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0176, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.1655,  0.1165,  0.1296,  0.0818,  0.1257,  0.1150,  0.1150,  0.1158,
         0.1154,  0.1154,  0.1152,  0.1166,  0.1196,  0.1150,  0.1151,  0.1257,
         0.1154,  0.1152,  0.1094,  0.1150,  0.1150,  0.1188,  0.1152,  0.1312,
         0.1160,  0.1154,  0.1152,  0.1151,  0.1152,  0.1219,  0.1205,  0.1154,
         0.1140,  0.1150,  0.1152,  0.1151,  0.1146,  0.1152,  0.1153,  0.1153,
         0.1150,  0.1154,  0.1150,  0.1148,  0.1150,  0.1213,  0.1150,  0.1150,
         0.1152,  0.1157,  0.1150,  0.1285,  0.1152,  0.1149,  0.1151,  0.1147,
         0.1194,  0.1111,  0.1151,  0.1149,  0.1152,  0.1150,  0.1394,  0.1206,
         0.1770,  0.1175,  0.1150,  0.1155,  0.1150,  0.1150,  0.1150,  0.1151,
         0.1150,  0.1151,  0.1149,  0.1153,  0.1151,  0.1083,  0.1151,  0.1455,
         0.1150,  0.1152,  0.1255,  0.1151,  0.1466,  0.1157,  0.1150,  0.1150,
         0.1299,  0.1152,  0.1150,  0.1252,  0.1154,  0.1151,  0.1150,  0.1153,
         0.1283,  0.1158,  0.1653,  0.1150,  0.1154,  0.1151,  0.1154,  0.1153,
         0.1152,  0.1150,  0.1151,  0.1154,  0.1160,  0.1272,  0.1154,  0.1152,
         0.1111,  0.1237,  0.1150,  0.1150,  0.1150,  0.1187,  0.1147,  0.1549,
         0.1150,  0.1150,  0.1221,  0.1151,  0.1132,  0.1154,  0.1227,  0.1150,
         0.1187,  0.1151,  0.1151,  0.1150,  0.1116,  0.1150,  0.1147,  0.1315,
         0.1152,  0.1152,  0.1144,  0.1153,  0.1147,  0.1094,  0.1155,  0.1152,
         0.1149,  0.1153,  0.1415,  0.1154,  0.1153,  0.1153,  0.1665,  0.1157,
         0.1155,  0.1158,  0.1279,  0.1151,  0.1152,  0.1151,  0.1149,  0.1150,
         0.1150,  0.1151,  0.1151,  0.1153,  0.1193,  0.1154,  0.1155,  0.1151,
         0.1154,  0.1153,  0.1150,  0.1151,  0.1149,  0.1155,  0.1151,  0.1152,
         0.1151,  0.1152,  0.1160,  0.1103,  0.1152,  0.1151,  0.1146,  0.1234,
         0.1153,  0.1184,  0.1299,  0.1150,  0.1150,  0.1154,  0.1152,  0.1150,
         0.1150,  0.1172,  0.1151,  0.1140,  0.1175,  0.1152,  0.1154,  0.1155,
         0.1093,  0.1152,  0.1158,  0.1150,  0.1152,  0.1152,  0.1196,  0.1144,
         0.1152,  0.1159,  0.1305,  0.1188,  0.1153,  0.1204,  0.1151,  0.1150,
         0.1199,  0.1151,  0.1467,  0.1150,  0.1153,  0.1152,  0.1153,  0.1150,
         0.1151,  0.1150,  0.1210,  0.1151,  0.1151,  0.1227,  0.1186,  0.1154,
         0.1151,  0.1151,  0.1151,  0.1153,  0.1184,  0.1150,  0.1149,  0.1124,
         0.1150,  0.1180,  0.1163,  0.1138,  0.1150,  0.1151,  0.1160,  0.1146,
         0.1150,  0.1150,  0.1150,  0.1150,  0.1650,  0.1151,  0.1409,  0.1151,
         0.1171,  0.1151,  0.1152,  0.1183,  0.1149,  0.1150,  0.1151,  0.1213,
         0.1154,  0.1151,  0.1153,  0.1150,  0.1150,  0.1152,  0.1140,  0.1151,
         0.1150,  0.1148,  0.1146,  0.1150,  0.1150,  0.1151,  0.1151, -0.0170,
         0.1228,  0.1150,  0.1566,  0.1150,  0.1151,  0.1155,  0.1151,  0.1152,
         0.1150,  0.1152,  0.1150,  0.1150,  0.1147,  0.1153,  0.1151,  0.1151,
         0.1312,  0.1151,  0.1116,  0.1150,  0.1154,  0.1102,  0.1151,  0.1149,
         0.1151,  0.1150,  0.1152,  0.1150,  0.1152,  0.1152,  0.1157,  0.1146,
         0.1152,  0.1225,  0.1151,  0.1152,  0.1453,  0.1154,  0.1151,  0.1151,
         0.1149,  0.1171,  0.1150,  0.1150,  0.1150,  0.1153,  0.1152,  0.1267,
         0.1368,  0.1152,  0.1165,  0.1150,  0.1283,  0.1285,  0.1161,  0.1152,
         0.1152,  0.1279,  0.1149,  0.1152,  0.1214,  0.1151,  0.1150,  0.1161,
         0.1151,  0.1121,  0.1128,  0.1344,  0.1154,  0.1150,  0.1169,  0.1150,
         0.1155,  0.1150,  0.1151,  0.1151,  0.1151,  0.1150,  0.1150,  0.1150,
         0.1157,  0.1151,  0.1149,  0.1152,  0.1433,  0.1249,  0.1150,  0.1150,
         0.1150,  0.1182,  0.1150,  0.1150,  0.1359,  0.1150,  0.1265,  0.1143,
         0.1153,  0.1151,  0.1150,  0.1111,  0.1150,  0.1152,  0.1150,  0.1150,
         0.1150,  0.1151,  0.1151,  0.1105,  0.1654,  0.1150,  0.1154,  0.1150,
         0.1282,  0.1150,  0.1150,  0.1150,  0.1152,  0.1151,  0.1151,  0.1193,
         0.1150,  0.1230,  0.1154,  0.1212,  0.1151,  0.1279,  0.1150,  0.1151,
         0.1146,  0.1154,  0.1151,  0.1151,  0.1151,  0.1151,  0.1153,  0.1151,
         0.1150,  0.1239,  0.1151,  0.1151,  0.1163,  0.1150,  0.1136,  0.1152,
         0.1152,  0.1147,  0.1315,  0.1151,  0.1168,  0.1151,  0.1150,  0.1133,
         0.1152,  0.1219,  0.1150,  0.1154,  0.1169,  0.1142,  0.1197,  0.1237,
         0.1152,  0.1151,  0.1164,  0.1149,  0.1266,  0.1149,  0.1154,  0.1150,
         0.0853,  0.1154,  0.1151,  0.1192,  0.1152,  0.1150,  0.1151,  0.1150,
         0.1151,  0.1154,  0.1153,  0.1312,  0.1740,  0.1152,  0.1151,  0.1151,
         0.1114,  0.1152,  0.1126,  0.1101,  0.1150,  0.1171,  0.1149,  0.1153,
         0.1153,  0.1150,  0.1154,  0.1151,  0.1392,  0.1127,  0.1150,  0.1238,
         0.1150,  0.1089,  0.1152,  0.1150,  0.1151,  0.1152,  0.1196,  0.1172,
         0.1151,  0.1151,  0.1152,  0.1151,  0.1150,  0.1150,  0.1131,  0.1151,
         0.1151,  0.1158,  0.1233,  0.1151,  0.1150,  0.1150,  0.1153,  0.1151,
         0.1151,  0.1150,  0.1210,  0.1153,  0.1150,  0.1108,  0.1150,  0.1265,
         0.1266,  0.1150,  0.1150,  0.1152,  0.1147,  0.1151,  0.1150,  0.1151,
         0.1151,  0.1150,  0.1150,  0.1151,  0.1122,  0.1215,  0.1150,  0.1152,
         0.1152,  0.1154,  0.1211,  0.1392,  0.1161,  0.1197,  0.1154,  0.1158,
         0.1151,  0.1150,  0.1158,  0.1154,  0.1575,  0.1151,  0.1193,  0.1150,
         0.1146,  0.1151,  0.1151,  0.1153,  0.1150,  0.1090,  0.1152,  0.1150,
         0.1150,  0.1169,  0.1150,  0.1154,  0.1150,  0.0966,  0.1383,  0.1150,
         0.1150,  0.1150,  0.1150,  0.1150,  0.1150,  0.1156,  0.1152,  0.1152,
         0.1133,  0.0660,  0.1100,  0.1153,  0.1288,  0.1151,  0.1154,  0.1151,
         0.1150,  0.1153,  0.1151,  0.1151,  0.1250,  0.1169,  0.1151,  0.1373,
         0.1150,  0.1151,  0.1150,  0.1154,  0.1442,  0.1152,  0.1150,  0.1150,
         0.1154,  0.1150,  0.1151,  0.1152,  0.1181,  0.1152,  0.1154,  0.1293,
         0.1148,  0.1150,  0.1183,  0.1150,  0.1153,  0.1151,  0.1154,  0.1153,
         0.1153,  0.1151,  0.1151,  0.1150,  0.1337,  0.1152,  0.1150,  0.1456,
         0.1259,  0.1135,  0.1210,  0.1153,  0.1368,  0.1155,  0.1248,  0.1146,
         0.0872,  0.1152,  0.1151,  0.1154,  0.1151,  0.1152,  0.1151,  0.1152,
         0.1150,  0.1150,  0.1150,  0.1196,  0.1151,  0.1150,  0.1154,  0.1150],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(3922.2012, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7500, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7995.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2695, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5165.4683, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 06:15:05,807][train_inner][INFO] - {"epoch": 12, "update": 11.56, "loss": "4.332", "ntokens": "149737", "nsentences": "537.625", "prob_perplexity": "73.814", "code_perplexity": "73.458", "temp": "1.942", "loss_0": "4.193", "loss_1": "0.128", "loss_2": "0.012", "accuracy": "0.29484", "wps": "37886.2", "ups": "0.25", "wpb": "149736", "bsz": "537.6", "num_updates": "6000", "lr": "9.375e-05", "gnorm": "0.582", "loss_scale": "2", "train_wall": "789", "gb_free": "13.2", "wall": "24172"}
loss: tensor(6312.6582, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2266, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 06:21:40,153][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
loss: tensor(7111.3794, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0625, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6877.8247, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7324, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.1926,  0.1332,  0.1545,  0.0764,  0.1458,  0.1316,  0.1316,  0.1329,
         0.1321,  0.1320,  0.1318,  0.1354,  0.1364,  0.1317,  0.1317,  0.1514,
         0.1320,  0.1318,  0.1261,  0.1316,  0.1317,  0.1370,  0.1318,  0.1487,
         0.1327,  0.1321,  0.1320,  0.1317,  0.1320,  0.1422,  0.1377,  0.1321,
         0.1306,  0.1317,  0.1318,  0.1317,  0.1312,  0.1322,  0.1320,  0.1320,
         0.1316,  0.1321,  0.1317,  0.1315,  0.1317,  0.1423,  0.1317,  0.1317,
         0.1318,  0.1323,  0.1317,  0.1458,  0.1318,  0.1316,  0.1317,  0.1313,
         0.1390,  0.1278,  0.1317,  0.1315,  0.1318,  0.1317,  0.1656,  0.1373,
         0.2018,  0.1370,  0.1317,  0.1322,  0.1317,  0.1316,  0.1316,  0.1318,
         0.1317,  0.1317,  0.1316,  0.1320,  0.1317,  0.1255,  0.1317,  0.1627,
         0.1316,  0.1318,  0.1422,  0.1318,  0.1636,  0.1326,  0.1317,  0.1316,
         0.1487,  0.1318,  0.1316,  0.1445,  0.1321,  0.1317,  0.1316,  0.1320,
         0.1493,  0.1324,  0.1863,  0.1317,  0.1320,  0.1318,  0.1321,  0.1320,
         0.1318,  0.1316,  0.1318,  0.1321,  0.1329,  0.1508,  0.1321,  0.1318,
         0.1278,  0.1415,  0.1317,  0.1316,  0.1316,  0.1354,  0.1313,  0.1735,
         0.1316,  0.1316,  0.1393,  0.1317,  0.1299,  0.1321,  0.1432,  0.1317,
         0.1394,  0.1317,  0.1317,  0.1316,  0.1283,  0.1317,  0.1313,  0.1534,
         0.1318,  0.1321,  0.1311,  0.1321,  0.1315,  0.1260,  0.1322,  0.1318,
         0.1316,  0.1320,  0.1671,  0.1323,  0.1320,  0.1320,  0.1880,  0.1324,
         0.1322,  0.1324,  0.1516,  0.1317,  0.1320,  0.1317,  0.1315,  0.1316,
         0.1317,  0.1318,  0.1317,  0.1320,  0.1411,  0.1321,  0.1322,  0.1318,
         0.1320,  0.1320,  0.1317,  0.1317,  0.1315,  0.1322,  0.1317,  0.1318,
         0.1317,  0.1318,  0.1327,  0.1270,  0.1318,  0.1317,  0.1313,  0.1401,
         0.1320,  0.1351,  0.1562,  0.1317,  0.1317,  0.1321,  0.1320,  0.1317,
         0.1317,  0.1350,  0.1317,  0.1334,  0.1342,  0.1318,  0.1322,  0.1321,
         0.1260,  0.1318,  0.1326,  0.1317,  0.1320,  0.1318,  0.1362,  0.1311,
         0.1318,  0.1349,  0.1473,  0.1367,  0.1321,  0.1401,  0.1318,  0.1317,
         0.1373,  0.1318,  0.1709,  0.1317,  0.1320,  0.1318,  0.1320,  0.1317,
         0.1317,  0.1317,  0.1434,  0.1317,  0.1318,  0.1395,  0.1359,  0.1321,
         0.1318,  0.1317,  0.1318,  0.1320,  0.1360,  0.1317,  0.1316,  0.1290,
         0.1317,  0.1350,  0.1329,  0.1305,  0.1316,  0.1317,  0.1327,  0.1312,
         0.1316,  0.1316,  0.1316,  0.1317,  0.1824,  0.1317,  0.1578,  0.1318,
         0.1338,  0.1317,  0.1320,  0.1350,  0.1316,  0.1317,  0.1324,  0.1379,
         0.1327,  0.1317,  0.1320,  0.1316,  0.1316,  0.1318,  0.1306,  0.1318,
         0.1317,  0.1315,  0.1312,  0.1316,  0.1316,  0.1317,  0.1317, -0.0211,
         0.1410,  0.1316,  0.1801,  0.1317,  0.1317,  0.1322,  0.1317,  0.1318,
         0.1317,  0.1318,  0.1316,  0.1316,  0.1313,  0.1320,  0.1317,  0.1317,
         0.1533,  0.1317,  0.1283,  0.1317,  0.1321,  0.1270,  0.1317,  0.1316,
         0.1317,  0.1317,  0.1320,  0.1317,  0.1318,  0.1318,  0.1335,  0.1313,
         0.1318,  0.1498,  0.1317,  0.1320,  0.1665,  0.1321,  0.1318,  0.1318,
         0.1316,  0.1338,  0.1316,  0.1316,  0.1317,  0.1320,  0.1318,  0.1433,
         0.1478,  0.1320,  0.1332,  0.1317,  0.1487,  0.1086,  0.1340,  0.1318,
         0.1318,  0.1487,  0.1316,  0.1318,  0.1420,  0.1318,  0.1317,  0.1332,
         0.1317,  0.1287,  0.1296,  0.1510,  0.1320,  0.1316,  0.1362,  0.1317,
         0.1322,  0.1317,  0.1317,  0.1317,  0.1317,  0.1316,  0.1317,  0.1316,
         0.1345,  0.1317,  0.1316,  0.1318,  0.1622,  0.1433,  0.1317,  0.1317,
         0.1316,  0.1349,  0.1317,  0.1317,  0.1559,  0.1317,  0.1142,  0.1310,
         0.1326,  0.1317,  0.1317,  0.1277,  0.1316,  0.1320,  0.1316,  0.1317,
         0.1316,  0.1317,  0.1317,  0.1272,  0.1755,  0.1316,  0.1320,  0.1316,
         0.1470,  0.1317,  0.1317,  0.1317,  0.1318,  0.1317,  0.1317,  0.1368,
         0.1317,  0.1399,  0.1321,  0.1434,  0.1317,  0.1493,  0.1316,  0.1317,
         0.1313,  0.1320,  0.1317,  0.1317,  0.1317,  0.1317,  0.1320,  0.1317,
         0.1317,  0.1405,  0.1317,  0.1317,  0.1342,  0.1316,  0.1304,  0.1318,
         0.1318,  0.1313,  0.1492,  0.1317,  0.1334,  0.1317,  0.1317,  0.1300,
         0.1318,  0.1454,  0.1317,  0.1321,  0.1343,  0.1309,  0.1422,  0.1405,
         0.1318,  0.1317,  0.1332,  0.1316,  0.1506,  0.1344,  0.1321,  0.1316,
         0.1018,  0.1320,  0.1317,  0.1360,  0.1318,  0.1317,  0.1318,  0.1317,
         0.1317,  0.1320,  0.1320,  0.1527,  0.2001,  0.1318,  0.1318,  0.1321,
         0.1281,  0.1318,  0.1294,  0.1276,  0.1317,  0.1351,  0.1316,  0.1320,
         0.1320,  0.1317,  0.1321,  0.1318,  0.1617,  0.1294,  0.1316,  0.1492,
         0.1317,  0.1256,  0.1318,  0.1317,  0.1317,  0.1318,  0.1373,  0.1339,
         0.1317,  0.1317,  0.1320,  0.1317,  0.1316,  0.1317,  0.1298,  0.1318,
         0.1318,  0.1326,  0.1454,  0.1317,  0.1316,  0.1317,  0.1320,  0.1317,
         0.1317,  0.1316,  0.1377,  0.1320,  0.1316,  0.1274,  0.1317,  0.1490,
         0.1406,  0.1317,  0.1317,  0.1318,  0.1315,  0.1317,  0.1317,  0.1317,
         0.1317,  0.1316,  0.1316,  0.1317,  0.1289,  0.1382,  0.1317,  0.1318,
         0.1318,  0.1320,  0.1416,  0.1570,  0.1334,  0.1394,  0.1320,  0.1326,
         0.1318,  0.1317,  0.1361,  0.1321,  0.1781,  0.1317,  0.1394,  0.1316,
         0.1312,  0.1317,  0.1318,  0.1320,  0.1317,  0.1257,  0.1318,  0.1317,
         0.1317,  0.1339,  0.1317,  0.1321,  0.1316,  0.1172,  0.1594,  0.1317,
         0.1316,  0.1316,  0.1316,  0.1316,  0.1317,  0.1323,  0.1318,  0.1320,
         0.1299,  0.0832,  0.1266,  0.1320,  0.1542,  0.1317,  0.1321,  0.1317,
         0.1317,  0.1320,  0.1318,  0.1317,  0.1494,  0.1335,  0.1317,  0.1541,
         0.1316,  0.1317,  0.1316,  0.1321,  0.1691,  0.1318,  0.1316,  0.1317,
         0.1321,  0.1316,  0.1317,  0.1320,  0.1387,  0.1318,  0.1321,  0.1459,
         0.1315,  0.1317,  0.1377,  0.1316,  0.1320,  0.1317,  0.1320,  0.1320,
         0.1320,  0.1318,  0.1317,  0.1317,  0.1558,  0.1320,  0.1316,  0.1643,
         0.1465,  0.1302,  0.1410,  0.1320,  0.1572,  0.1321,  0.1477,  0.1313,
         0.0726,  0.1318,  0.1318,  0.1320,  0.1317,  0.1318,  0.1317,  0.1318,
         0.1317,  0.1317,  0.1317,  0.1364,  0.1317,  0.1317,  0.1321,  0.1317],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 06:28:20,320][train_inner][INFO] - {"epoch": 12, "update": 11.946, "loss": "4.308", "ntokens": "149962", "nsentences": "540.725", "prob_perplexity": "75.456", "code_perplexity": "74.973", "temp": "1.94", "loss_0": "4.168", "loss_1": "0.127", "loss_2": "0.012", "accuracy": "0.29754", "wps": "37749.4", "ups": "0.25", "wpb": "149962", "bsz": "540.7", "num_updates": "6200", "lr": "9.6875e-05", "gnorm": "0.578", "loss_scale": "2", "train_wall": "793", "gb_free": "12.6", "wall": "24967"}
[2023-09-12 06:30:06,421][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 06:30:06,422][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 06:30:06,593][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2023-09-12 06:30:30,055][valid][INFO] - {"epoch": 12, "valid_loss": "4.055", "valid_ntokens": "7882.56", "valid_nsentences": "55.2525", "valid_prob_perplexity": "72.475", "valid_code_perplexity": "71.254", "valid_temp": "1.939", "valid_loss_0": "3.915", "valid_loss_1": "0.128", "valid_loss_2": "0.013", "valid_accuracy": "0.34698", "valid_wps": "33390.8", "valid_wpb": "7882.6", "valid_bsz": "55.3", "valid_num_updates": "6228", "valid_best_loss": "4.055"}
[2023-09-12 06:30:30,057][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 6228 updates
[2023-09-12 06:30:30,058][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 06:30:32,613][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 06:30:33,928][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 6228 updates, score 4.055) (writing took 3.871025427011773 seconds)
[2023-09-12 06:30:33,928][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2023-09-12 06:30:33,929][train][INFO] - {"epoch": 12, "train_loss": "4.33", "train_ntokens": "149519", "train_nsentences": "538.397", "train_prob_perplexity": "74.453", "train_code_perplexity": "74.036", "train_temp": "1.941", "train_loss_0": "4.191", "train_loss_1": "0.127", "train_loss_2": "0.012", "train_accuracy": "0.29465", "train_wps": "37209.1", "train_ups": "0.25", "train_wpb": "149519", "train_bsz": "538.4", "train_num_updates": "6228", "train_lr": "9.73125e-05", "train_gnorm": "0.591", "train_loss_scale": "2", "train_train_wall": "2054", "train_gb_free": "15", "train_wall": "25100"}
[2023-09-12 06:30:33,930][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 06:30:34,029][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2023-09-12 06:30:34,268][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 06:30:34,271][fairseq.trainer][INFO] - begin training epoch 13
[2023-09-12 06:30:34,272][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([ 0.1477,  0.1052,  0.1146,  0.0844,  0.1118,  0.1036,  0.1036,  0.1043,
         0.1041,  0.1040,  0.1038,  0.1048,  0.1080,  0.1036,  0.1038,  0.1111,
         0.1040,  0.1039,  0.0978,  0.1036,  0.1037,  0.1069,  0.1038,  0.1204,
         0.1046,  0.1041,  0.1039,  0.1037,  0.1038,  0.1097,  0.1086,  0.1041,
         0.1026,  0.1036,  0.1038,  0.1038,  0.1033,  0.1039,  0.1039,  0.1039,
         0.1036,  0.1041,  0.1037,  0.1035,  0.1037,  0.1087,  0.1036,  0.1036,
         0.1038,  0.1043,  0.1036,  0.1168,  0.1039,  0.1036,  0.1037,  0.1033,
         0.1067,  0.0997,  0.1037,  0.1035,  0.1038,  0.1036,  0.1261,  0.1089,
         0.1593,  0.1050,  0.1036,  0.1042,  0.1036,  0.1036,  0.1036,  0.1038,
         0.1036,  0.1037,  0.1036,  0.1039,  0.1037,  0.0968,  0.1037,  0.1339,
         0.1036,  0.1038,  0.1141,  0.1038,  0.1351,  0.1043,  0.1037,  0.1036,
         0.1174,  0.1038,  0.1036,  0.1119,  0.1041,  0.1038,  0.1036,  0.1039,
         0.1150,  0.1044,  0.1506,  0.1036,  0.1040,  0.1038,  0.1041,  0.1040,
         0.1039,  0.1036,  0.1038,  0.1041,  0.1047,  0.1119,  0.1040,  0.1039,
         0.0999,  0.1112,  0.1037,  0.1036,  0.1036,  0.1072,  0.1033,  0.1434,
         0.1036,  0.1036,  0.1102,  0.1037,  0.1017,  0.1041,  0.1089,  0.1036,
         0.1057,  0.1037,  0.1037,  0.1036,  0.1003,  0.1036,  0.1033,  0.1182,
         0.1038,  0.1038,  0.1031,  0.1039,  0.1035,  0.0980,  0.1042,  0.1038,
         0.1036,  0.1039,  0.1252,  0.1041,  0.1040,  0.1039,  0.1505,  0.1044,
         0.1042,  0.1045,  0.1140,  0.1037,  0.1039,  0.1037,  0.1035,  0.1036,
         0.1036,  0.1038,  0.1038,  0.1039,  0.1042,  0.1041,  0.1042,  0.1038,
         0.1040,  0.1039,  0.1036,  0.1037,  0.1035,  0.1042,  0.1037,  0.1038,
         0.1037,  0.1039,  0.1046,  0.0989,  0.1038,  0.1037,  0.1033,  0.1120,
         0.1039,  0.1071,  0.1155,  0.1036,  0.1037,  0.1041,  0.1039,  0.1036,
         0.1036,  0.1051,  0.1037,  0.1023,  0.1061,  0.1038,  0.1039,  0.1041,
         0.0980,  0.1038,  0.1046,  0.1037,  0.1039,  0.1038,  0.1082,  0.1031,
         0.1039,  0.1040,  0.1190,  0.1059,  0.1038,  0.1068,  0.1038,  0.1036,
         0.1083,  0.1038,  0.1337,  0.1036,  0.1039,  0.1039,  0.1039,  0.1036,
         0.1037,  0.1037,  0.1069,  0.1037,  0.1038,  0.1112,  0.1073,  0.1040,
         0.1038,  0.1037,  0.1038,  0.1039,  0.1059,  0.1037,  0.1035,  0.1010,
         0.1037,  0.1066,  0.1050,  0.1025,  0.1036,  0.1038,  0.1047,  0.1033,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1537,  0.1038,  0.1296,  0.1037,
         0.1058,  0.1037,  0.1039,  0.1070,  0.1036,  0.1036,  0.1037,  0.1100,
         0.1038,  0.1038,  0.1039,  0.1036,  0.1036,  0.1039,  0.1026,  0.1038,
         0.1036,  0.1035,  0.1033,  0.1036,  0.1036,  0.1037,  0.1038, -0.0047,
         0.1099,  0.1036,  0.1388,  0.1036,  0.1037,  0.1041,  0.1037,  0.1039,
         0.1036,  0.1039,  0.1036,  0.1036,  0.1034,  0.1039,  0.1037,  0.1038,
         0.1170,  0.1037,  0.1003,  0.1036,  0.1041,  0.0989,  0.1037,  0.1035,
         0.1038,  0.1037,  0.1039,  0.1036,  0.1038,  0.1039,  0.1042,  0.1033,
         0.1038,  0.1074,  0.1038,  0.1039,  0.1326,  0.1040,  0.1038,  0.1038,
         0.1036,  0.1057,  0.1036,  0.1036,  0.1036,  0.1039,  0.1038,  0.1154,
         0.1190,  0.1038,  0.1052,  0.1036,  0.1141,  0.1204,  0.1044,  0.1038,
         0.1038,  0.1151,  0.1036,  0.1038,  0.1085,  0.1038,  0.1037,  0.1047,
         0.1038,  0.1007,  0.1010,  0.1242,  0.1040,  0.1036,  0.1047,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1037,  0.1037,  0.1036,  0.1036,  0.1036,
         0.1041,  0.1038,  0.1036,  0.1038,  0.1317,  0.1130,  0.1037,  0.1036,
         0.1036,  0.1069,  0.1037,  0.1036,  0.1196,  0.1037,  0.1331,  0.1030,
         0.1038,  0.1037,  0.1036,  0.0997,  0.1036,  0.1039,  0.1036,  0.1036,
         0.1036,  0.1037,  0.1038,  0.0992,  0.1473,  0.1036,  0.1040,  0.1036,
         0.1167,  0.1036,  0.1036,  0.1036,  0.1038,  0.1037,  0.1037,  0.1086,
         0.1036,  0.1116,  0.1040,  0.1076,  0.1037,  0.1135,  0.1036,  0.1037,
         0.1033,  0.1040,  0.1038,  0.1037,  0.1037,  0.1037,  0.1039,  0.1038,
         0.1036,  0.1141,  0.1037,  0.1037,  0.1041,  0.1036,  0.1024,  0.1038,
         0.1039,  0.1033,  0.1194,  0.1037,  0.1055,  0.1037,  0.1036,  0.1020,
         0.1038,  0.1068,  0.1037,  0.1040,  0.1054,  0.1029,  0.1068,  0.1122,
         0.1039,  0.1038,  0.1050,  0.1036,  0.1118,  0.1028,  0.1041,  0.1036,
         0.0750,  0.1040,  0.1037,  0.1078,  0.1038,  0.1036,  0.1038,  0.1036,
         0.1038,  0.1040,  0.1039,  0.1183,  0.1614,  0.1039,  0.1038,  0.1037,
         0.1000,  0.1038,  0.1012,  0.0987,  0.1036,  0.1051,  0.1036,  0.1039,
         0.1039,  0.1036,  0.1040,  0.1038,  0.1256,  0.1013,  0.1036,  0.1084,
         0.1036,  0.0976,  0.1038,  0.1036,  0.1037,  0.1039,  0.1082,  0.1060,
         0.1038,  0.1038,  0.1039,  0.1038,  0.1036,  0.1036,  0.1018,  0.1038,
         0.1038,  0.1045,  0.1122,  0.1037,  0.1036,  0.1036,  0.1039,  0.1037,
         0.1038,  0.1036,  0.1097,  0.1039,  0.1036,  0.0995,  0.1037,  0.1118,
         0.1146,  0.1036,  0.1036,  0.1039,  0.1034,  0.1038,  0.1037,  0.1037,
         0.1038,  0.1036,  0.1036,  0.1037,  0.1009,  0.1102,  0.1037,  0.1038,
         0.1038,  0.1040,  0.1083,  0.1257,  0.1045,  0.1071,  0.1040,  0.1046,
         0.1038,  0.1036,  0.1036,  0.1041,  0.1444,  0.1037,  0.1063,  0.1036,
         0.1032,  0.1038,  0.1038,  0.1039,  0.1036,  0.0977,  0.1039,  0.1036,
         0.1036,  0.1055,  0.1037,  0.1041,  0.1036,  0.0839,  0.1267,  0.1036,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1036,  0.1042,  0.1039,  0.1039,
         0.1019,  0.0544,  0.0986,  0.1040,  0.1133,  0.1037,  0.1040,  0.1038,
         0.1036,  0.1040,  0.1038,  0.1037,  0.1115,  0.1056,  0.1037,  0.1260,
         0.1036,  0.1038,  0.1036,  0.1041,  0.1287,  0.1039,  0.1036,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1039,  0.1061,  0.1038,  0.1041,  0.1186,
         0.1035,  0.1036,  0.1057,  0.1036,  0.1039,  0.1037,  0.1040,  0.1039,
         0.1039,  0.1038,  0.1037,  0.1036,  0.1174,  0.1039,  0.1036,  0.1326,
         0.1133,  0.1022,  0.1085,  0.1039,  0.1249,  0.1041,  0.1110,  0.1033,
         0.1027,  0.1038,  0.1038,  0.1040,  0.1037,  0.1039,  0.1037,  0.1038,
         0.1036,  0.1037,  0.1037,  0.1083,  0.1038,  0.1036,  0.1041,  0.1036],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(4042.7456, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8545, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7226.7959, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3262, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6839.6201, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8057, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7302.0093, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3643, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5834.9277, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7539, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7340.5024, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8604, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7358.0234, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8672, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5059.0933, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9404, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5211.8550, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2090, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4117.9507, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5410, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6366.7100, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: Parameter containing:
tensor([ 0.1477,  0.1052,  0.1146,  0.0844,  0.1118,  0.1036,  0.1036,  0.1043,
         0.1041,  0.1040,  0.1038,  0.1048,  0.1080,  0.1036,  0.1038,  0.1111,
         0.1040,  0.1039,  0.0978,  0.1036,  0.1037,  0.1069,  0.1038,  0.1204,
         0.1046,  0.1041,  0.1039,  0.1037,  0.1038,  0.1097,  0.1086,  0.1041,
         0.1026,  0.1036,  0.1038,  0.1038,  0.1033,  0.1039,  0.1039,  0.1039,
         0.1036,  0.1041,  0.1037,  0.1035,  0.1037,  0.1087,  0.1036,  0.1036,
         0.1038,  0.1043,  0.1036,  0.1168,  0.1039,  0.1036,  0.1037,  0.1033,
         0.1067,  0.0997,  0.1037,  0.1035,  0.1038,  0.1036,  0.1261,  0.1089,
         0.1593,  0.1050,  0.1036,  0.1042,  0.1036,  0.1036,  0.1036,  0.1038,
         0.1036,  0.1037,  0.1036,  0.1039,  0.1037,  0.0968,  0.1037,  0.1339,
         0.1036,  0.1038,  0.1141,  0.1038,  0.1351,  0.1043,  0.1037,  0.1036,
         0.1174,  0.1038,  0.1036,  0.1119,  0.1041,  0.1038,  0.1036,  0.1039,
         0.1150,  0.1044,  0.1506,  0.1036,  0.1040,  0.1038,  0.1041,  0.1040,
         0.1039,  0.1036,  0.1038,  0.1041,  0.1047,  0.1119,  0.1040,  0.1039,
         0.0999,  0.1112,  0.1037,  0.1036,  0.1036,  0.1072,  0.1033,  0.1434,
         0.1036,  0.1036,  0.1102,  0.1037,  0.1017,  0.1041,  0.1089,  0.1036,
         0.1057,  0.1037,  0.1037,  0.1036,  0.1003,  0.1036,  0.1033,  0.1182,
         0.1038,  0.1038,  0.1031,  0.1039,  0.1035,  0.0980,  0.1042,  0.1038,
         0.1036,  0.1039,  0.1252,  0.1041,  0.1040,  0.1039,  0.1505,  0.1044,
         0.1042,  0.1045,  0.1140,  0.1037,  0.1039,  0.1037,  0.1035,  0.1036,
         0.1036,  0.1038,  0.1038,  0.1039,  0.1042,  0.1041,  0.1042,  0.1038,
         0.1040,  0.1039,  0.1036,  0.1037,  0.1035,  0.1042,  0.1037,  0.1038,
         0.1037,  0.1039,  0.1046,  0.0989,  0.1038,  0.1037,  0.1033,  0.1120,
         0.1039,  0.1071,  0.1155,  0.1036,  0.1037,  0.1041,  0.1039,  0.1036,
         0.1036,  0.1051,  0.1037,  0.1023,  0.1061,  0.1038,  0.1039,  0.1041,
         0.0980,  0.1038,  0.1046,  0.1037,  0.1039,  0.1038,  0.1082,  0.1031,
         0.1039,  0.1040,  0.1190,  0.1059,  0.1038,  0.1068,  0.1038,  0.1036,
         0.1083,  0.1038,  0.1337,  0.1036,  0.1039,  0.1039,  0.1039,  0.1036,
         0.1037,  0.1037,  0.1069,  0.1037,  0.1038,  0.1112,  0.1073,  0.1040,
         0.1038,  0.1037,  0.1038,  0.1039,  0.1059,  0.1037,  0.1035,  0.1010,
         0.1037,  0.1066,  0.1050,  0.1025,  0.1036,  0.1038,  0.1047,  0.1033,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1537,  0.1038,  0.1296,  0.1037,
         0.1058,  0.1037,  0.1039,  0.1070,  0.1036,  0.1036,  0.1037,  0.1100,
         0.1038,  0.1038,  0.1039,  0.1036,  0.1036,  0.1039,  0.1026,  0.1038,
         0.1036,  0.1035,  0.1033,  0.1036,  0.1036,  0.1037,  0.1038, -0.0047,
         0.1099,  0.1036,  0.1388,  0.1036,  0.1037,  0.1041,  0.1037,  0.1039,
         0.1036,  0.1039,  0.1036,  0.1036,  0.1034,  0.1039,  0.1037,  0.1038,
         0.1170,  0.1037,  0.1003,  0.1036,  0.1041,  0.0989,  0.1037,  0.1035,
         0.1038,  0.1037,  0.1039,  0.1036,  0.1038,  0.1039,  0.1042,  0.1033,
         0.1038,  0.1074,  0.1038,  0.1039,  0.1326,  0.1040,  0.1038,  0.1038,
         0.1036,  0.1057,  0.1036,  0.1036,  0.1036,  0.1039,  0.1038,  0.1154,
         0.1190,  0.1038,  0.1052,  0.1036,  0.1141,  0.1204,  0.1044,  0.1038,
         0.1038,  0.1151,  0.1036,  0.1038,  0.1085,  0.1038,  0.1037,  0.1047,
         0.1038,  0.1007,  0.1010,  0.1242,  0.1040,  0.1036,  0.1047,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1037,  0.1037,  0.1036,  0.1036,  0.1036,
         0.1041,  0.1038,  0.1036,  0.1038,  0.1317,  0.1130,  0.1037,  0.1036,
         0.1036,  0.1069,  0.1037,  0.1036,  0.1196,  0.1037,  0.1331,  0.1030,
         0.1038,  0.1037,  0.1036,  0.0997,  0.1036,  0.1039,  0.1036,  0.1036,
         0.1036,  0.1037,  0.1038,  0.0992,  0.1473,  0.1036,  0.1040,  0.1036,
         0.1167,  0.1036,  0.1036,  0.1036,  0.1038,  0.1037,  0.1037,  0.1086,
         0.1036,  0.1116,  0.1040,  0.1076,  0.1037,  0.1135,  0.1036,  0.1037,
         0.1033,  0.1040,  0.1038,  0.1037,  0.1037,  0.1037,  0.1039,  0.1038,
         0.1036,  0.1141,  0.1037,  0.1037,  0.1041,  0.1036,  0.1024,  0.1038,
         0.1039,  0.1033,  0.1194,  0.1037,  0.1055,  0.1037,  0.1036,  0.1020,
         0.1038,  0.1068,  0.1037,  0.1040,  0.1054,  0.1029,  0.1068,  0.1122,
         0.1039,  0.1038,  0.1050,  0.1036,  0.1118,  0.1028,  0.1041,  0.1036,
         0.0750,  0.1040,  0.1037,  0.1078,  0.1038,  0.1036,  0.1038,  0.1036,
         0.1038,  0.1040,  0.1039,  0.1183,  0.1614,  0.1039,  0.1038,  0.1037,
         0.1000,  0.1038,  0.1012,  0.0987,  0.1036,  0.1051,  0.1036,  0.1039,
         0.1039,  0.1036,  0.1040,  0.1038,  0.1256,  0.1013,  0.1036,  0.1084,
         0.1036,  0.0976,  0.1038,  0.1036,  0.1037,  0.1039,  0.1082,  0.1060,
         0.1038,  0.1038,  0.1039,  0.1038,  0.1036,  0.1036,  0.1018,  0.1038,
         0.1038,  0.1045,  0.1122,  0.1037,  0.1036,  0.1036,  0.1039,  0.1037,
         0.1038,  0.1036,  0.1097,  0.1039,  0.1036,  0.0995,  0.1037,  0.1118,
         0.1146,  0.1036,  0.1036,  0.1039,  0.1034,  0.1038,  0.1037,  0.1037,
         0.1038,  0.1036,  0.1036,  0.1037,  0.1009,  0.1102,  0.1037,  0.1038,
         0.1038,  0.1040,  0.1083,  0.1257,  0.1045,  0.1071,  0.1040,  0.1046,
         0.1038,  0.1036,  0.1036,  0.1041,  0.1444,  0.1037,  0.1063,  0.1036,
         0.1032,  0.1038,  0.1038,  0.1039,  0.1036,  0.0977,  0.1039,  0.1036,
         0.1036,  0.1055,  0.1037,  0.1041,  0.1036,  0.0839,  0.1267,  0.1036,
         0.1036,  0.1036,  0.1036,  0.1036,  0.1036,  0.1042,  0.1039,  0.1039,
         0.1019,  0.0544,  0.0986,  0.1040,  0.1133,  0.1037,  0.1040,  0.1038,
         0.1036,  0.1040,  0.1038,  0.1037,  0.1115,  0.1056,  0.1037,  0.1260,
         0.1036,  0.1038,  0.1036,  0.1041,  0.1287,  0.1039,  0.1036,  0.1036,
         0.1041,  0.1036,  0.1037,  0.1039,  0.1061,  0.1038,  0.1041,  0.1186,
         0.1035,  0.1036,  0.1057,  0.1036,  0.1039,  0.1037,  0.1040,  0.1039,
         0.1039,  0.1038,  0.1037,  0.1036,  0.1174,  0.1039,  0.1036,  0.1326,
         0.1133,  0.1022,  0.1085,  0.1039,  0.1249,  0.1041,  0.1110,  0.1033,
         0.1027,  0.1038,  0.1038,  0.1040,  0.1037,  0.1039,  0.1037,  0.1038,
         0.1036,  0.1037,  0.1037,  0.1083,  0.1038,  0.1036,  0.1041,  0.1036],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7519.3340, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3730, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7026.5732, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7588, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7937.8208, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2031, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7920.8174, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6484, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5024.3018, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8745, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8537.4062, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3135, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4787.2432, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6855, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7098.6729, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6377, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7435.0508, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1934, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7963.1826, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8018, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5451.1157, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(3798.8140, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3203, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7550.4062, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7783, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6249.6885, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9805, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7113.3569, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5527, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 06:40:16,557][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
tensor(-2.3848, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.1973,  0.1473,  0.1760,  0.0741,  0.1649,  0.1458,  0.1459,  0.1472,
         0.1462,  0.1462,  0.1460,  0.1515,  0.1508,  0.1459,  0.1460,  0.1703,
         0.1461,  0.1460,  0.1405,  0.1459,  0.1459,  0.1506,  0.1460,  0.1567,
         0.1470,  0.1462,  0.1461,  0.1459,  0.1461,  0.1587,  0.1521,  0.1462,
         0.1448,  0.1459,  0.1460,  0.1459,  0.1454,  0.1470,  0.1461,  0.1461,
         0.1459,  0.1462,  0.1459,  0.1456,  0.1459,  0.1599,  0.1459,  0.1459,
         0.1460,  0.1465,  0.1459,  0.1608,  0.1461,  0.1458,  0.1459,  0.1455,
         0.1549,  0.1420,  0.1459,  0.1456,  0.1460,  0.1459,  0.1775,  0.1517,
         0.2217,  0.1532,  0.1459,  0.1464,  0.1459,  0.1459,  0.1459,  0.1460,
         0.1459,  0.1459,  0.1458,  0.1461,  0.1459,  0.1406,  0.1459,  0.1772,
         0.1459,  0.1460,  0.1565,  0.1460,  0.1777,  0.1478,  0.1459,  0.1459,
         0.1632,  0.1460,  0.1458,  0.1609,  0.1462,  0.1459,  0.1459,  0.1461,
         0.1665,  0.1466,  0.2042,  0.1459,  0.1462,  0.1460,  0.1462,  0.1461,
         0.1461,  0.1458,  0.1460,  0.1462,  0.1473,  0.1698,  0.1462,  0.1460,
         0.1420,  0.1547,  0.1459,  0.1459,  0.1458,  0.1495,  0.1455,  0.1888,
         0.1459,  0.1459,  0.1537,  0.1459,  0.1443,  0.1462,  0.1593,  0.1459,
         0.1547,  0.1459,  0.1459,  0.1459,  0.1425,  0.1459,  0.1455,  0.1705,
         0.1460,  0.1465,  0.1453,  0.1465,  0.1456,  0.1401,  0.1464,  0.1460,
         0.1458,  0.1461,  0.1849,  0.1467,  0.1461,  0.1461,  0.2004,  0.1467,
         0.1464,  0.1467,  0.1683,  0.1459,  0.1464,  0.1459,  0.1456,  0.1459,
         0.1459,  0.1460,  0.1460,  0.1461,  0.1576,  0.1466,  0.1464,  0.1460,
         0.1461,  0.1461,  0.1459,  0.1459,  0.1458,  0.1465,  0.1459,  0.1460,
         0.1459,  0.1460,  0.1469,  0.1411,  0.1460,  0.1459,  0.1455,  0.1543,
         0.1461,  0.1493,  0.1764,  0.1459,  0.1459,  0.1462,  0.1461,  0.1459,
         0.1459,  0.1500,  0.1459,  0.1503,  0.1483,  0.1460,  0.1471,  0.1462,
         0.1401,  0.1460,  0.1467,  0.1459,  0.1461,  0.1460,  0.1505,  0.1453,
         0.1460,  0.1504,  0.1617,  0.1519,  0.1469,  0.1580,  0.1460,  0.1459,
         0.1520,  0.1460,  0.1815,  0.1459,  0.1461,  0.1461,  0.1461,  0.1460,
         0.1459,  0.1459,  0.1615,  0.1459,  0.1461,  0.1539,  0.1508,  0.1462,
         0.1460,  0.1459,  0.1460,  0.1461,  0.1503,  0.1459,  0.1460,  0.1432,
         0.1459,  0.1494,  0.1471,  0.1447,  0.1459,  0.1459,  0.1469,  0.1454,
         0.1459,  0.1459,  0.1459,  0.1459,  0.1985,  0.1459,  0.1721,  0.1462,
         0.1479,  0.1459,  0.1461,  0.1492,  0.1458,  0.1459,  0.1471,  0.1521,
         0.1476,  0.1459,  0.1464,  0.1459,  0.1459,  0.1461,  0.1448,  0.1460,
         0.1459,  0.1456,  0.1455,  0.1459,  0.1459,  0.1459,  0.1460, -0.0063,
         0.1550,  0.1459,  0.1986,  0.1459,  0.1459,  0.1464,  0.1459,  0.1460,
         0.1459,  0.1460,  0.1459,  0.1459,  0.1455,  0.1461,  0.1459,  0.1460,
         0.1703,  0.1459,  0.1425,  0.1459,  0.1462,  0.1411,  0.1459,  0.1458,
         0.1459,  0.1459,  0.1461,  0.1459,  0.1460,  0.1460,  0.1490,  0.1455,
         0.1460,  0.1680,  0.1459,  0.1461,  0.1833,  0.1462,  0.1460,  0.1460,
         0.1458,  0.1479,  0.1458,  0.1458,  0.1459,  0.1461,  0.1460,  0.1575,
         0.1504,  0.1461,  0.1473,  0.1459,  0.1649,  0.0898,  0.1490,  0.1460,
         0.1462,  0.1656,  0.1458,  0.1460,  0.1589,  0.1460,  0.1459,  0.1475,
         0.1459,  0.1428,  0.1443,  0.1650,  0.1462,  0.1459,  0.1517,  0.1459,
         0.1464,  0.1459,  0.1459,  0.1459,  0.1459,  0.1459,  0.1459,  0.1458,
         0.1509,  0.1460,  0.1458,  0.1460,  0.1749,  0.1592,  0.1459,  0.1459,
         0.1458,  0.1490,  0.1459,  0.1459,  0.1724,  0.1459,  0.1132,  0.1451,
         0.1493,  0.1459,  0.1459,  0.1418,  0.1459,  0.1461,  0.1459,  0.1459,
         0.1459,  0.1459,  0.1459,  0.1414,  0.1777,  0.1459,  0.1462,  0.1459,
         0.1632,  0.1459,  0.1459,  0.1459,  0.1460,  0.1459,  0.1459,  0.1519,
         0.1459,  0.1543,  0.1462,  0.1615,  0.1459,  0.1658,  0.1458,  0.1459,
         0.1460,  0.1461,  0.1459,  0.1459,  0.1459,  0.1459,  0.1461,  0.1460,
         0.1459,  0.1583,  0.1459,  0.1459,  0.1488,  0.1459,  0.1445,  0.1460,
         0.1461,  0.1455,  0.1650,  0.1459,  0.1476,  0.1459,  0.1459,  0.1442,
         0.1460,  0.1635,  0.1459,  0.1462,  0.1486,  0.1450,  0.1624,  0.1552,
         0.1461,  0.1459,  0.1475,  0.1458,  0.1711,  0.1534,  0.1462,  0.1459,
         0.1166,  0.1461,  0.1459,  0.1503,  0.1460,  0.1459,  0.1460,  0.1459,
         0.1459,  0.1461,  0.1461,  0.1694,  0.2128,  0.1461,  0.1460,  0.1469,
         0.1422,  0.1460,  0.1437,  0.1427,  0.1459,  0.1501,  0.1458,  0.1461,
         0.1461,  0.1459,  0.1464,  0.1460,  0.1803,  0.1436,  0.1458,  0.1703,
         0.1459,  0.1398,  0.1460,  0.1459,  0.1459,  0.1460,  0.1532,  0.1481,
         0.1460,  0.1459,  0.1461,  0.1459,  0.1459,  0.1459,  0.1439,  0.1460,
         0.1461,  0.1467,  0.1605,  0.1459,  0.1459,  0.1459,  0.1461,  0.1459,
         0.1459,  0.1459,  0.1519,  0.1461,  0.1459,  0.1417,  0.1459,  0.1681,
         0.1550,  0.1459,  0.1459,  0.1460,  0.1456,  0.1460,  0.1459,  0.1459,
         0.1459,  0.1459,  0.1459,  0.1459,  0.1431,  0.1523,  0.1459,  0.1460,
         0.1460,  0.1461,  0.1567,  0.1724,  0.1483,  0.1558,  0.1461,  0.1467,
         0.1460,  0.1459,  0.1532,  0.1462,  0.1940,  0.1459,  0.1530,  0.1459,
         0.1454,  0.1459,  0.1460,  0.1461,  0.1459,  0.1399,  0.1460,  0.1459,
         0.1459,  0.1482,  0.1459,  0.1462,  0.1459,  0.1331,  0.1758,  0.1459,
         0.1459,  0.1458,  0.1459,  0.1458,  0.1459,  0.1465,  0.1460,  0.1461,
         0.1440,  0.0973,  0.1409,  0.1461,  0.1742,  0.1459,  0.1462,  0.1459,
         0.1459,  0.1461,  0.1460,  0.1459,  0.1678,  0.1478,  0.1459,  0.1682,
         0.1459,  0.1460,  0.1459,  0.1462,  0.1864,  0.1460,  0.1459,  0.1459,
         0.1462,  0.1459,  0.1459,  0.1462,  0.1537,  0.1460,  0.1462,  0.1600,
         0.1456,  0.1459,  0.1547,  0.1459,  0.1461,  0.1459,  0.1461,  0.1461,
         0.1461,  0.1460,  0.1459,  0.1459,  0.1729,  0.1461,  0.1459,  0.1796,
         0.1635,  0.1444,  0.1575,  0.1461,  0.1737,  0.1464,  0.1637,  0.1455,
         0.0575,  0.1460,  0.1466,  0.1462,  0.1459,  0.1460,  0.1459,  0.1460,
         0.1459,  0.1459,  0.1459,  0.1505,  0.1460,  0.1459,  0.1462,  0.1459],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(7081.1504, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3027, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 06:41:58,748][train_inner][INFO] - {"epoch": 13, "update": 12.332, "loss": "4.34", "ntokens": "148698", "nsentences": "533.715", "prob_perplexity": "80.466", "code_perplexity": "79.495", "temp": "1.938", "loss_0": "4.202", "loss_1": "0.126", "loss_2": "0.013", "accuracy": "0.28443", "wps": "36337.4", "ups": "0.24", "wpb": "148698", "bsz": "533.7", "num_updates": "6400", "lr": "0.0001", "gnorm": "0.613", "loss_scale": "2", "train_wall": "789", "gb_free": "12.9", "wall": "25785"}
Parameter containing:
tensor([ 0.1754,  0.1222,  0.1378,  0.0800,  0.1315,  0.1207,  0.1207,  0.1216,
         0.1211,  0.1210,  0.1208,  0.1226,  0.1252,  0.1207,  0.1208,  0.1345,
         0.1210,  0.1209,  0.1151,  0.1207,  0.1207,  0.1246,  0.1208,  0.1368,
         0.1217,  0.1211,  0.1210,  0.1207,  0.1209,  0.1284,  0.1262,  0.1211,
         0.1196,  0.1207,  0.1208,  0.1208,  0.1203,  0.1211,  0.1210,  0.1210,
         0.1207,  0.1211,  0.1207,  0.1205,  0.1207,  0.1281,  0.1207,  0.1207,
         0.1208,  0.1213,  0.1207,  0.1345,  0.1209,  0.1206,  0.1207,  0.1204,
         0.1265,  0.1168,  0.1207,  0.1205,  0.1208,  0.1207,  0.1475,  0.1262,
         0.1842,  0.1241,  0.1207,  0.1212,  0.1207,  0.1207,  0.1207,  0.1208,
         0.1207,  0.1207,  0.1206,  0.1210,  0.1207,  0.1141,  0.1207,  0.1515,
         0.1207,  0.1208,  0.1312,  0.1208,  0.1525,  0.1215,  0.1207,  0.1207,
         0.1367,  0.1208,  0.1207,  0.1317,  0.1211,  0.1208,  0.1207,  0.1210,
         0.1339,  0.1215,  0.1721,  0.1207,  0.1210,  0.1208,  0.1210,  0.1210,
         0.1209,  0.1207,  0.1208,  0.1211,  0.1218,  0.1356,  0.1210,  0.1209,
         0.1168,  0.1299,  0.1207,  0.1207,  0.1207,  0.1244,  0.1204,  0.1613,
         0.1207,  0.1207,  0.1278,  0.1207,  0.1189,  0.1211,  0.1306,  0.1207,
         0.1261,  0.1207,  0.1207,  0.1207,  0.1172,  0.1207,  0.1204,  0.1390,
         0.1208,  0.1209,  0.1201,  0.1210,  0.1204,  0.1151,  0.1212,  0.1208,
         0.1206,  0.1210,  0.1503,  0.1212,  0.1210,  0.1210,  0.1748,  0.1215,
         0.1212,  0.1215,  0.1362,  0.1207,  0.1209,  0.1207,  0.1205,  0.1207,
         0.1207,  0.1208,  0.1208,  0.1210,  0.1259,  0.1211,  0.1212,  0.1208,
         0.1210,  0.1210,  0.1207,  0.1207,  0.1205,  0.1212,  0.1207,  0.1208,
         0.1207,  0.1209,  0.1216,  0.1160,  0.1208,  0.1207,  0.1203,  0.1290,
         0.1210,  0.1241,  0.1388,  0.1207,  0.1207,  0.1211,  0.1209,  0.1207,
         0.1207,  0.1231,  0.1207,  0.1205,  0.1232,  0.1208,  0.1211,  0.1212,
         0.1150,  0.1208,  0.1215,  0.1207,  0.1209,  0.1208,  0.1252,  0.1201,
         0.1209,  0.1220,  0.1362,  0.1248,  0.1210,  0.1271,  0.1208,  0.1207,
         0.1259,  0.1208,  0.1548,  0.1207,  0.1210,  0.1209,  0.1210,  0.1207,
         0.1207,  0.1207,  0.1288,  0.1207,  0.1208,  0.1284,  0.1245,  0.1210,
         0.1208,  0.1207,  0.1208,  0.1210,  0.1239,  0.1207,  0.1205,  0.1180,
         0.1207,  0.1238,  0.1220,  0.1195,  0.1207,  0.1207,  0.1217,  0.1203,
         0.1207,  0.1207,  0.1207,  0.1207,  0.1711,  0.1208,  0.1467,  0.1208,
         0.1228,  0.1207,  0.1209,  0.1240,  0.1206,  0.1207,  0.1211,  0.1270,
         0.1212,  0.1208,  0.1210,  0.1207,  0.1207,  0.1209,  0.1196,  0.1208,
         0.1207,  0.1205,  0.1203,  0.1207,  0.1207,  0.1207,  0.1208, -0.0213,
         0.1283,  0.1207,  0.1655,  0.1207,  0.1207,  0.1212,  0.1207,  0.1209,
         0.1207,  0.1208,  0.1207,  0.1207,  0.1204,  0.1210,  0.1207,  0.1208,
         0.1392,  0.1207,  0.1173,  0.1207,  0.1211,  0.1160,  0.1207,  0.1205,
         0.1208,  0.1207,  0.1210,  0.1207,  0.1208,  0.1208,  0.1217,  0.1204,
         0.1208,  0.1320,  0.1207,  0.1209,  0.1532,  0.1210,  0.1208,  0.1208,
         0.1206,  0.1228,  0.1207,  0.1207,  0.1207,  0.1210,  0.1208,  0.1323,
         0.1428,  0.1209,  0.1222,  0.1207,  0.1349,  0.1224,  0.1219,  0.1208,
         0.1208,  0.1338,  0.1206,  0.1208,  0.1278,  0.1208,  0.1207,  0.1219,
         0.1208,  0.1177,  0.1184,  0.1394,  0.1210,  0.1207,  0.1232,  0.1207,
         0.1212,  0.1207,  0.1207,  0.1207,  0.1207,  0.1207,  0.1207,  0.1207,
         0.1220,  0.1208,  0.1206,  0.1208,  0.1497,  0.1311,  0.1207,  0.1207,
         0.1207,  0.1239,  0.1207,  0.1207,  0.1411,  0.1207,  0.1240,  0.1200,
         0.1213,  0.1207,  0.1207,  0.1168,  0.1207,  0.1210,  0.1207,  0.1207,
         0.1207,  0.1207,  0.1208,  0.1162,  0.1730,  0.1207,  0.1210,  0.1207,
         0.1338,  0.1207,  0.1207,  0.1207,  0.1208,  0.1207,  0.1207,  0.1252,
         0.1207,  0.1289,  0.1210,  0.1285,  0.1207,  0.1355,  0.1207,  0.1207,
         0.1203,  0.1210,  0.1208,  0.1207,  0.1207,  0.1207,  0.1210,  0.1208,
         0.1207,  0.1277,  0.1207,  0.1207,  0.1223,  0.1207,  0.1194,  0.1208,
         0.1209,  0.1204,  0.1373,  0.1207,  0.1225,  0.1207,  0.1207,  0.1190,
         0.1208,  0.1305,  0.1207,  0.1210,  0.1230,  0.1199,  0.1270,  0.1293,
         0.1209,  0.1208,  0.1221,  0.1206,  0.1355,  0.1212,  0.1211,  0.1207,
         0.0906,  0.1210,  0.1207,  0.1249,  0.1208,  0.1207,  0.1208,  0.1207,
         0.1208,  0.1210,  0.1210,  0.1388,  0.1833,  0.1209,  0.1208,  0.1209,
         0.1171,  0.1208,  0.1183,  0.1161,  0.1207,  0.1230,  0.1206,  0.1210,
         0.1210,  0.1207,  0.1210,  0.1208,  0.1472,  0.1183,  0.1207,  0.1328,
         0.1207,  0.1146,  0.1208,  0.1207,  0.1207,  0.1209,  0.1261,  0.1230,
         0.1208,  0.1208,  0.1209,  0.1208,  0.1207,  0.1207,  0.1188,  0.1208,
         0.1208,  0.1215,  0.1304,  0.1207,  0.1207,  0.1207,  0.1210,  0.1207,
         0.1207,  0.1207,  0.1267,  0.1210,  0.1207,  0.1165,  0.1207,  0.1348,
         0.1311,  0.1207,  0.1207,  0.1208,  0.1204,  0.1208,  0.1207,  0.1207,
         0.1207,  0.1207,  0.1207,  0.1207,  0.1179,  0.1272,  0.1207,  0.1208,
         0.1208,  0.1210,  0.1277,  0.1451,  0.1220,  0.1263,  0.1210,  0.1216,
         0.1208,  0.1207,  0.1219,  0.1211,  0.1649,  0.1207,  0.1271,  0.1207,
         0.1202,  0.1208,  0.1208,  0.1210,  0.1207,  0.1147,  0.1209,  0.1207,
         0.1207,  0.1228,  0.1207,  0.1211,  0.1207,  0.1038,  0.1456,  0.1207,
         0.1207,  0.1207,  0.1207,  0.1207,  0.1207,  0.1213,  0.1208,  0.1210,
         0.1190,  0.0720,  0.1157,  0.1210,  0.1361,  0.1207,  0.1210,  0.1208,
         0.1207,  0.1210,  0.1208,  0.1207,  0.1324,  0.1226,  0.1207,  0.1431,
         0.1207,  0.1208,  0.1207,  0.1211,  0.1531,  0.1209,  0.1207,  0.1207,
         0.1211,  0.1207,  0.1207,  0.1210,  0.1246,  0.1208,  0.1211,  0.1349,
         0.1205,  0.1207,  0.1248,  0.1207,  0.1210,  0.1207,  0.1210,  0.1210,
         0.1210,  0.1208,  0.1207,  0.1207,  0.1420,  0.1209,  0.1207,  0.1527,
         0.1324,  0.1192,  0.1278,  0.1210,  0.1443,  0.1212,  0.1318,  0.1203,
         0.0817,  0.1208,  0.1208,  0.1210,  0.1207,  0.1209,  0.1207,  0.1208,
         0.1207,  0.1207,  0.1207,  0.1254,  0.1208,  0.1207,  0.1211,  0.1207],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7466.9736, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7021, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6553.5020, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0508, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7970.8452, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1816, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6147.5015, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7070, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5333.0957, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3535, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6638.2295, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9790, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7553.2544, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0840, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7053.7964, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4277, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8439.0674, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2910, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7005.6558, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7910, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7212.5293, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(7793.5586, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8799, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7749.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3477, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1931, 0.1593, 0.1927, 0.0728, 0.1826, 0.1578, 0.1578, 0.1595, 0.1582,
        0.1582, 0.1580, 0.1671, 0.1636, 0.1578, 0.1580, 0.1854, 0.1582, 0.1581,
        0.1527, 0.1578, 0.1578, 0.1635, 0.1580, 0.1674, 0.1591, 0.1582, 0.1581,
        0.1580, 0.1581, 0.1741, 0.1644, 0.1582, 0.1567, 0.1578, 0.1581, 0.1580,
        0.1575, 0.1600, 0.1581, 0.1581, 0.1578, 0.1583, 0.1578, 0.1577, 0.1578,
        0.1743, 0.1578, 0.1578, 0.1581, 0.1584, 0.1578, 0.1729, 0.1581, 0.1578,
        0.1578, 0.1576, 0.1682, 0.1539, 0.1578, 0.1577, 0.1580, 0.1578, 0.1923,
        0.1637, 0.2375, 0.1689, 0.1578, 0.1583, 0.1578, 0.1578, 0.1578, 0.1580,
        0.1578, 0.1580, 0.1577, 0.1581, 0.1578, 0.1541, 0.1578, 0.1895, 0.1578,
        0.1581, 0.1685, 0.1580, 0.1897, 0.1606, 0.1578, 0.1578, 0.1760, 0.1581,
        0.1578, 0.1748, 0.1582, 0.1580, 0.1578, 0.1581, 0.1821, 0.1586, 0.2159,
        0.1578, 0.1582, 0.1580, 0.1582, 0.1582, 0.1581, 0.1578, 0.1580, 0.1582,
        0.1602, 0.1844, 0.1582, 0.1581, 0.1541, 0.1669, 0.1578, 0.1578, 0.1578,
        0.1616, 0.1575, 0.2019, 0.1578, 0.1578, 0.1659, 0.1581, 0.1567, 0.1583,
        0.1726, 0.1578, 0.1692, 0.1578, 0.1578, 0.1578, 0.1544, 0.1578, 0.1575,
        0.1853, 0.1581, 0.1588, 0.1572, 0.1593, 0.1576, 0.1522, 0.1583, 0.1580,
        0.1577, 0.1581, 0.2015, 0.1591, 0.1582, 0.1581, 0.2070, 0.1588, 0.1583,
        0.1587, 0.1838, 0.1580, 0.1583, 0.1578, 0.1577, 0.1578, 0.1578, 0.1580,
        0.1580, 0.1582, 0.1729, 0.1592, 0.1583, 0.1580, 0.1582, 0.1581, 0.1578,
        0.1578, 0.1577, 0.1586, 0.1578, 0.1580, 0.1578, 0.1581, 0.1588, 0.1532,
        0.1580, 0.1578, 0.1575, 0.1663, 0.1581, 0.1613, 0.1920, 0.1578, 0.1580,
        0.1582, 0.1581, 0.1578, 0.1580, 0.1622, 0.1580, 0.1643, 0.1603, 0.1580,
        0.1605, 0.1583, 0.1521, 0.1580, 0.1587, 0.1578, 0.1581, 0.1581, 0.1625,
        0.1572, 0.1581, 0.1642, 0.1740, 0.1652, 0.1598, 0.1748, 0.1580, 0.1578,
        0.1644, 0.1580, 0.1927, 0.1578, 0.1582, 0.1581, 0.1581, 0.1580, 0.1578,
        0.1578, 0.1785, 0.1580, 0.1581, 0.1663, 0.1631, 0.1582, 0.1580, 0.1578,
        0.1580, 0.1581, 0.1624, 0.1578, 0.1584, 0.1553, 0.1578, 0.1616, 0.1592,
        0.1567, 0.1578, 0.1580, 0.1588, 0.1575, 0.1578, 0.1578, 0.1578, 0.1578,
        0.2124, 0.1580, 0.1843, 0.1583, 0.1599, 0.1578, 0.1581, 0.1613, 0.1577,
        0.1578, 0.1602, 0.1642, 0.1602, 0.1580, 0.1584, 0.1578, 0.1578, 0.1581,
        0.1567, 0.1580, 0.1578, 0.1577, 0.1575, 0.1578, 0.1578, 0.1580, 0.1580,
        0.0082, 0.1672, 0.1578, 0.2128, 0.1578, 0.1580, 0.1583, 0.1578, 0.1581,
        0.1581, 0.1581, 0.1578, 0.1578, 0.1576, 0.1581, 0.1578, 0.1580, 0.1860,
        0.1580, 0.1544, 0.1578, 0.1582, 0.1531, 0.1578, 0.1577, 0.1580, 0.1578,
        0.1581, 0.1578, 0.1580, 0.1581, 0.1630, 0.1575, 0.1580, 0.1816, 0.1580,
        0.1581, 0.2001, 0.1582, 0.1580, 0.1580, 0.1577, 0.1600, 0.1578, 0.1578,
        0.1578, 0.1581, 0.1580, 0.1696, 0.1562, 0.1583, 0.1593, 0.1578, 0.1779,
        0.0743, 0.1625, 0.1580, 0.1583, 0.1812, 0.1578, 0.1581, 0.1736, 0.1580,
        0.1578, 0.1595, 0.1580, 0.1549, 0.1565, 0.1775, 0.1582, 0.1578, 0.1654,
        0.1578, 0.1583, 0.1578, 0.1580, 0.1578, 0.1578, 0.1578, 0.1578, 0.1578,
        0.1652, 0.1580, 0.1578, 0.1580, 0.1870, 0.1716, 0.1578, 0.1578, 0.1578,
        0.1610, 0.1578, 0.1578, 0.1884, 0.1578, 0.1096, 0.1571, 0.1635, 0.1578,
        0.1578, 0.1539, 0.1578, 0.1581, 0.1578, 0.1578, 0.1578, 0.1578, 0.1580,
        0.1534, 0.1838, 0.1578, 0.1582, 0.1578, 0.1757, 0.1578, 0.1578, 0.1578,
        0.1580, 0.1578, 0.1578, 0.1652, 0.1578, 0.1664, 0.1584, 0.1766, 0.1578,
        0.1808, 0.1578, 0.1578, 0.1586, 0.1582, 0.1580, 0.1578, 0.1578, 0.1578,
        0.1581, 0.1580, 0.1578, 0.1720, 0.1578, 0.1578, 0.1619, 0.1578, 0.1565,
        0.1580, 0.1581, 0.1576, 0.1785, 0.1578, 0.1597, 0.1578, 0.1580, 0.1561,
        0.1580, 0.1802, 0.1578, 0.1582, 0.1609, 0.1571, 0.1765, 0.1683, 0.1581,
        0.1580, 0.1595, 0.1577, 0.1865, 0.1699, 0.1583, 0.1578, 0.1282, 0.1582,
        0.1580, 0.1622, 0.1580, 0.1578, 0.1580, 0.1578, 0.1580, 0.1582, 0.1583,
        0.1838, 0.2167, 0.1581, 0.1580, 0.1600, 0.1543, 0.1581, 0.1558, 0.1562,
        0.1578, 0.1628, 0.1577, 0.1582, 0.1582, 0.1578, 0.1587, 0.1580, 0.1946,
        0.1555, 0.1578, 0.1875, 0.1580, 0.1519, 0.1580, 0.1578, 0.1578, 0.1581,
        0.1644, 0.1602, 0.1580, 0.1580, 0.1581, 0.1580, 0.1578, 0.1578, 0.1560,
        0.1580, 0.1582, 0.1587, 0.1742, 0.1580, 0.1578, 0.1578, 0.1581, 0.1580,
        0.1580, 0.1578, 0.1638, 0.1582, 0.1578, 0.1537, 0.1578, 0.1820, 0.1685,
        0.1578, 0.1578, 0.1581, 0.1576, 0.1580, 0.1578, 0.1578, 0.1580, 0.1578,
        0.1578, 0.1578, 0.1552, 0.1644, 0.1578, 0.1580, 0.1580, 0.1582, 0.1714,
        0.1858, 0.1617, 0.1699, 0.1582, 0.1587, 0.1580, 0.1578, 0.1672, 0.1583,
        0.2074, 0.1578, 0.1665, 0.1578, 0.1573, 0.1580, 0.1580, 0.1581, 0.1578,
        0.1519, 0.1581, 0.1578, 0.1578, 0.1602, 0.1578, 0.1582, 0.1578, 0.1471,
        0.1865, 0.1578, 0.1578, 0.1578, 0.1578, 0.1578, 0.1578, 0.1584, 0.1581,
        0.1581, 0.1561, 0.1098, 0.1528, 0.1582, 0.1896, 0.1578, 0.1582, 0.1580,
        0.1578, 0.1582, 0.1580, 0.1578, 0.1849, 0.1598, 0.1580, 0.1802, 0.1578,
        0.1580, 0.1578, 0.1582, 0.1973, 0.1581, 0.1578, 0.1578, 0.1583, 0.1578,
        0.1578, 0.1583, 0.1687, 0.1580, 0.1582, 0.1720, 0.1576, 0.1578, 0.1720,
        0.1578, 0.1581, 0.1578, 0.1582, 0.1581, 0.1581, 0.1580, 0.1578, 0.1578,
        0.1871, 0.1581, 0.1578, 0.1936, 0.1797, 0.1564, 0.1729, 0.1581, 0.1843,
        0.1583, 0.1791, 0.1575, 0.0477, 0.1580, 0.1595, 0.1582, 0.1578, 0.1581,
        0.1580, 0.1580, 0.1578, 0.1578, 0.1578, 0.1625, 0.1580, 0.1578, 0.1582,
        0.1578], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 06:55:18,801][train_inner][INFO] - {"epoch": 13, "update": 12.716, "loss": "4.313", "ntokens": "149866", "nsentences": "539.13", "prob_perplexity": "82.933", "code_perplexity": "82.036", "temp": "1.936", "loss_0": "4.175", "loss_1": "0.126", "loss_2": "0.013", "accuracy": "0.28687", "wps": "37464.1", "ups": "0.25", "wpb": "149866", "bsz": "539.1", "num_updates": "6600", "lr": "0.000103125", "gnorm": "0.591", "loss_scale": "2", "train_wall": "799", "gb_free": "12.7", "wall": "26585"}
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1918, 0.1672, 0.2031, 0.0735, 0.1942, 0.1656, 0.1656, 0.1676, 0.1661,
        0.1660, 0.1659, 0.1769, 0.1714, 0.1656, 0.1659, 0.1963, 0.1660, 0.1659,
        0.1606, 0.1656, 0.1658, 0.1722, 0.1659, 0.1757, 0.1670, 0.1661, 0.1660,
        0.1658, 0.1660, 0.1837, 0.1727, 0.1661, 0.1647, 0.1658, 0.1659, 0.1658,
        0.1655, 0.1685, 0.1660, 0.1660, 0.1656, 0.1661, 0.1658, 0.1655, 0.1658,
        0.1848, 0.1658, 0.1658, 0.1659, 0.1664, 0.1658, 0.1810, 0.1659, 0.1656,
        0.1658, 0.1654, 0.1772, 0.1619, 0.1658, 0.1655, 0.1659, 0.1658, 0.2002,
        0.1716, 0.2485, 0.1790, 0.1658, 0.1663, 0.1658, 0.1656, 0.1656, 0.1659,
        0.1658, 0.1658, 0.1656, 0.1660, 0.1658, 0.1628, 0.1658, 0.1974, 0.1656,
        0.1659, 0.1764, 0.1659, 0.1976, 0.1693, 0.1658, 0.1656, 0.1843, 0.1659,
        0.1656, 0.1835, 0.1661, 0.1658, 0.1656, 0.1660, 0.1924, 0.1665, 0.2241,
        0.1658, 0.1660, 0.1659, 0.1661, 0.1661, 0.1660, 0.1656, 0.1659, 0.1661,
        0.1686, 0.1938, 0.1660, 0.1659, 0.1619, 0.1759, 0.1658, 0.1656, 0.1656,
        0.1694, 0.1654, 0.2108, 0.1656, 0.1656, 0.1741, 0.1663, 0.1649, 0.1661,
        0.1829, 0.1658, 0.1777, 0.1658, 0.1658, 0.1656, 0.1624, 0.1658, 0.1654,
        0.1956, 0.1659, 0.1670, 0.1652, 0.1675, 0.1655, 0.1600, 0.1663, 0.1659,
        0.1656, 0.1660, 0.2118, 0.1671, 0.1660, 0.1660, 0.2155, 0.1667, 0.1663,
        0.1665, 0.1904, 0.1658, 0.1664, 0.1658, 0.1655, 0.1656, 0.1658, 0.1659,
        0.1658, 0.1660, 0.1819, 0.1674, 0.1663, 0.1659, 0.1660, 0.1660, 0.1658,
        0.1658, 0.1655, 0.1665, 0.1658, 0.1659, 0.1658, 0.1659, 0.1667, 0.1610,
        0.1659, 0.1658, 0.1654, 0.1742, 0.1660, 0.1692, 0.1990, 0.1658, 0.1659,
        0.1661, 0.1660, 0.1658, 0.1658, 0.1703, 0.1658, 0.1738, 0.1682, 0.1659,
        0.1688, 0.1661, 0.1600, 0.1659, 0.1666, 0.1658, 0.1660, 0.1659, 0.1704,
        0.1652, 0.1659, 0.1746, 0.1820, 0.1733, 0.1682, 0.1853, 0.1659, 0.1658,
        0.1726, 0.1659, 0.2006, 0.1658, 0.1660, 0.1659, 0.1660, 0.1659, 0.1658,
        0.1658, 0.1868, 0.1658, 0.1661, 0.1743, 0.1714, 0.1661, 0.1659, 0.1658,
        0.1659, 0.1660, 0.1707, 0.1658, 0.1664, 0.1631, 0.1658, 0.1694, 0.1670,
        0.1646, 0.1656, 0.1658, 0.1667, 0.1653, 0.1656, 0.1658, 0.1656, 0.1658,
        0.2208, 0.1658, 0.1923, 0.1663, 0.1678, 0.1658, 0.1660, 0.1691, 0.1656,
        0.1658, 0.1692, 0.1720, 0.1687, 0.1658, 0.1665, 0.1656, 0.1656, 0.1659,
        0.1647, 0.1659, 0.1658, 0.1655, 0.1653, 0.1656, 0.1656, 0.1658, 0.1658,
        0.0179, 0.1754, 0.1656, 0.2224, 0.1658, 0.1658, 0.1663, 0.1658, 0.1659,
        0.1660, 0.1659, 0.1656, 0.1656, 0.1654, 0.1660, 0.1658, 0.1658, 0.1964,
        0.1658, 0.1624, 0.1658, 0.1661, 0.1610, 0.1658, 0.1656, 0.1658, 0.1658,
        0.1660, 0.1658, 0.1659, 0.1659, 0.1729, 0.1654, 0.1659, 0.1899, 0.1658,
        0.1660, 0.2080, 0.1661, 0.1659, 0.1659, 0.1656, 0.1678, 0.1656, 0.1656,
        0.1658, 0.1660, 0.1659, 0.1774, 0.1643, 0.1665, 0.1672, 0.1658, 0.1857,
        0.0646, 0.1708, 0.1659, 0.1663, 0.1908, 0.1658, 0.1659, 0.1833, 0.1658,
        0.1658, 0.1674, 0.1658, 0.1627, 0.1647, 0.1855, 0.1660, 0.1656, 0.1738,
        0.1658, 0.1663, 0.1658, 0.1658, 0.1658, 0.1658, 0.1656, 0.1658, 0.1656,
        0.1738, 0.1658, 0.1656, 0.1659, 0.1947, 0.1807, 0.1658, 0.1658, 0.1656,
        0.1689, 0.1658, 0.1656, 0.1971, 0.1658, 0.1076, 0.1650, 0.1721, 0.1658,
        0.1658, 0.1617, 0.1656, 0.1660, 0.1656, 0.1658, 0.1656, 0.1658, 0.1658,
        0.1614, 0.1879, 0.1656, 0.1660, 0.1656, 0.1841, 0.1658, 0.1658, 0.1658,
        0.1659, 0.1658, 0.1658, 0.1732, 0.1656, 0.1742, 0.1664, 0.1875, 0.1658,
        0.1886, 0.1656, 0.1658, 0.1669, 0.1660, 0.1658, 0.1658, 0.1658, 0.1658,
        0.1660, 0.1658, 0.1658, 0.1798, 0.1658, 0.1658, 0.1703, 0.1656, 0.1644,
        0.1659, 0.1659, 0.1654, 0.1877, 0.1658, 0.1675, 0.1658, 0.1658, 0.1641,
        0.1659, 0.1903, 0.1658, 0.1661, 0.1688, 0.1649, 0.1853, 0.1775, 0.1659,
        0.1658, 0.1675, 0.1656, 0.1895, 0.1782, 0.1661, 0.1656, 0.1356, 0.1660,
        0.1658, 0.1702, 0.1659, 0.1658, 0.1659, 0.1658, 0.1658, 0.1660, 0.1661,
        0.1956, 0.2212, 0.1659, 0.1659, 0.1691, 0.1621, 0.1659, 0.1638, 0.1648,
        0.1658, 0.1711, 0.1656, 0.1660, 0.1660, 0.1658, 0.1672, 0.1659, 0.2052,
        0.1635, 0.1656, 0.1973, 0.1660, 0.1599, 0.1659, 0.1658, 0.1658, 0.1659,
        0.1731, 0.1680, 0.1658, 0.1658, 0.1660, 0.1659, 0.1656, 0.1658, 0.1638,
        0.1659, 0.1661, 0.1666, 0.1825, 0.1658, 0.1656, 0.1658, 0.1660, 0.1658,
        0.1658, 0.1656, 0.1718, 0.1660, 0.1656, 0.1615, 0.1658, 0.1920, 0.1772,
        0.1656, 0.1658, 0.1659, 0.1655, 0.1658, 0.1658, 0.1658, 0.1658, 0.1656,
        0.1656, 0.1658, 0.1630, 0.1722, 0.1658, 0.1659, 0.1659, 0.1660, 0.1798,
        0.1941, 0.1700, 0.1798, 0.1660, 0.1666, 0.1658, 0.1658, 0.1764, 0.1661,
        0.2168, 0.1658, 0.1755, 0.1656, 0.1653, 0.1658, 0.1658, 0.1660, 0.1658,
        0.1598, 0.1659, 0.1658, 0.1658, 0.1681, 0.1658, 0.1661, 0.1656, 0.1559,
        0.1935, 0.1656, 0.1656, 0.1656, 0.1656, 0.1656, 0.1658, 0.1664, 0.1659,
        0.1660, 0.1639, 0.1182, 0.1608, 0.1660, 0.1992, 0.1658, 0.1660, 0.1658,
        0.1658, 0.1660, 0.1659, 0.1658, 0.1946, 0.1677, 0.1658, 0.1881, 0.1656,
        0.1659, 0.1656, 0.1661, 0.2021, 0.1659, 0.1656, 0.1658, 0.1661, 0.1656,
        0.1658, 0.1663, 0.1770, 0.1659, 0.1661, 0.1799, 0.1655, 0.1658, 0.1825,
        0.1656, 0.1660, 0.1658, 0.1660, 0.1660, 0.1660, 0.1659, 0.1658, 0.1658,
        0.1959, 0.1660, 0.1656, 0.2028, 0.1908, 0.1643, 0.1827, 0.1660, 0.1921,
        0.1661, 0.1887, 0.1654, 0.0404, 0.1659, 0.1694, 0.1660, 0.1658, 0.1659,
        0.1658, 0.1660, 0.1658, 0.1658, 0.1658, 0.1704, 0.1658, 0.1658, 0.1661,
        0.1658], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(6899.7266, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4844, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1896, 0.1705, 0.2061, 0.0750, 0.1991, 0.1689, 0.1689, 0.1710, 0.1694,
        0.1693, 0.1692, 0.1816, 0.1748, 0.1689, 0.1693, 0.2012, 0.1693, 0.1692,
        0.1642, 0.1689, 0.1691, 0.1761, 0.1692, 0.1792, 0.1703, 0.1694, 0.1693,
        0.1691, 0.1693, 0.1884, 0.1760, 0.1694, 0.1680, 0.1689, 0.1692, 0.1691,
        0.1689, 0.1730, 0.1693, 0.1693, 0.1689, 0.1694, 0.1691, 0.1688, 0.1691,
        0.1887, 0.1689, 0.1691, 0.1692, 0.1697, 0.1689, 0.1844, 0.1692, 0.1689,
        0.1691, 0.1687, 0.1803, 0.1650, 0.1691, 0.1688, 0.1692, 0.1689, 0.2019,
        0.1749, 0.2529, 0.1836, 0.1691, 0.1696, 0.1691, 0.1689, 0.1689, 0.1691,
        0.1689, 0.1691, 0.1689, 0.1693, 0.1691, 0.1666, 0.1691, 0.2007, 0.1689,
        0.1692, 0.1797, 0.1692, 0.2009, 0.1735, 0.1691, 0.1689, 0.1877, 0.1692,
        0.1689, 0.1875, 0.1694, 0.1691, 0.1689, 0.1693, 0.1960, 0.1698, 0.2275,
        0.1691, 0.1693, 0.1692, 0.1696, 0.1694, 0.1693, 0.1689, 0.1691, 0.1694,
        0.1720, 0.1984, 0.1693, 0.1692, 0.1652, 0.1794, 0.1691, 0.1689, 0.1689,
        0.1727, 0.1687, 0.2140, 0.1689, 0.1689, 0.1776, 0.1697, 0.1686, 0.1694,
        0.1865, 0.1689, 0.1818, 0.1691, 0.1691, 0.1689, 0.1656, 0.1689, 0.1687,
        0.2002, 0.1693, 0.1705, 0.1685, 0.1709, 0.1688, 0.1633, 0.1696, 0.1692,
        0.1689, 0.1693, 0.2161, 0.1705, 0.1693, 0.1693, 0.2175, 0.1699, 0.1696,
        0.1698, 0.1948, 0.1691, 0.1697, 0.1691, 0.1688, 0.1689, 0.1691, 0.1692,
        0.1691, 0.1693, 0.1857, 0.1708, 0.1696, 0.1692, 0.1693, 0.1693, 0.1689,
        0.1691, 0.1688, 0.1698, 0.1691, 0.1692, 0.1691, 0.1692, 0.1700, 0.1643,
        0.1692, 0.1691, 0.1686, 0.1775, 0.1693, 0.1725, 0.2013, 0.1689, 0.1692,
        0.1694, 0.1693, 0.1689, 0.1691, 0.1736, 0.1691, 0.1781, 0.1715, 0.1692,
        0.1732, 0.1694, 0.1633, 0.1692, 0.1699, 0.1691, 0.1692, 0.1692, 0.1737,
        0.1685, 0.1692, 0.1787, 0.1853, 0.1774, 0.1722, 0.1890, 0.1692, 0.1689,
        0.1763, 0.1692, 0.2043, 0.1691, 0.1693, 0.1692, 0.1693, 0.1692, 0.1691,
        0.1691, 0.1915, 0.1691, 0.1696, 0.1777, 0.1748, 0.1693, 0.1692, 0.1691,
        0.1691, 0.1693, 0.1743, 0.1691, 0.1697, 0.1664, 0.1691, 0.1729, 0.1703,
        0.1678, 0.1689, 0.1691, 0.1700, 0.1686, 0.1689, 0.1691, 0.1689, 0.1691,
        0.2249, 0.1691, 0.1957, 0.1698, 0.1711, 0.1691, 0.1693, 0.1724, 0.1689,
        0.1689, 0.1732, 0.1753, 0.1726, 0.1691, 0.1699, 0.1689, 0.1689, 0.1692,
        0.1680, 0.1692, 0.1689, 0.1688, 0.1686, 0.1689, 0.1689, 0.1691, 0.1691,
        0.0219, 0.1791, 0.1689, 0.2262, 0.1691, 0.1691, 0.1694, 0.1691, 0.1692,
        0.1697, 0.1692, 0.1689, 0.1689, 0.1687, 0.1693, 0.1691, 0.1691, 0.2004,
        0.1691, 0.1656, 0.1691, 0.1694, 0.1643, 0.1691, 0.1688, 0.1691, 0.1691,
        0.1693, 0.1689, 0.1692, 0.1692, 0.1776, 0.1687, 0.1692, 0.1941, 0.1691,
        0.1692, 0.2126, 0.1694, 0.1692, 0.1691, 0.1689, 0.1711, 0.1689, 0.1689,
        0.1691, 0.1693, 0.1692, 0.1807, 0.1680, 0.1698, 0.1705, 0.1689, 0.1899,
        0.0609, 0.1744, 0.1692, 0.1697, 0.1948, 0.1691, 0.1692, 0.1880, 0.1691,
        0.1691, 0.1707, 0.1691, 0.1660, 0.1680, 0.1891, 0.1693, 0.1689, 0.1787,
        0.1689, 0.1694, 0.1689, 0.1691, 0.1691, 0.1691, 0.1689, 0.1689, 0.1689,
        0.1785, 0.1691, 0.1689, 0.1692, 0.1984, 0.1848, 0.1691, 0.1689, 0.1689,
        0.1722, 0.1691, 0.1689, 0.2013, 0.1691, 0.1059, 0.1683, 0.1768, 0.1691,
        0.1689, 0.1650, 0.1689, 0.1693, 0.1689, 0.1689, 0.1689, 0.1691, 0.1691,
        0.1647, 0.1864, 0.1689, 0.1693, 0.1689, 0.1877, 0.1689, 0.1689, 0.1689,
        0.1692, 0.1691, 0.1691, 0.1772, 0.1689, 0.1775, 0.1698, 0.1923, 0.1691,
        0.1937, 0.1689, 0.1691, 0.1707, 0.1693, 0.1691, 0.1691, 0.1691, 0.1691,
        0.1693, 0.1691, 0.1689, 0.1835, 0.1691, 0.1691, 0.1740, 0.1689, 0.1677,
        0.1692, 0.1692, 0.1687, 0.1912, 0.1691, 0.1708, 0.1691, 0.1692, 0.1674,
        0.1692, 0.1954, 0.1691, 0.1693, 0.1721, 0.1682, 0.1896, 0.1815, 0.1692,
        0.1691, 0.1708, 0.1689, 0.1871, 0.1827, 0.1694, 0.1689, 0.1390, 0.1693,
        0.1691, 0.1735, 0.1692, 0.1689, 0.1691, 0.1689, 0.1691, 0.1693, 0.1696,
        0.2007, 0.2236, 0.1692, 0.1692, 0.1730, 0.1654, 0.1692, 0.1672, 0.1686,
        0.1689, 0.1747, 0.1689, 0.1693, 0.1693, 0.1689, 0.1708, 0.1691, 0.2097,
        0.1666, 0.1689, 0.2006, 0.1693, 0.1632, 0.1692, 0.1689, 0.1691, 0.1692,
        0.1775, 0.1713, 0.1691, 0.1691, 0.1692, 0.1692, 0.1689, 0.1691, 0.1671,
        0.1692, 0.1694, 0.1699, 0.1860, 0.1691, 0.1689, 0.1689, 0.1693, 0.1691,
        0.1691, 0.1689, 0.1750, 0.1693, 0.1689, 0.1648, 0.1691, 0.1960, 0.1819,
        0.1689, 0.1689, 0.1692, 0.1687, 0.1691, 0.1691, 0.1691, 0.1691, 0.1689,
        0.1689, 0.1691, 0.1663, 0.1755, 0.1691, 0.1692, 0.1692, 0.1693, 0.1842,
        0.1980, 0.1740, 0.1842, 0.1693, 0.1699, 0.1691, 0.1689, 0.1804, 0.1694,
        0.2212, 0.1691, 0.1792, 0.1689, 0.1686, 0.1691, 0.1691, 0.1693, 0.1689,
        0.1630, 0.1692, 0.1691, 0.1689, 0.1714, 0.1691, 0.1694, 0.1689, 0.1597,
        0.1929, 0.1689, 0.1689, 0.1689, 0.1689, 0.1689, 0.1689, 0.1696, 0.1692,
        0.1693, 0.1672, 0.1220, 0.1641, 0.1693, 0.2032, 0.1691, 0.1693, 0.1691,
        0.1691, 0.1693, 0.1692, 0.1691, 0.1998, 0.1710, 0.1691, 0.1913, 0.1689,
        0.1692, 0.1689, 0.1694, 0.2036, 0.1692, 0.1689, 0.1691, 0.1694, 0.1689,
        0.1691, 0.1697, 0.1814, 0.1692, 0.1694, 0.1832, 0.1688, 0.1689, 0.1877,
        0.1689, 0.1693, 0.1691, 0.1693, 0.1693, 0.1693, 0.1692, 0.1691, 0.1689,
        0.2002, 0.1693, 0.1689, 0.2070, 0.1954, 0.1676, 0.1870, 0.1693, 0.1962,
        0.1694, 0.1932, 0.1686, 0.0376, 0.1692, 0.1744, 0.1693, 0.1691, 0.1692,
        0.1691, 0.1693, 0.1689, 0.1691, 0.1691, 0.1736, 0.1691, 0.1689, 0.1694,
        0.1689], device='cuda:0', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1893, 0.1715, 0.2067, 0.0756, 0.2003, 0.1699, 0.1700, 0.1721, 0.1704,
        0.1704, 0.1702, 0.1830, 0.1759, 0.1700, 0.1704, 0.2024, 0.1703, 0.1702,
        0.1652, 0.1700, 0.1700, 0.1774, 0.1702, 0.1803, 0.1713, 0.1704, 0.1703,
        0.1700, 0.1703, 0.1899, 0.1771, 0.1704, 0.1689, 0.1700, 0.1702, 0.1700,
        0.1699, 0.1741, 0.1703, 0.1703, 0.1700, 0.1704, 0.1700, 0.1698, 0.1700,
        0.1901, 0.1700, 0.1700, 0.1702, 0.1707, 0.1700, 0.1855, 0.1703, 0.1699,
        0.1700, 0.1697, 0.1814, 0.1661, 0.1700, 0.1698, 0.1702, 0.1700, 0.2026,
        0.1759, 0.2537, 0.1846, 0.1700, 0.1705, 0.1700, 0.1700, 0.1700, 0.1702,
        0.1700, 0.1700, 0.1699, 0.1703, 0.1700, 0.1677, 0.1700, 0.2018, 0.1700,
        0.1702, 0.1807, 0.1702, 0.2019, 0.1747, 0.1700, 0.1700, 0.1888, 0.1702,
        0.1699, 0.1886, 0.1704, 0.1700, 0.1700, 0.1703, 0.1970, 0.1708, 0.2289,
        0.1700, 0.1703, 0.1702, 0.1705, 0.1704, 0.1703, 0.1699, 0.1702, 0.1704,
        0.1731, 0.1995, 0.1704, 0.1702, 0.1661, 0.1808, 0.1700, 0.1700, 0.1699,
        0.1737, 0.1697, 0.2148, 0.1700, 0.1700, 0.1786, 0.1707, 0.1696, 0.1704,
        0.1882, 0.1700, 0.1827, 0.1700, 0.1700, 0.1700, 0.1666, 0.1700, 0.1697,
        0.2015, 0.1703, 0.1715, 0.1694, 0.1719, 0.1698, 0.1643, 0.1705, 0.1702,
        0.1699, 0.1703, 0.2174, 0.1716, 0.1703, 0.1703, 0.2184, 0.1710, 0.1705,
        0.1709, 0.1960, 0.1700, 0.1707, 0.1700, 0.1698, 0.1700, 0.1700, 0.1702,
        0.1702, 0.1703, 0.1864, 0.1720, 0.1705, 0.1702, 0.1703, 0.1703, 0.1700,
        0.1700, 0.1699, 0.1708, 0.1700, 0.1702, 0.1700, 0.1702, 0.1710, 0.1653,
        0.1702, 0.1700, 0.1697, 0.1785, 0.1703, 0.1735, 0.2020, 0.1700, 0.1702,
        0.1704, 0.1703, 0.1700, 0.1702, 0.1746, 0.1700, 0.1794, 0.1725, 0.1702,
        0.1744, 0.1704, 0.1643, 0.1702, 0.1709, 0.1700, 0.1703, 0.1702, 0.1747,
        0.1694, 0.1702, 0.1798, 0.1863, 0.1785, 0.1733, 0.1899, 0.1702, 0.1700,
        0.1775, 0.1702, 0.2053, 0.1700, 0.1703, 0.1703, 0.1703, 0.1702, 0.1700,
        0.1700, 0.1925, 0.1700, 0.1707, 0.1787, 0.1759, 0.1704, 0.1702, 0.1700,
        0.1702, 0.1703, 0.1753, 0.1700, 0.1708, 0.1674, 0.1700, 0.1740, 0.1713,
        0.1688, 0.1700, 0.1700, 0.1710, 0.1696, 0.1700, 0.1700, 0.1700, 0.1700,
        0.2261, 0.1700, 0.1967, 0.1708, 0.1721, 0.1700, 0.1703, 0.1733, 0.1699,
        0.1700, 0.1744, 0.1763, 0.1737, 0.1700, 0.1709, 0.1700, 0.1700, 0.1703,
        0.1689, 0.1702, 0.1700, 0.1698, 0.1697, 0.1699, 0.1700, 0.1700, 0.1702,
        0.0231, 0.1801, 0.1699, 0.2273, 0.1700, 0.1700, 0.1705, 0.1700, 0.1702,
        0.1708, 0.1702, 0.1700, 0.1700, 0.1697, 0.1703, 0.1700, 0.1702, 0.2017,
        0.1700, 0.1666, 0.1700, 0.1704, 0.1653, 0.1700, 0.1699, 0.1700, 0.1700,
        0.1703, 0.1700, 0.1702, 0.1702, 0.1790, 0.1697, 0.1702, 0.1952, 0.1700,
        0.1703, 0.2141, 0.1704, 0.1702, 0.1702, 0.1699, 0.1722, 0.1699, 0.1699,
        0.1700, 0.1703, 0.1702, 0.1816, 0.1692, 0.1709, 0.1715, 0.1700, 0.1909,
        0.0595, 0.1757, 0.1702, 0.1707, 0.1959, 0.1702, 0.1702, 0.1892, 0.1702,
        0.1700, 0.1718, 0.1700, 0.1670, 0.1689, 0.1902, 0.1704, 0.1700, 0.1799,
        0.1700, 0.1705, 0.1700, 0.1700, 0.1700, 0.1700, 0.1700, 0.1700, 0.1699,
        0.1796, 0.1702, 0.1699, 0.1702, 0.1995, 0.1859, 0.1700, 0.1700, 0.1699,
        0.1732, 0.1700, 0.1700, 0.2026, 0.1700, 0.1051, 0.1693, 0.1777, 0.1700,
        0.1700, 0.1660, 0.1700, 0.1703, 0.1700, 0.1700, 0.1700, 0.1700, 0.1700,
        0.1656, 0.1866, 0.1700, 0.1704, 0.1700, 0.1888, 0.1700, 0.1700, 0.1700,
        0.1702, 0.1700, 0.1700, 0.1785, 0.1700, 0.1786, 0.1708, 0.1936, 0.1700,
        0.1949, 0.1699, 0.1700, 0.1718, 0.1703, 0.1700, 0.1700, 0.1700, 0.1700,
        0.1703, 0.1702, 0.1700, 0.1846, 0.1700, 0.1700, 0.1749, 0.1699, 0.1687,
        0.1702, 0.1703, 0.1697, 0.1924, 0.1700, 0.1718, 0.1700, 0.1702, 0.1683,
        0.1702, 0.1968, 0.1700, 0.1704, 0.1732, 0.1692, 0.1909, 0.1826, 0.1703,
        0.1700, 0.1719, 0.1699, 0.1864, 0.1841, 0.1704, 0.1700, 0.1400, 0.1703,
        0.1700, 0.1744, 0.1702, 0.1700, 0.1702, 0.1700, 0.1700, 0.1703, 0.1705,
        0.2023, 0.2246, 0.1703, 0.1702, 0.1741, 0.1664, 0.1702, 0.1683, 0.1697,
        0.1700, 0.1757, 0.1699, 0.1703, 0.1703, 0.1700, 0.1719, 0.1702, 0.2107,
        0.1677, 0.1699, 0.2018, 0.1703, 0.1642, 0.1702, 0.1700, 0.1700, 0.1702,
        0.1785, 0.1722, 0.1702, 0.1700, 0.1703, 0.1703, 0.1700, 0.1700, 0.1681,
        0.1702, 0.1704, 0.1709, 0.1873, 0.1700, 0.1700, 0.1700, 0.1703, 0.1700,
        0.1700, 0.1700, 0.1760, 0.1703, 0.1700, 0.1659, 0.1700, 0.1971, 0.1830,
        0.1700, 0.1700, 0.1702, 0.1698, 0.1702, 0.1700, 0.1700, 0.1700, 0.1700,
        0.1700, 0.1700, 0.1672, 0.1765, 0.1700, 0.1702, 0.1702, 0.1703, 0.1862,
        0.1991, 0.1750, 0.1858, 0.1703, 0.1709, 0.1702, 0.1700, 0.1819, 0.1704,
        0.2222, 0.1700, 0.1801, 0.1700, 0.1696, 0.1700, 0.1702, 0.1703, 0.1700,
        0.1641, 0.1702, 0.1700, 0.1700, 0.1725, 0.1700, 0.1704, 0.1699, 0.1606,
        0.1931, 0.1700, 0.1700, 0.1699, 0.1699, 0.1699, 0.1700, 0.1707, 0.1702,
        0.1703, 0.1682, 0.1230, 0.1650, 0.1703, 0.2041, 0.1700, 0.1704, 0.1700,
        0.1700, 0.1703, 0.1702, 0.1700, 0.2013, 0.1721, 0.1700, 0.1924, 0.1700,
        0.1702, 0.1700, 0.1704, 0.2042, 0.1702, 0.1700, 0.1700, 0.1704, 0.1700,
        0.1700, 0.1707, 0.1825, 0.1702, 0.1704, 0.1842, 0.1698, 0.1700, 0.1890,
        0.1700, 0.1703, 0.1700, 0.1703, 0.1703, 0.1703, 0.1702, 0.1700, 0.1700,
        0.2013, 0.1703, 0.1700, 0.2081, 0.1965, 0.1686, 0.1884, 0.1703, 0.1975,
        0.1704, 0.1946, 0.1697, 0.0372, 0.1702, 0.1757, 0.1704, 0.1700, 0.1702,
        0.1700, 0.1704, 0.1700, 0.1700, 0.1700, 0.1747, 0.1702, 0.1700, 0.1704,
        0.1700], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 07:05:01,405][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 07:05:01,406][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 07:05:01,500][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2023-09-12 07:05:25,012][valid][INFO] - {"epoch": 13, "valid_loss": "3.998", "valid_ntokens": "7889.36", "valid_nsentences": "55.2525", "valid_prob_perplexity": "77.288", "valid_code_perplexity": "76.245", "valid_temp": "1.934", "valid_loss_0": "3.858", "valid_loss_1": "0.127", "valid_loss_2": "0.013", "valid_accuracy": "0.3585", "valid_wps": "33397.2", "valid_wpb": "7889.4", "valid_bsz": "55.3", "valid_num_updates": "6748", "valid_best_loss": "3.998"}
[2023-09-12 07:05:25,014][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 6748 updates
[2023-09-12 07:05:25,015][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 07:05:27,502][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 07:05:28,884][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 6748 updates, score 3.998) (writing took 3.870049064978957 seconds)
[2023-09-12 07:05:28,885][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2023-09-12 07:05:28,886][train][INFO] - {"epoch": 13, "train_loss": "4.317", "train_ntokens": "149481", "train_nsentences": "538.438", "train_prob_perplexity": "82.859", "train_code_perplexity": "81.936", "train_temp": "1.936", "train_loss_0": "4.179", "train_loss_1": "0.126", "train_loss_2": "0.013", "train_accuracy": "0.28652", "train_wps": "37103.6", "train_ups": "0.25", "train_wpb": "149482", "train_bsz": "538.4", "train_num_updates": "6748", "train_lr": "0.000105438", "train_gnorm": "0.594", "train_loss_scale": "4", "train_train_wall": "2064", "train_gb_free": "15.3", "train_wall": "27195"}
[2023-09-12 07:05:28,889][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 07:05:28,985][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2023-09-12 07:05:29,226][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 07:05:29,229][fairseq.trainer][INFO] - begin training epoch 14
[2023-09-12 07:05:29,230][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 07:08:55,433][train_inner][INFO] - {"epoch": 14, "update": 13.1, "loss": "4.295", "ntokens": "149148", "nsentences": "541.395", "prob_perplexity": "85.19", "code_perplexity": "84.316", "temp": "1.934", "loss_0": "4.157", "loss_1": "0.125", "loss_2": "0.013", "accuracy": "0.29047", "wps": "36527.6", "ups": "0.24", "wpb": "149148", "bsz": "541.4", "num_updates": "6800", "lr": "0.00010625", "gnorm": "0.594", "loss_scale": "4", "train_wall": "787", "gb_free": "12.8", "wall": "27402"}
loss: tensor(3744.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4355, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 07:14:37,387][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
loss: tensor(3732.4819, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7031, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 07:15:24,321][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
Parameter containing:
tensor([ 0.1970,  0.1476,  0.1764,  0.0740,  0.1653,  0.1461,  0.1461,  0.1475,
         0.1465,  0.1465,  0.1462,  0.1519,  0.1510,  0.1461,  0.1462,  0.1705,
         0.1465,  0.1464,  0.1407,  0.1461,  0.1461,  0.1509,  0.1462,  0.1569,
         0.1473,  0.1465,  0.1464,  0.1461,  0.1464,  0.1591,  0.1525,  0.1465,
         0.1450,  0.1461,  0.1462,  0.1462,  0.1456,  0.1472,  0.1464,  0.1464,
         0.1461,  0.1465,  0.1461,  0.1459,  0.1461,  0.1602,  0.1461,  0.1461,
         0.1462,  0.1467,  0.1461,  0.1610,  0.1464,  0.1460,  0.1461,  0.1458,
         0.1552,  0.1422,  0.1461,  0.1460,  0.1462,  0.1461,  0.1777,  0.1520,
         0.2220,  0.1536,  0.1461,  0.1466,  0.1461,  0.1461,  0.1461,  0.1462,
         0.1461,  0.1461,  0.1460,  0.1464,  0.1461,  0.1409,  0.1461,  0.1775,
         0.1461,  0.1462,  0.1567,  0.1462,  0.1780,  0.1481,  0.1461,  0.1461,
         0.1636,  0.1462,  0.1461,  0.1613,  0.1465,  0.1462,  0.1461,  0.1464,
         0.1669,  0.1469,  0.2046,  0.1461,  0.1465,  0.1462,  0.1465,  0.1465,
         0.1464,  0.1461,  0.1462,  0.1465,  0.1476,  0.1700,  0.1465,  0.1464,
         0.1422,  0.1550,  0.1461,  0.1461,  0.1461,  0.1499,  0.1458,  0.1891,
         0.1461,  0.1461,  0.1539,  0.1462,  0.1445,  0.1465,  0.1597,  0.1461,
         0.1549,  0.1461,  0.1461,  0.1461,  0.1427,  0.1461,  0.1458,  0.1709,
         0.1462,  0.1469,  0.1455,  0.1467,  0.1459,  0.1405,  0.1466,  0.1462,
         0.1460,  0.1464,  0.1852,  0.1470,  0.1464,  0.1464,  0.2008,  0.1470,
         0.1466,  0.1470,  0.1686,  0.1461,  0.1466,  0.1461,  0.1460,  0.1461,
         0.1461,  0.1462,  0.1462,  0.1464,  0.1580,  0.1469,  0.1466,  0.1462,
         0.1465,  0.1464,  0.1461,  0.1461,  0.1460,  0.1467,  0.1461,  0.1462,
         0.1461,  0.1464,  0.1471,  0.1414,  0.1462,  0.1461,  0.1458,  0.1545,
         0.1464,  0.1495,  0.1768,  0.1461,  0.1461,  0.1465,  0.1464,  0.1461,
         0.1461,  0.1504,  0.1461,  0.1506,  0.1486,  0.1462,  0.1473,  0.1466,
         0.1404,  0.1462,  0.1470,  0.1461,  0.1464,  0.1462,  0.1508,  0.1455,
         0.1464,  0.1506,  0.1621,  0.1522,  0.1472,  0.1583,  0.1462,  0.1461,
         0.1522,  0.1462,  0.1816,  0.1461,  0.1464,  0.1464,  0.1464,  0.1462,
         0.1461,  0.1461,  0.1617,  0.1461,  0.1464,  0.1542,  0.1510,  0.1465,
         0.1462,  0.1461,  0.1462,  0.1464,  0.1505,  0.1461,  0.1462,  0.1434,
         0.1461,  0.1497,  0.1473,  0.1449,  0.1461,  0.1461,  0.1471,  0.1458,
         0.1461,  0.1461,  0.1461,  0.1461,  0.1989,  0.1461,  0.1725,  0.1465,
         0.1482,  0.1461,  0.1464,  0.1494,  0.1460,  0.1461,  0.1475,  0.1525,
         0.1478,  0.1462,  0.1466,  0.1461,  0.1461,  0.1464,  0.1450,  0.1462,
         0.1461,  0.1460,  0.1458,  0.1461,  0.1461,  0.1461,  0.1462, -0.0061,
         0.1553,  0.1461,  0.1990,  0.1461,  0.1461,  0.1466,  0.1461,  0.1464,
         0.1461,  0.1462,  0.1461,  0.1461,  0.1459,  0.1464,  0.1461,  0.1462,
         0.1708,  0.1461,  0.1427,  0.1461,  0.1465,  0.1414,  0.1461,  0.1460,
         0.1462,  0.1461,  0.1464,  0.1461,  0.1462,  0.1462,  0.1493,  0.1458,
         0.1462,  0.1682,  0.1461,  0.1464,  0.1837,  0.1465,  0.1462,  0.1462,
         0.1460,  0.1483,  0.1461,  0.1461,  0.1461,  0.1464,  0.1462,  0.1578,
         0.1504,  0.1464,  0.1476,  0.1461,  0.1653,  0.0895,  0.1493,  0.1462,
         0.1465,  0.1659,  0.1461,  0.1462,  0.1593,  0.1462,  0.1461,  0.1477,
         0.1462,  0.1432,  0.1445,  0.1653,  0.1465,  0.1461,  0.1520,  0.1461,
         0.1466,  0.1461,  0.1461,  0.1461,  0.1461,  0.1461,  0.1461,  0.1461,
         0.1512,  0.1462,  0.1460,  0.1462,  0.1753,  0.1594,  0.1461,  0.1461,
         0.1461,  0.1493,  0.1461,  0.1461,  0.1727,  0.1461,  0.1132,  0.1454,
         0.1497,  0.1461,  0.1461,  0.1422,  0.1461,  0.1464,  0.1461,  0.1461,
         0.1461,  0.1461,  0.1461,  0.1416,  0.1779,  0.1461,  0.1465,  0.1461,
         0.1635,  0.1461,  0.1461,  0.1461,  0.1462,  0.1461,  0.1461,  0.1521,
         0.1461,  0.1545,  0.1465,  0.1619,  0.1461,  0.1663,  0.1461,  0.1461,
         0.1464,  0.1465,  0.1462,  0.1461,  0.1461,  0.1461,  0.1464,  0.1462,
         0.1461,  0.1587,  0.1461,  0.1461,  0.1490,  0.1461,  0.1448,  0.1462,
         0.1464,  0.1458,  0.1653,  0.1461,  0.1479,  0.1461,  0.1461,  0.1444,
         0.1462,  0.1639,  0.1461,  0.1465,  0.1488,  0.1453,  0.1627,  0.1554,
         0.1464,  0.1462,  0.1477,  0.1460,  0.1715,  0.1538,  0.1465,  0.1461,
         0.1168,  0.1465,  0.1461,  0.1505,  0.1462,  0.1461,  0.1462,  0.1461,
         0.1462,  0.1465,  0.1465,  0.1699,  0.2131,  0.1464,  0.1462,  0.1471,
         0.1425,  0.1462,  0.1439,  0.1429,  0.1461,  0.1504,  0.1460,  0.1464,
         0.1464,  0.1461,  0.1466,  0.1462,  0.1805,  0.1438,  0.1461,  0.1707,
         0.1461,  0.1400,  0.1462,  0.1461,  0.1461,  0.1462,  0.1534,  0.1484,
         0.1462,  0.1462,  0.1464,  0.1462,  0.1461,  0.1461,  0.1443,  0.1462,
         0.1464,  0.1470,  0.1608,  0.1461,  0.1461,  0.1461,  0.1464,  0.1461,
         0.1461,  0.1461,  0.1521,  0.1464,  0.1461,  0.1420,  0.1461,  0.1686,
         0.1554,  0.1461,  0.1461,  0.1462,  0.1459,  0.1462,  0.1461,  0.1461,
         0.1461,  0.1461,  0.1461,  0.1461,  0.1433,  0.1526,  0.1461,  0.1462,
         0.1462,  0.1465,  0.1570,  0.1727,  0.1486,  0.1561,  0.1465,  0.1470,
         0.1462,  0.1461,  0.1536,  0.1465,  0.1943,  0.1461,  0.1532,  0.1461,
         0.1456,  0.1462,  0.1462,  0.1464,  0.1461,  0.1401,  0.1464,  0.1461,
         0.1461,  0.1484,  0.1461,  0.1465,  0.1461,  0.1333,  0.1761,  0.1461,
         0.1461,  0.1461,  0.1461,  0.1461,  0.1461,  0.1467,  0.1462,  0.1464,
         0.1444,  0.0975,  0.1411,  0.1464,  0.1747,  0.1461,  0.1465,  0.1462,
         0.1461,  0.1465,  0.1462,  0.1461,  0.1682,  0.1481,  0.1461,  0.1685,
         0.1461,  0.1462,  0.1461,  0.1465,  0.1868,  0.1464,  0.1461,  0.1461,
         0.1465,  0.1461,  0.1461,  0.1465,  0.1539,  0.1462,  0.1465,  0.1603,
         0.1459,  0.1461,  0.1549,  0.1461,  0.1464,  0.1461,  0.1465,  0.1464,
         0.1464,  0.1462,  0.1461,  0.1461,  0.1731,  0.1464,  0.1461,  0.1798,
         0.1638,  0.1447,  0.1578,  0.1464,  0.1741,  0.1466,  0.1641,  0.1458,
         0.0572,  0.1462,  0.1469,  0.1465,  0.1461,  0.1464,  0.1461,  0.1462,
         0.1461,  0.1461,  0.1461,  0.1508,  0.1462,  0.1461,  0.1465,  0.1461],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(4779.3813, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2588, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6000.0186, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8789, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6608.1284, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8438, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5720.1724, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4355, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6710.0972, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6543, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7686.9561, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0879, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6444.8955, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6914, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0.1818, 0.1855, 0.2153, 0.0850, 0.2125, 0.1841, 0.1841, 0.1880, 0.1846,
        0.1844, 0.1842, 0.2012, 0.1912, 0.1841, 0.1854, 0.2196, 0.1844, 0.1843,
        0.1796, 0.1841, 0.1841, 0.1937, 0.1842, 0.1951, 0.1854, 0.1844, 0.1843,
        0.1842, 0.1843, 0.2094, 0.1880, 0.1844, 0.1830, 0.1841, 0.1843, 0.1842,
        0.1848, 0.1904, 0.1843, 0.1844, 0.1841, 0.1846, 0.1841, 0.1840, 0.1841,
        0.2073, 0.1841, 0.1841, 0.1843, 0.1847, 0.1841, 0.2008, 0.1843, 0.1841,
        0.1841, 0.1838, 0.1986, 0.1802, 0.1841, 0.1840, 0.1842, 0.1841, 0.2173,
        0.1903, 0.2722, 0.2041, 0.1841, 0.1846, 0.1841, 0.1841, 0.1841, 0.1842,
        0.1841, 0.1842, 0.1840, 0.1844, 0.1841, 0.1842, 0.1842, 0.2162, 0.1841,
        0.1843, 0.1948, 0.1842, 0.2161, 0.1912, 0.1841, 0.1841, 0.2043, 0.1843,
        0.1841, 0.2051, 0.1844, 0.1842, 0.1841, 0.1844, 0.2095, 0.1848, 0.2415,
        0.1841, 0.1844, 0.1842, 0.1857, 0.1847, 0.1844, 0.1841, 0.1842, 0.1844,
        0.1880, 0.2191, 0.1844, 0.1843, 0.1803, 0.1976, 0.1841, 0.1841, 0.1841,
        0.1879, 0.1837, 0.2277, 0.1841, 0.1841, 0.1930, 0.1853, 0.1857, 0.1846,
        0.2025, 0.1842, 0.1984, 0.1841, 0.1842, 0.1841, 0.1807, 0.1841, 0.1837,
        0.2185, 0.1846, 0.1860, 0.1836, 0.1870, 0.1838, 0.1785, 0.1846, 0.1842,
        0.1840, 0.1843, 0.2361, 0.1859, 0.1844, 0.1844, 0.2290, 0.1853, 0.1847,
        0.1849, 0.2167, 0.1842, 0.1849, 0.1841, 0.1840, 0.1841, 0.1841, 0.1842,
        0.1842, 0.1844, 0.1974, 0.1880, 0.1847, 0.1842, 0.1844, 0.1843, 0.1841,
        0.1841, 0.1840, 0.1852, 0.1842, 0.1842, 0.1841, 0.1843, 0.1851, 0.1794,
        0.1842, 0.1841, 0.1837, 0.1926, 0.1843, 0.1875, 0.2120, 0.1841, 0.1843,
        0.1844, 0.1843, 0.1841, 0.1844, 0.1891, 0.1842, 0.1979, 0.1865, 0.1842,
        0.1914, 0.1846, 0.1783, 0.1843, 0.1849, 0.1841, 0.1843, 0.1843, 0.1888,
        0.1835, 0.1843, 0.1947, 0.2007, 0.1953, 0.1899, 0.2089, 0.1842, 0.1841,
        0.1921, 0.1842, 0.2229, 0.1841, 0.1844, 0.1843, 0.1844, 0.1843, 0.1842,
        0.1841, 0.2104, 0.1842, 0.1855, 0.1931, 0.1903, 0.1844, 0.1842, 0.1841,
        0.1842, 0.1843, 0.1898, 0.1841, 0.1851, 0.1815, 0.1842, 0.1882, 0.1854,
        0.1830, 0.1841, 0.1842, 0.1851, 0.1837, 0.1841, 0.1842, 0.1841, 0.1841,
        0.2445, 0.1842, 0.2112, 0.1851, 0.1862, 0.1841, 0.1843, 0.1875, 0.1840,
        0.1842, 0.1918, 0.1904, 0.1895, 0.1842, 0.1857, 0.1841, 0.1841, 0.1843,
        0.1830, 0.1842, 0.1841, 0.1840, 0.1837, 0.1841, 0.1841, 0.1842, 0.1842,
        0.0419, 0.1946, 0.1841, 0.2452, 0.1841, 0.1842, 0.1846, 0.1841, 0.1843,
        0.1849, 0.1843, 0.1841, 0.1841, 0.1838, 0.1843, 0.1841, 0.1842, 0.2214,
        0.1842, 0.1807, 0.1841, 0.1844, 0.1793, 0.1842, 0.1840, 0.1842, 0.1841,
        0.1843, 0.1841, 0.1842, 0.1843, 0.1953, 0.1837, 0.1842, 0.2095, 0.1842,
        0.1843, 0.2306, 0.1844, 0.1842, 0.1842, 0.1840, 0.1864, 0.1841, 0.1841,
        0.1841, 0.1843, 0.1843, 0.1958, 0.1859, 0.1855, 0.1855, 0.1841, 0.2081,
        0.0490, 0.1915, 0.1842, 0.1849, 0.2145, 0.1844, 0.1843, 0.2078, 0.1842,
        0.1841, 0.1860, 0.1842, 0.1812, 0.1836, 0.2046, 0.1844, 0.1841, 0.1979,
        0.1841, 0.1846, 0.1841, 0.1842, 0.1842, 0.1841, 0.1841, 0.1841, 0.1841,
        0.1984, 0.1842, 0.1841, 0.1842, 0.2136, 0.2023, 0.1841, 0.1841, 0.1841,
        0.1874, 0.1841, 0.1841, 0.2206, 0.1841, 0.1044, 0.1835, 0.1970, 0.1841,
        0.1841, 0.1802, 0.1841, 0.1843, 0.1841, 0.1841, 0.1841, 0.1841, 0.1842,
        0.1799, 0.1915, 0.1841, 0.1844, 0.1841, 0.2047, 0.1841, 0.1841, 0.1841,
        0.1842, 0.1841, 0.1842, 0.1941, 0.1841, 0.1926, 0.1854, 0.2100, 0.1841,
        0.2148, 0.1841, 0.1841, 0.1864, 0.1844, 0.1842, 0.1841, 0.1841, 0.1841,
        0.1843, 0.1842, 0.1841, 0.2020, 0.1841, 0.1842, 0.1909, 0.1841, 0.1827,
        0.1842, 0.1843, 0.1838, 0.2083, 0.1841, 0.1859, 0.1841, 0.1855, 0.1824,
        0.1842, 0.2184, 0.1841, 0.1844, 0.1874, 0.1833, 0.2073, 0.1985, 0.1843,
        0.1842, 0.1860, 0.1841, 0.1670, 0.2057, 0.1846, 0.1841, 0.1553, 0.1844,
        0.1842, 0.1886, 0.1843, 0.1841, 0.1842, 0.1841, 0.1842, 0.1844, 0.1847,
        0.2212, 0.2350, 0.1843, 0.1842, 0.1906, 0.1805, 0.1843, 0.1827, 0.1853,
        0.1841, 0.1910, 0.1840, 0.1844, 0.1844, 0.1841, 0.1870, 0.1842, 0.2294,
        0.1818, 0.1841, 0.2129, 0.1848, 0.1785, 0.1843, 0.1841, 0.1841, 0.1843,
        0.1926, 0.1864, 0.1842, 0.1842, 0.1843, 0.1847, 0.1841, 0.1841, 0.1823,
        0.1842, 0.1846, 0.1851, 0.2030, 0.1842, 0.1841, 0.1841, 0.1843, 0.1842,
        0.1842, 0.1841, 0.1901, 0.1844, 0.1841, 0.1799, 0.1841, 0.2153, 0.1990,
        0.1841, 0.1841, 0.1843, 0.1838, 0.1842, 0.1841, 0.1841, 0.1842, 0.1841,
        0.1841, 0.1841, 0.1814, 0.1907, 0.1842, 0.1842, 0.1842, 0.1844, 0.2026,
        0.2139, 0.1914, 0.2029, 0.1844, 0.1849, 0.1842, 0.1841, 0.2002, 0.1846,
        0.2390, 0.1841, 0.1951, 0.1841, 0.1837, 0.1842, 0.1842, 0.1843, 0.1841,
        0.1781, 0.1843, 0.1841, 0.1841, 0.1866, 0.1842, 0.1844, 0.1841, 0.1766,
        0.1910, 0.1841, 0.1841, 0.1841, 0.1841, 0.1841, 0.1841, 0.1847, 0.1843,
        0.1843, 0.1824, 0.1381, 0.1792, 0.1844, 0.2209, 0.1841, 0.1844, 0.1842,
        0.1841, 0.1844, 0.1842, 0.1841, 0.2200, 0.1864, 0.1842, 0.2064, 0.1841,
        0.1846, 0.1841, 0.1844, 0.2063, 0.1843, 0.1841, 0.1841, 0.1846, 0.1841,
        0.1841, 0.1851, 0.1985, 0.1843, 0.1844, 0.1982, 0.1838, 0.1841, 0.2081,
        0.1841, 0.1843, 0.1841, 0.1844, 0.1843, 0.1843, 0.1843, 0.1841, 0.1841,
        0.2181, 0.1843, 0.1841, 0.2233, 0.2168, 0.1826, 0.2073, 0.1843, 0.2126,
        0.1846, 0.2126, 0.1837, 0.0261, 0.1843, 0.1949, 0.1844, 0.1842, 0.1843,
        0.1842, 0.1848, 0.1841, 0.1841, 0.1841, 0.1887, 0.1842, 0.1841, 0.1844,
        0.1841], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 07:22:07,270][train_inner][INFO] - {"epoch": 14, "update": 13.488, "loss": "4.245", "ntokens": "149717", "nsentences": "539.985", "prob_perplexity": "88.276", "code_perplexity": "87.252", "temp": "1.932", "loss_0": "4.107", "loss_1": "0.124", "loss_2": "0.014", "accuracy": "0.30026", "wps": "37815.2", "ups": "0.25", "wpb": "149717", "bsz": "540", "num_updates": "7000", "lr": "0.000109375", "gnorm": "0.546", "loss_scale": "2", "train_wall": "791", "gb_free": "12.4", "wall": "28194"}
tensor(-2.6582, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5593.8208, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.2363, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.1945,  0.1520,  0.1831,  0.0726,  0.1720,  0.1504,  0.1504,  0.1519,
         0.1509,  0.1508,  0.1506,  0.1578,  0.1555,  0.1504,  0.1505,  0.1770,
         0.1508,  0.1506,  0.1451,  0.1504,  0.1505,  0.1558,  0.1506,  0.1602,
         0.1516,  0.1509,  0.1508,  0.1505,  0.1508,  0.1655,  0.1570,  0.1509,
         0.1494,  0.1505,  0.1506,  0.1505,  0.1500,  0.1519,  0.1508,  0.1508,
         0.1504,  0.1509,  0.1505,  0.1503,  0.1505,  0.1656,  0.1505,  0.1505,
         0.1506,  0.1511,  0.1505,  0.1654,  0.1506,  0.1504,  0.1505,  0.1501,
         0.1602,  0.1466,  0.1505,  0.1503,  0.1506,  0.1504,  0.1835,  0.1564,
         0.2278,  0.1595,  0.1505,  0.1510,  0.1505,  0.1504,  0.1504,  0.1506,
         0.1505,  0.1505,  0.1504,  0.1508,  0.1505,  0.1458,  0.1505,  0.1820,
         0.1504,  0.1506,  0.1611,  0.1506,  0.1824,  0.1527,  0.1505,  0.1504,
         0.1682,  0.1506,  0.1504,  0.1669,  0.1509,  0.1505,  0.1504,  0.1508,
         0.1732,  0.1512,  0.2096,  0.1505,  0.1508,  0.1506,  0.1509,  0.1509,
         0.1508,  0.1504,  0.1506,  0.1509,  0.1523,  0.1757,  0.1508,  0.1506,
         0.1466,  0.1593,  0.1505,  0.1504,  0.1504,  0.1542,  0.1501,  0.1943,
         0.1504,  0.1504,  0.1584,  0.1508,  0.1490,  0.1509,  0.1659,  0.1505,
         0.1609,  0.1505,  0.1505,  0.1504,  0.1471,  0.1505,  0.1501,  0.1770,
         0.1506,  0.1512,  0.1499,  0.1514,  0.1503,  0.1448,  0.1510,  0.1506,
         0.1504,  0.1508,  0.1917,  0.1515,  0.1508,  0.1508,  0.2026,  0.1514,
         0.1510,  0.1512,  0.1742,  0.1505,  0.1509,  0.1505,  0.1503,  0.1504,
         0.1505,  0.1506,  0.1505,  0.1508,  0.1642,  0.1515,  0.1510,  0.1506,
         0.1508,  0.1508,  0.1505,  0.1505,  0.1503,  0.1511,  0.1505,  0.1506,
         0.1505,  0.1506,  0.1515,  0.1458,  0.1506,  0.1505,  0.1500,  0.1589,
         0.1508,  0.1539,  0.1831,  0.1505,  0.1505,  0.1509,  0.1508,  0.1504,
         0.1505,  0.1549,  0.1505,  0.1561,  0.1530,  0.1506,  0.1522,  0.1509,
         0.1448,  0.1506,  0.1514,  0.1505,  0.1508,  0.1506,  0.1550,  0.1499,
         0.1506,  0.1562,  0.1665,  0.1569,  0.1520,  0.1652,  0.1506,  0.1504,
         0.1569,  0.1506,  0.1855,  0.1505,  0.1508,  0.1506,  0.1508,  0.1505,
         0.1505,  0.1505,  0.1687,  0.1505,  0.1508,  0.1587,  0.1558,  0.1508,
         0.1506,  0.1505,  0.1506,  0.1508,  0.1549,  0.1505,  0.1508,  0.1478,
         0.1505,  0.1541,  0.1517,  0.1493,  0.1504,  0.1505,  0.1515,  0.1500,
         0.1504,  0.1504,  0.1504,  0.1505,  0.2041,  0.1505,  0.1769,  0.1509,
         0.1526,  0.1505,  0.1508,  0.1538,  0.1504,  0.1504,  0.1521,  0.1567,
         0.1523,  0.1505,  0.1510,  0.1504,  0.1504,  0.1506,  0.1494,  0.1506,
         0.1504,  0.1503,  0.1500,  0.1504,  0.1504,  0.1505,  0.1505, -0.0009,
         0.1598,  0.1504,  0.2046,  0.1505,  0.1505,  0.1509,  0.1505,  0.1506,
         0.1505,  0.1506,  0.1504,  0.1504,  0.1501,  0.1508,  0.1505,  0.1505,
         0.1770,  0.1505,  0.1471,  0.1505,  0.1509,  0.1458,  0.1505,  0.1504,
         0.1505,  0.1505,  0.1508,  0.1504,  0.1506,  0.1506,  0.1544,  0.1501,
         0.1506,  0.1740,  0.1505,  0.1508,  0.1899,  0.1509,  0.1506,  0.1506,
         0.1504,  0.1526,  0.1504,  0.1504,  0.1505,  0.1508,  0.1506,  0.1621,
         0.1516,  0.1508,  0.1520,  0.1505,  0.1692,  0.0840,  0.1541,  0.1506,
         0.1509,  0.1713,  0.1505,  0.1506,  0.1649,  0.1505,  0.1505,  0.1521,
         0.1505,  0.1475,  0.1490,  0.1698,  0.1508,  0.1504,  0.1567,  0.1504,
         0.1510,  0.1504,  0.1505,  0.1505,  0.1505,  0.1504,  0.1504,  0.1504,
         0.1571,  0.1505,  0.1504,  0.1506,  0.1799,  0.1639,  0.1505,  0.1505,
         0.1504,  0.1537,  0.1505,  0.1504,  0.1793,  0.1505,  0.1103,  0.1498,
         0.1549,  0.1505,  0.1504,  0.1465,  0.1504,  0.1508,  0.1504,  0.1505,
         0.1504,  0.1505,  0.1505,  0.1460,  0.1807,  0.1504,  0.1508,  0.1504,
         0.1683,  0.1505,  0.1505,  0.1504,  0.1506,  0.1505,  0.1505,  0.1569,
         0.1504,  0.1589,  0.1509,  0.1682,  0.1505,  0.1727,  0.1504,  0.1505,
         0.1510,  0.1508,  0.1505,  0.1505,  0.1505,  0.1505,  0.1508,  0.1505,
         0.1505,  0.1633,  0.1505,  0.1505,  0.1536,  0.1504,  0.1492,  0.1506,
         0.1506,  0.1501,  0.1700,  0.1505,  0.1522,  0.1505,  0.1505,  0.1488,
         0.1506,  0.1704,  0.1505,  0.1509,  0.1533,  0.1497,  0.1681,  0.1603,
         0.1506,  0.1505,  0.1522,  0.1504,  0.1777,  0.1598,  0.1509,  0.1504,
         0.1216,  0.1508,  0.1505,  0.1549,  0.1506,  0.1505,  0.1506,  0.1505,
         0.1505,  0.1508,  0.1508,  0.1761,  0.2136,  0.1506,  0.1506,  0.1519,
         0.1469,  0.1506,  0.1483,  0.1483,  0.1505,  0.1556,  0.1504,  0.1508,
         0.1508,  0.1505,  0.1511,  0.1506,  0.1868,  0.1481,  0.1504,  0.1771,
         0.1505,  0.1444,  0.1506,  0.1505,  0.1505,  0.1506,  0.1569,  0.1527,
         0.1505,  0.1505,  0.1506,  0.1505,  0.1504,  0.1505,  0.1486,  0.1506,
         0.1508,  0.1514,  0.1664,  0.1505,  0.1504,  0.1505,  0.1508,  0.1505,
         0.1505,  0.1504,  0.1565,  0.1508,  0.1504,  0.1462,  0.1505,  0.1736,
         0.1603,  0.1504,  0.1505,  0.1506,  0.1503,  0.1505,  0.1505,  0.1505,
         0.1505,  0.1504,  0.1504,  0.1505,  0.1477,  0.1570,  0.1505,  0.1506,
         0.1506,  0.1508,  0.1626,  0.1777,  0.1537,  0.1614,  0.1508,  0.1514,
         0.1505,  0.1504,  0.1594,  0.1509,  0.1997,  0.1505,  0.1583,  0.1504,
         0.1500,  0.1505,  0.1505,  0.1508,  0.1505,  0.1445,  0.1506,  0.1505,
         0.1504,  0.1528,  0.1505,  0.1509,  0.1504,  0.1384,  0.1807,  0.1504,
         0.1504,  0.1504,  0.1504,  0.1504,  0.1504,  0.1511,  0.1506,  0.1508,
         0.1487,  0.1021,  0.1455,  0.1508,  0.1805,  0.1505,  0.1508,  0.1505,
         0.1505,  0.1508,  0.1506,  0.1505,  0.1746,  0.1525,  0.1505,  0.1729,
         0.1504,  0.1505,  0.1504,  0.1509,  0.1915,  0.1506,  0.1504,  0.1505,
         0.1509,  0.1504,  0.1505,  0.1509,  0.1588,  0.1506,  0.1509,  0.1647,
         0.1503,  0.1505,  0.1613,  0.1504,  0.1508,  0.1505,  0.1508,  0.1508,
         0.1508,  0.1506,  0.1505,  0.1505,  0.1785,  0.1508,  0.1504,  0.1854,
         0.1697,  0.1490,  0.1635,  0.1508,  0.1777,  0.1509,  0.1688,  0.1501,
         0.0536,  0.1506,  0.1516,  0.1508,  0.1505,  0.1506,  0.1505,  0.1506,
         0.1504,  0.1505,  0.1505,  0.1552,  0.1505,  0.1505,  0.1509,  0.1505],
       device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(5377.6011, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7092.8955, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5312, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6443.7236, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8066, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7937.9951, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3691, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6672.4521, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0020, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5949.3311, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6504, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6786.9189, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1895, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8486.0654, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6211, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6546.7754, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: [2023-09-12 07:24:48,687][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(6945.9043, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1973, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 07:35:19,515][train_inner][INFO] - {"epoch": 14, "update": 13.873, "loss": "4.192", "ntokens": "149794", "nsentences": "538.34", "prob_perplexity": "91.703", "code_perplexity": "90.627", "temp": "1.93", "loss_0": "4.054", "loss_1": "0.124", "loss_2": "0.014", "accuracy": "0.30815", "wps": "37815.2", "ups": "0.25", "wpb": "149794", "bsz": "538.3", "num_updates": "7200", "lr": "0.0001125", "gnorm": "0.556", "loss_scale": "1", "train_wall": "791", "gb_free": "12.6", "wall": "28986"}
[2023-09-12 07:39:37,612][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 07:39:37,613][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 07:39:37,817][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2023-09-12 07:40:01,430][valid][INFO] - {"epoch": 14, "valid_loss": "3.975", "valid_ntokens": "7902.75", "valid_nsentences": "55.2525", "valid_prob_perplexity": "84.555", "valid_code_perplexity": "83.469", "valid_temp": "1.929", "valid_loss_0": "3.835", "valid_loss_1": "0.125", "valid_loss_2": "0.015", "valid_accuracy": "0.35745", "valid_wps": "33349.5", "valid_wpb": "7902.7", "valid_bsz": "55.3", "valid_num_updates": "7266", "valid_best_loss": "3.975"}
[2023-09-12 07:40:01,432][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 7266 updates
[2023-09-12 07:40:01,433][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 07:40:03,851][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 07:40:05,294][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 7266 updates, score 3.975) (writing took 3.8621418830007315 seconds)
[2023-09-12 07:40:05,295][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2023-09-12 07:40:05,296][train][INFO] - {"epoch": 14, "train_loss": "4.221", "train_ntokens": "149482", "train_nsentences": "538.454", "train_prob_perplexity": "90.111", "train_code_perplexity": "89.072", "train_temp": "1.931", "train_loss_0": "4.083", "train_loss_1": "0.124", "train_loss_2": "0.014", "train_accuracy": "0.30379", "train_wps": "37291.2", "train_ups": "0.25", "train_wpb": "149482", "train_bsz": "538.5", "train_num_updates": "7266", "train_lr": "0.000113531", "train_gnorm": "0.558", "train_loss_scale": "1", "train_train_wall": "2045", "train_gb_free": "13.2", "train_wall": "29272"}
[2023-09-12 07:40:05,301][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 07:40:05,411][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2023-09-12 07:40:05,650][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 07:40:05,653][fairseq.trainer][INFO] - begin training epoch 15
[2023-09-12 07:40:05,653][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(6295.5117, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3711, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4505.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0938, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6931.4629, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3926, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4325.0098, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2344, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7404.1113, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2324, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7889.9033, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4043, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7167.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3066, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4431.1611, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4609, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3670.7944, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2148, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 07:48:52,938][train_inner][INFO] - {"epoch": 15, "update": 14.257, "loss": "4.175", "ntokens": "148899", "nsentences": "538.005", "prob_perplexity": "95.523", "code_perplexity": "94.409", "temp": "1.928", "loss_0": "4.038", "loss_1": "0.123", "loss_2": "0.015", "accuracy": "0.30957", "wps": "36610.4", "ups": "0.25", "wpb": "148898", "bsz": "538", "num_updates": "7400", "lr": "0.000115625", "gnorm": "0.59", "loss_scale": "2", "train_wall": "784", "gb_free": "12.8", "wall": "29799"}
[2023-09-12 07:59:09,570][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
Parameter containing:
tensor([0.1792, 0.1893, 0.2150, 0.0887, 0.2136, 0.1879, 0.1879, 0.1921, 0.1884,
        0.1882, 0.1881, 0.2069, 0.1953, 0.1879, 0.1896, 0.2255, 0.1882, 0.1881,
        0.1835, 0.1879, 0.1879, 0.1980, 0.1880, 0.1995, 0.1892, 0.1882, 0.1881,
        0.1880, 0.1882, 0.2147, 0.1918, 0.1882, 0.1869, 0.1879, 0.1881, 0.1880,
        0.1891, 0.1952, 0.1881, 0.1882, 0.1879, 0.1884, 0.1879, 0.1877, 0.1879,
        0.2125, 0.1879, 0.1879, 0.1881, 0.1885, 0.1879, 0.2053, 0.1881, 0.1879,
        0.1880, 0.1876, 0.2031, 0.1840, 0.1879, 0.1877, 0.1880, 0.1879, 0.2196,
        0.1941, 0.2773, 0.2095, 0.1880, 0.1885, 0.1879, 0.1879, 0.1879, 0.1880,
        0.1879, 0.1880, 0.1877, 0.1882, 0.1879, 0.1892, 0.1880, 0.2200, 0.1879,
        0.1881, 0.1986, 0.1880, 0.2198, 0.1965, 0.1879, 0.1879, 0.2085, 0.1881,
        0.1879, 0.2107, 0.1884, 0.1880, 0.1879, 0.1882, 0.2145, 0.1887, 0.2435,
        0.1879, 0.1882, 0.1880, 0.1898, 0.1885, 0.1882, 0.1879, 0.1880, 0.1884,
        0.1920, 0.2246, 0.1882, 0.1881, 0.1841, 0.2025, 0.1879, 0.1879, 0.1879,
        0.1917, 0.1875, 0.2312, 0.1879, 0.1879, 0.1971, 0.1895, 0.1898, 0.1884,
        0.2067, 0.1880, 0.2048, 0.1880, 0.1880, 0.1879, 0.1844, 0.1879, 0.1875,
        0.2235, 0.1886, 0.1903, 0.1874, 0.1910, 0.1876, 0.1823, 0.1884, 0.1881,
        0.1877, 0.1882, 0.2411, 0.1898, 0.1882, 0.1882, 0.2301, 0.1891, 0.1885,
        0.1887, 0.2223, 0.1880, 0.1887, 0.1880, 0.1877, 0.1879, 0.1879, 0.1880,
        0.1880, 0.1882, 0.2018, 0.1925, 0.1885, 0.1880, 0.1882, 0.1882, 0.1879,
        0.1880, 0.1877, 0.1890, 0.1880, 0.1881, 0.1880, 0.1881, 0.1888, 0.1832,
        0.1881, 0.1879, 0.1875, 0.1965, 0.1881, 0.1913, 0.2166, 0.1879, 0.1881,
        0.1882, 0.1881, 0.1879, 0.1882, 0.1931, 0.1880, 0.2015, 0.1904, 0.1881,
        0.1967, 0.1884, 0.1821, 0.1881, 0.1887, 0.1879, 0.1881, 0.1881, 0.1926,
        0.1873, 0.1881, 0.1990, 0.2046, 0.1995, 0.1943, 0.2145, 0.1880, 0.1879,
        0.1962, 0.1880, 0.2280, 0.1879, 0.1882, 0.1881, 0.1882, 0.1881, 0.1880,
        0.1879, 0.2162, 0.1880, 0.1901, 0.1971, 0.1942, 0.1882, 0.1880, 0.1879,
        0.1880, 0.1881, 0.1938, 0.1879, 0.1888, 0.1853, 0.1880, 0.1921, 0.1892,
        0.1869, 0.1879, 0.1880, 0.1890, 0.1875, 0.1879, 0.1881, 0.1879, 0.1879,
        0.2491, 0.1880, 0.2151, 0.1890, 0.1901, 0.1880, 0.1881, 0.1913, 0.1877,
        0.1880, 0.1956, 0.1942, 0.1941, 0.1880, 0.1897, 0.1879, 0.1879, 0.1881,
        0.1869, 0.1880, 0.1879, 0.1877, 0.1875, 0.1879, 0.1879, 0.1880, 0.1880,
        0.0466, 0.1987, 0.1879, 0.2498, 0.1879, 0.1880, 0.1884, 0.1880, 0.1881,
        0.1887, 0.1881, 0.1879, 0.1879, 0.1876, 0.1882, 0.1880, 0.1880, 0.2269,
        0.1880, 0.1844, 0.1880, 0.1882, 0.1831, 0.1880, 0.1877, 0.1880, 0.1879,
        0.1881, 0.1879, 0.1880, 0.1881, 0.1997, 0.1875, 0.1881, 0.2135, 0.1880,
        0.1881, 0.2365, 0.1882, 0.1880, 0.1880, 0.1879, 0.1903, 0.1879, 0.1879,
        0.1879, 0.1881, 0.1881, 0.1996, 0.1906, 0.1897, 0.1893, 0.1879, 0.2128,
        0.0468, 0.1963, 0.1880, 0.1887, 0.2202, 0.1884, 0.1881, 0.2128, 0.1880,
        0.1879, 0.1898, 0.1880, 0.1849, 0.1875, 0.2085, 0.1882, 0.1879, 0.2036,
        0.1879, 0.1884, 0.1879, 0.1880, 0.1880, 0.1880, 0.1879, 0.1879, 0.1879,
        0.2043, 0.1880, 0.1879, 0.1881, 0.2178, 0.2067, 0.1879, 0.1879, 0.1879,
        0.1912, 0.1879, 0.1879, 0.2258, 0.1879, 0.1052, 0.1873, 0.2024, 0.1880,
        0.1879, 0.1840, 0.1879, 0.1881, 0.1879, 0.1879, 0.1879, 0.1879, 0.1880,
        0.1837, 0.1899, 0.1879, 0.1882, 0.1879, 0.2090, 0.1879, 0.1879, 0.1879,
        0.1881, 0.1879, 0.1880, 0.1985, 0.1879, 0.1965, 0.1895, 0.2150, 0.1880,
        0.2209, 0.1879, 0.1879, 0.1904, 0.1882, 0.1880, 0.1880, 0.1879, 0.1879,
        0.1881, 0.1880, 0.1879, 0.2069, 0.1879, 0.1880, 0.1952, 0.1879, 0.1865,
        0.1880, 0.1881, 0.1876, 0.2125, 0.1880, 0.1897, 0.1879, 0.1901, 0.1862,
        0.1880, 0.2233, 0.1879, 0.1882, 0.1913, 0.1871, 0.2114, 0.2028, 0.1881,
        0.1880, 0.1898, 0.1879, 0.1619, 0.2114, 0.1884, 0.1879, 0.1594, 0.1882,
        0.1880, 0.1924, 0.1881, 0.1879, 0.1880, 0.1879, 0.1880, 0.1882, 0.1886,
        0.2267, 0.2401, 0.1881, 0.1880, 0.1951, 0.1843, 0.1881, 0.1866, 0.1897,
        0.1879, 0.1951, 0.1877, 0.1882, 0.1882, 0.1879, 0.1912, 0.1880, 0.2344,
        0.1855, 0.1879, 0.2142, 0.1891, 0.1824, 0.1881, 0.1879, 0.1880, 0.1881,
        0.1967, 0.1902, 0.1880, 0.1880, 0.1881, 0.1886, 0.1879, 0.1879, 0.1860,
        0.1880, 0.1884, 0.1888, 0.2069, 0.1880, 0.1879, 0.1879, 0.1881, 0.1880,
        0.1880, 0.1879, 0.1938, 0.1882, 0.1879, 0.1837, 0.1879, 0.2196, 0.2030,
        0.1879, 0.1879, 0.1881, 0.1876, 0.1880, 0.1879, 0.1880, 0.1880, 0.1879,
        0.1879, 0.1880, 0.1852, 0.1945, 0.1880, 0.1881, 0.1881, 0.1882, 0.2061,
        0.2179, 0.1959, 0.2068, 0.1882, 0.1887, 0.1880, 0.1879, 0.2048, 0.1884,
        0.2444, 0.1879, 0.1993, 0.1879, 0.1875, 0.1880, 0.1880, 0.1882, 0.1879,
        0.1819, 0.1881, 0.1879, 0.1879, 0.1904, 0.1880, 0.1882, 0.1879, 0.1818,
        0.1920, 0.1879, 0.1879, 0.1879, 0.1879, 0.1879, 0.1879, 0.1885, 0.1881,
        0.1881, 0.1862, 0.1421, 0.1831, 0.1882, 0.2264, 0.1880, 0.1882, 0.1880,
        0.1879, 0.1882, 0.1880, 0.1879, 0.2252, 0.1903, 0.1880, 0.2102, 0.1879,
        0.1885, 0.1879, 0.1884, 0.2072, 0.1881, 0.1879, 0.1879, 0.1884, 0.1879,
        0.1880, 0.1891, 0.2034, 0.1881, 0.1882, 0.2021, 0.1877, 0.1879, 0.2134,
        0.1879, 0.1882, 0.1879, 0.1882, 0.1881, 0.1881, 0.1881, 0.1879, 0.1879,
        0.2236, 0.1881, 0.1879, 0.2271, 0.2223, 0.1864, 0.2131, 0.1881, 0.2163,
        0.1884, 0.2166, 0.1875, 0.0239, 0.1881, 0.1997, 0.1882, 0.1880, 0.1881,
        0.1880, 0.1888, 0.1879, 0.1879, 0.1879, 0.1925, 0.1880, 0.1879, 0.1884,
        0.1879], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6337.7627, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2051, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8115.5205, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7871, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7263.2026, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0020, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7469.0200, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2012, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6827.6958, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4492, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6796.2407, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3379, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7077.4336, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4453, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7013.9448, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5059, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6289.9233, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3555, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7113.0659, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6016, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7463.4160, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5840, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6346.7935, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3984, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1436, 0.2280, 0.1970, 0.1370, 0.1794, 0.2266, 0.2266, 0.2378, 0.2271,
        0.2269, 0.2267, 0.2654, 0.2410, 0.2266, 0.2385, 0.2812, 0.2269, 0.2268,
        0.2273, 0.2266, 0.2266, 0.2476, 0.2267, 0.2494, 0.2285, 0.2269, 0.2268,
        0.2267, 0.2269, 0.2449, 0.2330, 0.2269, 0.2255, 0.2266, 0.2268, 0.2267,
        0.2362, 0.2524, 0.2268, 0.2278, 0.2266, 0.2271, 0.2266, 0.2264, 0.2266,
        0.2688, 0.2266, 0.2266, 0.2268, 0.2272, 0.2266, 0.2502, 0.2268, 0.2266,
        0.2266, 0.2263, 0.2571, 0.2227, 0.2266, 0.2264, 0.2267, 0.2268, 0.2043,
        0.2333, 0.3176, 0.2666, 0.2271, 0.2272, 0.2267, 0.2266, 0.2266, 0.2267,
        0.2266, 0.2272, 0.2264, 0.2269, 0.2266, 0.2484, 0.2267, 0.2605, 0.2266,
        0.2268, 0.2373, 0.2267, 0.2585, 0.2563, 0.2266, 0.2266, 0.2524, 0.2268,
        0.2266, 0.2646, 0.2269, 0.2267, 0.2266, 0.2269, 0.2507, 0.2277, 0.2773,
        0.2266, 0.2269, 0.2267, 0.2352, 0.2296, 0.2274, 0.2266, 0.2267, 0.2271,
        0.2424, 0.2844, 0.2269, 0.2268, 0.2228, 0.2544, 0.2266, 0.2266, 0.2266,
        0.2303, 0.2262, 0.2793, 0.2266, 0.2266, 0.2365, 0.2341, 0.2410, 0.2271,
        0.2544, 0.2272, 0.2605, 0.2267, 0.2267, 0.2266, 0.2231, 0.2266, 0.2262,
        0.2825, 0.2301, 0.2351, 0.2261, 0.2358, 0.2263, 0.2209, 0.2271, 0.2268,
        0.2264, 0.2269, 0.2998, 0.2312, 0.2269, 0.2269, 0.2115, 0.2300, 0.2272,
        0.2274, 0.2820, 0.2267, 0.2280, 0.2267, 0.2264, 0.2266, 0.2266, 0.2267,
        0.2267, 0.2269, 0.2354, 0.2415, 0.2278, 0.2268, 0.2269, 0.2269, 0.2266,
        0.2267, 0.2264, 0.2306, 0.2267, 0.2267, 0.2267, 0.2268, 0.2275, 0.2219,
        0.2267, 0.2266, 0.2262, 0.2357, 0.2268, 0.2301, 0.2461, 0.2266, 0.2294,
        0.2269, 0.2268, 0.2266, 0.2346, 0.2345, 0.2267, 0.2563, 0.2291, 0.2268,
        0.2524, 0.2271, 0.2208, 0.2281, 0.2274, 0.2266, 0.2268, 0.2268, 0.2322,
        0.2260, 0.2268, 0.2469, 0.2454, 0.2500, 0.2440, 0.2739, 0.2267, 0.2266,
        0.2417, 0.2267, 0.2839, 0.2266, 0.2269, 0.2268, 0.2269, 0.2274, 0.2267,
        0.2266, 0.2729, 0.2267, 0.2402, 0.2389, 0.2361, 0.2269, 0.2267, 0.2266,
        0.2267, 0.2268, 0.2400, 0.2266, 0.2339, 0.2240, 0.2302, 0.2313, 0.2279,
        0.2264, 0.2266, 0.2267, 0.2277, 0.2262, 0.2266, 0.2290, 0.2266, 0.2266,
        0.3042, 0.2267, 0.2585, 0.2305, 0.2288, 0.2267, 0.2268, 0.2300, 0.2264,
        0.2271, 0.2522, 0.2329, 0.2454, 0.2267, 0.2336, 0.2266, 0.2266, 0.2268,
        0.2256, 0.2267, 0.2266, 0.2264, 0.2262, 0.2266, 0.2266, 0.2267, 0.2267,
        0.1015, 0.2406, 0.2266, 0.2571, 0.2266, 0.2272, 0.2271, 0.2267, 0.2268,
        0.2295, 0.2268, 0.2266, 0.2266, 0.2263, 0.2268, 0.2266, 0.2267, 0.2803,
        0.2267, 0.2231, 0.2280, 0.2269, 0.2218, 0.2267, 0.2264, 0.2267, 0.2266,
        0.2268, 0.2266, 0.2267, 0.2268, 0.2522, 0.2262, 0.2267, 0.2605, 0.2267,
        0.2268, 0.2932, 0.2269, 0.2269, 0.2267, 0.2266, 0.2310, 0.2266, 0.2266,
        0.2266, 0.2268, 0.2268, 0.2383, 0.2454, 0.2372, 0.2280, 0.2266, 0.2644,
        0.0353, 0.2510, 0.2267, 0.2306, 0.2773, 0.2284, 0.2268, 0.2705, 0.2267,
        0.2266, 0.2288, 0.2267, 0.2236, 0.2301, 0.2498, 0.2269, 0.2266, 0.2622,
        0.2266, 0.2271, 0.2266, 0.2267, 0.2267, 0.2266, 0.2266, 0.2266, 0.2266,
        0.1995, 0.2267, 0.2281, 0.2268, 0.2605, 0.2551, 0.2266, 0.2266, 0.2266,
        0.2300, 0.2267, 0.2266, 0.2795, 0.2266, 0.1084, 0.2267, 0.2576, 0.2267,
        0.2266, 0.2227, 0.2266, 0.2268, 0.2266, 0.2266, 0.2266, 0.2266, 0.2267,
        0.2253, 0.1548, 0.2266, 0.2271, 0.2266, 0.2549, 0.2266, 0.2266, 0.2266,
        0.2267, 0.2266, 0.2269, 0.2499, 0.2266, 0.2352, 0.2349, 0.2739, 0.2266,
        0.2800, 0.2266, 0.2268, 0.2321, 0.2269, 0.2267, 0.2267, 0.2266, 0.2266,
        0.2268, 0.2267, 0.2266, 0.2627, 0.2266, 0.2267, 0.2490, 0.2266, 0.2252,
        0.2267, 0.2268, 0.2263, 0.2595, 0.2266, 0.2284, 0.2266, 0.2393, 0.2249,
        0.2267, 0.2300, 0.2266, 0.2269, 0.2336, 0.2258, 0.2529, 0.2489, 0.2268,
        0.2267, 0.2290, 0.2266, 0.1315, 0.2515, 0.2271, 0.2266, 0.2039, 0.2269,
        0.2267, 0.2311, 0.2268, 0.2266, 0.2267, 0.2266, 0.2267, 0.2269, 0.2300,
        0.2839, 0.2419, 0.2268, 0.2267, 0.2468, 0.2230, 0.2268, 0.2280, 0.2410,
        0.2266, 0.2394, 0.2264, 0.2269, 0.2269, 0.2266, 0.2368, 0.2267, 0.2869,
        0.2242, 0.2266, 0.1855, 0.2352, 0.2249, 0.2268, 0.2266, 0.2267, 0.2268,
        0.2423, 0.2289, 0.2267, 0.2267, 0.2268, 0.2297, 0.2266, 0.2266, 0.2247,
        0.2267, 0.2290, 0.2277, 0.2598, 0.2267, 0.2266, 0.2266, 0.2268, 0.2267,
        0.2267, 0.2266, 0.2325, 0.2269, 0.2266, 0.2224, 0.2266, 0.2798, 0.2554,
        0.2266, 0.2266, 0.2268, 0.2263, 0.2267, 0.2266, 0.2266, 0.2267, 0.2267,
        0.2266, 0.2267, 0.2239, 0.2332, 0.2271, 0.2267, 0.2268, 0.2269, 0.2598,
        0.2639, 0.2491, 0.2620, 0.2269, 0.2274, 0.2272, 0.2266, 0.2605, 0.2271,
        0.2983, 0.2266, 0.2437, 0.2266, 0.2262, 0.2267, 0.2267, 0.2268, 0.2266,
        0.2206, 0.2268, 0.2266, 0.2266, 0.2292, 0.2272, 0.2269, 0.2266, 0.2306,
        0.2070, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2266, 0.2272, 0.2268,
        0.2268, 0.2249, 0.1868, 0.2218, 0.2271, 0.2742, 0.2267, 0.2269, 0.2267,
        0.2266, 0.2269, 0.2268, 0.2266, 0.2517, 0.2322, 0.2267, 0.2489, 0.2266,
        0.2297, 0.2266, 0.2271, 0.1870, 0.2268, 0.2266, 0.2266, 0.2278, 0.2266,
        0.2266, 0.2350, 0.2571, 0.2268, 0.2269, 0.2407, 0.2264, 0.2266, 0.2664,
        0.2266, 0.2269, 0.2266, 0.2269, 0.2268, 0.2268, 0.2278, 0.2266, 0.2266,
        0.2742, 0.2269, 0.2266, 0.2798, 0.2549, 0.2251, 0.2739, 0.2268, 0.2588,
        0.2271, 0.2739, 0.2262, 0.0020, 0.2268, 0.2372, 0.2269, 0.2267, 0.2268,
        0.2267, 0.2281, 0.2266, 0.2266, 0.2266, 0.2312, 0.2267, 0.2266, 0.2269,
        0.2266], device='cuda:2', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: [2023-09-12 08:02:07,746][train_inner][INFO] - {"epoch": 15, "update": 14.643, "loss": "4.156", "ntokens": "149754", "nsentences": "538.075", "prob_perplexity": "99.494", "code_perplexity": "98.287", "temp": "1.926", "loss_0": "4.019", "loss_1": "0.122", "loss_2": "0.015", "accuracy": "0.31047", "wps": "37682.9", "ups": "0.25", "wpb": "149754", "bsz": "538.1", "num_updates": "7600", "lr": "0.00011875", "gnorm": "0.555", "loss_scale": "2", "train_wall": "794", "gb_free": "12.9", "wall": "30594"}
loss: tensor(5893.4893, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5430, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7181.1655, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6133, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:05:06,017][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(7215.1040, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4082, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5661.9570, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4512, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6899.5518, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7227, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6501.6406, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4844, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:14:22,593][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 08:14:22,594][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 08:14:22,665][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2023-09-12 08:14:46,222][valid][INFO] - {"epoch": 15, "valid_loss": "3.923", "valid_ntokens": "7899.18", "valid_nsentences": "55.2525", "valid_prob_perplexity": "92.858", "valid_code_perplexity": "91.427", "valid_temp": "1.924", "valid_loss_0": "3.784", "valid_loss_1": "0.123", "valid_loss_2": "0.016", "valid_accuracy": "0.36487", "valid_wps": "33448.6", "valid_wpb": "7899.2", "valid_bsz": "55.3", "valid_num_updates": "7785", "valid_best_loss": "3.923"}
[2023-09-12 08:14:46,224][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 7785 updates
[2023-09-12 08:14:46,226][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 08:14:48,752][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 08:14:50,165][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 7785 updates, score 3.923) (writing took 3.9404279169393703 seconds)
[2023-09-12 08:14:50,165][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2023-09-12 08:14:50,166][train][INFO] - {"epoch": 15, "train_loss": "4.156", "train_ntokens": "149462", "train_nsentences": "538.495", "train_prob_perplexity": "99.888", "train_code_perplexity": "98.679", "train_temp": "1.926", "train_loss_0": "4.019", "train_loss_1": "0.122", "train_loss_2": "0.015", "train_accuracy": "0.31045", "train_wps": "37206.6", "train_ups": "0.25", "train_wpb": "149462", "train_bsz": "538.5", "train_num_updates": "7785", "train_lr": "0.000121641", "train_gnorm": "0.558", "train_loss_scale": "1", "train_train_wall": "2053", "train_gb_free": "13.1", "train_wall": "31357"}
[2023-09-12 08:14:50,168][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 08:14:50,263][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2023-09-12 08:14:50,513][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 08:14:50,517][fairseq.trainer][INFO] - begin training epoch 16
[2023-09-12 08:14:50,517][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(5395.9316, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0430, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:15:49,003][train_inner][INFO] - {"epoch": 16, "update": 15.029, "loss": "4.149", "ntokens": "149097", "nsentences": "537.665", "prob_perplexity": "102.971", "code_perplexity": "101.697", "temp": "1.924", "loss_0": "4.012", "loss_1": "0.121", "loss_2": "0.015", "accuracy": "0.31038", "wps": "36309.5", "ups": "0.24", "wpb": "149097", "bsz": "537.7", "num_updates": "7800", "lr": "0.000121875", "gnorm": "0.576", "loss_scale": "1", "train_wall": "792", "gb_free": "13", "wall": "31415"}
loss: tensor(7266.1357, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4453, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7372.9004, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7891, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5988.5693, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9004, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6872.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2773, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5191.2749, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4688, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6828.5195, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2285, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6267.5044, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8301, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:23:32,556][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(6422.7749, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9277, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7784.7534, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8848, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:29:04,981][train_inner][INFO] - {"epoch": 16, "update": 15.415, "loss": "4.124", "ntokens": "149722", "nsentences": "540.205", "prob_perplexity": "106.195", "code_perplexity": "104.873", "temp": "1.923", "loss_0": "3.988", "loss_1": "0.12", "loss_2": "0.016", "accuracy": "0.31246", "wps": "37619.7", "ups": "0.25", "wpb": "149722", "bsz": "540.2", "num_updates": "8000", "lr": "0.000125", "gnorm": "0.536", "loss_scale": "1", "train_wall": "795", "gb_free": "13.1", "wall": "32211"}
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.1167,  0.2600,  0.1957,  0.1825,  0.1499,  0.2585,  0.2585,  0.2783,
         0.2590,  0.2590,  0.2588,  0.3149,  0.2812,  0.2585,  0.2830,  0.3296,
         0.2590,  0.2588,  0.2683,  0.2585,  0.2585,  0.2939,  0.2588,  0.2942,
         0.2612,  0.2590,  0.2588,  0.2588,  0.2590,  0.2452,  0.2678,  0.2590,
         0.2576,  0.2585,  0.2588,  0.2588,  0.2820,  0.3005,  0.2588,  0.2620,
         0.2585,  0.2590,  0.2585,  0.2585,  0.2585,  0.2795,  0.2585,  0.2585,
         0.2588,  0.2593,  0.2585,  0.2908,  0.2588,  0.2585,  0.2588,  0.2583,
         0.3052,  0.2546,  0.2585,  0.2585,  0.2588,  0.2590,  0.1897,  0.2659,
         0.3420,  0.3149,  0.2598,  0.2593,  0.2588,  0.2585,  0.2585,  0.2588,
         0.2585,  0.2615,  0.2585,  0.2590,  0.2585,  0.3008,  0.2588,  0.2964,
         0.2585,  0.2588,  0.2693,  0.2588,  0.2905,  0.3003,  0.2585,  0.2585,
         0.2881,  0.2588,  0.2588,  0.2739,  0.2590,  0.2588,  0.2585,  0.2590,
         0.2610,  0.2615,  0.2981,  0.2585,  0.2590,  0.2588,  0.2749,  0.2678,
         0.2612,  0.2585,  0.2588,  0.2590,  0.2910,  0.3188,  0.2590,  0.2588,
         0.2549,  0.3010,  0.2585,  0.2585,  0.2585,  0.2625,  0.2583,  0.3264,
         0.2585,  0.2585,  0.2693,  0.2769,  0.2888,  0.2590,  0.2957,  0.2605,
         0.3088,  0.2588,  0.2588,  0.2585,  0.2551,  0.2585,  0.2583,  0.3083,
         0.2661,  0.2751,  0.2581,  0.2737,  0.2583,  0.2529,  0.2590,  0.2588,
         0.2585,  0.2590,  0.3462,  0.2668,  0.2590,  0.2590,  0.2089,  0.2651,
         0.2593,  0.2595,  0.3323,  0.2588,  0.2607,  0.2588,  0.2585,  0.2585,
         0.2588,  0.2588,  0.2588,  0.2590,  0.2551,  0.2881,  0.2615,  0.2590,
         0.2590,  0.2590,  0.2585,  0.2588,  0.2585,  0.2693,  0.2588,  0.2588,
         0.2588,  0.2588,  0.2598,  0.2539,  0.2588,  0.2585,  0.2583,  0.2695,
         0.2588,  0.2622,  0.2482,  0.2585,  0.2661,  0.2590,  0.2588,  0.2585,
         0.2869,  0.2703,  0.2588,  0.3071,  0.2612,  0.2588,  0.3013,  0.2590,
         0.2529,  0.2644,  0.2595,  0.2585,  0.2588,  0.2588,  0.2651,  0.2581,
         0.2588,  0.2922,  0.2803,  0.2905,  0.2891,  0.3262,  0.2588,  0.2585,
         0.2854,  0.2588,  0.2998,  0.2585,  0.2590,  0.2588,  0.2590,  0.2600,
         0.2588,  0.2585,  0.2686,  0.2588,  0.2839,  0.2771,  0.2715,  0.2590,
         0.2588,  0.2585,  0.2588,  0.2588,  0.2830,  0.2585,  0.2795,  0.2561,
         0.2751,  0.2642,  0.2600,  0.2603,  0.2585,  0.2588,  0.2598,  0.2583,
         0.2585,  0.2656,  0.2585,  0.2585,  0.2944,  0.2588,  0.2988,  0.2661,
         0.2607,  0.2588,  0.2588,  0.2620,  0.2585,  0.2595,  0.3052,  0.2651,
         0.2932,  0.2588,  0.2749,  0.2585,  0.2585,  0.2588,  0.2576,  0.2588,
         0.2585,  0.2585,  0.2583,  0.2585,  0.2585,  0.2588,  0.2588,  0.1501,
         0.2791,  0.2585,  0.2230,  0.2585,  0.2622,  0.2590,  0.2588,  0.2588,
         0.2686,  0.2588,  0.2585,  0.2585,  0.2583,  0.2590,  0.2588,  0.2588,
         0.3218,  0.2588,  0.2551,  0.2639,  0.2590,  0.2539,  0.2588,  0.2585,
         0.2588,  0.2585,  0.2588,  0.2585,  0.2588,  0.2588,  0.2998,  0.2583,
         0.2588,  0.3018,  0.2588,  0.2588,  0.3440,  0.2590,  0.2593,  0.2588,
         0.2585,  0.2659,  0.2585,  0.2585,  0.2585,  0.2588,  0.2588,  0.2703,
         0.2335,  0.2832,  0.2600,  0.2585,  0.3125,  0.0696,  0.2976,  0.2588,
         0.2690,  0.3291,  0.2629,  0.2588,  0.3032,  0.2588,  0.2585,  0.2607,
         0.2588,  0.2556,  0.2698,  0.2864,  0.2590,  0.2593,  0.3137,  0.2585,
         0.2590,  0.2585,  0.2588,  0.2588,  0.2588,  0.2585,  0.2585,  0.2585,
         0.2014,  0.2588,  0.2642,  0.2588,  0.2986,  0.2971,  0.2585,  0.2585,
         0.2585,  0.2637,  0.2590,  0.2585,  0.3298,  0.2585,  0.1396,  0.2646,
         0.3064,  0.2588,  0.2585,  0.2546,  0.2585,  0.2588,  0.2585,  0.2585,
         0.2585,  0.2585,  0.2588,  0.2629,  0.1215,  0.2585,  0.2590,  0.2585,
         0.2954,  0.2585,  0.2585,  0.2585,  0.2588,  0.2585,  0.2598,  0.2976,
         0.2585,  0.2673,  0.2820,  0.3123,  0.2588,  0.2861,  0.2585,  0.2612,
         0.2688,  0.2590,  0.2588,  0.2588,  0.2585,  0.2588,  0.2588,  0.2588,
         0.2585,  0.3086,  0.2585,  0.2588,  0.2979,  0.2585,  0.2573,  0.2588,
         0.2588,  0.2583,  0.3013,  0.2588,  0.2605,  0.2588,  0.2791,  0.2568,
         0.2588,  0.2351,  0.2585,  0.2590,  0.2715,  0.2578,  0.2908,  0.2878,
         0.2588,  0.2588,  0.2615,  0.2585,  0.1281,  0.2566,  0.2590,  0.2585,
         0.2415,  0.2590,  0.2588,  0.2632,  0.2588,  0.2585,  0.2588,  0.2585,
         0.2588,  0.2590,  0.2678,  0.3342,  0.2416,  0.2588,  0.2588,  0.2927,
         0.2551,  0.2588,  0.2671,  0.2883,  0.2585,  0.2751,  0.2585,  0.2590,
         0.2590,  0.2585,  0.2808,  0.2588,  0.3347,  0.2563,  0.2585,  0.1512,
         0.2769,  0.2620,  0.2588,  0.2585,  0.2588,  0.2593,  0.2839,  0.2610,
         0.2588,  0.2588,  0.2590,  0.2673,  0.2585,  0.2585,  0.2568,  0.2588,
         0.2632,  0.2598,  0.3064,  0.2588,  0.2585,  0.2585,  0.2588,  0.2588,
         0.2588,  0.2585,  0.2646,  0.2590,  0.2585,  0.2544,  0.2585,  0.3193,
         0.3042,  0.2585,  0.2585,  0.2588,  0.2583,  0.2588,  0.2585,  0.2588,
         0.2588,  0.2600,  0.2585,  0.2588,  0.2559,  0.2654,  0.2603,  0.2588,
         0.2588,  0.2590,  0.3083,  0.3066,  0.2998,  0.3140,  0.2590,  0.2595,
         0.2595,  0.2585,  0.3110,  0.2590,  0.3423,  0.2585,  0.2825,  0.2585,
         0.2583,  0.2588,  0.2588,  0.2590,  0.2585,  0.2527,  0.2588,  0.2585,
         0.2585,  0.2615,  0.2607,  0.2590,  0.2585,  0.2710,  0.2279,  0.2585,
         0.2585,  0.2585,  0.2585,  0.2585,  0.2585,  0.2593,  0.2588,  0.2588,
         0.2568,  0.2251,  0.2539,  0.2593,  0.3010,  0.2588,  0.2590,  0.2588,
         0.2585,  0.2590,  0.2593,  0.2585,  0.2377,  0.2705,  0.2588,  0.2810,
         0.2585,  0.2683,  0.2585,  0.2590,  0.1606,  0.2588,  0.2585,  0.2588,
         0.2620,  0.2585,  0.2588,  0.2766,  0.3057,  0.2588,  0.2590,  0.2727,
         0.2585,  0.2585,  0.2808,  0.2585,  0.2590,  0.2585,  0.2590,  0.2588,
         0.2588,  0.2666,  0.2585,  0.2585,  0.3196,  0.2593,  0.2585,  0.3242,
         0.2426,  0.2571,  0.3066,  0.2588,  0.3015,  0.2590,  0.3267,  0.2583,
        -0.0117,  0.2588,  0.2046,  0.2590,  0.2588,  0.2590,  0.2588,  0.2727,
         0.2585,  0.2585,  0.2585,  0.2632,  0.2588,  0.2585,  0.2590,  0.2585],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(3171.6101, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7754, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6521.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8047, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6201.4497, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9414, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:37:42,436][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
loss: tensor(6258.0200, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7363, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:42:21,008][train_inner][INFO] - {"epoch": 16, "update": 15.8, "loss": "4.106", "ntokens": "149926", "nsentences": "539.86", "prob_perplexity": "108.901", "code_perplexity": "107.489", "temp": "1.921", "loss_0": "3.971", "loss_1": "0.12", "loss_2": "0.016", "accuracy": "0.31395", "wps": "37668.7", "ups": "0.25", "wpb": "149926", "bsz": "539.9", "num_updates": "8200", "lr": "0.000128125", "gnorm": "0.557", "loss_scale": "0.5", "train_wall": "795", "gb_free": "12.7", "wall": "33007"}
[2023-09-12 08:49:08,394][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 08:49:08,396][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 08:49:08,515][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2023-09-12 08:49:32,037][valid][INFO] - {"epoch": 16, "valid_loss": "3.832", "valid_ntokens": "7920.9", "valid_nsentences": "55.2525", "valid_prob_perplexity": "99.187", "valid_code_perplexity": "97.391", "valid_temp": "1.919", "valid_loss_0": "3.694", "valid_loss_1": "0.122", "valid_loss_2": "0.017", "valid_accuracy": "0.37542", "valid_wps": "33543", "valid_wpb": "7920.9", "valid_bsz": "55.3", "valid_num_updates": "8304", "valid_best_loss": "3.832"}
[2023-09-12 08:49:32,038][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 8304 updates
[2023-09-12 08:49:32,040][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 08:49:34,531][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 08:49:35,899][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 8304 updates, score 3.832) (writing took 3.860148726031184 seconds)
[2023-09-12 08:49:35,899][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2023-09-12 08:49:35,900][train][INFO] - {"epoch": 16, "train_loss": "4.114", "train_ntokens": "149504", "train_nsentences": "538.385", "train_prob_perplexity": "108.107", "train_code_perplexity": "106.716", "train_temp": "1.921", "train_loss_0": "3.978", "train_loss_1": "0.12", "train_loss_2": "0.016", "train_accuracy": "0.3132", "train_wps": "37201.5", "train_ups": "0.25", "train_wpb": "149504", "train_bsz": "538.4", "train_num_updates": "8304", "train_lr": "0.00012975", "train_gnorm": "0.559", "train_loss_scale": "0.5", "train_train_wall": "2054", "train_gb_free": "13.2", "train_wall": "33442"}
[2023-09-12 08:49:35,904][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 08:49:36,004][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2023-09-12 08:49:36,247][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 08:49:36,251][fairseq.trainer][INFO] - begin training epoch 17
[2023-09-12 08:49:36,251][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([0.1427, 0.2288, 0.1968, 0.1381, 0.1788, 0.2272, 0.2273, 0.2386, 0.2277,
        0.2277, 0.2274, 0.2666, 0.2419, 0.2273, 0.2395, 0.2820, 0.2275, 0.2274,
        0.2280, 0.2273, 0.2273, 0.2484, 0.2274, 0.2502, 0.2291, 0.2277, 0.2275,
        0.2273, 0.2275, 0.2450, 0.2336, 0.2277, 0.2262, 0.2273, 0.2274, 0.2273,
        0.2373, 0.2537, 0.2275, 0.2284, 0.2273, 0.2277, 0.2273, 0.2271, 0.2273,
        0.2695, 0.2273, 0.2273, 0.2274, 0.2279, 0.2273, 0.2510, 0.2275, 0.2272,
        0.2273, 0.2269, 0.2583, 0.2234, 0.2273, 0.2271, 0.2274, 0.2274, 0.2039,
        0.2340, 0.3181, 0.2676, 0.2278, 0.2278, 0.2273, 0.2273, 0.2273, 0.2274,
        0.2273, 0.2279, 0.2272, 0.2275, 0.2273, 0.2493, 0.2273, 0.2612, 0.2273,
        0.2274, 0.2379, 0.2274, 0.2590, 0.2573, 0.2273, 0.2273, 0.2532, 0.2274,
        0.2272, 0.2654, 0.2277, 0.2273, 0.2273, 0.2275, 0.2517, 0.2283, 0.2781,
        0.2273, 0.2275, 0.2274, 0.2360, 0.2303, 0.2281, 0.2272, 0.2274, 0.2277,
        0.2432, 0.2854, 0.2277, 0.2275, 0.2234, 0.2554, 0.2273, 0.2273, 0.2272,
        0.2311, 0.2269, 0.2800, 0.2273, 0.2273, 0.2372, 0.2349, 0.2419, 0.2277,
        0.2551, 0.2279, 0.2617, 0.2273, 0.2273, 0.2273, 0.2239, 0.2273, 0.2269,
        0.2834, 0.2308, 0.2358, 0.2267, 0.2368, 0.2271, 0.2217, 0.2278, 0.2274,
        0.2272, 0.2275, 0.3008, 0.2319, 0.2275, 0.2275, 0.2104, 0.2306, 0.2278,
        0.2281, 0.2832, 0.2273, 0.2286, 0.2273, 0.2271, 0.2272, 0.2273, 0.2274,
        0.2274, 0.2275, 0.2356, 0.2422, 0.2285, 0.2274, 0.2275, 0.2275, 0.2273,
        0.2273, 0.2272, 0.2313, 0.2273, 0.2274, 0.2273, 0.2274, 0.2283, 0.2225,
        0.2274, 0.2273, 0.2269, 0.2365, 0.2275, 0.2307, 0.2462, 0.2273, 0.2301,
        0.2277, 0.2275, 0.2273, 0.2356, 0.2352, 0.2273, 0.2576, 0.2297, 0.2274,
        0.2534, 0.2277, 0.2216, 0.2289, 0.2281, 0.2273, 0.2275, 0.2274, 0.2328,
        0.2267, 0.2274, 0.2476, 0.2462, 0.2510, 0.2446, 0.2751, 0.2274, 0.2273,
        0.2427, 0.2274, 0.2849, 0.2273, 0.2275, 0.2275, 0.2275, 0.2281, 0.2273,
        0.2273, 0.2734, 0.2273, 0.2411, 0.2396, 0.2368, 0.2277, 0.2274, 0.2273,
        0.2274, 0.2275, 0.2408, 0.2273, 0.2347, 0.2246, 0.2311, 0.2321, 0.2285,
        0.2272, 0.2273, 0.2273, 0.2283, 0.2268, 0.2273, 0.2297, 0.2273, 0.2273,
        0.3044, 0.2273, 0.2593, 0.2312, 0.2294, 0.2273, 0.2275, 0.2306, 0.2272,
        0.2278, 0.2529, 0.2336, 0.2465, 0.2273, 0.2346, 0.2273, 0.2273, 0.2275,
        0.2262, 0.2274, 0.2273, 0.2271, 0.2269, 0.2272, 0.2273, 0.2273, 0.2274,
        0.1027, 0.2413, 0.2272, 0.2568, 0.2273, 0.2278, 0.2278, 0.2273, 0.2274,
        0.2302, 0.2274, 0.2272, 0.2273, 0.2269, 0.2275, 0.2273, 0.2274, 0.2812,
        0.2273, 0.2239, 0.2288, 0.2277, 0.2225, 0.2273, 0.2272, 0.2273, 0.2273,
        0.2275, 0.2273, 0.2274, 0.2274, 0.2534, 0.2269, 0.2274, 0.2612, 0.2273,
        0.2275, 0.2942, 0.2277, 0.2275, 0.2274, 0.2272, 0.2317, 0.2272, 0.2272,
        0.2273, 0.2275, 0.2274, 0.2389, 0.2456, 0.2378, 0.2288, 0.2273, 0.2651,
        0.0357, 0.2522, 0.2274, 0.2313, 0.2786, 0.2291, 0.2274, 0.2715, 0.2274,
        0.2273, 0.2294, 0.2273, 0.2242, 0.2310, 0.2505, 0.2277, 0.2273, 0.2632,
        0.2273, 0.2278, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2272,
        0.1990, 0.2274, 0.2289, 0.2274, 0.2612, 0.2559, 0.2273, 0.2273, 0.2272,
        0.2307, 0.2274, 0.2273, 0.2805, 0.2273, 0.1088, 0.2274, 0.2585, 0.2273,
        0.2273, 0.2233, 0.2273, 0.2275, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2261, 0.1541, 0.2273, 0.2277, 0.2273, 0.2559, 0.2273, 0.2273, 0.2273,
        0.2274, 0.2273, 0.2277, 0.2510, 0.2273, 0.2358, 0.2360, 0.2749, 0.2273,
        0.2810, 0.2272, 0.2275, 0.2329, 0.2275, 0.2273, 0.2273, 0.2273, 0.2273,
        0.2275, 0.2274, 0.2273, 0.2634, 0.2273, 0.2273, 0.2505, 0.2272, 0.2260,
        0.2274, 0.2275, 0.2269, 0.2603, 0.2273, 0.2290, 0.2273, 0.2401, 0.2256,
        0.2274, 0.2302, 0.2273, 0.2277, 0.2344, 0.2264, 0.2542, 0.2496, 0.2275,
        0.2273, 0.2296, 0.2272, 0.1312, 0.2517, 0.2277, 0.2273, 0.2047, 0.2275,
        0.2273, 0.2317, 0.2274, 0.2273, 0.2274, 0.2273, 0.2273, 0.2275, 0.2307,
        0.2852, 0.2413, 0.2275, 0.2274, 0.2474, 0.2236, 0.2274, 0.2289, 0.2422,
        0.2273, 0.2405, 0.2272, 0.2275, 0.2275, 0.2273, 0.2377, 0.2274, 0.2878,
        0.2250, 0.2272, 0.1847, 0.2360, 0.2256, 0.2274, 0.2273, 0.2273, 0.2275,
        0.2432, 0.2295, 0.2274, 0.2273, 0.2275, 0.2305, 0.2273, 0.2273, 0.2253,
        0.2274, 0.2296, 0.2283, 0.2607, 0.2273, 0.2273, 0.2273, 0.2275, 0.2273,
        0.2273, 0.2273, 0.2333, 0.2275, 0.2273, 0.2231, 0.2273, 0.2810, 0.2563,
        0.2273, 0.2273, 0.2274, 0.2271, 0.2274, 0.2273, 0.2273, 0.2273, 0.2274,
        0.2273, 0.2273, 0.2245, 0.2338, 0.2278, 0.2274, 0.2274, 0.2275, 0.2605,
        0.2646, 0.2500, 0.2634, 0.2275, 0.2281, 0.2278, 0.2273, 0.2615, 0.2277,
        0.2993, 0.2273, 0.2445, 0.2273, 0.2268, 0.2273, 0.2274, 0.2275, 0.2273,
        0.2213, 0.2274, 0.2273, 0.2273, 0.2300, 0.2279, 0.2277, 0.2272, 0.2316,
        0.2070, 0.2273, 0.2273, 0.2272, 0.2272, 0.2272, 0.2273, 0.2279, 0.2274,
        0.2275, 0.2255, 0.1876, 0.2224, 0.2278, 0.2749, 0.2273, 0.2277, 0.2273,
        0.2273, 0.2275, 0.2274, 0.2273, 0.2517, 0.2329, 0.2273, 0.2496, 0.2273,
        0.2306, 0.2273, 0.2277, 0.1865, 0.2274, 0.2273, 0.2273, 0.2285, 0.2273,
        0.2273, 0.2358, 0.2578, 0.2274, 0.2277, 0.2415, 0.2271, 0.2273, 0.2671,
        0.2273, 0.2275, 0.2273, 0.2275, 0.2275, 0.2275, 0.2285, 0.2273, 0.2273,
        0.2751, 0.2277, 0.2273, 0.2808, 0.2549, 0.2258, 0.2751, 0.2275, 0.2595,
        0.2277, 0.2747, 0.2269, 0.0020, 0.2274, 0.2366, 0.2277, 0.2273, 0.2275,
        0.2273, 0.2290, 0.2273, 0.2273, 0.2273, 0.2319, 0.2274, 0.2273, 0.2277,
        0.2273], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6384.1152, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5742, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7779.5190, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5254, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7973.8208, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7109, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5965.4268, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9121, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6655.4526, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7871, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7396.8125, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9414, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8079.6641, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9023, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5704.0112, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9277, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8273.8916, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0234, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7848.4785, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7617, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5847.9346, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8184, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3880.2981, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0391, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5727.5498, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8672, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5596.0537, device='cuda:2')
loss_ent_max: tensor(-3.5645, device='cuda:2', dtype=torch.float16)
loss: tensor(5567.7710, device='cuda:2')
loss_ent_max: tensor(-3.5918, device='cuda:2', dtype=torch.float16)
loss: tensor(8405.3018, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(7343.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0273, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5819.4023, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0625, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6626.2109, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0195, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 08:55:55,238][train_inner][INFO] - {"epoch": 17, "update": 16.184, "loss": "4.095", "ntokens": "149067", "nsentences": "534.67", "prob_perplexity": "111.355", "code_perplexity": "109.857", "temp": "1.919", "loss_0": "3.959", "loss_1": "0.119", "loss_2": "0.017", "accuracy": "0.31482", "wps": "36615.4", "ups": "0.25", "wpb": "149067", "bsz": "534.7", "num_updates": "8400", "lr": "0.00013125", "gnorm": "0.545", "loss_scale": "1", "train_wall": "785", "gb_free": "12.6", "wall": "33822"}
loss: tensor(4674.1743, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0039, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6373.4492, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1211, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 09:08:46,573][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2023-09-12 09:09:13,138][train_inner][INFO] - {"epoch": 17, "update": 16.57, "loss": "4.07", "ntokens": "149848", "nsentences": "539.62", "prob_perplexity": "114.647", "code_perplexity": "113.043", "temp": "1.917", "loss_0": "3.935", "loss_1": "0.118", "loss_2": "0.017", "accuracy": "0.31701", "wps": "37560.7", "ups": "0.25", "wpb": "149848", "bsz": "539.6", "num_updates": "8600", "lr": "0.000134375", "gnorm": "0.564", "loss_scale": "0.5", "train_wall": "797", "gb_free": "12.8", "wall": "34620"}
loss: tensor(7530.6572, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2500, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5321.8052, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0469, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0970, 0.3005, 0.2343, 0.2310, 0.1613, 0.2991, 0.2991, 0.3362, 0.2993,
        0.2993, 0.2991, 0.3455, 0.3381, 0.2991, 0.3447, 0.3962, 0.2993, 0.2993,
        0.3247, 0.2991, 0.2991, 0.3491, 0.2991, 0.3564, 0.3044, 0.2993, 0.2993,
        0.2991, 0.2993, 0.2585, 0.3157, 0.2993, 0.2979, 0.2991, 0.2991, 0.2991,
        0.3430, 0.3640, 0.2993, 0.3071, 0.2991, 0.2993, 0.2991, 0.2988, 0.2991,
        0.3025, 0.2991, 0.2991, 0.2991, 0.2996, 0.2991, 0.3496, 0.2993, 0.2988,
        0.2991, 0.2986, 0.3677, 0.2952, 0.2991, 0.2988, 0.2991, 0.2998, 0.1870,
        0.3086, 0.3579, 0.3586, 0.3030, 0.2996, 0.2991, 0.2991, 0.2991, 0.2991,
        0.2991, 0.3096, 0.2988, 0.2993, 0.2991, 0.2812, 0.2991, 0.3455, 0.2991,
        0.2991, 0.3098, 0.2991, 0.3308, 0.3232, 0.2991, 0.2991, 0.3354, 0.2996,
        0.3020, 0.2491, 0.2993, 0.2991, 0.2991, 0.2993, 0.2539, 0.3074, 0.3196,
        0.2991, 0.2993, 0.2991, 0.3330, 0.3218, 0.3069, 0.2991, 0.2991, 0.2993,
        0.3599, 0.3413, 0.2993, 0.2993, 0.2952, 0.3625, 0.2991, 0.2991, 0.2991,
        0.3035, 0.2986, 0.3892, 0.2991, 0.2991, 0.3130, 0.3389, 0.3533, 0.2993,
        0.3525, 0.3042, 0.3718, 0.2991, 0.2991, 0.2991, 0.2957, 0.2991, 0.2986,
        0.2847, 0.3152, 0.3281, 0.2983, 0.3225, 0.2988, 0.2935, 0.2996, 0.2991,
        0.2988, 0.2993, 0.4128, 0.3115, 0.2993, 0.2993, 0.2319, 0.3154, 0.2996,
        0.2998, 0.3660, 0.2991, 0.3018, 0.3008, 0.2988, 0.2991, 0.3018, 0.3000,
        0.2991, 0.2993, 0.2839, 0.3474, 0.3066, 0.3000, 0.2993, 0.2993, 0.2991,
        0.2991, 0.2988, 0.3225, 0.2991, 0.2991, 0.2991, 0.3010, 0.3008, 0.2944,
        0.2991, 0.2991, 0.2986, 0.3157, 0.2993, 0.3027, 0.2355, 0.2991, 0.3174,
        0.2993, 0.2993, 0.2991, 0.3533, 0.3220, 0.2991, 0.3762, 0.3015, 0.2991,
        0.3635, 0.2996, 0.2932, 0.3193, 0.2998, 0.2991, 0.2993, 0.2991, 0.3062,
        0.2983, 0.2993, 0.3572, 0.3296, 0.3469, 0.3481, 0.3889, 0.2991, 0.2991,
        0.3477, 0.2991, 0.2678, 0.2991, 0.2993, 0.2993, 0.2993, 0.3025, 0.2991,
        0.2991, 0.2583, 0.2991, 0.3491, 0.3340, 0.3225, 0.2993, 0.2991, 0.2991,
        0.2991, 0.2993, 0.3420, 0.2991, 0.3464, 0.2964, 0.3428, 0.3066, 0.3003,
        0.3057, 0.2991, 0.2991, 0.3000, 0.2986, 0.2991, 0.3137, 0.2991, 0.2991,
        0.2515, 0.2991, 0.3562, 0.3188, 0.3010, 0.2991, 0.2993, 0.3022, 0.2988,
        0.3005, 0.3591, 0.3071, 0.3572, 0.2991, 0.3318, 0.2991, 0.2991, 0.2993,
        0.2979, 0.2991, 0.2991, 0.2988, 0.2986, 0.2991, 0.2991, 0.2991, 0.2991,
        0.2169, 0.3347, 0.2991, 0.1749, 0.2991, 0.3108, 0.2996, 0.2991, 0.2993,
        0.3157, 0.2991, 0.2991, 0.2991, 0.2988, 0.2993, 0.2991, 0.2993, 0.3643,
        0.2991, 0.2957, 0.3154, 0.2993, 0.2942, 0.2991, 0.2988, 0.2991, 0.2991,
        0.2993, 0.2991, 0.2991, 0.2991, 0.3396, 0.2986, 0.2991, 0.3625, 0.2991,
        0.2993, 0.4082, 0.2993, 0.2998, 0.2991, 0.2988, 0.3110, 0.2991, 0.2991,
        0.2991, 0.2993, 0.2991, 0.3105, 0.1942, 0.3477, 0.3005, 0.2991, 0.3779,
        0.1174, 0.3628, 0.2991, 0.3228, 0.3872, 0.3105, 0.2991, 0.3000, 0.2991,
        0.2991, 0.3022, 0.2991, 0.2961, 0.3298, 0.3379, 0.2993, 0.3054, 0.3794,
        0.2991, 0.2996, 0.2991, 0.2991, 0.2991, 0.2991, 0.2991, 0.2991, 0.2991,
        0.2417, 0.2991, 0.3125, 0.2991, 0.3506, 0.3533, 0.2991, 0.2991, 0.2991,
        0.3213, 0.3010, 0.2991, 0.3948, 0.2991, 0.1627, 0.3225, 0.3699, 0.2991,
        0.2991, 0.2952, 0.2991, 0.2993, 0.2991, 0.2991, 0.2991, 0.2991, 0.2991,
        0.3149, 0.0903, 0.2991, 0.2996, 0.2991, 0.3462, 0.2991, 0.2991, 0.2991,
        0.2991, 0.2991, 0.3040, 0.2898, 0.2991, 0.3076, 0.3484, 0.3037, 0.2991,
        0.2490, 0.2991, 0.3079, 0.3186, 0.2993, 0.2991, 0.2991, 0.2991, 0.2991,
        0.2993, 0.2991, 0.2991, 0.3738, 0.2991, 0.2991, 0.3604, 0.2991, 0.2979,
        0.2991, 0.2993, 0.2986, 0.3572, 0.2991, 0.3008, 0.2991, 0.3330, 0.2974,
        0.2991, 0.2247, 0.2991, 0.2993, 0.3247, 0.2983, 0.3049, 0.3396, 0.2993,
        0.2991, 0.3032, 0.2993, 0.1390, 0.2314, 0.2993, 0.2991, 0.2900, 0.2993,
        0.2991, 0.3035, 0.2991, 0.2991, 0.2991, 0.2991, 0.2991, 0.2993, 0.3247,
        0.4016, 0.2698, 0.2993, 0.2991, 0.3584, 0.2954, 0.2991, 0.3206, 0.3508,
        0.2991, 0.3186, 0.2988, 0.2993, 0.2993, 0.2991, 0.3430, 0.2991, 0.3965,
        0.2966, 0.2991, 0.1111, 0.3340, 0.3147, 0.2991, 0.2991, 0.2991, 0.3020,
        0.3472, 0.3013, 0.2991, 0.2991, 0.3003, 0.3196, 0.2991, 0.2991, 0.2971,
        0.2993, 0.3066, 0.3003, 0.3699, 0.2991, 0.2991, 0.2991, 0.2993, 0.2991,
        0.2991, 0.2991, 0.3049, 0.2993, 0.2991, 0.2949, 0.2991, 0.3008, 0.3259,
        0.2991, 0.2991, 0.2993, 0.2988, 0.2991, 0.2991, 0.2991, 0.2991, 0.3110,
        0.2991, 0.2991, 0.2961, 0.3071, 0.3027, 0.2993, 0.2991, 0.2993, 0.3733,
        0.3684, 0.3496, 0.3806, 0.2993, 0.2998, 0.3003, 0.2991, 0.3772, 0.2993,
        0.3486, 0.2991, 0.3337, 0.2991, 0.2991, 0.2991, 0.2991, 0.2993, 0.2991,
        0.2930, 0.2993, 0.2991, 0.2991, 0.3018, 0.3059, 0.2993, 0.2991, 0.3293,
        0.2688, 0.2991, 0.2991, 0.2991, 0.2991, 0.2991, 0.2991, 0.2996, 0.2991,
        0.2993, 0.2974, 0.2805, 0.2944, 0.3020, 0.3040, 0.2991, 0.2996, 0.2991,
        0.2991, 0.2993, 0.3022, 0.2991, 0.2256, 0.3252, 0.2991, 0.3213, 0.2991,
        0.3284, 0.2991, 0.2993, 0.1268, 0.2993, 0.2991, 0.2998, 0.3145, 0.2991,
        0.2991, 0.3350, 0.3701, 0.2991, 0.2993, 0.3132, 0.2991, 0.2991, 0.2695,
        0.2991, 0.2993, 0.2991, 0.2993, 0.2993, 0.2993, 0.3257, 0.2991, 0.2991,
        0.3828, 0.3000, 0.2991, 0.3723, 0.2273, 0.2976, 0.2915, 0.2993, 0.3555,
        0.2996, 0.3608, 0.2991, 0.0086, 0.2991, 0.1678, 0.2993, 0.2991, 0.3013,
        0.2991, 0.3354, 0.2998, 0.2991, 0.2991, 0.3037, 0.2991, 0.2991, 0.2993,
        0.2991], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(6337.7817, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3125, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5566.9951, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1172, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 09:22:25,654][train_inner][INFO] - {"epoch": 17, "update": 16.954, "loss": "4.052", "ntokens": "149553", "nsentences": "539.555", "prob_perplexity": "117.536", "code_perplexity": "115.845", "temp": "1.915", "loss_0": "3.917", "loss_1": "0.118", "loss_2": "0.017", "accuracy": "0.31884", "wps": "37741.3", "ups": "0.25", "wpb": "149553", "bsz": "539.6", "num_updates": "8800", "lr": "0.0001375", "gnorm": "0.552", "loss_scale": "0.5", "train_wall": "791", "gb_free": "12.9", "wall": "35412"}
tensor(-2.1562, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7608.3125, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4531, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7162.2622, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1172, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6363.8721, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4805, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7244.8066, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6699, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6387.7222, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0039, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6744.7871, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9609, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5336.1113, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5820, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6982.5996, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9980, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7256.2925, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6895, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5053.9053, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9863, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6890.3403, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8633, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6820.5195, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2422, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7658.6880, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5566, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5197.3447, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7656, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5757.1836, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7266, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7272.0498, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6816, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5812.0859, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7188, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7867.5903, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8887, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6766.7622, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7148, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7002.8564, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6895, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7281.3125, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6016, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5649.5874, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7383, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.1126,  0.2644,  0.1978,  0.1895,  0.1482,  0.2629,  0.2629,  0.2842,
         0.2634,  0.2634,  0.2632,  0.3215,  0.2869,  0.2629,  0.2888,  0.3367,
         0.2634,  0.2632,  0.2744,  0.2629,  0.2629,  0.3005,  0.2632,  0.3013,
         0.2659,  0.2634,  0.2632,  0.2632,  0.2632,  0.2467,  0.2725,  0.2634,
         0.2620,  0.2629,  0.2632,  0.2632,  0.2876,  0.3062,  0.2632,  0.2666,
         0.2629,  0.2634,  0.2629,  0.2627,  0.2629,  0.2800,  0.2629,  0.2629,
         0.2632,  0.2637,  0.2629,  0.2969,  0.2632,  0.2629,  0.2629,  0.2627,
         0.3115,  0.2590,  0.2629,  0.2629,  0.2632,  0.2634,  0.1886,  0.2703,
         0.3445,  0.3220,  0.2642,  0.2634,  0.2629,  0.2629,  0.2629,  0.2632,
         0.2629,  0.2664,  0.2629,  0.2632,  0.2629,  0.3035,  0.2629,  0.3013,
         0.2629,  0.2632,  0.2737,  0.2632,  0.2949,  0.3064,  0.2629,  0.2629,
         0.2932,  0.2632,  0.2632,  0.2725,  0.2634,  0.2632,  0.2629,  0.2632,
         0.2605,  0.2659,  0.3020,  0.2629,  0.2634,  0.2632,  0.2812,  0.2729,
         0.2659,  0.2629,  0.2632,  0.2634,  0.2981,  0.3228,  0.2634,  0.2632,
         0.2593,  0.3074,  0.2629,  0.2629,  0.2629,  0.2668,  0.2627,  0.3330,
         0.2629,  0.2629,  0.2739,  0.2822,  0.2957,  0.2634,  0.3015,  0.2649,
         0.3152,  0.2629,  0.2629,  0.2629,  0.2595,  0.2629,  0.2627,  0.3064,
         0.2712,  0.2808,  0.2625,  0.2791,  0.2627,  0.2573,  0.2634,  0.2632,
         0.2629,  0.2632,  0.3533,  0.2720,  0.2634,  0.2632,  0.2135,  0.2703,
         0.2634,  0.2639,  0.3379,  0.2629,  0.2654,  0.2632,  0.2629,  0.2629,
         0.2632,  0.2632,  0.2632,  0.2632,  0.2605,  0.2944,  0.2661,  0.2634,
         0.2634,  0.2634,  0.2629,  0.2629,  0.2629,  0.2739,  0.2629,  0.2632,
         0.2629,  0.2632,  0.2642,  0.2583,  0.2632,  0.2629,  0.2627,  0.2742,
         0.2632,  0.2666,  0.2476,  0.2629,  0.2712,  0.2634,  0.2632,  0.2629,
         0.2935,  0.2754,  0.2629,  0.3137,  0.2654,  0.2632,  0.3083,  0.2634,
         0.2573,  0.2698,  0.2639,  0.2629,  0.2632,  0.2632,  0.2695,  0.2625,
         0.2632,  0.2988,  0.2849,  0.2971,  0.2952,  0.3335,  0.2632,  0.2629,
         0.2917,  0.2632,  0.2983,  0.2629,  0.2632,  0.2632,  0.2632,  0.2646,
         0.2629,  0.2629,  0.2676,  0.2629,  0.2905,  0.2832,  0.2769,  0.2634,
         0.2632,  0.2629,  0.2632,  0.2632,  0.2893,  0.2629,  0.2861,  0.2603,
         0.2822,  0.2686,  0.2642,  0.2651,  0.2629,  0.2629,  0.2639,  0.2627,
         0.2629,  0.2708,  0.2629,  0.2629,  0.2908,  0.2632,  0.3047,  0.2712,
         0.2651,  0.2629,  0.2632,  0.2664,  0.2629,  0.2639,  0.3125,  0.2695,
         0.2993,  0.2632,  0.2803,  0.2629,  0.2629,  0.2632,  0.2620,  0.2632,
         0.2629,  0.2629,  0.2627,  0.2629,  0.2629,  0.2629,  0.2632,  0.1572,
         0.2852,  0.2629,  0.2179,  0.2629,  0.2671,  0.2634,  0.2629,  0.2632,
         0.2737,  0.2632,  0.2629,  0.2629,  0.2627,  0.2632,  0.2629,  0.2632,
         0.3276,  0.2629,  0.2595,  0.2688,  0.2634,  0.2583,  0.2629,  0.2629,
         0.2632,  0.2629,  0.2632,  0.2629,  0.2632,  0.2632,  0.3059,  0.2627,
         0.2632,  0.3081,  0.2629,  0.2632,  0.3511,  0.2634,  0.2634,  0.2632,
         0.2629,  0.2710,  0.2629,  0.2629,  0.2629,  0.2632,  0.2632,  0.2747,
         0.2301,  0.2898,  0.2644,  0.2629,  0.3186,  0.0768,  0.3044,  0.2632,
         0.2742,  0.3362,  0.2678,  0.2632,  0.3064,  0.2632,  0.2629,  0.2651,
         0.2632,  0.2600,  0.2759,  0.2915,  0.2634,  0.2637,  0.3208,  0.2629,
         0.2634,  0.2629,  0.2629,  0.2629,  0.2629,  0.2629,  0.2629,  0.2629,
         0.2042,  0.2632,  0.2695,  0.2632,  0.3037,  0.3032,  0.2629,  0.2629,
         0.2629,  0.2690,  0.2634,  0.2629,  0.3364,  0.2629,  0.1451,  0.2705,
         0.3130,  0.2629,  0.2629,  0.2590,  0.2629,  0.2632,  0.2629,  0.2629,
         0.2629,  0.2629,  0.2632,  0.2681,  0.1174,  0.2629,  0.2634,  0.2629,
         0.3005,  0.2629,  0.2629,  0.2629,  0.2632,  0.2629,  0.2644,  0.3005,
         0.2629,  0.2715,  0.2886,  0.3127,  0.2629,  0.2827,  0.2629,  0.2661,
         0.2739,  0.2634,  0.2632,  0.2629,  0.2629,  0.2629,  0.2632,  0.2632,
         0.2629,  0.3154,  0.2629,  0.2629,  0.3042,  0.2629,  0.2617,  0.2632,
         0.2632,  0.2627,  0.3066,  0.2629,  0.2649,  0.2629,  0.2847,  0.2612,
         0.2632,  0.2340,  0.2629,  0.2634,  0.2769,  0.2622,  0.2935,  0.2932,
         0.2632,  0.2632,  0.2659,  0.2629,  0.1300,  0.2551,  0.2634,  0.2629,
         0.2466,  0.2634,  0.2629,  0.2676,  0.2632,  0.2629,  0.2632,  0.2629,
         0.2632,  0.2634,  0.2732,  0.3416,  0.2434,  0.2632,  0.2632,  0.2996,
         0.2593,  0.2632,  0.2727,  0.2949,  0.2629,  0.2800,  0.2629,  0.2632,
         0.2632,  0.2629,  0.2869,  0.2632,  0.3411,  0.2607,  0.2629,  0.1461,
         0.2825,  0.2673,  0.2632,  0.2629,  0.2629,  0.2637,  0.2903,  0.2654,
         0.2632,  0.2632,  0.2634,  0.2729,  0.2629,  0.2629,  0.2612,  0.2632,
         0.2681,  0.2642,  0.3135,  0.2629,  0.2629,  0.2629,  0.2632,  0.2629,
         0.2629,  0.2629,  0.2690,  0.2632,  0.2629,  0.2588,  0.2629,  0.3242,
         0.3113,  0.2629,  0.2629,  0.2632,  0.2627,  0.2632,  0.2629,  0.2629,
         0.2629,  0.2646,  0.2629,  0.2629,  0.2603,  0.2698,  0.2649,  0.2632,
         0.2632,  0.2634,  0.3147,  0.3130,  0.3074,  0.3210,  0.2634,  0.2639,
         0.2639,  0.2629,  0.3174,  0.2634,  0.3430,  0.2629,  0.2881,  0.2629,
         0.2625,  0.2632,  0.2632,  0.2632,  0.2629,  0.2571,  0.2632,  0.2629,
         0.2629,  0.2659,  0.2656,  0.2634,  0.2629,  0.2771,  0.2339,  0.2629,
         0.2629,  0.2629,  0.2629,  0.2629,  0.2629,  0.2637,  0.2632,  0.2632,
         0.2612,  0.2311,  0.2583,  0.2639,  0.3037,  0.2629,  0.2634,  0.2632,
         0.2629,  0.2634,  0.2637,  0.2629,  0.2382,  0.2761,  0.2629,  0.2854,
         0.2629,  0.2739,  0.2629,  0.2634,  0.1564,  0.2632,  0.2629,  0.2632,
         0.2668,  0.2629,  0.2629,  0.2820,  0.3118,  0.2632,  0.2634,  0.2771,
         0.2629,  0.2629,  0.2798,  0.2629,  0.2632,  0.2629,  0.2634,  0.2632,
         0.2632,  0.2727,  0.2629,  0.2629,  0.3259,  0.2637,  0.2629,  0.3298,
         0.2404,  0.2615,  0.3062,  0.2632,  0.3074,  0.2634,  0.3333,  0.2627,
        -0.0116,  0.2632,  0.1995,  0.2634,  0.2629,  0.2634,  0.2629,  0.2793,
         0.2629,  0.2629,  0.2629,  0.2676,  0.2632,  0.2629,  0.2634,  0.2629],
       device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7808.5576, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8066, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7099.5654, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8438, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6786.7881, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6348, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4864.1533, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9512, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6512.7515, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0508, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7114.9189, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7812, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6854.0698, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0898, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3374.9849, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1406, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6326.3936, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1719, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0900, 0.3135, 0.2456, 0.2354, 0.1639, 0.3120, 0.3120, 0.3550, 0.3125,
        0.3125, 0.3123, 0.3523, 0.3584, 0.3120, 0.3665, 0.4185, 0.3125, 0.3123,
        0.3438, 0.3120, 0.3120, 0.3689, 0.3123, 0.3767, 0.3193, 0.3125, 0.3123,
        0.3123, 0.3125, 0.2703, 0.3330, 0.3125, 0.3110, 0.3120, 0.3123, 0.3123,
        0.3640, 0.3853, 0.3123, 0.3218, 0.3120, 0.3125, 0.3120, 0.3118, 0.3120,
        0.3157, 0.3120, 0.3120, 0.3123, 0.3127, 0.3120, 0.3706, 0.3123, 0.3120,
        0.3120, 0.3118, 0.3877, 0.3081, 0.3120, 0.3120, 0.3123, 0.3130, 0.1914,
        0.3232, 0.3623, 0.3562, 0.3179, 0.3125, 0.3123, 0.3120, 0.3120, 0.3123,
        0.3120, 0.3262, 0.3120, 0.3123, 0.3120, 0.2664, 0.3120, 0.3633, 0.3120,
        0.3123, 0.3232, 0.3123, 0.3440, 0.3198, 0.3120, 0.3120, 0.3518, 0.3127,
        0.3174, 0.2415, 0.3125, 0.3123, 0.3120, 0.3123, 0.2474, 0.3235, 0.3149,
        0.3120, 0.3125, 0.3123, 0.3521, 0.3401, 0.3223, 0.3120, 0.3123, 0.3125,
        0.3765, 0.3503, 0.3125, 0.3123, 0.3083, 0.3833, 0.3120, 0.3120, 0.3120,
        0.3174, 0.3118, 0.4062, 0.3120, 0.3120, 0.3279, 0.3596, 0.3738, 0.3125,
        0.3723, 0.3181, 0.3938, 0.3120, 0.3120, 0.3120, 0.3086, 0.3120, 0.3118,
        0.2761, 0.3323, 0.3462, 0.3115, 0.3386, 0.3118, 0.3064, 0.3125, 0.3123,
        0.3120, 0.3123, 0.4351, 0.3262, 0.3123, 0.3123, 0.2385, 0.3333, 0.3125,
        0.3130, 0.3726, 0.3120, 0.3152, 0.3147, 0.3120, 0.3120, 0.3167, 0.3135,
        0.3123, 0.3123, 0.2842, 0.3662, 0.3218, 0.3137, 0.3125, 0.3125, 0.3120,
        0.3120, 0.3120, 0.3418, 0.3120, 0.3123, 0.3120, 0.3159, 0.3145, 0.3074,
        0.3123, 0.3120, 0.3118, 0.3328, 0.3123, 0.3159, 0.2277, 0.3120, 0.3340,
        0.3125, 0.3123, 0.3120, 0.3745, 0.3403, 0.3120, 0.3984, 0.3145, 0.3123,
        0.3784, 0.3125, 0.3064, 0.3386, 0.3130, 0.3120, 0.3123, 0.3123, 0.3198,
        0.3115, 0.3123, 0.3799, 0.3477, 0.3655, 0.3691, 0.4082, 0.3123, 0.3120,
        0.3687, 0.3123, 0.2546, 0.3120, 0.3123, 0.3123, 0.3123, 0.3176, 0.3120,
        0.3120, 0.2524, 0.3120, 0.3655, 0.3547, 0.3401, 0.3125, 0.3123, 0.3120,
        0.3123, 0.3123, 0.3630, 0.3120, 0.3691, 0.3093, 0.3657, 0.3210, 0.3132,
        0.3213, 0.3120, 0.3120, 0.3130, 0.3115, 0.3120, 0.3296, 0.3120, 0.3120,
        0.2371, 0.3120, 0.3755, 0.3394, 0.3142, 0.3120, 0.3123, 0.3154, 0.3120,
        0.3137, 0.3718, 0.3220, 0.3787, 0.3123, 0.3528, 0.3120, 0.3120, 0.3123,
        0.3110, 0.3123, 0.3120, 0.3118, 0.3118, 0.3120, 0.3120, 0.3120, 0.3123,
        0.2368, 0.3542, 0.3120, 0.1586, 0.3120, 0.3274, 0.3125, 0.3120, 0.3123,
        0.3328, 0.3123, 0.3120, 0.3120, 0.3118, 0.3123, 0.3120, 0.3123, 0.3706,
        0.3120, 0.3086, 0.3335, 0.3125, 0.3074, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3123, 0.3120, 0.3123, 0.3123, 0.3372, 0.3118, 0.3123, 0.3833, 0.3120,
        0.3123, 0.4307, 0.3125, 0.3130, 0.3123, 0.3120, 0.3259, 0.3120, 0.3120,
        0.3120, 0.3123, 0.3123, 0.3237, 0.1805, 0.3687, 0.3135, 0.3120, 0.4001,
        0.1279, 0.3855, 0.3123, 0.3394, 0.4021, 0.3271, 0.3123, 0.2893, 0.3123,
        0.3120, 0.3164, 0.3123, 0.3091, 0.3528, 0.3550, 0.3125, 0.3225, 0.3989,
        0.3120, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.2542, 0.3123, 0.3296, 0.3123, 0.3699, 0.3721, 0.3120, 0.3120, 0.3120,
        0.3418, 0.3149, 0.3120, 0.4167, 0.3120, 0.1625, 0.3433, 0.3911, 0.3120,
        0.3120, 0.3081, 0.3120, 0.3123, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3323, 0.0854, 0.3120, 0.3127, 0.3120, 0.3635, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3120, 0.3193, 0.2795, 0.3120, 0.3206, 0.3701, 0.2976, 0.3120,
        0.2389, 0.3120, 0.3237, 0.3350, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3123, 0.3120, 0.3933, 0.3120, 0.3120, 0.3799, 0.3120, 0.3110,
        0.3123, 0.3123, 0.3118, 0.3755, 0.3120, 0.3140, 0.3120, 0.3518, 0.3103,
        0.3123, 0.2186, 0.3120, 0.3125, 0.3442, 0.3113, 0.3000, 0.3562, 0.3123,
        0.3120, 0.3169, 0.3125, 0.1344, 0.2224, 0.3125, 0.3120, 0.3069, 0.3125,
        0.3120, 0.3167, 0.3123, 0.3120, 0.3123, 0.3120, 0.3123, 0.3125, 0.3442,
        0.4238, 0.2856, 0.3123, 0.3123, 0.3816, 0.3083, 0.3123, 0.3391, 0.3726,
        0.3120, 0.3320, 0.3120, 0.3123, 0.3123, 0.3120, 0.3640, 0.3123, 0.4182,
        0.3098, 0.3120, 0.1025, 0.3547, 0.3342, 0.3123, 0.3120, 0.3120, 0.3171,
        0.3687, 0.3142, 0.3123, 0.3123, 0.3147, 0.3374, 0.3120, 0.3120, 0.3103,
        0.3123, 0.3213, 0.3135, 0.3867, 0.3120, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3120, 0.3120, 0.3181, 0.3123, 0.3120, 0.3079, 0.3120, 0.2871, 0.3218,
        0.3120, 0.3120, 0.3123, 0.3118, 0.3123, 0.3120, 0.3120, 0.3120, 0.3296,
        0.3120, 0.3120, 0.3093, 0.3213, 0.3169, 0.3123, 0.3123, 0.3125, 0.3960,
        0.3889, 0.3630, 0.3999, 0.3125, 0.3130, 0.3135, 0.3120, 0.3989, 0.3125,
        0.3472, 0.3120, 0.3513, 0.3120, 0.3132, 0.3123, 0.3123, 0.3123, 0.3120,
        0.3062, 0.3123, 0.3120, 0.3120, 0.3149, 0.3220, 0.3125, 0.3120, 0.3489,
        0.2751, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3127, 0.3123,
        0.3123, 0.3103, 0.2993, 0.3076, 0.3164, 0.2937, 0.3120, 0.3127, 0.3123,
        0.3120, 0.3123, 0.3169, 0.3120, 0.2244, 0.3440, 0.3120, 0.3342, 0.3120,
        0.3506, 0.3120, 0.3125, 0.1175, 0.3123, 0.3120, 0.3130, 0.3340, 0.3120,
        0.3120, 0.3569, 0.3918, 0.3123, 0.3125, 0.3262, 0.3120, 0.3120, 0.2646,
        0.3120, 0.3123, 0.3120, 0.3125, 0.3123, 0.3123, 0.3457, 0.3120, 0.3120,
        0.4028, 0.3132, 0.3120, 0.3723, 0.2212, 0.3105, 0.2800, 0.3123, 0.3745,
        0.3125, 0.3564, 0.3125, 0.0183, 0.3123, 0.1609, 0.3125, 0.3120, 0.3154,
        0.3120, 0.3574, 0.3137, 0.3120, 0.3120, 0.3167, 0.3123, 0.3120, 0.3125,
        0.3120], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 09:23:58,548][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 09:23:58,549][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 09:23:58,692][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2023-09-12 09:24:22,493][valid][INFO] - {"epoch": 17, "valid_loss": "3.85", "valid_ntokens": "7904.06", "valid_nsentences": "55.2525", "valid_prob_perplexity": "97.643", "valid_code_perplexity": "95.65", "valid_temp": "1.914", "valid_loss_0": "3.71", "valid_loss_1": "0.122", "valid_loss_2": "0.018", "valid_accuracy": "0.37658", "valid_wps": "33044.9", "valid_wpb": "7904.1", "valid_bsz": "55.3", "valid_num_updates": "8824", "valid_best_loss": "3.832"}
[2023-09-12 09:24:22,495][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 8824 updates
[2023-09-12 09:24:22,496][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_last.pt
[2023-09-12 09:24:25,096][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_last.pt
[2023-09-12 09:24:25,162][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 17 @ 8824 updates, score 3.85) (writing took 2.667058176943101 seconds)
[2023-09-12 09:24:25,162][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2023-09-12 09:24:25,163][train][INFO] - {"epoch": 17, "train_loss": "4.067", "train_ntokens": "149451", "train_nsentences": "538.487", "train_prob_perplexity": "115.397", "train_code_perplexity": "113.771", "train_temp": "1.916", "train_loss_0": "3.932", "train_loss_1": "0.118", "train_loss_2": "0.017", "train_accuracy": "0.3173", "train_wps": "37197", "train_ups": "0.25", "train_wpb": "149451", "train_bsz": "538.5", "train_num_updates": "8824", "train_lr": "0.000137875", "train_gnorm": "0.557", "train_loss_scale": "0.5", "train_train_wall": "2059", "train_gb_free": "13.4", "train_wall": "35532"}
[2023-09-12 09:24:25,165][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 09:24:25,288][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2023-09-12 09:24:25,570][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 09:24:25,574][fairseq.trainer][INFO] - begin training epoch 18
[2023-09-12 09:24:25,574][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(7931.6118, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1055, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7743.1362, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 09:36:06,790][train_inner][INFO] - {"epoch": 18, "update": 17.338, "loss": "4.046", "ntokens": "149148", "nsentences": "538.065", "prob_perplexity": "119.515", "code_perplexity": "117.753", "temp": "1.913", "loss_0": "3.911", "loss_1": "0.117", "loss_2": "0.018", "accuracy": "0.3191", "wps": "36327.2", "ups": "0.24", "wpb": "149148", "bsz": "538.1", "num_updates": "9000", "lr": "0.000140625", "gnorm": "0.54", "loss_scale": "1", "train_wall": "793", "gb_free": "12.9", "wall": "36233"}
tensor(-3.1348, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6604.6260, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0898, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6910.7900, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6138.3496, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9824, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6431.4355, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4258, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6645.1572, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0840, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6143.5815, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3340, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4524.9390, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3477, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6898.0010, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3., device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5580.0493, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8281, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5645.9834, device='cuda:1')
loss_ent_max: tensor(-2.8262, device='cuda:1', dtype=torch.float16)
loss: tensor(6388.0444, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6875, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7939.8169, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8574, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6060.3740, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7793, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8245.4863, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7246, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6827.0405, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8887, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6286.9219, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0195, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6991.6367, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9102, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6672.5547, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7383, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6659.9785, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0234, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7356.5142, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9727, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4903.3262, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0625, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7687.5801, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0391, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4135.0679, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2461, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3941.5085, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0195, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0900, 0.3135, 0.2456, 0.2354, 0.1639, 0.3120, 0.3120, 0.3550, 0.3125,
        0.3125, 0.3123, 0.3523, 0.3584, 0.3120, 0.3665, 0.4185, 0.3125, 0.3123,
        0.3438, 0.3120, 0.3120, 0.3689, 0.3123, 0.3767, 0.3193, 0.3125, 0.3123,
        0.3123, 0.3125, 0.2703, 0.3330, 0.3125, 0.3110, 0.3120, 0.3123, 0.3123,
        0.3640, 0.3853, 0.3123, 0.3218, 0.3120, 0.3125, 0.3120, 0.3118, 0.3120,
        0.3157, 0.3120, 0.3120, 0.3123, 0.3127, 0.3120, 0.3706, 0.3123, 0.3120,
        0.3120, 0.3118, 0.3877, 0.3081, 0.3120, 0.3120, 0.3123, 0.3130, 0.1914,
        0.3232, 0.3623, 0.3562, 0.3179, 0.3125, 0.3123, 0.3120, 0.3120, 0.3123,
        0.3120, 0.3262, 0.3120, 0.3123, 0.3120, 0.2664, 0.3120, 0.3633, 0.3120,
        0.3123, 0.3232, 0.3123, 0.3440, 0.3198, 0.3120, 0.3120, 0.3518, 0.3127,
        0.3174, 0.2415, 0.3125, 0.3123, 0.3120, 0.3123, 0.2474, 0.3235, 0.3149,
        0.3120, 0.3125, 0.3123, 0.3521, 0.3401, 0.3223, 0.3120, 0.3123, 0.3125,
        0.3765, 0.3503, 0.3125, 0.3123, 0.3083, 0.3833, 0.3120, 0.3120, 0.3120,
        0.3174, 0.3118, 0.4062, 0.3120, 0.3120, 0.3279, 0.3596, 0.3738, 0.3125,
        0.3723, 0.3181, 0.3938, 0.3120, 0.3120, 0.3120, 0.3086, 0.3120, 0.3118,
        0.2761, 0.3323, 0.3462, 0.3115, 0.3386, 0.3118, 0.3064, 0.3125, 0.3123,
        0.3120, 0.3123, 0.4351, 0.3262, 0.3123, 0.3123, 0.2385, 0.3333, 0.3125,
        0.3130, 0.3726, 0.3120, 0.3152, 0.3147, 0.3120, 0.3120, 0.3167, 0.3135,
        0.3123, 0.3123, 0.2842, 0.3662, 0.3218, 0.3137, 0.3125, 0.3125, 0.3120,
        0.3120, 0.3120, 0.3418, 0.3120, 0.3123, 0.3120, 0.3159, 0.3145, 0.3074,
        0.3123, 0.3120, 0.3118, 0.3328, 0.3123, 0.3159, 0.2277, 0.3120, 0.3340,
        0.3125, 0.3123, 0.3120, 0.3745, 0.3403, 0.3120, 0.3984, 0.3145, 0.3123,
        0.3784, 0.3125, 0.3064, 0.3386, 0.3130, 0.3120, 0.3123, 0.3123, 0.3198,
        0.3115, 0.3123, 0.3799, 0.3477, 0.3655, 0.3691, 0.4082, 0.3123, 0.3120,
        0.3687, 0.3123, 0.2546, 0.3120, 0.3123, 0.3123, 0.3123, 0.3176, 0.3120,
        0.3120, 0.2524, 0.3120, 0.3655, 0.3547, 0.3401, 0.3125, 0.3123, 0.3120,
        0.3123, 0.3123, 0.3630, 0.3120, 0.3691, 0.3093, 0.3657, 0.3210, 0.3132,
        0.3213, 0.3120, 0.3120, 0.3130, 0.3115, 0.3120, 0.3296, 0.3120, 0.3120,
        0.2371, 0.3120, 0.3755, 0.3394, 0.3142, 0.3120, 0.3123, 0.3154, 0.3120,
        0.3137, 0.3718, 0.3220, 0.3787, 0.3123, 0.3528, 0.3120, 0.3120, 0.3123,
        0.3110, 0.3123, 0.3120, 0.3118, 0.3118, 0.3120, 0.3120, 0.3120, 0.3123,
        0.2368, 0.3542, 0.3120, 0.1586, 0.3120, 0.3274, 0.3125, 0.3120, 0.3123,
        0.3328, 0.3123, 0.3120, 0.3120, 0.3118, 0.3123, 0.3120, 0.3123, 0.3706,
        0.3120, 0.3086, 0.3335, 0.3125, 0.3074, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3123, 0.3120, 0.3123, 0.3123, 0.3372, 0.3118, 0.3123, 0.3833, 0.3120,
        0.3123, 0.4307, 0.3125, 0.3130, 0.3123, 0.3120, 0.3259, 0.3120, 0.3120,
        0.3120, 0.3123, 0.3123, 0.3237, 0.1805, 0.3687, 0.3135, 0.3120, 0.4001,
        0.1279, 0.3855, 0.3123, 0.3394, 0.4021, 0.3271, 0.3123, 0.2893, 0.3123,
        0.3120, 0.3164, 0.3123, 0.3091, 0.3528, 0.3550, 0.3125, 0.3225, 0.3989,
        0.3120, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.2542, 0.3123, 0.3296, 0.3123, 0.3699, 0.3721, 0.3120, 0.3120, 0.3120,
        0.3418, 0.3149, 0.3120, 0.4167, 0.3120, 0.1625, 0.3433, 0.3911, 0.3120,
        0.3120, 0.3081, 0.3120, 0.3123, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3323, 0.0854, 0.3120, 0.3127, 0.3120, 0.3635, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3120, 0.3193, 0.2795, 0.3120, 0.3206, 0.3701, 0.2976, 0.3120,
        0.2389, 0.3120, 0.3237, 0.3350, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3123, 0.3120, 0.3933, 0.3120, 0.3120, 0.3799, 0.3120, 0.3110,
        0.3123, 0.3123, 0.3118, 0.3755, 0.3120, 0.3140, 0.3120, 0.3518, 0.3103,
        0.3123, 0.2186, 0.3120, 0.3125, 0.3442, 0.3113, 0.3000, 0.3562, 0.3123,
        0.3120, 0.3169, 0.3125, 0.1344, 0.2224, 0.3125, 0.3120, 0.3069, 0.3125,
        0.3120, 0.3167, 0.3123, 0.3120, 0.3123, 0.3120, 0.3123, 0.3125, 0.3442,
        0.4238, 0.2856, 0.3123, 0.3123, 0.3816, 0.3083, 0.3123, 0.3391, 0.3726,
        0.3120, 0.3320, 0.3120, 0.3123, 0.3123, 0.3120, 0.3640, 0.3123, 0.4182,
        0.3098, 0.3120, 0.1025, 0.3547, 0.3342, 0.3123, 0.3120, 0.3120, 0.3171,
        0.3687, 0.3142, 0.3123, 0.3123, 0.3147, 0.3374, 0.3120, 0.3120, 0.3103,
        0.3123, 0.3213, 0.3135, 0.3867, 0.3120, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3120, 0.3120, 0.3181, 0.3123, 0.3120, 0.3079, 0.3120, 0.2871, 0.3218,
        0.3120, 0.3120, 0.3123, 0.3118, 0.3123, 0.3120, 0.3120, 0.3120, 0.3296,
        0.3120, 0.3120, 0.3093, 0.3213, 0.3169, 0.3123, 0.3123, 0.3125, 0.3960,
        0.3889, 0.3630, 0.3999, 0.3125, 0.3130, 0.3135, 0.3120, 0.3989, 0.3125,
        0.3472, 0.3120, 0.3513, 0.3120, 0.3132, 0.3123, 0.3123, 0.3123, 0.3120,
        0.3062, 0.3123, 0.3120, 0.3120, 0.3149, 0.3220, 0.3125, 0.3120, 0.3489,
        0.2751, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3127, 0.3123,
        0.3123, 0.3103, 0.2993, 0.3076, 0.3164, 0.2937, 0.3120, 0.3127, 0.3123,
        0.3120, 0.3123, 0.3169, 0.3120, 0.2244, 0.3440, 0.3120, 0.3342, 0.3120,
        0.3506, 0.3120, 0.3125, 0.1175, 0.3123, 0.3120, 0.3130, 0.3340, 0.3120,
        0.3120, 0.3569, 0.3918, 0.3123, 0.3125, 0.3262, 0.3120, 0.3120, 0.2646,
        0.3120, 0.3123, 0.3120, 0.3125, 0.3123, 0.3123, 0.3457, 0.3120, 0.3120,
        0.4028, 0.3132, 0.3120, 0.3723, 0.2212, 0.3105, 0.2800, 0.3123, 0.3745,
        0.3125, 0.3564, 0.3125, 0.0183, 0.3123, 0.1609, 0.3125, 0.3120, 0.3154,
        0.3120, 0.3574, 0.3137, 0.3120, 0.3120, 0.3167, 0.3123, 0.3120, 0.3125,
        0.3120], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6012.6050, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1719, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7355.3867, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3633, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6365.1387, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1367, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6671.3120, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1992, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6929.7993, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3164, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6630.8013, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3789, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6084.8774, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2734, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5350.2656, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3242, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 09:43:59,294][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
tensor(-3.2324, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3824.4138, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8555, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6698.7383, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0273, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6749.9116, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0469, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4971.4907, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1055, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7195.6455, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0977, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0900, 0.3135, 0.2456, 0.2354, 0.1639, 0.3120, 0.3120, 0.3550, 0.3125,
        0.3125, 0.3123, 0.3523, 0.3584, 0.3120, 0.3665, 0.4185, 0.3125, 0.3123,
        0.3438, 0.3120, 0.3120, 0.3689, 0.3123, 0.3767, 0.3193, 0.3125, 0.3123,
        0.3123, 0.3125, 0.2703, 0.3330, 0.3125, 0.3110, 0.3120, 0.3123, 0.3123,
        0.3640, 0.3853, 0.3123, 0.3218, 0.3120, 0.3125, 0.3120, 0.3118, 0.3120,
        0.3157, 0.3120, 0.3120, 0.3123, 0.3127, 0.3120, 0.3706, 0.3123, 0.3120,
        0.3120, 0.3118, 0.3877, 0.3081, 0.3120, 0.3120, 0.3123, 0.3130, 0.1914,
        0.3232, 0.3623, 0.3562, 0.3179, 0.3125, 0.3123, 0.3120, 0.3120, 0.3123,
        0.3120, 0.3262, 0.3120, 0.3123, 0.3120, 0.2664, 0.3120, 0.3633, 0.3120,
        0.3123, 0.3232, 0.3123, 0.3440, 0.3198, 0.3120, 0.3120, 0.3518, 0.3127,
        0.3174, 0.2415, 0.3125, 0.3123, 0.3120, 0.3123, 0.2474, 0.3235, 0.3149,
        0.3120, 0.3125, 0.3123, 0.3521, 0.3401, 0.3223, 0.3120, 0.3123, 0.3125,
        0.3765, 0.3503, 0.3125, 0.3123, 0.3083, 0.3833, 0.3120, 0.3120, 0.3120,
        0.3174, 0.3118, 0.4062, 0.3120, 0.3120, 0.3279, 0.3596, 0.3738, 0.3125,
        0.3723, 0.3181, 0.3938, 0.3120, 0.3120, 0.3120, 0.3086, 0.3120, 0.3118,
        0.2761, 0.3323, 0.3462, 0.3115, 0.3386, 0.3118, 0.3064, 0.3125, 0.3123,
        0.3120, 0.3123, 0.4351, 0.3262, 0.3123, 0.3123, 0.2385, 0.3333, 0.3125,
        0.3130, 0.3726, 0.3120, 0.3152, 0.3147, 0.3120, 0.3120, 0.3167, 0.3135,
        0.3123, 0.3123, 0.2842, 0.3662, 0.3218, 0.3137, 0.3125, 0.3125, 0.3120,
        0.3120, 0.3120, 0.3418, 0.3120, 0.3123, 0.3120, 0.3159, 0.3145, 0.3074,
        0.3123, 0.3120, 0.3118, 0.3328, 0.3123, 0.3159, 0.2277, 0.3120, 0.3340,
        0.3125, 0.3123, 0.3120, 0.3745, 0.3403, 0.3120, 0.3984, 0.3145, 0.3123,
        0.3784, 0.3125, 0.3064, 0.3386, 0.3130, 0.3120, 0.3123, 0.3123, 0.3198,
        0.3115, 0.3123, 0.3799, 0.3477, 0.3655, 0.3691, 0.4082, 0.3123, 0.3120,
        0.3687, 0.3123, 0.2546, 0.3120, 0.3123, 0.3123, 0.3123, 0.3176, 0.3120,
        0.3120, 0.2524, 0.3120, 0.3655, 0.3547, 0.3401, 0.3125, 0.3123, 0.3120,
        0.3123, 0.3123, 0.3630, 0.3120, 0.3691, 0.3093, 0.3657, 0.3210, 0.3132,
        0.3213, 0.3120, 0.3120, 0.3130, 0.3115, 0.3120, 0.3296, 0.3120, 0.3120,
        0.2371, 0.3120, 0.3755, 0.3394, 0.3142, 0.3120, 0.3123, 0.3154, 0.3120,
        0.3137, 0.3718, 0.3220, 0.3787, 0.3123, 0.3528, 0.3120, 0.3120, 0.3123,
        0.3110, 0.3123, 0.3120, 0.3118, 0.3118, 0.3120, 0.3120, 0.3120, 0.3123,
        0.2368, 0.3542, 0.3120, 0.1586, 0.3120, 0.3274, 0.3125, 0.3120, 0.3123,
        0.3328, 0.3123, 0.3120, 0.3120, 0.3118, 0.3123, 0.3120, 0.3123, 0.3706,
        0.3120, 0.3086, 0.3335, 0.3125, 0.3074, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3123, 0.3120, 0.3123, 0.3123, 0.3372, 0.3118, 0.3123, 0.3833, 0.3120,
        0.3123, 0.4307, 0.3125, 0.3130, 0.3123, 0.3120, 0.3259, 0.3120, 0.3120,
        0.3120, 0.3123, 0.3123, 0.3237, 0.1805, 0.3687, 0.3135, 0.3120, 0.4001,
        0.1279, 0.3855, 0.3123, 0.3394, 0.4021, 0.3271, 0.3123, 0.2893, 0.3123,
        0.3120, 0.3164, 0.3123, 0.3091, 0.3528, 0.3550, 0.3125, 0.3225, 0.3989,
        0.3120, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.2542, 0.3123, 0.3296, 0.3123, 0.3699, 0.3721, 0.3120, 0.3120, 0.3120,
        0.3418, 0.3149, 0.3120, 0.4167, 0.3120, 0.1625, 0.3433, 0.3911, 0.3120,
        0.3120, 0.3081, 0.3120, 0.3123, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3323, 0.0854, 0.3120, 0.3127, 0.3120, 0.3635, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3120, 0.3193, 0.2795, 0.3120, 0.3206, 0.3701, 0.2976, 0.3120,
        0.2389, 0.3120, 0.3237, 0.3350, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3123, 0.3120, 0.3933, 0.3120, 0.3120, 0.3799, 0.3120, 0.3110,
        0.3123, 0.3123, 0.3118, 0.3755, 0.3120, 0.3140, 0.3120, 0.3518, 0.3103,
        0.3123, 0.2186, 0.3120, 0.3125, 0.3442, 0.3113, 0.3000, 0.3562, 0.3123,
        0.3120, 0.3169, 0.3125, 0.1344, 0.2224, 0.3125, 0.3120, 0.3069, 0.3125,
        0.3120, 0.3167, 0.3123, 0.3120, 0.3123, 0.3120, 0.3123, 0.3125, 0.3442,
        0.4238, 0.2856, 0.3123, 0.3123, 0.3816, 0.3083, 0.3123, 0.3391, 0.3726,
        0.3120, 0.3320, 0.3120, 0.3123, 0.3123, 0.3120, 0.3640, 0.3123, 0.4182,
        0.3098, 0.3120, 0.1025, 0.3547, 0.3342, 0.3123, 0.3120, 0.3120, 0.3171,
        0.3687, 0.3142, 0.3123, 0.3123, 0.3147, 0.3374, 0.3120, 0.3120, 0.3103,
        0.3123, 0.3213, 0.3135, 0.3867, 0.3120, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3120, 0.3120, 0.3181, 0.3123, 0.3120, 0.3079, 0.3120, 0.2871, 0.3218,
        0.3120, 0.3120, 0.3123, 0.3118, 0.3123, 0.3120, 0.3120, 0.3120, 0.3296,
        0.3120, 0.3120, 0.3093, 0.3213, 0.3169, 0.3123, 0.3123, 0.3125, 0.3960,
        0.3889, 0.3630, 0.3999, 0.3125, 0.3130, 0.3135, 0.3120, 0.3989, 0.3125,
        0.3472, 0.3120, 0.3513, 0.3120, 0.3132, 0.3123, 0.3123, 0.3123, 0.3120,
        0.3062, 0.3123, 0.3120, 0.3120, 0.3149, 0.3220, 0.3125, 0.3120, 0.3489,
        0.2751, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3127, 0.3123,
        0.3123, 0.3103, 0.2993, 0.3076, 0.3164, 0.2937, 0.3120, 0.3127, 0.3123,
        0.3120, 0.3123, 0.3169, 0.3120, 0.2244, 0.3440, 0.3120, 0.3342, 0.3120,
        0.3506, 0.3120, 0.3125, 0.1175, 0.3123, 0.3120, 0.3130, 0.3340, 0.3120,
        0.3120, 0.3569, 0.3918, 0.3123, 0.3125, 0.3262, 0.3120, 0.3120, 0.2646,
        0.3120, 0.3123, 0.3120, 0.3125, 0.3123, 0.3123, 0.3457, 0.3120, 0.3120,
        0.4028, 0.3132, 0.3120, 0.3723, 0.2212, 0.3105, 0.2800, 0.3123, 0.3745,
        0.3125, 0.3564, 0.3125, 0.0183, 0.3123, 0.1609, 0.3125, 0.3120, 0.3154,
        0.3120, 0.3574, 0.3137, 0.3120, 0.3120, 0.3167, 0.3123, 0.3120, 0.3125,
        0.3120], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6817.9834, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4531, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6755.4585, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3984, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5876.4058, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2891, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6926.7212, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1133, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8105.5322, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2930, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6514.5493, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3711, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6372.7871, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3984, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 09:49:28,215][train_inner][INFO] - {"epoch": 18, "update": 17.724, "loss": "4.019", "ntokens": "149525", "nsentences": "539.325", "prob_perplexity": "122.933", "code_perplexity": "121.034", "temp": "1.911", "loss_0": "3.885", "loss_1": "0.117", "loss_2": "0.018", "accuracy": "0.32164", "wps": "37314.8", "ups": "0.25", "wpb": "149525", "bsz": "539.3", "num_updates": "9200", "lr": "0.00014375", "gnorm": "0.528", "loss_scale": "1", "train_wall": "800", "gb_free": "13", "wall": "37035"}
Parameter containing:
tensor([0.0854, 0.3308, 0.2522, 0.2375, 0.1663, 0.3293, 0.3293, 0.3777, 0.3298,
        0.3296, 0.3296, 0.3586, 0.3857, 0.3293, 0.3821, 0.4468, 0.3296, 0.3296,
        0.3704, 0.3293, 0.3293, 0.3899, 0.3296, 0.3855, 0.3396, 0.3296, 0.3296,
        0.3293, 0.3301, 0.2822, 0.3547, 0.3296, 0.3284, 0.3293, 0.3296, 0.3293,
        0.3918, 0.4128, 0.3296, 0.3423, 0.3293, 0.3298, 0.3293, 0.3291, 0.3293,
        0.3335, 0.3293, 0.3293, 0.3296, 0.3298, 0.3293, 0.3965, 0.3296, 0.3293,
        0.3293, 0.3291, 0.4143, 0.3254, 0.3293, 0.3291, 0.3296, 0.3306, 0.1979,
        0.3430, 0.3625, 0.3528, 0.3379, 0.3298, 0.3296, 0.3293, 0.3293, 0.3293,
        0.3293, 0.3479, 0.3293, 0.3296, 0.3293, 0.2476, 0.3293, 0.3875, 0.3293,
        0.3296, 0.3413, 0.3293, 0.3613, 0.3103, 0.3293, 0.3293, 0.3733, 0.3298,
        0.3386, 0.2311, 0.3298, 0.3293, 0.3298, 0.3296, 0.2391, 0.3442, 0.3037,
        0.3293, 0.3296, 0.3293, 0.3787, 0.3635, 0.3433, 0.3293, 0.3293, 0.3298,
        0.3823, 0.3501, 0.3296, 0.3296, 0.3254, 0.4094, 0.3293, 0.3293, 0.3293,
        0.3359, 0.3289, 0.4158, 0.3293, 0.3293, 0.3479, 0.3853, 0.4016, 0.3298,
        0.3989, 0.3376, 0.3989, 0.3293, 0.3296, 0.3293, 0.3259, 0.3293, 0.3291,
        0.2698, 0.3538, 0.3699, 0.3289, 0.3604, 0.3291, 0.3237, 0.3298, 0.3296,
        0.3293, 0.3296, 0.4626, 0.3452, 0.3296, 0.3296, 0.2450, 0.3579, 0.3298,
        0.3301, 0.3784, 0.3293, 0.3335, 0.3337, 0.3291, 0.3293, 0.3374, 0.3330,
        0.3293, 0.3296, 0.2832, 0.3887, 0.3425, 0.3320, 0.3296, 0.3301, 0.3293,
        0.3293, 0.3291, 0.3665, 0.3293, 0.3296, 0.3293, 0.3359, 0.3335, 0.3247,
        0.3296, 0.3293, 0.3289, 0.3550, 0.3296, 0.3337, 0.2136, 0.3293, 0.3569,
        0.3296, 0.3296, 0.3293, 0.3936, 0.3640, 0.3293, 0.4187, 0.3318, 0.3296,
        0.3735, 0.3298, 0.3237, 0.3652, 0.3301, 0.3293, 0.3296, 0.3296, 0.3381,
        0.3289, 0.3296, 0.4084, 0.3718, 0.3914, 0.3967, 0.4265, 0.3293, 0.3293,
        0.3960, 0.3293, 0.2366, 0.3293, 0.3296, 0.3296, 0.3296, 0.3376, 0.3293,
        0.3293, 0.2446, 0.3293, 0.3750, 0.3806, 0.3647, 0.3296, 0.3293, 0.3293,
        0.3293, 0.3296, 0.3892, 0.3293, 0.3926, 0.3267, 0.3882, 0.3406, 0.3306,
        0.3430, 0.3293, 0.3293, 0.3303, 0.3289, 0.3293, 0.3499, 0.3293, 0.3293,
        0.2197, 0.3293, 0.4016, 0.3667, 0.3315, 0.3293, 0.3296, 0.3328, 0.3291,
        0.3315, 0.3892, 0.3423, 0.4062, 0.3293, 0.3792, 0.3293, 0.3293, 0.3296,
        0.3284, 0.3293, 0.3293, 0.3291, 0.3289, 0.3293, 0.3293, 0.3293, 0.3293,
        0.2563, 0.3806, 0.3293, 0.1378, 0.3293, 0.3484, 0.3298, 0.3293, 0.3296,
        0.3464, 0.3296, 0.3293, 0.3293, 0.3291, 0.3296, 0.3293, 0.3298, 0.3699,
        0.3293, 0.3259, 0.3572, 0.3298, 0.3247, 0.3293, 0.3291, 0.3293, 0.3293,
        0.3296, 0.3293, 0.3293, 0.3296, 0.3301, 0.3289, 0.3296, 0.3943, 0.3293,
        0.3296, 0.4539, 0.3296, 0.3306, 0.3296, 0.3293, 0.3457, 0.3293, 0.3293,
        0.3293, 0.3296, 0.3296, 0.3411, 0.1627, 0.3940, 0.3308, 0.3293, 0.4153,
        0.1305, 0.4119, 0.3296, 0.3618, 0.4189, 0.3499, 0.3296, 0.2739, 0.3293,
        0.3293, 0.3357, 0.3293, 0.3264, 0.3826, 0.3779, 0.3296, 0.3455, 0.4272,
        0.3293, 0.3298, 0.3293, 0.3293, 0.3293, 0.3293, 0.3293, 0.3293, 0.3293,
        0.2644, 0.3293, 0.3530, 0.3296, 0.3955, 0.3955, 0.3293, 0.3293, 0.3293,
        0.3682, 0.3345, 0.3293, 0.4421, 0.3293, 0.1625, 0.3684, 0.4187, 0.3293,
        0.3293, 0.3254, 0.3293, 0.3296, 0.3293, 0.3293, 0.3293, 0.3293, 0.3293,
        0.3572, 0.0854, 0.3293, 0.3303, 0.3298, 0.3850, 0.3296, 0.3293, 0.3293,
        0.3296, 0.3293, 0.3401, 0.2681, 0.3293, 0.3379, 0.3984, 0.2900, 0.3293,
        0.2273, 0.3293, 0.3403, 0.3567, 0.3296, 0.3293, 0.3293, 0.3293, 0.3293,
        0.3296, 0.3293, 0.3293, 0.4211, 0.3293, 0.3293, 0.4060, 0.3293, 0.3298,
        0.3296, 0.3298, 0.3291, 0.3989, 0.3293, 0.3311, 0.3293, 0.3770, 0.3276,
        0.3296, 0.2124, 0.3293, 0.3296, 0.3691, 0.3286, 0.2927, 0.3679, 0.3296,
        0.3293, 0.3362, 0.3315, 0.1289, 0.2145, 0.3298, 0.3293, 0.3298, 0.3296,
        0.3293, 0.3342, 0.3296, 0.3293, 0.3293, 0.3293, 0.3293, 0.3296, 0.3704,
        0.4521, 0.3064, 0.3296, 0.3293, 0.4104, 0.3257, 0.3296, 0.3635, 0.4001,
        0.3293, 0.3506, 0.3293, 0.3296, 0.3296, 0.3293, 0.3918, 0.3293, 0.4456,
        0.3269, 0.3293, 0.0936, 0.3809, 0.3579, 0.3296, 0.3296, 0.3293, 0.3376,
        0.3855, 0.3315, 0.3293, 0.3296, 0.3347, 0.3611, 0.3293, 0.3293, 0.3274,
        0.3301, 0.3406, 0.3311, 0.4043, 0.3293, 0.3293, 0.3293, 0.3296, 0.3293,
        0.3293, 0.3293, 0.3354, 0.3296, 0.3293, 0.3252, 0.3293, 0.2710, 0.3142,
        0.3293, 0.3293, 0.3303, 0.3291, 0.3293, 0.3293, 0.3293, 0.3293, 0.3525,
        0.3293, 0.3293, 0.3267, 0.3413, 0.3359, 0.3298, 0.3296, 0.3296, 0.4197,
        0.4148, 0.3638, 0.4211, 0.3296, 0.3303, 0.3313, 0.3293, 0.4270, 0.3298,
        0.3467, 0.3293, 0.3726, 0.3293, 0.3323, 0.3293, 0.3293, 0.3296, 0.3293,
        0.3235, 0.3296, 0.3293, 0.3293, 0.3323, 0.3452, 0.3298, 0.3293, 0.3745,
        0.2759, 0.3293, 0.3293, 0.3293, 0.3293, 0.3293, 0.3293, 0.3298, 0.3296,
        0.3296, 0.3276, 0.3186, 0.3254, 0.3364, 0.2771, 0.3293, 0.3308, 0.3293,
        0.3293, 0.3296, 0.3369, 0.3293, 0.2236, 0.3667, 0.3293, 0.3516, 0.3293,
        0.3789, 0.3293, 0.3298, 0.1094, 0.3296, 0.3293, 0.3318, 0.3589, 0.3293,
        0.3293, 0.3833, 0.4187, 0.3296, 0.3296, 0.3435, 0.3296, 0.3293, 0.2603,
        0.3293, 0.3296, 0.3293, 0.3296, 0.3296, 0.3296, 0.3718, 0.3293, 0.3293,
        0.4297, 0.3313, 0.3293, 0.3704, 0.2107, 0.3279, 0.2629, 0.3296, 0.3979,
        0.3298, 0.3523, 0.3311, 0.0260, 0.3296, 0.1572, 0.3296, 0.3293, 0.3352,
        0.3293, 0.3811, 0.3333, 0.3293, 0.3296, 0.3340, 0.3293, 0.3293, 0.3298,
        0.3293], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(5084.5952, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2969, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6302.8857, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4453, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6842.3999, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4453, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(3961.8665, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4766, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6014.9136, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4258, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7223.8242, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4180, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 09:59:01,305][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 09:59:01,306][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 09:59:01,417][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2023-09-12 09:59:24,584][valid][INFO] - {"epoch": 18, "valid_loss": "3.825", "valid_ntokens": "7918.72", "valid_nsentences": "55.2525", "valid_prob_perplexity": "112.408", "valid_code_perplexity": "109.943", "valid_temp": "1.909", "valid_loss_0": "3.688", "valid_loss_1": "0.119", "valid_loss_2": "0.019", "valid_accuracy": "0.37441", "valid_wps": "34103.2", "valid_wpb": "7918.7", "valid_bsz": "55.3", "valid_num_updates": "9344", "valid_best_loss": "3.825"}
[2023-09-12 09:59:24,586][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 9344 updates
[2023-09-12 09:59:24,587][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 09:59:27,219][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 09:59:28,664][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 9344 updates, score 3.825) (writing took 4.078445975086652 seconds)
[2023-09-12 09:59:28,665][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2023-09-12 09:59:28,666][train][INFO] - {"epoch": 18, "train_loss": "4.024", "train_ntokens": "149409", "train_nsentences": "538.367", "train_prob_perplexity": "122.565", "train_code_perplexity": "120.679", "train_temp": "1.911", "train_loss_0": "3.889", "train_loss_1": "0.117", "train_loss_2": "0.018", "train_accuracy": "0.32121", "train_wps": "36935", "train_ups": "0.25", "train_wpb": "149410", "train_bsz": "538.4", "train_num_updates": "9344", "train_lr": "0.000146", "train_gnorm": "0.542", "train_loss_scale": "1", "train_train_wall": "2072", "train_gb_free": "13.2", "train_wall": "37635"}
[2023-09-12 09:59:28,669][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 09:59:28,770][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2023-09-12 09:59:29,011][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 09:59:29,014][fairseq.trainer][INFO] - begin training epoch 19
[2023-09-12 09:59:29,015][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 10:01:55,921][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2023-09-12 10:03:15,995][train_inner][INFO] - {"epoch": 19, "update": 18.109, "loss": "4.011", "ntokens": "149050", "nsentences": "535.79", "prob_perplexity": "125.253", "code_perplexity": "123.259", "temp": "1.909", "loss_0": "3.877", "loss_1": "0.116", "loss_2": "0.018", "accuracy": "0.32231", "wps": "36012", "ups": "0.24", "wpb": "149050", "bsz": "535.8", "num_updates": "9400", "lr": "0.000146875", "gnorm": "0.565", "loss_scale": "1", "train_wall": "799", "gb_free": "12.9", "wall": "37862"}
Parameter containing:
tensor([0.0900, 0.3135, 0.2456, 0.2354, 0.1639, 0.3120, 0.3120, 0.3550, 0.3125,
        0.3125, 0.3123, 0.3523, 0.3584, 0.3120, 0.3665, 0.4185, 0.3125, 0.3123,
        0.3438, 0.3120, 0.3120, 0.3689, 0.3123, 0.3767, 0.3193, 0.3125, 0.3123,
        0.3123, 0.3125, 0.2703, 0.3330, 0.3125, 0.3110, 0.3120, 0.3123, 0.3123,
        0.3640, 0.3853, 0.3123, 0.3218, 0.3120, 0.3125, 0.3120, 0.3118, 0.3120,
        0.3157, 0.3120, 0.3120, 0.3123, 0.3127, 0.3120, 0.3706, 0.3123, 0.3120,
        0.3120, 0.3118, 0.3877, 0.3081, 0.3120, 0.3120, 0.3123, 0.3130, 0.1914,
        0.3232, 0.3623, 0.3562, 0.3179, 0.3125, 0.3123, 0.3120, 0.3120, 0.3123,
        0.3120, 0.3262, 0.3120, 0.3123, 0.3120, 0.2664, 0.3120, 0.3633, 0.3120,
        0.3123, 0.3232, 0.3123, 0.3440, 0.3198, 0.3120, 0.3120, 0.3518, 0.3127,
        0.3174, 0.2415, 0.3125, 0.3123, 0.3120, 0.3123, 0.2474, 0.3235, 0.3149,
        0.3120, 0.3125, 0.3123, 0.3521, 0.3401, 0.3223, 0.3120, 0.3123, 0.3125,
        0.3765, 0.3503, 0.3125, 0.3123, 0.3083, 0.3833, 0.3120, 0.3120, 0.3120,
        0.3174, 0.3118, 0.4062, 0.3120, 0.3120, 0.3279, 0.3596, 0.3738, 0.3125,
        0.3723, 0.3181, 0.3938, 0.3120, 0.3120, 0.3120, 0.3086, 0.3120, 0.3118,
        0.2761, 0.3323, 0.3462, 0.3115, 0.3386, 0.3118, 0.3064, 0.3125, 0.3123,
        0.3120, 0.3123, 0.4351, 0.3262, 0.3123, 0.3123, 0.2385, 0.3333, 0.3125,
        0.3130, 0.3726, 0.3120, 0.3152, 0.3147, 0.3120, 0.3120, 0.3167, 0.3135,
        0.3123, 0.3123, 0.2842, 0.3662, 0.3218, 0.3137, 0.3125, 0.3125, 0.3120,
        0.3120, 0.3120, 0.3418, 0.3120, 0.3123, 0.3120, 0.3159, 0.3145, 0.3074,
        0.3123, 0.3120, 0.3118, 0.3328, 0.3123, 0.3159, 0.2277, 0.3120, 0.3340,
        0.3125, 0.3123, 0.3120, 0.3745, 0.3403, 0.3120, 0.3984, 0.3145, 0.3123,
        0.3784, 0.3125, 0.3064, 0.3386, 0.3130, 0.3120, 0.3123, 0.3123, 0.3198,
        0.3115, 0.3123, 0.3799, 0.3477, 0.3655, 0.3691, 0.4082, 0.3123, 0.3120,
        0.3687, 0.3123, 0.2546, 0.3120, 0.3123, 0.3123, 0.3123, 0.3176, 0.3120,
        0.3120, 0.2524, 0.3120, 0.3655, 0.3547, 0.3401, 0.3125, 0.3123, 0.3120,
        0.3123, 0.3123, 0.3630, 0.3120, 0.3691, 0.3093, 0.3657, 0.3210, 0.3132,
        0.3213, 0.3120, 0.3120, 0.3130, 0.3115, 0.3120, 0.3296, 0.3120, 0.3120,
        0.2371, 0.3120, 0.3755, 0.3394, 0.3142, 0.3120, 0.3123, 0.3154, 0.3120,
        0.3137, 0.3718, 0.3220, 0.3787, 0.3123, 0.3528, 0.3120, 0.3120, 0.3123,
        0.3110, 0.3123, 0.3120, 0.3118, 0.3118, 0.3120, 0.3120, 0.3120, 0.3123,
        0.2368, 0.3542, 0.3120, 0.1586, 0.3120, 0.3274, 0.3125, 0.3120, 0.3123,
        0.3328, 0.3123, 0.3120, 0.3120, 0.3118, 0.3123, 0.3120, 0.3123, 0.3706,
        0.3120, 0.3086, 0.3335, 0.3125, 0.3074, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3123, 0.3120, 0.3123, 0.3123, 0.3372, 0.3118, 0.3123, 0.3833, 0.3120,
        0.3123, 0.4307, 0.3125, 0.3130, 0.3123, 0.3120, 0.3259, 0.3120, 0.3120,
        0.3120, 0.3123, 0.3123, 0.3237, 0.1805, 0.3687, 0.3135, 0.3120, 0.4001,
        0.1279, 0.3855, 0.3123, 0.3394, 0.4021, 0.3271, 0.3123, 0.2893, 0.3123,
        0.3120, 0.3164, 0.3123, 0.3091, 0.3528, 0.3550, 0.3125, 0.3225, 0.3989,
        0.3120, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.2542, 0.3123, 0.3296, 0.3123, 0.3699, 0.3721, 0.3120, 0.3120, 0.3120,
        0.3418, 0.3149, 0.3120, 0.4167, 0.3120, 0.1625, 0.3433, 0.3911, 0.3120,
        0.3120, 0.3081, 0.3120, 0.3123, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3323, 0.0854, 0.3120, 0.3127, 0.3120, 0.3635, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3120, 0.3193, 0.2795, 0.3120, 0.3206, 0.3701, 0.2976, 0.3120,
        0.2389, 0.3120, 0.3237, 0.3350, 0.3125, 0.3120, 0.3120, 0.3120, 0.3120,
        0.3123, 0.3123, 0.3120, 0.3933, 0.3120, 0.3120, 0.3799, 0.3120, 0.3110,
        0.3123, 0.3123, 0.3118, 0.3755, 0.3120, 0.3140, 0.3120, 0.3518, 0.3103,
        0.3123, 0.2186, 0.3120, 0.3125, 0.3442, 0.3113, 0.3000, 0.3562, 0.3123,
        0.3120, 0.3169, 0.3125, 0.1344, 0.2224, 0.3125, 0.3120, 0.3069, 0.3125,
        0.3120, 0.3167, 0.3123, 0.3120, 0.3123, 0.3120, 0.3123, 0.3125, 0.3442,
        0.4238, 0.2856, 0.3123, 0.3123, 0.3816, 0.3083, 0.3123, 0.3391, 0.3726,
        0.3120, 0.3320, 0.3120, 0.3123, 0.3123, 0.3120, 0.3640, 0.3123, 0.4182,
        0.3098, 0.3120, 0.1025, 0.3547, 0.3342, 0.3123, 0.3120, 0.3120, 0.3171,
        0.3687, 0.3142, 0.3123, 0.3123, 0.3147, 0.3374, 0.3120, 0.3120, 0.3103,
        0.3123, 0.3213, 0.3135, 0.3867, 0.3120, 0.3120, 0.3120, 0.3123, 0.3120,
        0.3120, 0.3120, 0.3181, 0.3123, 0.3120, 0.3079, 0.3120, 0.2871, 0.3218,
        0.3120, 0.3120, 0.3123, 0.3118, 0.3123, 0.3120, 0.3120, 0.3120, 0.3296,
        0.3120, 0.3120, 0.3093, 0.3213, 0.3169, 0.3123, 0.3123, 0.3125, 0.3960,
        0.3889, 0.3630, 0.3999, 0.3125, 0.3130, 0.3135, 0.3120, 0.3989, 0.3125,
        0.3472, 0.3120, 0.3513, 0.3120, 0.3132, 0.3123, 0.3123, 0.3123, 0.3120,
        0.3062, 0.3123, 0.3120, 0.3120, 0.3149, 0.3220, 0.3125, 0.3120, 0.3489,
        0.2751, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3120, 0.3127, 0.3123,
        0.3123, 0.3103, 0.2993, 0.3076, 0.3164, 0.2937, 0.3120, 0.3127, 0.3123,
        0.3120, 0.3123, 0.3169, 0.3120, 0.2244, 0.3440, 0.3120, 0.3342, 0.3120,
        0.3506, 0.3120, 0.3125, 0.1175, 0.3123, 0.3120, 0.3130, 0.3340, 0.3120,
        0.3120, 0.3569, 0.3918, 0.3123, 0.3125, 0.3262, 0.3120, 0.3120, 0.2646,
        0.3120, 0.3123, 0.3120, 0.3125, 0.3123, 0.3123, 0.3457, 0.3120, 0.3120,
        0.4028, 0.3132, 0.3120, 0.3723, 0.2212, 0.3105, 0.2800, 0.3123, 0.3745,
        0.3125, 0.3564, 0.3125, 0.0183, 0.3123, 0.1609, 0.3125, 0.3120, 0.3154,
        0.3120, 0.3574, 0.3137, 0.3120, 0.3120, 0.3167, 0.3123, 0.3120, 0.3125,
        0.3120], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7439.4790, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3867, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6385.6123, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5352, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6683.6484, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3711, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6288.0464, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4492, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5703.9146, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3633, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5777.9399, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5117, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7748.9106, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3750, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6857.0068, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5938, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4402.2363, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4297, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7315.6870, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6289, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5474.3193, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6602, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 10:16:25,546][train_inner][INFO] - {"epoch": 19, "update": 18.493, "loss": "3.985", "ntokens": "149890", "nsentences": "537.555", "prob_perplexity": "129.523", "code_perplexity": "127.412", "temp": "1.907", "loss_0": "3.852", "loss_1": "0.115", "loss_2": "0.018", "accuracy": "0.32503", "wps": "37968.4", "ups": "0.25", "wpb": "149890", "bsz": "537.6", "num_updates": "9600", "lr": "0.00015", "gnorm": "0.519", "loss_scale": "1", "train_wall": "788", "gb_free": "12.7", "wall": "38652"}
loss: tensor(7921.9346, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4062, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 10:18:58,133][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(6619.8691, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3867, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0682, 0.3772, 0.2883, 0.2251, 0.1783, 0.3757, 0.3762, 0.4260, 0.3765,
        0.3760, 0.3760, 0.3645, 0.4568, 0.3757, 0.4604, 0.5220, 0.3760, 0.3760,
        0.4446, 0.3757, 0.3757, 0.4097, 0.3760, 0.3914, 0.3936, 0.3762, 0.3760,
        0.3762, 0.3813, 0.2954, 0.4231, 0.3762, 0.3748, 0.3757, 0.3760, 0.3757,
        0.4656, 0.4541, 0.3760, 0.3948, 0.3757, 0.3762, 0.3757, 0.3755, 0.3757,
        0.3367, 0.3757, 0.3757, 0.3760, 0.3765, 0.3757, 0.4695, 0.3760, 0.3757,
        0.3757, 0.3755, 0.4795, 0.3718, 0.3757, 0.3755, 0.3760, 0.3777, 0.2036,
        0.4016, 0.3674, 0.3308, 0.3928, 0.3762, 0.3767, 0.3757, 0.3757, 0.3760,
        0.3757, 0.4094, 0.3757, 0.3760, 0.3757, 0.1949, 0.3757, 0.4565, 0.3757,
        0.3760, 0.3936, 0.3760, 0.4089, 0.2771, 0.3757, 0.3757, 0.4326, 0.3787,
        0.3987, 0.1930, 0.3762, 0.3757, 0.3787, 0.3760, 0.2092, 0.3997, 0.2600,
        0.3757, 0.3760, 0.3760, 0.4336, 0.4297, 0.3999, 0.3760, 0.3760, 0.3762,
        0.3704, 0.3516, 0.3760, 0.3762, 0.3718, 0.4636, 0.3757, 0.3757, 0.3757,
        0.3906, 0.3755, 0.4001, 0.3757, 0.3757, 0.4058, 0.4253, 0.4753, 0.3762,
        0.4709, 0.3904, 0.4683, 0.3757, 0.3767, 0.3757, 0.3723, 0.3757, 0.3760,
        0.2502, 0.4175, 0.4316, 0.3752, 0.4189, 0.3755, 0.3704, 0.3762, 0.3760,
        0.3757, 0.3760, 0.5381, 0.3960, 0.3760, 0.3760, 0.2764, 0.4277, 0.3762,
        0.3765, 0.4019, 0.3760, 0.3857, 0.3877, 0.3755, 0.3757, 0.3972, 0.3911,
        0.3757, 0.3760, 0.2620, 0.4343, 0.3989, 0.3838, 0.3760, 0.3777, 0.3762,
        0.3757, 0.3757, 0.4417, 0.3757, 0.3760, 0.3757, 0.3953, 0.3870, 0.3711,
        0.3760, 0.3757, 0.3755, 0.4204, 0.3760, 0.3838, 0.1677, 0.3757, 0.4197,
        0.3762, 0.3760, 0.3770, 0.4167, 0.4294, 0.3757, 0.4692, 0.3784, 0.3760,
        0.3328, 0.3762, 0.3701, 0.4368, 0.3770, 0.3757, 0.3762, 0.3760, 0.3877,
        0.3752, 0.3760, 0.4207, 0.4407, 0.4624, 0.4727, 0.4890, 0.3760, 0.3757,
        0.4590, 0.3760, 0.1843, 0.3757, 0.3760, 0.3760, 0.3760, 0.3940, 0.3757,
        0.3757, 0.2106, 0.3757, 0.3552, 0.4563, 0.4336, 0.3760, 0.3760, 0.3757,
        0.3760, 0.3760, 0.4380, 0.3757, 0.4011, 0.3730, 0.4053, 0.3948, 0.3770,
        0.4038, 0.3757, 0.3757, 0.3767, 0.3757, 0.3757, 0.4060, 0.3770, 0.3757,
        0.1667, 0.3757, 0.4700, 0.4426, 0.3779, 0.3757, 0.3760, 0.3799, 0.3757,
        0.3799, 0.4380, 0.4016, 0.4836, 0.3757, 0.4529, 0.3757, 0.3760, 0.3760,
        0.3748, 0.3760, 0.3757, 0.3755, 0.3752, 0.3772, 0.3757, 0.3757, 0.3757,
        0.2712, 0.4546, 0.3757, 0.0839, 0.3757, 0.4092, 0.3762, 0.3757, 0.3760,
        0.3799, 0.3760, 0.3757, 0.3757, 0.3755, 0.3760, 0.3757, 0.3772, 0.3589,
        0.3760, 0.3723, 0.4233, 0.3762, 0.3711, 0.3757, 0.3757, 0.3757, 0.3757,
        0.3760, 0.3757, 0.3760, 0.3760, 0.2974, 0.3755, 0.3760, 0.4199, 0.3757,
        0.3760, 0.5127, 0.3762, 0.3787, 0.3762, 0.3757, 0.3965, 0.3757, 0.3757,
        0.3757, 0.3760, 0.3762, 0.3875, 0.1206, 0.4333, 0.3772, 0.3757, 0.4700,
        0.1213, 0.4785, 0.3760, 0.4224, 0.4890, 0.4128, 0.3760, 0.2260, 0.3760,
        0.3772, 0.3889, 0.3757, 0.3728, 0.4551, 0.4431, 0.3760, 0.4114, 0.4905,
        0.3757, 0.3762, 0.3757, 0.3757, 0.3760, 0.3757, 0.3757, 0.3767, 0.3757,
        0.2764, 0.3757, 0.4165, 0.3760, 0.4529, 0.4629, 0.3757, 0.3757, 0.3757,
        0.4380, 0.3882, 0.3757, 0.4939, 0.3757, 0.1495, 0.4385, 0.4570, 0.3757,
        0.3757, 0.3718, 0.3757, 0.3762, 0.3757, 0.3757, 0.3757, 0.3757, 0.3757,
        0.4277, 0.0834, 0.3757, 0.3777, 0.3794, 0.4458, 0.3760, 0.3757, 0.3757,
        0.3760, 0.3762, 0.4028, 0.2379, 0.3757, 0.3843, 0.4749, 0.2561, 0.3757,
        0.1885, 0.3757, 0.3965, 0.4167, 0.3772, 0.3757, 0.3757, 0.3757, 0.3757,
        0.3760, 0.3757, 0.3757, 0.4919, 0.3757, 0.3757, 0.4734, 0.3757, 0.3809,
        0.3760, 0.3735, 0.3755, 0.4609, 0.3760, 0.3774, 0.3757, 0.4451, 0.3743,
        0.3760, 0.2026, 0.3757, 0.3762, 0.4050, 0.3750, 0.2620, 0.4280, 0.3760,
        0.3757, 0.3889, 0.3828, 0.1182, 0.1818, 0.3765, 0.3770, 0.3958, 0.3762,
        0.3757, 0.3826, 0.3760, 0.3757, 0.3760, 0.3757, 0.3760, 0.3760, 0.4441,
        0.5107, 0.3418, 0.3760, 0.3760, 0.4751, 0.3721, 0.3760, 0.4280, 0.4739,
        0.3762, 0.4001, 0.3779, 0.3760, 0.3760, 0.3757, 0.4614, 0.3760, 0.5195,
        0.3735, 0.3757, 0.0757, 0.4546, 0.4287, 0.3760, 0.3777, 0.3757, 0.4014,
        0.4321, 0.3779, 0.3757, 0.3779, 0.3914, 0.4236, 0.3757, 0.3757, 0.3738,
        0.3796, 0.3928, 0.3816, 0.4397, 0.3757, 0.3757, 0.3757, 0.3760, 0.3757,
        0.3757, 0.3757, 0.3826, 0.3760, 0.3757, 0.3716, 0.3757, 0.2336, 0.2996,
        0.3757, 0.3757, 0.3804, 0.3755, 0.3757, 0.3757, 0.3757, 0.3757, 0.4304,
        0.3757, 0.3757, 0.3730, 0.4001, 0.3901, 0.3789, 0.3760, 0.3760, 0.4827,
        0.4907, 0.3403, 0.4307, 0.3760, 0.3767, 0.3828, 0.3757, 0.4995, 0.3762,
        0.3472, 0.3757, 0.4380, 0.3757, 0.3850, 0.3757, 0.3757, 0.3767, 0.3757,
        0.3704, 0.3760, 0.3757, 0.3757, 0.3804, 0.4150, 0.3762, 0.3757, 0.4456,
        0.2717, 0.3757, 0.3757, 0.3757, 0.3757, 0.3757, 0.3757, 0.3765, 0.3760,
        0.3760, 0.3740, 0.3889, 0.3738, 0.3945, 0.2229, 0.3757, 0.3794, 0.3757,
        0.3760, 0.3760, 0.3953, 0.3799, 0.2542, 0.4380, 0.3762, 0.3982, 0.3757,
        0.4504, 0.3757, 0.3762, 0.0862, 0.3760, 0.3757, 0.3843, 0.4182, 0.3757,
        0.3757, 0.4199, 0.4915, 0.3760, 0.3762, 0.3914, 0.3774, 0.3757, 0.2351,
        0.3757, 0.3760, 0.3760, 0.3760, 0.3760, 0.3760, 0.4380, 0.3757, 0.3760,
        0.4956, 0.3804, 0.3774, 0.3474, 0.1863, 0.3755, 0.2101, 0.3760, 0.4614,
        0.3762, 0.3738, 0.3833, 0.0348, 0.3848, 0.1422, 0.3760, 0.3757, 0.3870,
        0.3767, 0.4146, 0.3979, 0.3757, 0.3821, 0.3804, 0.3757, 0.3760, 0.3762,
        0.3757], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(6596.3735, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6094, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6642.2217, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6172, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 10:29:40,632][train_inner][INFO] - {"epoch": 19, "update": 18.879, "loss": "3.965", "ntokens": "149581", "nsentences": "540.515", "prob_perplexity": "132.614", "code_perplexity": "130.406", "temp": "1.905", "loss_0": "3.833", "loss_1": "0.114", "loss_2": "0.018", "accuracy": "0.32696", "wps": "37626.3", "ups": "0.25", "wpb": "149580", "bsz": "540.5", "num_updates": "9800", "lr": "0.000153125", "gnorm": "0.497", "loss_scale": "1", "train_wall": "794", "gb_free": "13.3", "wall": "39447"}
[2023-09-12 10:33:48,681][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 10:33:48,682][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 10:33:48,850][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2023-09-12 10:34:12,571][valid][INFO] - {"epoch": 19, "valid_loss": "3.751", "valid_ntokens": "7860.63", "valid_nsentences": "55.2525", "valid_prob_perplexity": "112.237", "valid_code_perplexity": "109.421", "valid_temp": "1.904", "valid_loss_0": "3.613", "valid_loss_1": "0.119", "valid_loss_2": "0.019", "valid_accuracy": "0.38357", "valid_wps": "32787.5", "valid_wpb": "7860.6", "valid_bsz": "55.3", "valid_num_updates": "9863", "valid_best_loss": "3.751"}
[2023-09-12 10:34:12,574][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 9863 updates
[2023-09-12 10:34:12,575][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 10:34:15,090][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-11/23-32-06/checkpoints/checkpoint_best.pt
[2023-09-12 10:34:16,518][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 19 @ 9863 updates, score 3.751) (writing took 3.9447880930965766 seconds)
[2023-09-12 10:34:16,519][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2023-09-12 10:34:16,519][train][INFO] - {"epoch": 19, "train_loss": "3.979", "train_ntokens": "149476", "train_nsentences": "538.416", "train_prob_perplexity": "130.735", "train_code_perplexity": "128.589", "train_temp": "1.906", "train_loss_0": "3.846", "train_loss_1": "0.115", "train_loss_2": "0.018", "train_accuracy": "0.32558", "train_wps": "37156.9", "train_ups": "0.25", "train_wpb": "149476", "train_bsz": "538.4", "train_num_updates": "9863", "train_lr": "0.000154109", "train_gnorm": "0.519", "train_loss_scale": "1", "train_train_wall": "2056", "train_gb_free": "13.3", "train_wall": "39723"}
[2023-09-12 10:34:16,521][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 10:34:16,614][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2023-09-12 10:34:16,866][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 10:34:16,869][fairseq.trainer][INFO] - begin training epoch 20
[2023-09-12 10:34:16,870][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 10:36:38,841][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Parameter containing:
tensor([0.0739, 0.3618, 0.2749, 0.2316, 0.1748, 0.3604, 0.3606, 0.4099, 0.3611,
        0.3608, 0.3606, 0.3601, 0.4382, 0.3604, 0.4351, 0.4966, 0.3608, 0.3606,
        0.4199, 0.3604, 0.3604, 0.4070, 0.3606, 0.3960, 0.3755, 0.3608, 0.3606,
        0.3608, 0.3635, 0.2932, 0.3992, 0.3608, 0.3594, 0.3604, 0.3606, 0.3606,
        0.4409, 0.4478, 0.3606, 0.3777, 0.3604, 0.3608, 0.3604, 0.3601, 0.3604,
        0.3391, 0.3604, 0.3604, 0.3606, 0.3611, 0.3604, 0.4487, 0.3606, 0.3604,
        0.3604, 0.3601, 0.4568, 0.3564, 0.3604, 0.3604, 0.3606, 0.3621, 0.2043,
        0.3828, 0.3674, 0.3403, 0.3735, 0.3608, 0.3608, 0.3604, 0.3604, 0.3606,
        0.3604, 0.3879, 0.3604, 0.3606, 0.3604, 0.2126, 0.3604, 0.4336, 0.3604,
        0.3606, 0.3757, 0.3606, 0.3928, 0.2886, 0.3604, 0.3604, 0.4133, 0.3623,
        0.3787, 0.2062, 0.3608, 0.3606, 0.3623, 0.3606, 0.2190, 0.3809, 0.2756,
        0.3604, 0.3608, 0.3606, 0.4163, 0.4070, 0.3804, 0.3606, 0.3606, 0.3608,
        0.3794, 0.3506, 0.3608, 0.3608, 0.3567, 0.4517, 0.3604, 0.3604, 0.3604,
        0.3723, 0.3601, 0.4065, 0.3604, 0.3604, 0.3857, 0.4209, 0.4512, 0.3608,
        0.4465, 0.3728, 0.4429, 0.3604, 0.3611, 0.3604, 0.3569, 0.3604, 0.3604,
        0.2554, 0.3958, 0.4116, 0.3599, 0.4001, 0.3601, 0.3550, 0.3608, 0.3606,
        0.3604, 0.3606, 0.5132, 0.3792, 0.3606, 0.3606, 0.2651, 0.4036, 0.3608,
        0.3613, 0.3879, 0.3606, 0.3677, 0.3694, 0.3604, 0.3604, 0.3767, 0.3708,
        0.3606, 0.3606, 0.2686, 0.4216, 0.3804, 0.3662, 0.3608, 0.3621, 0.3606,
        0.3604, 0.3604, 0.4167, 0.3604, 0.3606, 0.3604, 0.3740, 0.3689, 0.3557,
        0.3606, 0.3604, 0.3601, 0.3984, 0.3606, 0.3669, 0.1846, 0.3604, 0.3979,
        0.3608, 0.3606, 0.3608, 0.4128, 0.4084, 0.3604, 0.4534, 0.3628, 0.3606,
        0.3486, 0.3608, 0.3547, 0.4138, 0.3616, 0.3604, 0.3608, 0.3606, 0.3711,
        0.3599, 0.3606, 0.4280, 0.4172, 0.4397, 0.4482, 0.4683, 0.3606, 0.3604,
        0.4419, 0.3606, 0.2020, 0.3604, 0.3606, 0.3606, 0.3606, 0.3752, 0.3604,
        0.3604, 0.2219, 0.3604, 0.3650, 0.4312, 0.4102, 0.3608, 0.3606, 0.3604,
        0.3606, 0.3606, 0.4287, 0.3604, 0.4045, 0.3577, 0.4102, 0.3767, 0.3618,
        0.3840, 0.3604, 0.3604, 0.3613, 0.3601, 0.3604, 0.3877, 0.3611, 0.3604,
        0.1847, 0.3604, 0.4470, 0.4214, 0.3625, 0.3604, 0.3606, 0.3640, 0.3604,
        0.3640, 0.4255, 0.3813, 0.4570, 0.3606, 0.4282, 0.3604, 0.3606, 0.3606,
        0.3594, 0.3606, 0.3604, 0.3604, 0.3601, 0.3613, 0.3604, 0.3604, 0.3606,
        0.2703, 0.4294, 0.3604, 0.1011, 0.3604, 0.3901, 0.3608, 0.3604, 0.3606,
        0.3689, 0.3606, 0.3604, 0.3604, 0.3601, 0.3606, 0.3604, 0.3613, 0.3687,
        0.3604, 0.3569, 0.4011, 0.3608, 0.3557, 0.3604, 0.3604, 0.3606, 0.3604,
        0.3606, 0.3604, 0.3606, 0.3606, 0.3098, 0.3601, 0.3606, 0.4202, 0.3604,
        0.3606, 0.4934, 0.3608, 0.3628, 0.3606, 0.3604, 0.3801, 0.3604, 0.3604,
        0.3604, 0.3606, 0.3606, 0.3721, 0.1359, 0.4314, 0.3618, 0.3604, 0.4543,
        0.1274, 0.4565, 0.3606, 0.4026, 0.4656, 0.3921, 0.3606, 0.2437, 0.3606,
        0.3611, 0.3713, 0.3606, 0.3574, 0.4326, 0.4207, 0.3608, 0.3889, 0.4719,
        0.3604, 0.3608, 0.3604, 0.3604, 0.3606, 0.3604, 0.3604, 0.3611, 0.3604,
        0.2771, 0.3606, 0.3943, 0.3606, 0.4426, 0.4395, 0.3604, 0.3604, 0.3604,
        0.4143, 0.3701, 0.3604, 0.4915, 0.3604, 0.1541, 0.4150, 0.4526, 0.3604,
        0.3604, 0.3564, 0.3604, 0.3606, 0.3604, 0.3604, 0.3604, 0.3604, 0.3606,
        0.4036, 0.0804, 0.3604, 0.3618, 0.3628, 0.4250, 0.3608, 0.3604, 0.3604,
        0.3606, 0.3606, 0.3796, 0.2477, 0.3604, 0.3689, 0.4492, 0.2705, 0.3604,
        0.2037, 0.3604, 0.3779, 0.3972, 0.3613, 0.3604, 0.3604, 0.3604, 0.3604,
        0.3606, 0.3606, 0.3604, 0.4692, 0.3604, 0.3604, 0.4509, 0.3604, 0.3635,
        0.3606, 0.3589, 0.3601, 0.4407, 0.3606, 0.3623, 0.3604, 0.4214, 0.3589,
        0.3606, 0.2059, 0.3604, 0.3608, 0.4026, 0.3596, 0.2727, 0.4080, 0.3606,
        0.3604, 0.3716, 0.3652, 0.1230, 0.1925, 0.3608, 0.3608, 0.3733, 0.3608,
        0.3604, 0.3662, 0.3606, 0.3604, 0.3606, 0.3604, 0.3606, 0.3608, 0.4199,
        0.4902, 0.3291, 0.3606, 0.3606, 0.4600, 0.3567, 0.3606, 0.4070, 0.4519,
        0.3608, 0.3838, 0.3616, 0.3606, 0.3606, 0.3604, 0.4377, 0.3606, 0.4944,
        0.3582, 0.3604, 0.0845, 0.4312, 0.4045, 0.3606, 0.3618, 0.3604, 0.3796,
        0.4214, 0.3625, 0.3606, 0.3616, 0.3726, 0.4031, 0.3604, 0.3604, 0.3586,
        0.3630, 0.3750, 0.3643, 0.4265, 0.3604, 0.3604, 0.3604, 0.3606, 0.3604,
        0.3604, 0.3604, 0.3667, 0.3606, 0.3604, 0.3562, 0.3604, 0.2482, 0.3015,
        0.3604, 0.3604, 0.3638, 0.3601, 0.3606, 0.3604, 0.3604, 0.3604, 0.4045,
        0.3604, 0.3604, 0.3577, 0.3796, 0.3723, 0.3625, 0.3606, 0.3608, 0.4604,
        0.4656, 0.3494, 0.4353, 0.3608, 0.3613, 0.3650, 0.3604, 0.4751, 0.3608,
        0.3452, 0.3604, 0.4158, 0.3604, 0.3665, 0.3606, 0.3606, 0.3611, 0.3604,
        0.3550, 0.3606, 0.3604, 0.3604, 0.3643, 0.3901, 0.3608, 0.3604, 0.4214,
        0.2778, 0.3604, 0.3604, 0.3604, 0.3604, 0.3604, 0.3604, 0.3611, 0.3606,
        0.3606, 0.3586, 0.3662, 0.3577, 0.3750, 0.2422, 0.3604, 0.3630, 0.3606,
        0.3606, 0.3608, 0.3752, 0.3618, 0.2401, 0.4128, 0.3604, 0.3828, 0.3604,
        0.4307, 0.3604, 0.3608, 0.0959, 0.3606, 0.3604, 0.3667, 0.4006, 0.3604,
        0.3604, 0.4119, 0.4680, 0.3606, 0.3608, 0.3750, 0.3613, 0.3604, 0.2452,
        0.3604, 0.3606, 0.3606, 0.3608, 0.3606, 0.3606, 0.4214, 0.3604, 0.3604,
        0.4790, 0.3643, 0.3613, 0.3567, 0.1989, 0.3596, 0.2299, 0.3606, 0.4390,
        0.3608, 0.3662, 0.3660, 0.0370, 0.3650, 0.1482, 0.3608, 0.3604, 0.3696,
        0.3611, 0.4089, 0.3743, 0.3604, 0.3635, 0.3650, 0.3606, 0.3606, 0.3608,
        0.3604], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(3604.9453, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5703, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5845.5713, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5898, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5183.9868, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5039, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6186.9805, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5469, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6331.6157, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3711, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6732.6733, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5273, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6829.7559, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5742, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4646.4956, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5195, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5053.1704, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6211, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4260.2549, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7031, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6494.2573, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5156, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(5750.2715, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7695, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 10:42:53,964][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0627, 0.3994, 0.3091, 0.2152, 0.1827, 0.3979, 0.3984, 0.4570, 0.3989,
        0.3982, 0.3982, 0.3677, 0.4617, 0.3979, 0.4705, 0.5586, 0.3982, 0.3982,
        0.4797, 0.3979, 0.3979, 0.4109, 0.3982, 0.3835, 0.4194, 0.3982, 0.3982,
        0.3989, 0.4087, 0.3003, 0.4553, 0.3982, 0.3970, 0.3979, 0.3982, 0.3979,
        0.5005, 0.4456, 0.3982, 0.4192, 0.3979, 0.3989, 0.3979, 0.3977, 0.3979,
        0.3403, 0.3979, 0.3979, 0.3982, 0.3987, 0.3979, 0.4861, 0.3982, 0.3979,
        0.3979, 0.3977, 0.5146, 0.3940, 0.3979, 0.3977, 0.3982, 0.4001, 0.2036,
        0.4304, 0.3723, 0.3147, 0.4224, 0.3984, 0.3997, 0.3979, 0.3979, 0.3979,
        0.3979, 0.4399, 0.3979, 0.3982, 0.3979, 0.1731, 0.3979, 0.4905, 0.3979,
        0.3982, 0.4207, 0.3979, 0.4331, 0.2612, 0.3979, 0.3979, 0.4607, 0.4028,
        0.4277, 0.1766, 0.3984, 0.3979, 0.4023, 0.3982, 0.1990, 0.4277, 0.2388,
        0.3979, 0.3982, 0.3979, 0.4404, 0.4624, 0.4270, 0.3984, 0.3979, 0.3984,
        0.3547, 0.3496, 0.3982, 0.3984, 0.3940, 0.4739, 0.3979, 0.3979, 0.3979,
        0.4189, 0.3977, 0.3901, 0.3979, 0.3979, 0.4348, 0.4275, 0.5112, 0.3984,
        0.5059, 0.4160, 0.4944, 0.3979, 0.4004, 0.3979, 0.3945, 0.3979, 0.3989,
        0.2477, 0.4495, 0.4597, 0.3975, 0.4465, 0.3977, 0.3926, 0.3984, 0.3982,
        0.3979, 0.3982, 0.5654, 0.4202, 0.3982, 0.3982, 0.2927, 0.4622, 0.3984,
        0.3987, 0.4136, 0.3982, 0.4124, 0.4138, 0.3977, 0.3979, 0.4270, 0.4224,
        0.3979, 0.3982, 0.2495, 0.4661, 0.4255, 0.4089, 0.3982, 0.4004, 0.3994,
        0.3979, 0.3979, 0.4788, 0.3979, 0.3984, 0.3979, 0.4285, 0.4133, 0.3933,
        0.3982, 0.3979, 0.3975, 0.4526, 0.3982, 0.4084, 0.1439, 0.3979, 0.4507,
        0.3982, 0.3982, 0.4004, 0.4231, 0.4595, 0.3979, 0.4861, 0.4006, 0.3982,
        0.3123, 0.3984, 0.3923, 0.4688, 0.4001, 0.3979, 0.3984, 0.3982, 0.4124,
        0.3975, 0.3982, 0.4033, 0.4771, 0.4966, 0.5073, 0.5122, 0.3979, 0.3979,
        0.4685, 0.3979, 0.1633, 0.3979, 0.3982, 0.3982, 0.3982, 0.4226, 0.3979,
        0.3979, 0.1979, 0.3979, 0.3352, 0.4873, 0.4670, 0.3982, 0.3979, 0.3979,
        0.3979, 0.3982, 0.4402, 0.3979, 0.3950, 0.3953, 0.3906, 0.4209, 0.3992,
        0.4336, 0.3979, 0.3979, 0.3989, 0.3982, 0.3979, 0.4326, 0.3999, 0.3979,
        0.1392, 0.3979, 0.5020, 0.4421, 0.4001, 0.3979, 0.3982, 0.4026, 0.3979,
        0.4028, 0.4512, 0.4263, 0.5190, 0.3979, 0.4863, 0.3979, 0.3987, 0.3982,
        0.3970, 0.3979, 0.3979, 0.3977, 0.3975, 0.4009, 0.3979, 0.3979, 0.3979,
        0.2727, 0.4917, 0.3979, 0.0570, 0.3979, 0.4373, 0.3984, 0.3979, 0.3984,
        0.3862, 0.3982, 0.3979, 0.3979, 0.3977, 0.3982, 0.3979, 0.4001, 0.3435,
        0.3984, 0.3945, 0.4521, 0.3984, 0.3933, 0.3979, 0.3977, 0.3979, 0.3979,
        0.3982, 0.3979, 0.3982, 0.3982, 0.2805, 0.3975, 0.3982, 0.4153, 0.3979,
        0.3982, 0.5459, 0.3982, 0.4014, 0.3984, 0.3979, 0.4207, 0.3979, 0.3979,
        0.3979, 0.3982, 0.3984, 0.4097, 0.1006, 0.4336, 0.3994, 0.3979, 0.4868,
        0.1116, 0.4958, 0.3982, 0.4509, 0.5166, 0.4417, 0.3982, 0.2017, 0.3982,
        0.4006, 0.4150, 0.3979, 0.3950, 0.4717, 0.4753, 0.3982, 0.4431, 0.5269,
        0.3979, 0.3984, 0.3979, 0.3979, 0.3979, 0.3979, 0.3982, 0.3997, 0.3979,
        0.2734, 0.3979, 0.4482, 0.3982, 0.4634, 0.4929, 0.3982, 0.3979, 0.3979,
        0.4702, 0.4141, 0.3979, 0.4944, 0.3979, 0.1478, 0.4712, 0.4868, 0.3979,
        0.3979, 0.3940, 0.3979, 0.3984, 0.3979, 0.3979, 0.3979, 0.3979, 0.3979,
        0.4595, 0.0810, 0.3979, 0.4001, 0.4053, 0.4768, 0.3989, 0.3979, 0.3979,
        0.3982, 0.3984, 0.4302, 0.2252, 0.3979, 0.4065, 0.5044, 0.2356, 0.3979,
        0.1691, 0.3979, 0.4226, 0.4458, 0.4006, 0.3979, 0.3979, 0.3979, 0.3982,
        0.3982, 0.3979, 0.3979, 0.5273, 0.3979, 0.3979, 0.5049, 0.3979, 0.4065,
        0.3982, 0.3977, 0.3977, 0.4912, 0.3982, 0.3997, 0.3979, 0.4800, 0.3965,
        0.3982, 0.1959, 0.3979, 0.3982, 0.4172, 0.3972, 0.2490, 0.4585, 0.3982,
        0.3979, 0.4143, 0.4084, 0.1116, 0.1683, 0.3989, 0.4014, 0.4304, 0.3984,
        0.3979, 0.4067, 0.3982, 0.3979, 0.3979, 0.3979, 0.3984, 0.3982, 0.4775,
        0.5459, 0.3657, 0.3982, 0.3979, 0.4800, 0.3943, 0.3982, 0.4575, 0.4961,
        0.3992, 0.4236, 0.4016, 0.3982, 0.3982, 0.3979, 0.4805, 0.3979, 0.5503,
        0.3955, 0.3979, 0.0646, 0.4897, 0.4619, 0.3982, 0.4014, 0.3979, 0.4326,
        0.4265, 0.4001, 0.3979, 0.4021, 0.4180, 0.4519, 0.3979, 0.3979, 0.3962,
        0.4031, 0.4180, 0.4082, 0.4478, 0.3979, 0.3979, 0.3979, 0.3982, 0.3979,
        0.3979, 0.3979, 0.4058, 0.3982, 0.3979, 0.3938, 0.3979, 0.2142, 0.2961,
        0.3979, 0.3979, 0.4048, 0.3977, 0.3979, 0.3979, 0.3979, 0.3979, 0.4604,
        0.3979, 0.3979, 0.3953, 0.4299, 0.4160, 0.4031, 0.3982, 0.3982, 0.5044,
        0.5264, 0.3201, 0.4194, 0.3982, 0.3989, 0.4094, 0.3979, 0.5332, 0.3984,
        0.3481, 0.3979, 0.4690, 0.3979, 0.4133, 0.3979, 0.3979, 0.3992, 0.3979,
        0.3928, 0.3984, 0.3979, 0.3979, 0.4041, 0.4504, 0.3984, 0.3979, 0.4783,
        0.2539, 0.3979, 0.3979, 0.3979, 0.3979, 0.3979, 0.3979, 0.3984, 0.3982,
        0.3982, 0.3962, 0.4236, 0.3965, 0.4236, 0.1924, 0.3979, 0.4026, 0.3979,
        0.3982, 0.3982, 0.4246, 0.4070, 0.2783, 0.4680, 0.3997, 0.4209, 0.3979,
        0.4634, 0.3979, 0.3984, 0.0775, 0.3982, 0.3979, 0.4087, 0.4302, 0.3982,
        0.3979, 0.4358, 0.5244, 0.3982, 0.3984, 0.4153, 0.4006, 0.3979, 0.2251,
        0.3979, 0.3982, 0.3982, 0.3982, 0.3982, 0.3982, 0.4524, 0.3979, 0.3987,
        0.5161, 0.4041, 0.4004, 0.3296, 0.1685, 0.3987, 0.1821, 0.3982, 0.4802,
        0.3984, 0.3955, 0.4087, 0.0302, 0.4136, 0.1348, 0.3982, 0.3979, 0.4131,
        0.3997, 0.4094, 0.4324, 0.3979, 0.4104, 0.4026, 0.3979, 0.3987, 0.3984,
        0.3982], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 10:43:26,153][train_inner][INFO] - {"epoch": 20, "update": 19.267, "loss": "3.962", "ntokens": "149017", "nsentences": "536.81", "prob_perplexity": "134.933", "code_perplexity": "132.664", "temp": "1.903", "loss_0": "3.83", "loss_1": "0.114", "loss_2": "0.019", "accuracy": "0.32712", "wps": "36102.5", "ups": "0.24", "wpb": "149017", "bsz": "536.8", "num_updates": "10000", "lr": "0.00015625", "gnorm": "0.53", "loss_scale": "0.5", "train_wall": "796", "gb_free": "13", "wall": "40273"}
Parameter containing:
tensor([0.0765, 0.3496, 0.2659, 0.2324, 0.1733, 0.3481, 0.3481, 0.4060, 0.3486,
        0.3484, 0.3481, 0.3586, 0.4189, 0.3481, 0.4143, 0.4788, 0.3484, 0.3484,
        0.3999, 0.3481, 0.3481, 0.4009, 0.3481, 0.3975, 0.3613, 0.3484, 0.3484,
        0.3484, 0.3499, 0.2883, 0.3811, 0.3484, 0.3469, 0.3481, 0.3484, 0.3481,
        0.4221, 0.4370, 0.3484, 0.3638, 0.3481, 0.3486, 0.3481, 0.3479, 0.3481,
        0.3374, 0.3481, 0.3481, 0.3484, 0.3486, 0.3481, 0.4285, 0.3484, 0.3481,
        0.3481, 0.3479, 0.4438, 0.3442, 0.3481, 0.3479, 0.3481, 0.3494, 0.2003,
        0.3667, 0.3662, 0.3438, 0.3611, 0.3486, 0.3484, 0.3481, 0.3481, 0.3481,
        0.3481, 0.3745, 0.3479, 0.3484, 0.3481, 0.2239, 0.3481, 0.4158, 0.3481,
        0.3484, 0.3618, 0.3481, 0.3804, 0.2974, 0.3481, 0.3481, 0.3972, 0.3496,
        0.3628, 0.2150, 0.3484, 0.3481, 0.3494, 0.3484, 0.2246, 0.3682, 0.2859,
        0.3481, 0.3484, 0.3481, 0.4094, 0.3909, 0.3657, 0.3481, 0.3481, 0.3484,
        0.3821, 0.3499, 0.3484, 0.3484, 0.3442, 0.4387, 0.3481, 0.3481, 0.3481,
        0.3577, 0.3477, 0.4102, 0.3481, 0.3481, 0.3706, 0.4185, 0.4326, 0.3486,
        0.4287, 0.3589, 0.4255, 0.3481, 0.3486, 0.3481, 0.3447, 0.3481, 0.3479,
        0.2576, 0.3799, 0.3955, 0.3477, 0.3838, 0.3479, 0.3425, 0.3486, 0.3481,
        0.3479, 0.3484, 0.4939, 0.3660, 0.3484, 0.3484, 0.2583, 0.3860, 0.3486,
        0.3489, 0.3828, 0.3481, 0.3542, 0.3552, 0.3479, 0.3481, 0.3611, 0.3555,
        0.3481, 0.3484, 0.2769, 0.4175, 0.3655, 0.3525, 0.3484, 0.3494, 0.3481,
        0.3481, 0.3479, 0.3970, 0.3481, 0.3481, 0.3481, 0.3611, 0.3545, 0.3435,
        0.3481, 0.3481, 0.3477, 0.3816, 0.3484, 0.3538, 0.1954, 0.3481, 0.3826,
        0.3484, 0.3484, 0.3484, 0.4077, 0.3916, 0.3481, 0.4463, 0.3506, 0.3481,
        0.3574, 0.3486, 0.3423, 0.3962, 0.3491, 0.3481, 0.3484, 0.3484, 0.3579,
        0.3474, 0.3484, 0.4243, 0.3994, 0.4207, 0.4292, 0.4539, 0.3481, 0.3481,
        0.4250, 0.3481, 0.2135, 0.3481, 0.3484, 0.3484, 0.3484, 0.3604, 0.3481,
        0.3481, 0.2297, 0.3481, 0.3689, 0.4119, 0.3918, 0.3484, 0.3481, 0.3481,
        0.3481, 0.3484, 0.4172, 0.3481, 0.4043, 0.3455, 0.4099, 0.3623, 0.3494,
        0.3677, 0.3481, 0.3481, 0.3491, 0.3477, 0.3481, 0.3728, 0.3484, 0.3481,
        0.1975, 0.3481, 0.4294, 0.4009, 0.3501, 0.3481, 0.3484, 0.3518, 0.3479,
        0.3511, 0.4106, 0.3657, 0.4380, 0.3481, 0.4082, 0.3481, 0.3481, 0.3484,
        0.3469, 0.3481, 0.3481, 0.3479, 0.3477, 0.3484, 0.3481, 0.3481, 0.3481,
        0.2668, 0.4102, 0.3481, 0.1146, 0.3481, 0.3730, 0.3486, 0.3481, 0.3484,
        0.3645, 0.3484, 0.3481, 0.3481, 0.3479, 0.3484, 0.3481, 0.3489, 0.3728,
        0.3481, 0.3447, 0.3853, 0.3484, 0.3433, 0.3481, 0.3479, 0.3481, 0.3481,
        0.3484, 0.3481, 0.3481, 0.3484, 0.3174, 0.3477, 0.3481, 0.4219, 0.3481,
        0.3484, 0.4817, 0.3484, 0.3501, 0.3484, 0.3479, 0.3665, 0.3481, 0.3481,
        0.3481, 0.3484, 0.3484, 0.3596, 0.1453, 0.4185, 0.3496, 0.3481, 0.4380,
        0.1315, 0.4421, 0.3481, 0.3862, 0.4456, 0.3757, 0.3484, 0.2563, 0.3481,
        0.3484, 0.3579, 0.3481, 0.3452, 0.4148, 0.4038, 0.3484, 0.3716, 0.4529,
        0.3481, 0.3486, 0.3481, 0.3481, 0.3481, 0.3481, 0.3481, 0.3484, 0.3481,
        0.2722, 0.3481, 0.3779, 0.3481, 0.4241, 0.4231, 0.3481, 0.3481, 0.3481,
        0.3965, 0.3560, 0.3481, 0.4734, 0.3481, 0.1562, 0.3979, 0.4497, 0.3481,
        0.3481, 0.3442, 0.3481, 0.3484, 0.3481, 0.3481, 0.3481, 0.3481, 0.3481,
        0.3860, 0.0850, 0.3481, 0.3494, 0.3494, 0.4102, 0.3486, 0.3481, 0.3481,
        0.3481, 0.3484, 0.3647, 0.2539, 0.3481, 0.3567, 0.4292, 0.2766, 0.3481,
        0.2117, 0.3481, 0.3630, 0.3811, 0.3486, 0.3481, 0.3481, 0.3481, 0.3481,
        0.3484, 0.3481, 0.3481, 0.4504, 0.3481, 0.3481, 0.4336, 0.3481, 0.3501,
        0.3481, 0.3494, 0.3479, 0.4248, 0.3481, 0.3499, 0.3481, 0.4038, 0.3464,
        0.3481, 0.2092, 0.3481, 0.3484, 0.4014, 0.3474, 0.2798, 0.3921, 0.3484,
        0.3481, 0.3572, 0.3518, 0.1243, 0.2001, 0.3486, 0.3481, 0.3557, 0.3484,
        0.3481, 0.3535, 0.3481, 0.3481, 0.3481, 0.3481, 0.3481, 0.3484, 0.4004,
        0.4832, 0.3213, 0.3484, 0.3481, 0.4429, 0.3445, 0.3484, 0.3906, 0.4329,
        0.3484, 0.3706, 0.3489, 0.3484, 0.3484, 0.3481, 0.4226, 0.3481, 0.4763,
        0.3457, 0.3481, 0.0861, 0.4124, 0.3872, 0.3481, 0.3486, 0.3481, 0.3628,
        0.4109, 0.3503, 0.3481, 0.3489, 0.3572, 0.3870, 0.3481, 0.3481, 0.3462,
        0.3499, 0.3618, 0.3508, 0.4163, 0.3481, 0.3481, 0.3481, 0.3484, 0.3481,
        0.3481, 0.3481, 0.3542, 0.3484, 0.3481, 0.3440, 0.3481, 0.2563, 0.3032,
        0.3481, 0.3481, 0.3503, 0.3479, 0.3481, 0.3481, 0.3481, 0.3481, 0.3840,
        0.3481, 0.3481, 0.3455, 0.3643, 0.3577, 0.3496, 0.3481, 0.3484, 0.4512,
        0.4460, 0.3572, 0.4355, 0.3484, 0.3489, 0.3513, 0.3481, 0.4575, 0.3486,
        0.3464, 0.3481, 0.3987, 0.3481, 0.3540, 0.3481, 0.3481, 0.3486, 0.3481,
        0.3423, 0.3484, 0.3481, 0.3481, 0.3516, 0.3726, 0.3484, 0.3481, 0.4031,
        0.2793, 0.3481, 0.3481, 0.3481, 0.3481, 0.3481, 0.3481, 0.3486, 0.3484,
        0.3484, 0.3464, 0.3477, 0.3450, 0.3596, 0.2556, 0.3481, 0.3503, 0.3481,
        0.3481, 0.3484, 0.3596, 0.3486, 0.2363, 0.3967, 0.3484, 0.3704, 0.3481,
        0.4111, 0.3481, 0.3484, 0.0980, 0.3484, 0.3481, 0.3525, 0.3833, 0.3481,
        0.3481, 0.4089, 0.4490, 0.3481, 0.3484, 0.3628, 0.3489, 0.3481, 0.2522,
        0.3481, 0.3484, 0.3481, 0.3484, 0.3484, 0.3484, 0.4023, 0.3481, 0.3481,
        0.4595, 0.3511, 0.3486, 0.3621, 0.2047, 0.3472, 0.2427, 0.3484, 0.4258,
        0.3486, 0.3589, 0.3518, 0.0349, 0.3496, 0.1494, 0.3484, 0.3481, 0.3572,
        0.3484, 0.4026, 0.3574, 0.3481, 0.3496, 0.3528, 0.3481, 0.3481, 0.3484,
        0.3481], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(4942.8770, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4805, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6272.8037, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5234, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4994.2617, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5000, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6297.1421, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5547, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5481.0474, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2148, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5479.4917, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5156, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7368.1729, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6758, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5629.9521, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5898, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5607.8892, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5938, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5279.2295, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5312, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6573.4800, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7051.6060, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6758, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6298.3589, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7500, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0819, 0.3406, 0.2605, 0.2363, 0.1710, 0.3391, 0.3391, 0.3928, 0.3396,
        0.3396, 0.3394, 0.3591, 0.4033, 0.3391, 0.3992, 0.4639, 0.3396, 0.3394,
        0.3865, 0.3391, 0.3391, 0.3972, 0.3394, 0.3911, 0.3508, 0.3396, 0.3394,
        0.3394, 0.3403, 0.2856, 0.3687, 0.3396, 0.3381, 0.3391, 0.3394, 0.3394,
        0.4084, 0.4265, 0.3394, 0.3533, 0.3391, 0.3396, 0.3391, 0.3389, 0.3391,
        0.3384, 0.3391, 0.3391, 0.3394, 0.3398, 0.3391, 0.4133, 0.3394, 0.3391,
        0.3391, 0.3389, 0.4294, 0.3352, 0.3391, 0.3391, 0.3394, 0.3403, 0.1992,
        0.3550, 0.3630, 0.3479, 0.3503, 0.3396, 0.3394, 0.3391, 0.3391, 0.3394,
        0.3391, 0.3616, 0.3391, 0.3394, 0.3391, 0.2365, 0.3391, 0.4021, 0.3391,
        0.3394, 0.3521, 0.3394, 0.3711, 0.3035, 0.3391, 0.3391, 0.3860, 0.3401,
        0.3513, 0.2217, 0.3396, 0.3394, 0.3401, 0.3394, 0.2310, 0.3562, 0.2949,
        0.3391, 0.3396, 0.3394, 0.3948, 0.3777, 0.3547, 0.3391, 0.3394, 0.3396,
        0.3833, 0.3513, 0.3396, 0.3394, 0.3352, 0.4243, 0.3391, 0.3391, 0.3391,
        0.3474, 0.3389, 0.4143, 0.3391, 0.3391, 0.3599, 0.4026, 0.4180, 0.3396,
        0.4141, 0.3486, 0.4109, 0.3391, 0.3394, 0.3391, 0.3357, 0.3391, 0.3389,
        0.2620, 0.3674, 0.3838, 0.3386, 0.3728, 0.3389, 0.3335, 0.3396, 0.3394,
        0.3391, 0.3394, 0.4797, 0.3560, 0.3394, 0.3394, 0.2489, 0.3728, 0.3396,
        0.3401, 0.3833, 0.3391, 0.3442, 0.3447, 0.3391, 0.3391, 0.3496, 0.3445,
        0.3394, 0.3394, 0.2810, 0.4036, 0.3545, 0.3428, 0.3396, 0.3401, 0.3391,
        0.3391, 0.3391, 0.3826, 0.3391, 0.3394, 0.3391, 0.3484, 0.3445, 0.3345,
        0.3394, 0.3391, 0.3389, 0.3689, 0.3394, 0.3442, 0.2052, 0.3391, 0.3706,
        0.3396, 0.3394, 0.3394, 0.4058, 0.3787, 0.3391, 0.4329, 0.3416, 0.3394,
        0.3657, 0.3396, 0.3335, 0.3813, 0.3401, 0.3391, 0.3394, 0.3394, 0.3481,
        0.3386, 0.3394, 0.4199, 0.3860, 0.4065, 0.4138, 0.4407, 0.3394, 0.3391,
        0.4121, 0.3394, 0.2247, 0.3391, 0.3394, 0.3394, 0.3394, 0.3496, 0.3391,
        0.3391, 0.2380, 0.3391, 0.3743, 0.3967, 0.3782, 0.3396, 0.3394, 0.3391,
        0.3394, 0.3394, 0.4041, 0.3391, 0.4033, 0.3364, 0.4016, 0.3518, 0.3403,
        0.3564, 0.3391, 0.3391, 0.3401, 0.3389, 0.3391, 0.3621, 0.3394, 0.3391,
        0.2086, 0.3391, 0.4158, 0.3850, 0.3413, 0.3391, 0.3394, 0.3428, 0.3391,
        0.3418, 0.3987, 0.3545, 0.4231, 0.3394, 0.3948, 0.3391, 0.3391, 0.3394,
        0.3381, 0.3394, 0.3391, 0.3389, 0.3389, 0.3394, 0.3391, 0.3391, 0.3394,
        0.2646, 0.3958, 0.3391, 0.1249, 0.3391, 0.3613, 0.3396, 0.3391, 0.3394,
        0.3582, 0.3394, 0.3391, 0.3391, 0.3389, 0.3394, 0.3391, 0.3398, 0.3701,
        0.3391, 0.3357, 0.3718, 0.3396, 0.3345, 0.3391, 0.3391, 0.3391, 0.3391,
        0.3394, 0.3391, 0.3394, 0.3394, 0.3240, 0.3389, 0.3394, 0.4087, 0.3391,
        0.3394, 0.4688, 0.3396, 0.3408, 0.3394, 0.3391, 0.3564, 0.3391, 0.3391,
        0.3391, 0.3394, 0.3394, 0.3508, 0.1532, 0.4075, 0.3406, 0.3391, 0.4304,
        0.1322, 0.4287, 0.3394, 0.3748, 0.4314, 0.3633, 0.3394, 0.2649, 0.3394,
        0.3394, 0.3474, 0.3394, 0.3362, 0.3994, 0.3911, 0.3396, 0.3594, 0.4412,
        0.3391, 0.3396, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3394, 0.3391,
        0.2673, 0.3394, 0.3660, 0.3394, 0.4097, 0.4104, 0.3391, 0.3391, 0.3391,
        0.3833, 0.3455, 0.3391, 0.4597, 0.3391, 0.1595, 0.3835, 0.4348, 0.3391,
        0.3391, 0.3352, 0.3391, 0.3394, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391,
        0.3721, 0.0853, 0.3391, 0.3403, 0.3398, 0.3984, 0.3396, 0.3391, 0.3391,
        0.3394, 0.3394, 0.3525, 0.2603, 0.3391, 0.3477, 0.4143, 0.2825, 0.3391,
        0.2192, 0.3391, 0.3521, 0.3696, 0.3396, 0.3391, 0.3391, 0.3391, 0.3391,
        0.3394, 0.3394, 0.3391, 0.4370, 0.3391, 0.3391, 0.4204, 0.3391, 0.3403,
        0.3394, 0.3401, 0.3389, 0.4121, 0.3391, 0.3408, 0.3391, 0.3906, 0.3374,
        0.3394, 0.2102, 0.3391, 0.3396, 0.3855, 0.3384, 0.2859, 0.3801, 0.3394,
        0.3391, 0.3472, 0.3420, 0.1273, 0.2070, 0.3396, 0.3391, 0.3430, 0.3394,
        0.3391, 0.3442, 0.3394, 0.3391, 0.3394, 0.3391, 0.3394, 0.3396, 0.3862,
        0.4690, 0.3167, 0.3394, 0.3394, 0.4277, 0.3354, 0.3394, 0.3770, 0.4172,
        0.3394, 0.3611, 0.3396, 0.3394, 0.3394, 0.3391, 0.4077, 0.3394, 0.4614,
        0.3369, 0.3391, 0.0892, 0.3975, 0.3728, 0.3394, 0.3396, 0.3391, 0.3508,
        0.3979, 0.3413, 0.3394, 0.3396, 0.3464, 0.3750, 0.3391, 0.3391, 0.3374,
        0.3403, 0.3518, 0.3413, 0.4109, 0.3391, 0.3391, 0.3391, 0.3394, 0.3391,
        0.3391, 0.3391, 0.3452, 0.3394, 0.3391, 0.3350, 0.3391, 0.2632, 0.3066,
        0.3391, 0.3391, 0.3408, 0.3389, 0.3394, 0.3391, 0.3391, 0.3391, 0.3689,
        0.3391, 0.3391, 0.3364, 0.3535, 0.3472, 0.3401, 0.3394, 0.3394, 0.4355,
        0.4309, 0.3601, 0.4329, 0.3396, 0.3401, 0.3418, 0.3391, 0.4431, 0.3396,
        0.3469, 0.3391, 0.3860, 0.3391, 0.3435, 0.3394, 0.3394, 0.3396, 0.3391,
        0.3333, 0.3394, 0.3391, 0.3391, 0.3423, 0.3591, 0.3396, 0.3391, 0.3889,
        0.2756, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3398, 0.3394,
        0.3394, 0.3374, 0.3337, 0.3357, 0.3489, 0.2666, 0.3391, 0.3411, 0.3391,
        0.3391, 0.3394, 0.3491, 0.3394, 0.2264, 0.3823, 0.3394, 0.3613, 0.3391,
        0.3958, 0.3391, 0.3396, 0.1031, 0.3394, 0.3391, 0.3425, 0.3716, 0.3391,
        0.3391, 0.3979, 0.4346, 0.3394, 0.3396, 0.3535, 0.3396, 0.3391, 0.2559,
        0.3391, 0.3394, 0.3391, 0.3396, 0.3394, 0.3394, 0.3877, 0.3391, 0.3391,
        0.4456, 0.3418, 0.3394, 0.3660, 0.2095, 0.3379, 0.2534, 0.3394, 0.4126,
        0.3396, 0.3555, 0.3418, 0.0314, 0.3394, 0.1531, 0.3396, 0.3391, 0.3469,
        0.3394, 0.3911, 0.3452, 0.3391, 0.3398, 0.3438, 0.3394, 0.3391, 0.3396,
        0.3391], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(4164.7012, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4609, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5341.2998, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3867, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7485.8208, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4258, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6424.7393, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5938, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6251.9897, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5938, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6805.5391, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5781, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6137.5791, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5412.1050, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6211, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6363.2310, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6719, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5914.0874, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6641, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7392.1748, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4727, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6311.8237, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5508, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5160.6133, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5508, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7864.6299, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3516, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5606.3003, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6797, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5007.3545, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0626, 0.4045, 0.3140, 0.2117, 0.1826, 0.4031, 0.4038, 0.4639, 0.4041,
        0.4033, 0.4033, 0.3699, 0.4604, 0.4031, 0.4683, 0.5669, 0.4033, 0.4033,
        0.4880, 0.4031, 0.4031, 0.4109, 0.4033, 0.3804, 0.4250, 0.4036, 0.4033,
        0.4041, 0.4155, 0.3018, 0.4634, 0.4036, 0.4021, 0.4031, 0.4033, 0.4031,
        0.5093, 0.4421, 0.4033, 0.4248, 0.4031, 0.4043, 0.4031, 0.4028, 0.4031,
        0.3396, 0.4031, 0.4031, 0.4033, 0.4038, 0.4031, 0.4871, 0.4033, 0.4031,
        0.4031, 0.4028, 0.5229, 0.3992, 0.4031, 0.4028, 0.4033, 0.4055, 0.2039,
        0.4370, 0.3716, 0.3105, 0.4292, 0.4036, 0.4053, 0.4031, 0.4031, 0.4033,
        0.4031, 0.4473, 0.4031, 0.4033, 0.4031, 0.1681, 0.4031, 0.4983, 0.4031,
        0.4033, 0.4275, 0.4033, 0.4390, 0.2566, 0.4031, 0.4031, 0.4673, 0.4084,
        0.4343, 0.1735, 0.4036, 0.4031, 0.4080, 0.4033, 0.1958, 0.4343, 0.2341,
        0.4033, 0.4033, 0.4033, 0.4436, 0.4709, 0.4338, 0.4036, 0.4031, 0.4036,
        0.3521, 0.3496, 0.4033, 0.4036, 0.3992, 0.4749, 0.4031, 0.4031, 0.4031,
        0.4260, 0.4028, 0.3889, 0.4031, 0.4031, 0.4419, 0.4260, 0.5195, 0.4036,
        0.5142, 0.4221, 0.5005, 0.4031, 0.4058, 0.4031, 0.3997, 0.4031, 0.4043,
        0.2477, 0.4570, 0.4661, 0.4026, 0.4531, 0.4028, 0.3977, 0.4036, 0.4033,
        0.4031, 0.4033, 0.5713, 0.4255, 0.4033, 0.4033, 0.2959, 0.4707, 0.4036,
        0.4041, 0.4155, 0.4036, 0.4189, 0.4199, 0.4028, 0.4031, 0.4343, 0.4299,
        0.4033, 0.4033, 0.2455, 0.4734, 0.4321, 0.4150, 0.4036, 0.4055, 0.4045,
        0.4031, 0.4031, 0.4875, 0.4031, 0.4036, 0.4031, 0.4360, 0.4197, 0.3984,
        0.4033, 0.4031, 0.4028, 0.4602, 0.4033, 0.4141, 0.1383, 0.4031, 0.4580,
        0.4036, 0.4033, 0.4058, 0.4241, 0.4663, 0.4031, 0.4900, 0.4058, 0.4033,
        0.3062, 0.4038, 0.3975, 0.4722, 0.4055, 0.4031, 0.4038, 0.4033, 0.4182,
        0.4026, 0.4033, 0.3997, 0.4854, 0.5039, 0.5127, 0.5137, 0.4033, 0.4031,
        0.4719, 0.4033, 0.1591, 0.4031, 0.4033, 0.4033, 0.4033, 0.4292, 0.4031,
        0.4031, 0.1953, 0.4033, 0.3303, 0.4934, 0.4751, 0.4033, 0.4033, 0.4031,
        0.4033, 0.4033, 0.4395, 0.4031, 0.3928, 0.4004, 0.3872, 0.4272, 0.4043,
        0.4402, 0.4031, 0.4031, 0.4041, 0.4036, 0.4031, 0.4387, 0.4053, 0.4031,
        0.1337, 0.4031, 0.5093, 0.4404, 0.4053, 0.4031, 0.4033, 0.4080, 0.4031,
        0.4082, 0.4580, 0.4326, 0.5273, 0.4031, 0.4932, 0.4033, 0.4038, 0.4033,
        0.4021, 0.4033, 0.4031, 0.4028, 0.4026, 0.4065, 0.4031, 0.4031, 0.4031,
        0.2703, 0.4990, 0.4031, 0.0526, 0.4031, 0.4441, 0.4036, 0.4031, 0.4036,
        0.3865, 0.4033, 0.4031, 0.4031, 0.4028, 0.4033, 0.4031, 0.4055, 0.3401,
        0.4038, 0.3997, 0.4597, 0.4036, 0.3984, 0.4031, 0.4028, 0.4031, 0.4031,
        0.4033, 0.4031, 0.4033, 0.4033, 0.2747, 0.4028, 0.4033, 0.4121, 0.4031,
        0.4033, 0.5537, 0.4036, 0.4067, 0.4038, 0.4031, 0.4263, 0.4031, 0.4031,
        0.4031, 0.4033, 0.4036, 0.4148, 0.0973, 0.4326, 0.4045, 0.4031, 0.4929,
        0.1085, 0.4963, 0.4033, 0.4573, 0.5239, 0.4482, 0.4036, 0.1952, 0.4033,
        0.4062, 0.4204, 0.4031, 0.4001, 0.4709, 0.4827, 0.4033, 0.4502, 0.5352,
        0.4031, 0.4036, 0.4031, 0.4031, 0.4033, 0.4033, 0.4033, 0.4048, 0.4031,
        0.2725, 0.4031, 0.4553, 0.4033, 0.4639, 0.5000, 0.4033, 0.4031, 0.4031,
        0.4775, 0.4199, 0.4031, 0.4968, 0.4031, 0.1459, 0.4785, 0.4954, 0.4031,
        0.4031, 0.3992, 0.4031, 0.4038, 0.4031, 0.4031, 0.4031, 0.4031, 0.4031,
        0.4648, 0.0818, 0.4031, 0.4055, 0.4116, 0.4841, 0.4043, 0.4031, 0.4031,
        0.4033, 0.4038, 0.4375, 0.2222, 0.4031, 0.4116, 0.5127, 0.2305, 0.4031,
        0.1644, 0.4031, 0.4285, 0.4524, 0.4058, 0.4031, 0.4031, 0.4031, 0.4033,
        0.4033, 0.4031, 0.4031, 0.5352, 0.4031, 0.4031, 0.5122, 0.4031, 0.4124,
        0.4033, 0.4033, 0.4028, 0.4980, 0.4036, 0.4048, 0.4031, 0.4880, 0.4016,
        0.4033, 0.1941, 0.4031, 0.4036, 0.4194, 0.4023, 0.2472, 0.4658, 0.4033,
        0.4031, 0.4202, 0.4141, 0.1096, 0.1648, 0.4043, 0.4075, 0.4387, 0.4036,
        0.4031, 0.4126, 0.4033, 0.4031, 0.4033, 0.4031, 0.4036, 0.4033, 0.4856,
        0.5532, 0.3730, 0.4033, 0.4033, 0.4788, 0.3994, 0.4033, 0.4646, 0.5005,
        0.4045, 0.4292, 0.4072, 0.4033, 0.4033, 0.4031, 0.4868, 0.4033, 0.5576,
        0.4009, 0.4031, 0.0636, 0.4985, 0.4692, 0.4033, 0.4070, 0.4031, 0.4392,
        0.4246, 0.4053, 0.4031, 0.4077, 0.4241, 0.4578, 0.4031, 0.4031, 0.4014,
        0.4087, 0.4238, 0.4146, 0.4485, 0.4031, 0.4031, 0.4031, 0.4033, 0.4031,
        0.4031, 0.4031, 0.4111, 0.4033, 0.4031, 0.3989, 0.4031, 0.2095, 0.2942,
        0.4031, 0.4031, 0.4104, 0.4028, 0.4031, 0.4031, 0.4031, 0.4031, 0.4646,
        0.4031, 0.4031, 0.4004, 0.4373, 0.4224, 0.4087, 0.4033, 0.4033, 0.5078,
        0.5342, 0.3152, 0.4160, 0.4033, 0.4041, 0.4163, 0.4031, 0.5400, 0.4036,
        0.3469, 0.4031, 0.4761, 0.4031, 0.4207, 0.4031, 0.4031, 0.4045, 0.4031,
        0.3979, 0.4036, 0.4031, 0.4031, 0.4097, 0.4578, 0.4036, 0.4031, 0.4861,
        0.2487, 0.4031, 0.4031, 0.4031, 0.4031, 0.4031, 0.4031, 0.4038, 0.4033,
        0.4033, 0.4014, 0.4265, 0.4019, 0.4309, 0.1849, 0.4031, 0.4080, 0.4031,
        0.4036, 0.4033, 0.4319, 0.4138, 0.2854, 0.4724, 0.4053, 0.4263, 0.4031,
        0.4656, 0.4031, 0.4036, 0.0752, 0.4033, 0.4031, 0.4143, 0.4312, 0.4036,
        0.4031, 0.4392, 0.5322, 0.4033, 0.4036, 0.4209, 0.4062, 0.4031, 0.2224,
        0.4031, 0.4033, 0.4033, 0.4033, 0.4033, 0.4033, 0.4509, 0.4031, 0.4041,
        0.5186, 0.4097, 0.4060, 0.3259, 0.1636, 0.4041, 0.1766, 0.4033, 0.4817,
        0.4036, 0.3984, 0.4146, 0.0298, 0.4202, 0.1324, 0.4033, 0.4031, 0.4192,
        0.4053, 0.4087, 0.4397, 0.4031, 0.4170, 0.4077, 0.4031, 0.4041, 0.4036,
        0.4033], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7619.7666, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6875, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0610, 0.4094, 0.3179, 0.2095, 0.1842, 0.4080, 0.4087, 0.4705, 0.4092,
        0.4084, 0.4082, 0.3701, 0.4600, 0.4080, 0.4663, 0.5747, 0.4084, 0.4082,
        0.4954, 0.4080, 0.4080, 0.4109, 0.4082, 0.3789, 0.4307, 0.4084, 0.4082,
        0.4092, 0.4219, 0.3027, 0.4709, 0.4084, 0.4070, 0.4080, 0.4082, 0.4082,
        0.5171, 0.4402, 0.4082, 0.4299, 0.4080, 0.4094, 0.4080, 0.4080, 0.4080,
        0.3386, 0.4080, 0.4080, 0.4082, 0.4087, 0.4080, 0.4890, 0.4082, 0.4080,
        0.4080, 0.4077, 0.5303, 0.4041, 0.4080, 0.4080, 0.4082, 0.4104, 0.2018,
        0.4429, 0.3728, 0.3071, 0.4360, 0.4084, 0.4104, 0.4080, 0.4080, 0.4082,
        0.4080, 0.4543, 0.4080, 0.4082, 0.4080, 0.1624, 0.4080, 0.5059, 0.4080,
        0.4082, 0.4341, 0.4082, 0.4446, 0.2520, 0.4082, 0.4082, 0.4734, 0.4138,
        0.4409, 0.1699, 0.4084, 0.4082, 0.4131, 0.4082, 0.1924, 0.4409, 0.2294,
        0.4082, 0.4084, 0.4082, 0.4451, 0.4785, 0.4397, 0.4087, 0.4082, 0.4084,
        0.3486, 0.3481, 0.4084, 0.4084, 0.4043, 0.4771, 0.4080, 0.4080, 0.4080,
        0.4324, 0.4077, 0.3862, 0.4080, 0.4080, 0.4485, 0.4229, 0.5269, 0.4084,
        0.5215, 0.4277, 0.5073, 0.4080, 0.4111, 0.4080, 0.4045, 0.4082, 0.4094,
        0.2467, 0.4636, 0.4722, 0.4075, 0.4587, 0.4077, 0.4026, 0.4084, 0.4082,
        0.4080, 0.4082, 0.5791, 0.4307, 0.4084, 0.4082, 0.2998, 0.4780, 0.4084,
        0.4089, 0.4185, 0.4084, 0.4255, 0.4255, 0.4080, 0.4080, 0.4414, 0.4370,
        0.4082, 0.4082, 0.2415, 0.4802, 0.4380, 0.4207, 0.4084, 0.4106, 0.4097,
        0.4080, 0.4080, 0.4946, 0.4080, 0.4087, 0.4080, 0.4438, 0.4258, 0.4033,
        0.4082, 0.4080, 0.4077, 0.4670, 0.4082, 0.4197, 0.1333, 0.4080, 0.4646,
        0.4084, 0.4082, 0.4109, 0.4243, 0.4734, 0.4082, 0.4958, 0.4106, 0.4082,
        0.3010, 0.4087, 0.4026, 0.4775, 0.4106, 0.4080, 0.4087, 0.4082, 0.4238,
        0.4075, 0.4082, 0.3962, 0.4932, 0.5122, 0.5186, 0.5166, 0.4082, 0.4080,
        0.4756, 0.4082, 0.1544, 0.4080, 0.4084, 0.4082, 0.4082, 0.4358, 0.4080,
        0.4082, 0.1934, 0.4082, 0.3252, 0.4985, 0.4827, 0.4084, 0.4082, 0.4080,
        0.4082, 0.4082, 0.4380, 0.4080, 0.3918, 0.4055, 0.3833, 0.4333, 0.4094,
        0.4465, 0.4080, 0.4082, 0.4089, 0.4084, 0.4080, 0.4443, 0.4104, 0.4080,
        0.1285, 0.4082, 0.5161, 0.4377, 0.4102, 0.4080, 0.4082, 0.4131, 0.4080,
        0.4131, 0.4651, 0.4395, 0.5356, 0.4082, 0.5010, 0.4082, 0.4089, 0.4082,
        0.4070, 0.4082, 0.4080, 0.4080, 0.4077, 0.4116, 0.4080, 0.4080, 0.4082,
        0.2700, 0.5054, 0.4080, 0.0480, 0.4080, 0.4504, 0.4084, 0.4080, 0.4084,
        0.3857, 0.4082, 0.4080, 0.4080, 0.4077, 0.4082, 0.4080, 0.4109, 0.3354,
        0.4089, 0.4045, 0.4670, 0.4084, 0.4033, 0.4080, 0.4080, 0.4082, 0.4080,
        0.4082, 0.4080, 0.4082, 0.4082, 0.2703, 0.4077, 0.4082, 0.4082, 0.4082,
        0.4082, 0.5620, 0.4084, 0.4116, 0.4087, 0.4080, 0.4319, 0.4080, 0.4080,
        0.4080, 0.4082, 0.4084, 0.4197, 0.0920, 0.4321, 0.4094, 0.4080, 0.5000,
        0.1045, 0.4988, 0.4082, 0.4641, 0.5312, 0.4551, 0.4084, 0.1885, 0.4082,
        0.4114, 0.4263, 0.4082, 0.4050, 0.4729, 0.4900, 0.4084, 0.4573, 0.5425,
        0.4080, 0.4084, 0.4080, 0.4082, 0.4082, 0.4082, 0.4082, 0.4099, 0.4080,
        0.2725, 0.4082, 0.4617, 0.4082, 0.4644, 0.5068, 0.4082, 0.4080, 0.4080,
        0.4851, 0.4258, 0.4080, 0.4973, 0.4080, 0.1443, 0.4851, 0.5029, 0.4080,
        0.4080, 0.4041, 0.4080, 0.4087, 0.4080, 0.4080, 0.4080, 0.4080, 0.4082,
        0.4683, 0.0804, 0.4080, 0.4104, 0.4182, 0.4917, 0.4094, 0.4080, 0.4080,
        0.4082, 0.4087, 0.4448, 0.2200, 0.4080, 0.4165, 0.5205, 0.2274, 0.4082,
        0.1605, 0.4080, 0.4341, 0.4592, 0.4111, 0.4082, 0.4080, 0.4080, 0.4082,
        0.4082, 0.4082, 0.4080, 0.5425, 0.4080, 0.4080, 0.5190, 0.4080, 0.4180,
        0.4082, 0.4089, 0.4077, 0.5044, 0.4084, 0.4099, 0.4080, 0.4951, 0.4065,
        0.4082, 0.1921, 0.4080, 0.4084, 0.4216, 0.4072, 0.2450, 0.4731, 0.4082,
        0.4082, 0.4255, 0.4197, 0.1064, 0.1621, 0.4094, 0.4136, 0.4465, 0.4084,
        0.4082, 0.4182, 0.4082, 0.4080, 0.4082, 0.4080, 0.4087, 0.4084, 0.4932,
        0.5615, 0.3801, 0.4082, 0.4082, 0.4766, 0.4045, 0.4082, 0.4709, 0.5049,
        0.4094, 0.4343, 0.4124, 0.4084, 0.4084, 0.4080, 0.4900, 0.4082, 0.5649,
        0.4058, 0.4080, 0.0614, 0.5044, 0.4771, 0.4082, 0.4126, 0.4080, 0.4463,
        0.4219, 0.4104, 0.4082, 0.4133, 0.4302, 0.4641, 0.4080, 0.4080, 0.4062,
        0.4141, 0.4299, 0.4209, 0.4482, 0.4080, 0.4080, 0.4080, 0.4082, 0.4082,
        0.4082, 0.4080, 0.4165, 0.4082, 0.4080, 0.4038, 0.4080, 0.2047, 0.2935,
        0.4080, 0.4080, 0.4158, 0.4077, 0.4082, 0.4082, 0.4080, 0.4082, 0.4690,
        0.4080, 0.4080, 0.4053, 0.4441, 0.4280, 0.4138, 0.4082, 0.4084, 0.5127,
        0.5420, 0.3098, 0.4124, 0.4084, 0.4092, 0.4229, 0.4080, 0.5479, 0.4084,
        0.3472, 0.4080, 0.4832, 0.4080, 0.4272, 0.4082, 0.4082, 0.4097, 0.4080,
        0.4031, 0.4087, 0.4080, 0.4080, 0.4150, 0.4661, 0.4084, 0.4080, 0.4934,
        0.2432, 0.4080, 0.4080, 0.4080, 0.4080, 0.4082, 0.4080, 0.4087, 0.4084,
        0.4082, 0.4062, 0.4319, 0.4070, 0.4375, 0.1772, 0.4080, 0.4131, 0.4082,
        0.4084, 0.4084, 0.4385, 0.4202, 0.2917, 0.4766, 0.4106, 0.4314, 0.4080,
        0.4683, 0.4080, 0.4084, 0.0724, 0.4082, 0.4080, 0.4199, 0.4299, 0.4084,
        0.4080, 0.4419, 0.5391, 0.4082, 0.4084, 0.4265, 0.4116, 0.4080, 0.2194,
        0.4080, 0.4082, 0.4082, 0.4084, 0.4082, 0.4082, 0.4507, 0.4080, 0.4092,
        0.5215, 0.4148, 0.4111, 0.3215, 0.1583, 0.4092, 0.1705, 0.4082, 0.4846,
        0.4084, 0.4045, 0.4204, 0.0260, 0.4272, 0.1301, 0.4084, 0.4080, 0.4248,
        0.4104, 0.4080, 0.4475, 0.4080, 0.4236, 0.4126, 0.4082, 0.4094, 0.4084,
        0.4084], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(6534.4443, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7734, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6834.6450, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7812, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6500.1602, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7969, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0584, 0.4150, 0.3223, 0.2069, 0.1855, 0.4136, 0.4143, 0.4790, 0.4148,
        0.4138, 0.4138, 0.3704, 0.4587, 0.4136, 0.4629, 0.5835, 0.4138, 0.4138,
        0.5029, 0.4136, 0.4136, 0.4126, 0.4136, 0.3760, 0.4370, 0.4138, 0.4138,
        0.4148, 0.4294, 0.3035, 0.4785, 0.4138, 0.4126, 0.4136, 0.4138, 0.4136,
        0.5259, 0.4368, 0.4138, 0.4358, 0.4136, 0.4153, 0.4136, 0.4133, 0.4136,
        0.3411, 0.4136, 0.4136, 0.4138, 0.4141, 0.4136, 0.4924, 0.4138, 0.4136,
        0.4136, 0.4133, 0.5381, 0.4097, 0.4136, 0.4133, 0.4138, 0.4160, 0.2010,
        0.4502, 0.3755, 0.3037, 0.4431, 0.4141, 0.4165, 0.4136, 0.4136, 0.4136,
        0.4136, 0.4617, 0.4133, 0.4138, 0.4136, 0.1562, 0.4136, 0.5146, 0.4136,
        0.4138, 0.4412, 0.4136, 0.4512, 0.2478, 0.4136, 0.4138, 0.4802, 0.4197,
        0.4478, 0.1648, 0.4141, 0.4136, 0.4189, 0.4138, 0.1903, 0.4482, 0.2239,
        0.4136, 0.4138, 0.4136, 0.4448, 0.4871, 0.4463, 0.4141, 0.4136, 0.4141,
        0.3435, 0.3464, 0.4138, 0.4141, 0.4097, 0.4792, 0.4136, 0.4136, 0.4136,
        0.4399, 0.4131, 0.3823, 0.4136, 0.4136, 0.4558, 0.4167, 0.5342, 0.4141,
        0.5293, 0.4341, 0.5156, 0.4136, 0.4170, 0.4136, 0.4102, 0.4136, 0.4153,
        0.2449, 0.4709, 0.4788, 0.4131, 0.4651, 0.4133, 0.4082, 0.4141, 0.4138,
        0.4133, 0.4138, 0.5869, 0.4365, 0.4138, 0.4138, 0.3005, 0.4868, 0.4141,
        0.4143, 0.4243, 0.4141, 0.4329, 0.4319, 0.4133, 0.4136, 0.4490, 0.4463,
        0.4138, 0.4138, 0.2368, 0.4885, 0.4451, 0.4270, 0.4141, 0.4163, 0.4153,
        0.4136, 0.4136, 0.5024, 0.4136, 0.4143, 0.4136, 0.4524, 0.4326, 0.4089,
        0.4138, 0.4136, 0.4131, 0.4746, 0.4138, 0.4260, 0.1271, 0.4136, 0.4727,
        0.4138, 0.4138, 0.4170, 0.4258, 0.4807, 0.4136, 0.5024, 0.4163, 0.4138,
        0.2939, 0.4143, 0.4080, 0.4839, 0.4165, 0.4136, 0.4143, 0.4138, 0.4302,
        0.4128, 0.4138, 0.3909, 0.5020, 0.5205, 0.5239, 0.5225, 0.4136, 0.4136,
        0.4802, 0.4136, 0.1500, 0.4136, 0.4138, 0.4138, 0.4138, 0.4426, 0.4136,
        0.4136, 0.1908, 0.4136, 0.3193, 0.5044, 0.4915, 0.4138, 0.4136, 0.4136,
        0.4136, 0.4138, 0.4355, 0.4136, 0.3901, 0.4109, 0.3799, 0.4402, 0.4148,
        0.4548, 0.4136, 0.4136, 0.4146, 0.4141, 0.4136, 0.4504, 0.4163, 0.4136,
        0.1230, 0.4136, 0.5239, 0.4355, 0.4158, 0.4136, 0.4138, 0.4189, 0.4133,
        0.4187, 0.4700, 0.4473, 0.5449, 0.4136, 0.5088, 0.4136, 0.4148, 0.4138,
        0.4126, 0.4136, 0.4136, 0.4133, 0.4131, 0.4175, 0.4136, 0.4136, 0.4136,
        0.2693, 0.5073, 0.4136, 0.0425, 0.4136, 0.4568, 0.4141, 0.4136, 0.4141,
        0.3877, 0.4138, 0.4136, 0.4136, 0.4133, 0.4138, 0.4136, 0.4167, 0.3298,
        0.4146, 0.4102, 0.4746, 0.4138, 0.4089, 0.4136, 0.4133, 0.4136, 0.4136,
        0.4138, 0.4136, 0.4136, 0.4138, 0.2651, 0.4131, 0.4138, 0.4062, 0.4136,
        0.4138, 0.5679, 0.4138, 0.4172, 0.4143, 0.4136, 0.4380, 0.4136, 0.4136,
        0.4136, 0.4138, 0.4141, 0.4250, 0.0868, 0.4304, 0.4150, 0.4136, 0.5083,
        0.1021, 0.5024, 0.4136, 0.4714, 0.5405, 0.4624, 0.4141, 0.1824, 0.4138,
        0.4175, 0.4329, 0.4136, 0.4106, 0.4731, 0.4980, 0.4138, 0.4653, 0.5493,
        0.4136, 0.4141, 0.4136, 0.4136, 0.4136, 0.4138, 0.4138, 0.4155, 0.4136,
        0.2722, 0.4136, 0.4697, 0.4138, 0.4641, 0.5146, 0.4138, 0.4136, 0.4136,
        0.4927, 0.4321, 0.4136, 0.4958, 0.4136, 0.1429, 0.4932, 0.5122, 0.4136,
        0.4136, 0.4097, 0.4136, 0.4143, 0.4136, 0.4136, 0.4136, 0.4136, 0.4136,
        0.4722, 0.0777, 0.4136, 0.4160, 0.4255, 0.4995, 0.4150, 0.4136, 0.4136,
        0.4138, 0.4143, 0.4541, 0.2166, 0.4136, 0.4221, 0.5293, 0.2220, 0.4136,
        0.1578, 0.4136, 0.4404, 0.4670, 0.4170, 0.4136, 0.4136, 0.4136, 0.4138,
        0.4138, 0.4136, 0.4136, 0.5508, 0.4136, 0.4136, 0.5269, 0.4136, 0.4243,
        0.4136, 0.4150, 0.4133, 0.5117, 0.4141, 0.4153, 0.4136, 0.5024, 0.4121,
        0.4138, 0.1903, 0.4136, 0.4138, 0.4263, 0.4128, 0.2430, 0.4810, 0.4138,
        0.4136, 0.4319, 0.4260, 0.1039, 0.1591, 0.4148, 0.4204, 0.4553, 0.4141,
        0.4136, 0.4246, 0.4138, 0.4136, 0.4136, 0.4136, 0.4143, 0.4138, 0.5015,
        0.5708, 0.3855, 0.4138, 0.4136, 0.4734, 0.4099, 0.4138, 0.4785, 0.5088,
        0.4150, 0.4402, 0.4182, 0.4138, 0.4138, 0.4136, 0.4924, 0.4136, 0.5737,
        0.4111, 0.4136, 0.0585, 0.5107, 0.4854, 0.4138, 0.4185, 0.4136, 0.4543,
        0.4192, 0.4158, 0.4136, 0.4197, 0.4368, 0.4709, 0.4136, 0.4136, 0.4119,
        0.4199, 0.4365, 0.4285, 0.4500, 0.4136, 0.4136, 0.4136, 0.4138, 0.4136,
        0.4136, 0.4136, 0.4224, 0.4138, 0.4136, 0.4094, 0.4136, 0.1993, 0.2917,
        0.4136, 0.4136, 0.4221, 0.4133, 0.4136, 0.4136, 0.4136, 0.4136, 0.4727,
        0.4136, 0.4136, 0.4109, 0.4517, 0.4343, 0.4202, 0.4138, 0.4138, 0.5195,
        0.5508, 0.3040, 0.4092, 0.4138, 0.4146, 0.4304, 0.4136, 0.5552, 0.4141,
        0.3457, 0.4136, 0.4910, 0.4136, 0.4346, 0.4136, 0.4136, 0.4153, 0.4136,
        0.4087, 0.4143, 0.4136, 0.4136, 0.4209, 0.4751, 0.4138, 0.4136, 0.5029,
        0.2386, 0.4136, 0.4136, 0.4136, 0.4136, 0.4136, 0.4136, 0.4141, 0.4138,
        0.4138, 0.4119, 0.4377, 0.4126, 0.4448, 0.1687, 0.4136, 0.4189, 0.4136,
        0.4141, 0.4138, 0.4465, 0.4277, 0.2981, 0.4797, 0.4167, 0.4373, 0.4136,
        0.4729, 0.4136, 0.4141, 0.0695, 0.4138, 0.4136, 0.4263, 0.4302, 0.4141,
        0.4136, 0.4436, 0.5464, 0.4138, 0.4138, 0.4326, 0.4177, 0.4136, 0.2155,
        0.4136, 0.4138, 0.4138, 0.4138, 0.4138, 0.4138, 0.4512, 0.4136, 0.4148,
        0.5269, 0.4207, 0.4172, 0.3162, 0.1526, 0.4148, 0.1636, 0.4138, 0.4900,
        0.4141, 0.4102, 0.4268, 0.0235, 0.4346, 0.1283, 0.4138, 0.4136, 0.4316,
        0.4163, 0.4060, 0.4548, 0.4136, 0.4304, 0.4182, 0.4136, 0.4153, 0.4141,
        0.4141], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 10:56:38,859][train_inner][INFO] - {"epoch": 20, "update": 19.651, "loss": "3.932", "ntokens": "149780", "nsentences": "539.75", "prob_perplexity": "138.288", "code_perplexity": "135.968", "temp": "1.902", "loss_0": "3.8", "loss_1": "0.113", "loss_2": "0.018", "accuracy": "0.33078", "wps": "37789.6", "ups": "0.25", "wpb": "149780", "bsz": "539.8", "num_updates": "10200", "lr": "0.000159375", "gnorm": "0.512", "loss_scale": "0.5", "train_wall": "791", "gb_free": "12.9", "wall": "41065"}
Parameter containing:
tensor([0.0623, 0.4036, 0.3127, 0.2123, 0.1829, 0.4021, 0.4026, 0.4624, 0.4031,
        0.4023, 0.4021, 0.3699, 0.4607, 0.4021, 0.4690, 0.5654, 0.4023, 0.4023,
        0.4866, 0.4021, 0.4021, 0.4111, 0.4021, 0.3809, 0.4238, 0.4023, 0.4023,
        0.4031, 0.4141, 0.3013, 0.4617, 0.4023, 0.4009, 0.4021, 0.4021, 0.4021,
        0.5073, 0.4426, 0.4023, 0.4238, 0.4021, 0.4033, 0.4021, 0.4019, 0.4021,
        0.3398, 0.4021, 0.4021, 0.4023, 0.4026, 0.4021, 0.4866, 0.4023, 0.4019,
        0.4021, 0.4016, 0.5215, 0.3982, 0.4021, 0.4019, 0.4021, 0.4043, 0.2040,
        0.4358, 0.3721, 0.3113, 0.4277, 0.4026, 0.4041, 0.4021, 0.4021, 0.4021,
        0.4021, 0.4458, 0.4019, 0.4023, 0.4021, 0.1691, 0.4021, 0.4966, 0.4021,
        0.4021, 0.4260, 0.4021, 0.4377, 0.2573, 0.4021, 0.4021, 0.4658, 0.4075,
        0.4329, 0.1740, 0.4023, 0.4021, 0.4067, 0.4023, 0.1964, 0.4331, 0.2350,
        0.4021, 0.4023, 0.4021, 0.4429, 0.4692, 0.4324, 0.4026, 0.4021, 0.4023,
        0.3523, 0.3501, 0.4023, 0.4023, 0.3982, 0.4746, 0.4021, 0.4021, 0.4021,
        0.4246, 0.4016, 0.3892, 0.4021, 0.4021, 0.4404, 0.4263, 0.5176, 0.4023,
        0.5122, 0.4209, 0.4993, 0.4021, 0.4045, 0.4021, 0.3987, 0.4021, 0.4031,
        0.2477, 0.4556, 0.4648, 0.4014, 0.4517, 0.4019, 0.3967, 0.4026, 0.4021,
        0.4019, 0.4023, 0.5698, 0.4243, 0.4023, 0.4023, 0.2957, 0.4690, 0.4026,
        0.4028, 0.4150, 0.4023, 0.4175, 0.4189, 0.4019, 0.4021, 0.4329, 0.4285,
        0.4021, 0.4023, 0.2465, 0.4719, 0.4309, 0.4138, 0.4023, 0.4045, 0.4036,
        0.4021, 0.4019, 0.4858, 0.4021, 0.4026, 0.4021, 0.4346, 0.4185, 0.3975,
        0.4021, 0.4021, 0.4016, 0.4587, 0.4023, 0.4128, 0.1395, 0.4021, 0.4565,
        0.4023, 0.4023, 0.4048, 0.4238, 0.4651, 0.4021, 0.4888, 0.4048, 0.4021,
        0.3071, 0.4026, 0.3965, 0.4717, 0.4043, 0.4021, 0.4026, 0.4023, 0.4167,
        0.4014, 0.4023, 0.3999, 0.4836, 0.5024, 0.5122, 0.5132, 0.4021, 0.4021,
        0.4712, 0.4021, 0.1599, 0.4021, 0.4023, 0.4023, 0.4023, 0.4280, 0.4021,
        0.4021, 0.1968, 0.4021, 0.3315, 0.4922, 0.4736, 0.4023, 0.4021, 0.4021,
        0.4021, 0.4023, 0.4399, 0.4021, 0.3933, 0.3994, 0.3877, 0.4258, 0.4033,
        0.4387, 0.4021, 0.4021, 0.4031, 0.4023, 0.4021, 0.4377, 0.4043, 0.4021,
        0.1349, 0.4021, 0.5073, 0.4414, 0.4041, 0.4021, 0.4023, 0.4067, 0.4019,
        0.4070, 0.4563, 0.4312, 0.5259, 0.4021, 0.4919, 0.4021, 0.4028, 0.4023,
        0.4009, 0.4021, 0.4021, 0.4019, 0.4016, 0.4055, 0.4021, 0.4021, 0.4021,
        0.2705, 0.4978, 0.4021, 0.0537, 0.4021, 0.4426, 0.4026, 0.4021, 0.4023,
        0.3867, 0.4021, 0.4021, 0.4021, 0.4019, 0.4023, 0.4021, 0.4045, 0.3408,
        0.4028, 0.3987, 0.4580, 0.4023, 0.3972, 0.4021, 0.4019, 0.4021, 0.4021,
        0.4023, 0.4021, 0.4021, 0.4021, 0.2759, 0.4016, 0.4021, 0.4128, 0.4021,
        0.4023, 0.5518, 0.4023, 0.4055, 0.4026, 0.4019, 0.4250, 0.4021, 0.4021,
        0.4021, 0.4023, 0.4026, 0.4136, 0.0984, 0.4324, 0.4036, 0.4021, 0.4915,
        0.1093, 0.4958, 0.4021, 0.4561, 0.5225, 0.4470, 0.4023, 0.1965, 0.4023,
        0.4050, 0.4194, 0.4021, 0.3992, 0.4709, 0.4812, 0.4023, 0.4487, 0.5332,
        0.4021, 0.4026, 0.4021, 0.4021, 0.4021, 0.4021, 0.4021, 0.4038, 0.4021,
        0.2727, 0.4021, 0.4536, 0.4021, 0.4639, 0.4985, 0.4023, 0.4021, 0.4021,
        0.4758, 0.4187, 0.4021, 0.4963, 0.4021, 0.1462, 0.4771, 0.4937, 0.4021,
        0.4021, 0.3982, 0.4021, 0.4028, 0.4021, 0.4021, 0.4021, 0.4021, 0.4021,
        0.4646, 0.0817, 0.4021, 0.4043, 0.4102, 0.4827, 0.4033, 0.4021, 0.4021,
        0.4021, 0.4026, 0.4358, 0.2229, 0.4021, 0.4106, 0.5107, 0.2316, 0.4021,
        0.1656, 0.4021, 0.4272, 0.4512, 0.4048, 0.4021, 0.4021, 0.4021, 0.4023,
        0.4023, 0.4021, 0.4021, 0.5337, 0.4021, 0.4021, 0.5107, 0.4021, 0.4111,
        0.4021, 0.4021, 0.4016, 0.4968, 0.4023, 0.4038, 0.4021, 0.4863, 0.4004,
        0.4021, 0.1943, 0.4021, 0.4023, 0.4189, 0.4014, 0.2477, 0.4644, 0.4023,
        0.4021, 0.4189, 0.4126, 0.1097, 0.1654, 0.4033, 0.4062, 0.4370, 0.4026,
        0.4021, 0.4114, 0.4021, 0.4021, 0.4021, 0.4021, 0.4026, 0.4023, 0.4841,
        0.5518, 0.3713, 0.4023, 0.4021, 0.4790, 0.3984, 0.4021, 0.4631, 0.4993,
        0.4033, 0.4280, 0.4060, 0.4023, 0.4023, 0.4021, 0.4861, 0.4021, 0.5566,
        0.3997, 0.4021, 0.0635, 0.4968, 0.4678, 0.4021, 0.4058, 0.4021, 0.4377,
        0.4248, 0.4043, 0.4021, 0.4067, 0.4229, 0.4565, 0.4021, 0.4021, 0.4001,
        0.4077, 0.4229, 0.4131, 0.4480, 0.4021, 0.4021, 0.4021, 0.4023, 0.4021,
        0.4021, 0.4021, 0.4102, 0.4023, 0.4021, 0.3979, 0.4021, 0.2103, 0.2949,
        0.4021, 0.4021, 0.4094, 0.4019, 0.4021, 0.4021, 0.4021, 0.4021, 0.4641,
        0.4021, 0.4021, 0.3994, 0.4358, 0.4211, 0.4075, 0.4021, 0.4023, 0.5073,
        0.5327, 0.3164, 0.4167, 0.4023, 0.4031, 0.4148, 0.4021, 0.5391, 0.4026,
        0.3469, 0.4021, 0.4746, 0.4021, 0.4189, 0.4021, 0.4021, 0.4036, 0.4021,
        0.3970, 0.4026, 0.4021, 0.4021, 0.4084, 0.4561, 0.4023, 0.4021, 0.4844,
        0.2499, 0.4021, 0.4021, 0.4021, 0.4021, 0.4021, 0.4021, 0.4026, 0.4023,
        0.4023, 0.4004, 0.4255, 0.4009, 0.4292, 0.1865, 0.4021, 0.4070, 0.4021,
        0.4023, 0.4023, 0.4304, 0.4124, 0.2844, 0.4717, 0.4041, 0.4253, 0.4021,
        0.4648, 0.4021, 0.4023, 0.0757, 0.4023, 0.4021, 0.4131, 0.4309, 0.4023,
        0.4021, 0.4382, 0.5308, 0.4021, 0.4023, 0.4197, 0.4050, 0.4021, 0.2233,
        0.4021, 0.4023, 0.4021, 0.4023, 0.4023, 0.4023, 0.4514, 0.4021, 0.4028,
        0.5176, 0.4084, 0.4048, 0.3267, 0.1647, 0.4028, 0.1777, 0.4023, 0.4810,
        0.4026, 0.3977, 0.4133, 0.0301, 0.4189, 0.1327, 0.4023, 0.4021, 0.4180,
        0.4041, 0.4092, 0.4382, 0.4021, 0.4158, 0.4067, 0.4021, 0.4031, 0.4026,
        0.4023], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6753.7114, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7266, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4874.0820, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7344, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Traceback (most recent call last):
  File "/root/anaconda3/bin/fairseq-hydra-train", line 8, in <module>
    sys.exit(cli_main())
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 87, in cli_main
    hydra_main()
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/main.py", line 32, in decorated_main
    _run_hydra(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 346, in _run_hydra
    run_and_report(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 198, in run_and_report
    return func()
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 347, in <lambda>
    lambda: hydra.run(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 107, in run
    return run_job(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/core/utils.py", line 129, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 27, in hydra_main
    _hydra_main(cfg)
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 56, in _hydra_main
    distributed_utils.call_main(cfg, pre_main, **kwargs)
  File "/home/Workspace/fairseq/fairseq/distributed/utils.py", line 379, in call_main
    torch.multiprocessing.spawn(
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 109, in join
    ready = multiprocessing.connection.wait(
  File "/root/anaconda3/lib/python3.9/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/root/anaconda3/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
Process SpawnProcess-4:
Process SpawnProcess-1:
Process SpawnProcess-2:
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
KeyboardInterrupt
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Parameter containing:
tensor([0.0587, 0.4202, 0.3284, 0.2053, 0.1879, 0.4187, 0.4194, 0.4858, 0.4202,
        0.4192, 0.4189, 0.3699, 0.4578, 0.4187, 0.4583, 0.5918, 0.4192, 0.4189,
        0.5112, 0.4187, 0.4187, 0.4126, 0.4189, 0.3738, 0.4429, 0.4192, 0.4189,
        0.4204, 0.4363, 0.3062, 0.4858, 0.4192, 0.4177, 0.4187, 0.4189, 0.4189,
        0.5337, 0.4336, 0.4189, 0.4412, 0.4187, 0.4207, 0.4187, 0.4185, 0.4187,
        0.3411, 0.4187, 0.4187, 0.4189, 0.4194, 0.4187, 0.4951, 0.4189, 0.4187,
        0.4187, 0.4185, 0.5459, 0.4148, 0.4187, 0.4187, 0.4189, 0.4211, 0.2006,
        0.4570, 0.3757, 0.3010, 0.4500, 0.4192, 0.4221, 0.4187, 0.4187, 0.4189,
        0.4187, 0.4697, 0.4187, 0.4189, 0.4187, 0.1510, 0.4187, 0.5229, 0.4187,
        0.4189, 0.4482, 0.4189, 0.4570, 0.2429, 0.4189, 0.4189, 0.4871, 0.4255,
        0.4548, 0.1616, 0.4192, 0.4189, 0.4246, 0.4189, 0.1866, 0.4548, 0.2185,
        0.4189, 0.4192, 0.4189, 0.4424, 0.4949, 0.4529, 0.4194, 0.4189, 0.4192,
        0.3391, 0.3455, 0.4192, 0.4192, 0.4150, 0.4790, 0.4187, 0.4187, 0.4187,
        0.4468, 0.4185, 0.3801, 0.4187, 0.4187, 0.4631, 0.4119, 0.5391, 0.4192,
        0.5371, 0.4402, 0.5234, 0.4187, 0.4226, 0.4187, 0.4153, 0.4189, 0.4207,
        0.2443, 0.4783, 0.4844, 0.4182, 0.4709, 0.4185, 0.4133, 0.4192, 0.4189,
        0.4187, 0.4189, 0.5952, 0.4421, 0.4192, 0.4189, 0.3022, 0.4951, 0.4192,
        0.4197, 0.4316, 0.4192, 0.4402, 0.4380, 0.4187, 0.4187, 0.4561, 0.4553,
        0.4189, 0.4189, 0.2344, 0.4956, 0.4517, 0.4331, 0.4192, 0.4214, 0.4207,
        0.4187, 0.4187, 0.5112, 0.4187, 0.4194, 0.4187, 0.4602, 0.4390, 0.4141,
        0.4189, 0.4187, 0.4185, 0.4824, 0.4189, 0.4319, 0.1215, 0.4187, 0.4800,
        0.4192, 0.4189, 0.4226, 0.4229, 0.4878, 0.4187, 0.5059, 0.4216, 0.4189,
        0.2878, 0.4194, 0.4133, 0.4888, 0.4219, 0.4187, 0.4194, 0.4189, 0.4358,
        0.4182, 0.4189, 0.3857, 0.5083, 0.5283, 0.5283, 0.5283, 0.4189, 0.4187,
        0.4858, 0.4189, 0.1464, 0.4187, 0.4189, 0.4189, 0.4189, 0.4497, 0.4187,
        0.4189, 0.1927, 0.4189, 0.3132, 0.5103, 0.4976, 0.4192, 0.4189, 0.4187,
        0.4189, 0.4189, 0.4333, 0.4187, 0.3884, 0.4163, 0.3752, 0.4468, 0.4202,
        0.4614, 0.4187, 0.4189, 0.4197, 0.4194, 0.4187, 0.4568, 0.4216, 0.4187,
        0.1179, 0.4189, 0.5317, 0.4319, 0.4209, 0.4187, 0.4189, 0.4241, 0.4187,
        0.4241, 0.4749, 0.4548, 0.5527, 0.4189, 0.5166, 0.4189, 0.4202, 0.4189,
        0.4177, 0.4189, 0.4187, 0.4187, 0.4185, 0.4231, 0.4187, 0.4187, 0.4189,
        0.2698, 0.5117, 0.4187, 0.0381, 0.4187, 0.4639, 0.4192, 0.4187, 0.4192,
        0.3911, 0.4189, 0.4187, 0.4187, 0.4185, 0.4189, 0.4187, 0.4221, 0.3242,
        0.4199, 0.4153, 0.4822, 0.4192, 0.4141, 0.4187, 0.4187, 0.4189, 0.4187,
        0.4189, 0.4187, 0.4189, 0.4189, 0.2600, 0.4185, 0.4189, 0.4036, 0.4187,
        0.4189, 0.5752, 0.4192, 0.4226, 0.4194, 0.4187, 0.4436, 0.4187, 0.4187,
        0.4187, 0.4189, 0.4192, 0.4304, 0.0811, 0.4290, 0.4202, 0.4187, 0.5161,
        0.0993, 0.5034, 0.4189, 0.4783, 0.5479, 0.4690, 0.4192, 0.1771, 0.4189,
        0.4236, 0.4387, 0.4189, 0.4158, 0.4771, 0.5054, 0.4192, 0.4734, 0.5571,
        0.4187, 0.4192, 0.4187, 0.4187, 0.4189, 0.4189, 0.4189, 0.4209, 0.4187,
        0.2698, 0.4189, 0.4768, 0.4189, 0.4624, 0.5220, 0.4192, 0.4187, 0.4187,
        0.4998, 0.4387, 0.4187, 0.4937, 0.4187, 0.1425, 0.5015, 0.5205, 0.4187,
        0.4187, 0.4148, 0.4187, 0.4194, 0.4187, 0.4187, 0.4187, 0.4187, 0.4189,
        0.4736, 0.0757, 0.4187, 0.4211, 0.4324, 0.5068, 0.4204, 0.4187, 0.4187,
        0.4189, 0.4194, 0.4622, 0.2136, 0.4187, 0.4272, 0.5352, 0.2167, 0.4189,
        0.1547, 0.4187, 0.4463, 0.4736, 0.4224, 0.4189, 0.4187, 0.4187, 0.4192,
        0.4189, 0.4189, 0.4187, 0.5576, 0.4187, 0.4187, 0.5342, 0.4187, 0.4302,
        0.4189, 0.4207, 0.4185, 0.5186, 0.4194, 0.4207, 0.4187, 0.5103, 0.4172,
        0.4189, 0.1899, 0.4187, 0.4192, 0.4292, 0.4180, 0.2402, 0.4883, 0.4189,
        0.4189, 0.4375, 0.4316, 0.1028, 0.1550, 0.4202, 0.4272, 0.4639, 0.4192,
        0.4187, 0.4304, 0.4189, 0.4187, 0.4189, 0.4187, 0.4197, 0.4192, 0.5093,
        0.5796, 0.3911, 0.4189, 0.4189, 0.4709, 0.4153, 0.4189, 0.4851, 0.5078,
        0.4204, 0.4456, 0.4238, 0.4189, 0.4189, 0.4187, 0.4934, 0.4189, 0.5830,
        0.4165, 0.4187, 0.0566, 0.5156, 0.4934, 0.4189, 0.4238, 0.4187, 0.4617,
        0.4158, 0.4209, 0.4189, 0.4260, 0.4434, 0.4768, 0.4187, 0.4187, 0.4170,
        0.4258, 0.4424, 0.4353, 0.4509, 0.4187, 0.4187, 0.4187, 0.4189, 0.4187,
        0.4187, 0.4187, 0.4280, 0.4189, 0.4187, 0.4146, 0.4187, 0.1947, 0.2915,
        0.4187, 0.4187, 0.4277, 0.4185, 0.4189, 0.4189, 0.4187, 0.4187, 0.4761,
        0.4187, 0.4187, 0.4160, 0.4590, 0.4402, 0.4258, 0.4189, 0.4192, 0.5273,
        0.5596, 0.2979, 0.4060, 0.4192, 0.4199, 0.4373, 0.4187, 0.5630, 0.4192,
        0.3450, 0.4187, 0.4983, 0.4187, 0.4414, 0.4189, 0.4189, 0.4204, 0.4187,
        0.4138, 0.4197, 0.4187, 0.4187, 0.4268, 0.4834, 0.4192, 0.4187, 0.5073,
        0.2335, 0.4187, 0.4187, 0.4187, 0.4187, 0.4189, 0.4187, 0.4194, 0.4192,
        0.4189, 0.4170, 0.4438, 0.4180, 0.4524, 0.1614, 0.4187, 0.4243, 0.4189,
        0.4192, 0.4192, 0.4534, 0.4346, 0.3037, 0.4827, 0.4226, 0.4426, 0.4187,
        0.4780, 0.4187, 0.4192, 0.0665, 0.4189, 0.4187, 0.4319, 0.4285, 0.4192,
        0.4187, 0.4441, 0.5542, 0.4189, 0.4192, 0.4385, 0.4231, 0.4187, 0.2124,
        0.4187, 0.4189, 0.4189, 0.4192, 0.4189, 0.4189, 0.4507, 0.4187, 0.4204,
        0.5273, 0.4263, 0.4226, 0.3103, 0.1473, 0.4202, 0.1571, 0.4189, 0.4951,
        0.4192, 0.4172, 0.4326, 0.0209, 0.4414, 0.1261, 0.4192, 0.4187, 0.4373,
        0.4216, 0.4036, 0.4622, 0.4187, 0.4373, 0.4233, 0.4189, 0.4211, 0.4192,
        0.4192], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(5637.5088, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4297, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0594, 0.4124, 0.3193, 0.2090, 0.1842, 0.4109, 0.4116, 0.4749, 0.4121,
        0.4111, 0.4109, 0.3704, 0.4597, 0.4109, 0.4641, 0.5791, 0.4111, 0.4111,
        0.4990, 0.4109, 0.4109, 0.4114, 0.4109, 0.3770, 0.4338, 0.4111, 0.4111,
        0.4121, 0.4255, 0.3035, 0.4749, 0.4111, 0.4097, 0.4109, 0.4109, 0.4109,
        0.5215, 0.4380, 0.4111, 0.4329, 0.4109, 0.4124, 0.4109, 0.4106, 0.4109,
        0.3398, 0.4109, 0.4109, 0.4111, 0.4114, 0.4109, 0.4915, 0.4111, 0.4109,
        0.4109, 0.4104, 0.5347, 0.4070, 0.4109, 0.4106, 0.4109, 0.4133, 0.2013,
        0.4465, 0.3735, 0.3054, 0.4395, 0.4114, 0.4136, 0.4109, 0.4109, 0.4109,
        0.4109, 0.4585, 0.4106, 0.4111, 0.4109, 0.1594, 0.4109, 0.5098, 0.4109,
        0.4111, 0.4377, 0.4109, 0.4480, 0.2498, 0.4109, 0.4109, 0.4771, 0.4167,
        0.4443, 0.1669, 0.4111, 0.4109, 0.4160, 0.4111, 0.1917, 0.4446, 0.2271,
        0.4109, 0.4111, 0.4109, 0.4448, 0.4827, 0.4431, 0.4114, 0.4109, 0.4111,
        0.3462, 0.3477, 0.4111, 0.4114, 0.4070, 0.4790, 0.4109, 0.4109, 0.4109,
        0.4363, 0.4104, 0.3840, 0.4109, 0.4109, 0.4524, 0.4197, 0.5312, 0.4111,
        0.5254, 0.4312, 0.5117, 0.4109, 0.4141, 0.4109, 0.4075, 0.4109, 0.4124,
        0.2455, 0.4675, 0.4753, 0.4102, 0.4619, 0.4106, 0.4055, 0.4114, 0.4109,
        0.4106, 0.4111, 0.5830, 0.4338, 0.4111, 0.4111, 0.2996, 0.4827, 0.4114,
        0.4116, 0.4211, 0.4114, 0.4292, 0.4285, 0.4106, 0.4109, 0.4453, 0.4417,
        0.4111, 0.4111, 0.2383, 0.4846, 0.4414, 0.4238, 0.4111, 0.4133, 0.4126,
        0.4109, 0.4109, 0.4990, 0.4109, 0.4116, 0.4109, 0.4482, 0.4292, 0.4062,
        0.4111, 0.4109, 0.4104, 0.4709, 0.4111, 0.4229, 0.1306, 0.4109, 0.4683,
        0.4111, 0.4111, 0.4141, 0.4250, 0.4773, 0.4109, 0.5000, 0.4136, 0.4109,
        0.2976, 0.4114, 0.4053, 0.4810, 0.4136, 0.4109, 0.4116, 0.4111, 0.4270,
        0.4102, 0.4111, 0.3933, 0.4976, 0.5161, 0.5215, 0.5195, 0.4109, 0.4109,
        0.4773, 0.4109, 0.1521, 0.4109, 0.4111, 0.4111, 0.4111, 0.4395, 0.4109,
        0.4109, 0.1921, 0.4109, 0.3223, 0.5010, 0.4871, 0.4111, 0.4109, 0.4109,
        0.4109, 0.4111, 0.4363, 0.4109, 0.3911, 0.4082, 0.3816, 0.4368, 0.4121,
        0.4507, 0.4109, 0.4109, 0.4119, 0.4114, 0.4109, 0.4475, 0.4136, 0.4109,
        0.1255, 0.4109, 0.5200, 0.4370, 0.4128, 0.4109, 0.4111, 0.4160, 0.4106,
        0.4160, 0.4670, 0.4434, 0.5405, 0.4109, 0.5049, 0.4109, 0.4119, 0.4111,
        0.4097, 0.4109, 0.4109, 0.4106, 0.4104, 0.4148, 0.4109, 0.4109, 0.4109,
        0.2693, 0.5068, 0.4109, 0.0450, 0.4109, 0.4539, 0.4114, 0.4109, 0.4111,
        0.3867, 0.4111, 0.4109, 0.4109, 0.4106, 0.4111, 0.4109, 0.4138, 0.3323,
        0.4119, 0.4075, 0.4705, 0.4111, 0.4060, 0.4109, 0.4106, 0.4109, 0.4109,
        0.4111, 0.4109, 0.4109, 0.4111, 0.2676, 0.4104, 0.4111, 0.4070, 0.4109,
        0.4111, 0.5640, 0.4111, 0.4146, 0.4116, 0.4106, 0.4351, 0.4109, 0.4109,
        0.4109, 0.4111, 0.4114, 0.4224, 0.0894, 0.4314, 0.4124, 0.4109, 0.5044,
        0.1033, 0.5010, 0.4109, 0.4680, 0.5356, 0.4587, 0.4111, 0.1853, 0.4111,
        0.4146, 0.4294, 0.4109, 0.4080, 0.4729, 0.4941, 0.4111, 0.4612, 0.5464,
        0.4109, 0.4114, 0.4109, 0.4109, 0.4109, 0.4109, 0.4111, 0.4128, 0.4109,
        0.2737, 0.4109, 0.4656, 0.4109, 0.4636, 0.5112, 0.4111, 0.4109, 0.4109,
        0.4890, 0.4292, 0.4109, 0.4973, 0.4109, 0.1443, 0.4893, 0.5073, 0.4109,
        0.4109, 0.4070, 0.4109, 0.4116, 0.4109, 0.4109, 0.4109, 0.4109, 0.4109,
        0.4707, 0.0794, 0.4109, 0.4133, 0.4219, 0.4958, 0.4121, 0.4109, 0.4109,
        0.4109, 0.4116, 0.4495, 0.2186, 0.4109, 0.4194, 0.5249, 0.2247, 0.4109,
        0.1595, 0.4109, 0.4373, 0.4631, 0.4141, 0.4109, 0.4109, 0.4109, 0.4111,
        0.4111, 0.4109, 0.4109, 0.5469, 0.4109, 0.4109, 0.5234, 0.4109, 0.4211,
        0.4109, 0.4119, 0.4104, 0.5083, 0.4114, 0.4126, 0.4109, 0.4985, 0.4094,
        0.4109, 0.1913, 0.4109, 0.4111, 0.4236, 0.4102, 0.2439, 0.4773, 0.4111,
        0.4109, 0.4290, 0.4229, 0.1049, 0.1610, 0.4121, 0.4170, 0.4509, 0.4114,
        0.4109, 0.4214, 0.4109, 0.4109, 0.4109, 0.4109, 0.4116, 0.4111, 0.4973,
        0.5664, 0.3843, 0.4111, 0.4109, 0.4749, 0.4072, 0.4109, 0.4749, 0.5078,
        0.4124, 0.4373, 0.4153, 0.4111, 0.4111, 0.4109, 0.4917, 0.4109, 0.5698,
        0.4084, 0.4109, 0.0598, 0.5073, 0.4812, 0.4109, 0.4155, 0.4109, 0.4507,
        0.4202, 0.4131, 0.4109, 0.4165, 0.4336, 0.4675, 0.4109, 0.4109, 0.4092,
        0.4170, 0.4331, 0.4248, 0.4490, 0.4109, 0.4109, 0.4109, 0.4111, 0.4109,
        0.4109, 0.4109, 0.4194, 0.4111, 0.4109, 0.4067, 0.4109, 0.2024, 0.2925,
        0.4109, 0.4109, 0.4192, 0.4106, 0.4109, 0.4109, 0.4109, 0.4109, 0.4700,
        0.4109, 0.4109, 0.4082, 0.4478, 0.4309, 0.4172, 0.4109, 0.4111, 0.5156,
        0.5464, 0.3071, 0.4102, 0.4111, 0.4119, 0.4265, 0.4109, 0.5518, 0.4114,
        0.3462, 0.4109, 0.4871, 0.4109, 0.4309, 0.4109, 0.4109, 0.4124, 0.4109,
        0.4058, 0.4114, 0.4109, 0.4109, 0.4180, 0.4705, 0.4111, 0.4109, 0.4980,
        0.2416, 0.4109, 0.4109, 0.4109, 0.4109, 0.4109, 0.4109, 0.4114, 0.4111,
        0.4111, 0.4092, 0.4346, 0.4099, 0.4412, 0.1731, 0.4109, 0.4160, 0.4109,
        0.4114, 0.4111, 0.4424, 0.4241, 0.2949, 0.4788, 0.4138, 0.4343, 0.4109,
        0.4709, 0.4109, 0.4111, 0.0712, 0.4111, 0.4109, 0.4231, 0.4302, 0.4114,
        0.4109, 0.4446, 0.5425, 0.4109, 0.4111, 0.4297, 0.4146, 0.4109, 0.2173,
        0.4109, 0.4111, 0.4111, 0.4111, 0.4111, 0.4111, 0.4512, 0.4109, 0.4121,
        0.5239, 0.4177, 0.4141, 0.3188, 0.1553, 0.4119, 0.1667, 0.4111, 0.4868,
        0.4114, 0.4080, 0.4233, 0.0250, 0.4309, 0.1287, 0.4111, 0.4109, 0.4282,
        0.4133, 0.4067, 0.4507, 0.4109, 0.4270, 0.4155, 0.4109, 0.4124, 0.4114,
        0.4111], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6383.2812, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6602, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Parameter containing:
tensor([0.0637, 0.3970, 0.3066, 0.2168, 0.1825, 0.3955, 0.3960, 0.4536, 0.3965,
        0.3960, 0.3958, 0.3669, 0.4614, 0.3955, 0.4707, 0.5552, 0.3960, 0.3958,
        0.4763, 0.3955, 0.3955, 0.4106, 0.3958, 0.3855, 0.4167, 0.3960, 0.3958,
        0.3965, 0.4058, 0.2996, 0.4517, 0.3960, 0.3945, 0.3955, 0.3958, 0.3958,
        0.4968, 0.4473, 0.3958, 0.4165, 0.3955, 0.3965, 0.3955, 0.3953, 0.3955,
        0.3408, 0.3955, 0.3955, 0.3958, 0.3962, 0.3955, 0.4856, 0.3958, 0.3955,
        0.3955, 0.3953, 0.5112, 0.3916, 0.3955, 0.3955, 0.3958, 0.3977, 0.2051,
        0.4272, 0.3711, 0.3164, 0.4192, 0.3960, 0.3972, 0.3955, 0.3955, 0.3958,
        0.3955, 0.4365, 0.3955, 0.3958, 0.3955, 0.1758, 0.3955, 0.4871, 0.3955,
        0.3958, 0.4177, 0.3958, 0.4304, 0.2632, 0.3955, 0.3955, 0.4575, 0.4001,
        0.4248, 0.1782, 0.3960, 0.3958, 0.3999, 0.3958, 0.2000, 0.4250, 0.2419,
        0.3958, 0.3960, 0.3958, 0.4397, 0.4590, 0.4243, 0.3960, 0.3958, 0.3960,
        0.3562, 0.3499, 0.3960, 0.3960, 0.3918, 0.4729, 0.3955, 0.3955, 0.3955,
        0.4158, 0.3953, 0.3914, 0.3955, 0.3955, 0.4316, 0.4285, 0.5073, 0.3960,
        0.5024, 0.4133, 0.4922, 0.3955, 0.3977, 0.3955, 0.3921, 0.3955, 0.3965,
        0.2485, 0.4458, 0.4570, 0.3950, 0.4436, 0.3953, 0.3901, 0.3960, 0.3958,
        0.3955, 0.3958, 0.5625, 0.4175, 0.3958, 0.3958, 0.2917, 0.4585, 0.3960,
        0.3965, 0.4131, 0.3960, 0.4094, 0.4109, 0.3955, 0.3955, 0.4236, 0.4187,
        0.3958, 0.3958, 0.2517, 0.4626, 0.4226, 0.4062, 0.3960, 0.3979, 0.3970,
        0.3955, 0.3955, 0.4749, 0.3955, 0.3960, 0.3955, 0.4255, 0.4102, 0.3909,
        0.3958, 0.3955, 0.3953, 0.4490, 0.3958, 0.4058, 0.1464, 0.3955, 0.4475,
        0.3960, 0.3958, 0.3979, 0.4221, 0.4561, 0.3955, 0.4846, 0.3982, 0.3958,
        0.3152, 0.3962, 0.3901, 0.4673, 0.3975, 0.3955, 0.3962, 0.3958, 0.4097,
        0.3950, 0.3958, 0.4050, 0.4729, 0.4932, 0.5034, 0.5088, 0.3958, 0.3955,
        0.4680, 0.3958, 0.1663, 0.3955, 0.3958, 0.3958, 0.3958, 0.4197, 0.3955,
        0.3958, 0.1992, 0.3958, 0.3367, 0.4851, 0.4636, 0.3960, 0.3958, 0.3955,
        0.3958, 0.3958, 0.4414, 0.3955, 0.3962, 0.3928, 0.3916, 0.4182, 0.3970,
        0.4304, 0.3955, 0.3955, 0.3965, 0.3958, 0.3955, 0.4299, 0.3975, 0.3955,
        0.1418, 0.3955, 0.4983, 0.4424, 0.3977, 0.3955, 0.3958, 0.4001, 0.3955,
        0.4004, 0.4485, 0.4233, 0.5151, 0.3958, 0.4827, 0.3958, 0.3962, 0.3958,
        0.3945, 0.3958, 0.3955, 0.3953, 0.3953, 0.3982, 0.3955, 0.3955, 0.3958,
        0.2732, 0.4878, 0.3955, 0.0601, 0.3955, 0.4343, 0.3960, 0.3955, 0.3960,
        0.3860, 0.3958, 0.3955, 0.3955, 0.3953, 0.3958, 0.3955, 0.3977, 0.3450,
        0.3962, 0.3921, 0.4492, 0.3960, 0.3909, 0.3955, 0.3955, 0.3955, 0.3955,
        0.3958, 0.3955, 0.3958, 0.3958, 0.2825, 0.3953, 0.3958, 0.4165, 0.3955,
        0.3958, 0.5420, 0.3960, 0.3989, 0.3960, 0.3955, 0.4182, 0.3955, 0.3955,
        0.3955, 0.3958, 0.3960, 0.4072, 0.1031, 0.4341, 0.3970, 0.3955, 0.4846,
        0.1129, 0.4954, 0.3958, 0.4480, 0.5142, 0.4387, 0.3958, 0.2050, 0.3958,
        0.3979, 0.4121, 0.3958, 0.3926, 0.4712, 0.4714, 0.3960, 0.4395, 0.5229,
        0.3955, 0.3960, 0.3955, 0.3955, 0.3958, 0.3958, 0.3958, 0.3972, 0.3955,
        0.2749, 0.3958, 0.4446, 0.3958, 0.4631, 0.4900, 0.3958, 0.3955, 0.3955,
        0.4668, 0.4111, 0.3955, 0.4937, 0.3955, 0.1476, 0.4678, 0.4829, 0.3955,
        0.3955, 0.3916, 0.3955, 0.3962, 0.3955, 0.3955, 0.3955, 0.3955, 0.3958,
        0.4558, 0.0817, 0.3955, 0.3979, 0.4026, 0.4739, 0.3965, 0.3955, 0.3955,
        0.3958, 0.3962, 0.4275, 0.2264, 0.3955, 0.4041, 0.5010, 0.2384, 0.3958,
        0.1708, 0.3955, 0.4199, 0.4426, 0.3979, 0.3955, 0.3955, 0.3955, 0.3958,
        0.3958, 0.3958, 0.3955, 0.5239, 0.3955, 0.3955, 0.5015, 0.3955, 0.4041,
        0.3958, 0.3950, 0.3953, 0.4880, 0.3958, 0.3972, 0.3955, 0.4768, 0.3940,
        0.3958, 0.1969, 0.3955, 0.3960, 0.4148, 0.3948, 0.2512, 0.4551, 0.3958,
        0.3955, 0.4114, 0.4055, 0.1136, 0.1689, 0.3965, 0.3987, 0.4265, 0.3960,
        0.3955, 0.4041, 0.3958, 0.3955, 0.3958, 0.3955, 0.3960, 0.3960, 0.4736,
        0.5420, 0.3630, 0.3958, 0.3958, 0.4797, 0.3918, 0.3958, 0.4551, 0.4939,
        0.3967, 0.4211, 0.3992, 0.3958, 0.3958, 0.3955, 0.4780, 0.3958, 0.5469,
        0.3933, 0.3955, 0.0660, 0.4861, 0.4580, 0.3958, 0.3987, 0.3955, 0.4290,
        0.4272, 0.3977, 0.3958, 0.3997, 0.4153, 0.4490, 0.3955, 0.3955, 0.3938,
        0.4006, 0.4155, 0.4053, 0.4485, 0.3955, 0.3955, 0.3955, 0.3958, 0.3955,
        0.3955, 0.3955, 0.4033, 0.3958, 0.3955, 0.3914, 0.3955, 0.2159, 0.2964,
        0.3955, 0.3955, 0.4023, 0.3953, 0.3958, 0.3955, 0.3955, 0.3955, 0.4587,
        0.3955, 0.3955, 0.3928, 0.4265, 0.4131, 0.4006, 0.3958, 0.3958, 0.5034,
        0.5225, 0.3228, 0.4204, 0.3960, 0.3967, 0.4067, 0.3955, 0.5298, 0.3960,
        0.3484, 0.3955, 0.4653, 0.3955, 0.4102, 0.3958, 0.3958, 0.3967, 0.3955,
        0.3904, 0.3960, 0.3955, 0.3955, 0.4014, 0.4468, 0.3960, 0.3955, 0.4751,
        0.2561, 0.3955, 0.3955, 0.3955, 0.3955, 0.3958, 0.3955, 0.3962, 0.3958,
        0.3958, 0.3938, 0.4199, 0.3940, 0.4204, 0.1962, 0.3955, 0.4001, 0.3955,
        0.3960, 0.3958, 0.4214, 0.4038, 0.2744, 0.4653, 0.3975, 0.4185, 0.3955,
        0.4626, 0.3955, 0.3960, 0.0790, 0.3958, 0.3955, 0.4060, 0.4297, 0.3960,
        0.3955, 0.4338, 0.5215, 0.3958, 0.3960, 0.4126, 0.3982, 0.3955, 0.2261,
        0.3955, 0.3958, 0.3958, 0.3960, 0.3958, 0.3958, 0.4509, 0.3955, 0.3962,
        0.5146, 0.4016, 0.3982, 0.3313, 0.1699, 0.3960, 0.1851, 0.3958, 0.4795,
        0.3960, 0.3931, 0.4062, 0.0309, 0.4104, 0.1365, 0.3960, 0.3955, 0.4102,
        0.3972, 0.4106, 0.4285, 0.3955, 0.4072, 0.4001, 0.3958, 0.3962, 0.3960,
        0.3958], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6140.7065, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7812, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5795.3594, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6875, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5724.1382, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7578, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7623.8076, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6250, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
/root/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
