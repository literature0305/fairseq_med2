DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=hydra_train
INFO:fairseq.distributed.utils:distributed init (rank 2): tcp://localhost:52399
[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:52399 (errno: 99 - Cannot assign requested address).
INFO:fairseq.distributed.utils:distributed init (rank 0): tcp://localhost:52399
INFO:fairseq.distributed.utils:distributed init (rank 3): tcp://localhost:52399
INFO:fairseq.distributed.utils:distributed init (rank 1): tcp://localhost:52399
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 3
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 0
INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 1
INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
INFO:fairseq.distributed.utils:initialized host 19e0b2a19b1c as rank 2
[2023-09-12 11:05:24,826][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:52399', 'distributed_port': 52399, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1900000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1900000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.1, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'audio_pretraining', 'data': '/home/Workspace/fairseq/data', 'labels': None, 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': 250000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': none, 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 1}, 'criterion': {'_name': 'wav2vec', 'infonce': True, 'loss_weights': [0.1, 10.0], 'log_keys': ['prob_perplexity', 'code_perplexity', 'temp']}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 32000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 400000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2023-09-12 11:05:26,270][fairseq_cli.train][INFO] - Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
        (3): GELU(approximate='none')
      )
      (1-4): 4 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
      (5-6): 2 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.1, inplace=False)
  (dropout_features): Dropout(p=0.1, inplace=False)
  (quantizer): GumbelVectorQuantizer(
    (weight_proj): Linear(in_features=512, out_features=640, bias=True)
  )
  (project_q): Linear(in_features=256, out_features=256, bias=True)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): Linear(in_features=768, out_features=256, bias=True)
)
[2023-09-12 11:05:26,272][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2023-09-12 11:05:26,272][fairseq_cli.train][INFO] - model: Wav2Vec2Model
[2023-09-12 11:05:26,272][fairseq_cli.train][INFO] - criterion: Wav2vecCriterion
[2023-09-12 11:05:26,273][fairseq_cli.train][INFO] - num. shared model params: 95,045,248 (num. trained: 95,045,248)
[2023-09-12 11:05:26,274][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2023-09-12 11:05:26,280][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 5474, skipped 93 samples
[2023-09-12 11:05:27,280][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0
[2023-09-12 11:05:27,331][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-09-12 11:05:27,332][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.1.0.bias
[2023-09-12 11:05:27,332][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.2.0.bias
[2023-09-12 11:05:27,332][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.3.0.bias
[2023-09-12 11:05:27,332][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.4.0.bias
[2023-09-12 11:05:27,332][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.5.0.bias
[2023-09-12 11:05:27,333][fairseq.trainer][INFO] - detected shared parameter: feature_extractor.conv_layers.0.0.bias <- feature_extractor.conv_layers.6.0.bias
[2023-09-12 11:05:27,448][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2023-09-12 11:05:27,448][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 11:05:27,448][fairseq.utils][INFO] - rank   1: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 11:05:27,448][fairseq.utils][INFO] - rank   2: capabilities =  8.9  ; total memory = 23.650 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 11:05:27,448][fairseq.utils][INFO] - rank   3: capabilities =  8.9  ; total memory = 23.648 GB ; name = NVIDIA GeForce RTX 4090                 
[2023-09-12 11:05:27,448][fairseq.utils][INFO] - ***********************CUDA enviroments for all 4 workers***********************
[2023-09-12 11:05:27,449][fairseq_cli.train][INFO] - training on 4 devices (GPUs/TPUs)
[2023-09-12 11:05:27,449][fairseq_cli.train][INFO] - max tokens per device = 1900000 and max sentences per device = None
[2023-09-12 11:05:27,450][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2023-09-12 11:05:27,450][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2023-09-12 11:05:27,450][fairseq.trainer][INFO] - loading train data for epoch 1
[2023-09-12 11:05:27,717][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 280531, skipped 710 samples
[2023-09-12 11:05:27,796][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 11:05:27,796][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2023-09-12 11:05:27,796][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2023-09-12 11:05:27,796][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2023-09-12 11:05:27,797][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2023-09-12 11:05:28,105][fairseq_cli.train][INFO] - begin dry-run validation on "valid" subset
[2023-09-12 11:05:28,106][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 11:05:28,106][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2023-09-12 11:05:28,106][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2023-09-12 11:05:28,106][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2023-09-12 11:05:28,106][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2023-09-12 11:05:44,413][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 11:05:44,420][fairseq.trainer][INFO] - begin training epoch 1
[2023-09-12 11:05:44,420][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 11:06:06,234][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
self.scaling_factor_for_vector: Parameter containing:
tensor([ 1.5259e-04,  1.5247e-04,  2.3234e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5378e-04,  1.5247e-04,  1.5247e-04,  1.6141e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,  1.5283e-04,
         1.5271e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5259e-04,  1.5259e-04,
         1.5438e-04,  1.5569e-04,  1.6606e-04,  1.6856e-04,  1.5247e-04,
         1.5247e-04,  1.8072e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,
         1.5271e-04,  1.5378e-04,  1.5247e-04,  1.8489e-04,  1.5247e-04,
         1.5247e-04,  1.5318e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5712e-04,  1.5247e-04,
         1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5390e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5247e-04,  1.5306e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.6260e-04,  1.5247e-04,  1.8561e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5247e-04,  1.5581e-04,  1.5259e-04,  1.5247e-04,
         1.5295e-04,  1.5426e-04,  2.1124e-04,  1.5247e-04,  1.5259e-04,
         1.8823e-04,  1.5247e-04,  1.6153e-04,  1.5259e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5271e-04,
         1.5259e-04,  1.5247e-04,  1.5354e-04,  1.7047e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5283e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.7464e-04,  1.5247e-04,
         1.5247e-04,  1.5318e-04,  1.5259e-04,  1.5247e-04,  1.5259e-04,
         1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5295e-04,  1.5247e-04,
         1.5283e-04,  1.5247e-04,  1.5259e-04,  1.9872e-04,  1.5259e-04,
         2.0754e-04,  1.5247e-04,  1.5247e-04,  1.9503e-04,  1.5247e-04,
         1.5271e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.6737e-04,
         1.5247e-04,  1.5259e-04,  1.5259e-04,  1.5485e-04,  1.5342e-04,
         1.5271e-04,  1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,
         1.5950e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5259e-04,  2.1064e-04,  1.5247e-04,  1.5604e-04,
         1.5569e-04,  1.5533e-04,  1.5295e-04,  1.5259e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.6630e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.6057e-04,  1.5545e-04,  1.5986e-04,  1.5247e-04,
         1.9968e-04,  1.2839e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5306e-04,  1.5259e-04,  1.5259e-04,  1.5259e-04,  1.5378e-04,
         1.5247e-04,  1.5247e-04,  2.2876e-04,  1.5247e-04,  1.0496e-04,
         1.5247e-04,  1.5271e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5295e-04,
         1.5259e-04,  1.6701e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.9956e-04,  1.5271e-04,  1.5247e-04,  1.5247e-04,  1.7190e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5271e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  9.9719e-05,  1.5247e-04,
         1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5259e-04,  2.3901e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5497e-04,  1.5247e-04,  1.5259e-04,  1.5259e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5366e-04,  1.5247e-04,  1.7869e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5295e-04,  1.5259e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5318e-04,  1.5271e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.8430e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5414e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5414e-04,  1.5247e-04,  1.6475e-04,  1.5247e-04,
         1.9634e-04,  1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5259e-04,  1.5247e-04,  1.5271e-04,  1.5283e-04,
         1.6296e-04,  2.1195e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5378e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5724e-04,  1.5247e-04,  1.5259e-04,  1.5867e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5295e-04,  1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5724e-04,  1.5247e-04,  1.5295e-04,
         1.7655e-04,  1.5247e-04,  1.5247e-04,  1.6189e-04,  1.5247e-04,
         1.9681e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5342e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         2.2888e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,
         1.5283e-04,  1.5247e-04,  1.6928e-04,  1.6165e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5581e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.9896e-04,  1.5521e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.8930e-04,
         1.5271e-04,  1.5247e-04,  1.5247e-04,  2.0313e-04,  1.5247e-04,
         1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5032e-04,  1.5247e-04,
         1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,  1.5247e-04,
         1.5283e-04,  1.5247e-04,  1.7488e-04,  1.5247e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  2.3317e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.3292e-04,  1.5247e-04,
         2.4247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  2.2388e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5414e-04,
         1.5247e-04,  1.5247e-04,  1.6344e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  2.2578e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5974e-04,  1.5247e-04,  1.5247e-04,  1.5378e-04,  1.5283e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,  1.5950e-04,
         1.5247e-04,  2.0087e-04,  1.5247e-04,  1.5259e-04,  1.5247e-04,
         1.5247e-04,  1.6975e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5366e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5295e-04,  1.5247e-04,  1.5247e-04,  1.5509e-04,  1.5247e-04,
         1.5247e-04,  1.5438e-04,  1.5247e-04,  1.5247e-04,  1.5521e-04,
         1.5247e-04,  1.5247e-04,  1.7822e-04,  1.5247e-04, -2.2280e-04,
         1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5366e-04,  1.5259e-04,  1.5259e-04,  1.7107e-04,
         1.5259e-04,  1.5247e-04,  1.5640e-04,  1.5247e-04,  1.5247e-04,
         1.5259e-04,  1.5247e-04,  1.7059e-04,  1.5247e-04,  1.5247e-04,
         2.1017e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5330e-04,  1.5247e-04,  2.1446e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5926e-04,
         1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5295e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  2.2709e-04,  1.5247e-04,
         1.5283e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,  1.5247e-04,
         1.5247e-04,  1.5259e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5283e-04,  1.5247e-04,  1.5295e-04,
         1.5295e-04,  1.5295e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5354e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5259e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  2.0385e-04,
         1.5247e-04,  1.5247e-04,  1.5295e-04,  1.5247e-04,  1.5247e-04,
         1.5271e-04,  1.5247e-04,  1.5259e-04,  1.5295e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5247e-04,
         1.5271e-04,  1.5295e-04,  1.5247e-04,  1.5259e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5283e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5318e-04,  1.5247e-04,  2.0719e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5271e-04,  1.5783e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5903e-04,  1.5247e-04,  1.5247e-04,
         1.5247e-04,  1.5247e-04,  1.5247e-04,  1.5962e-04,  1.5247e-04],
       device='cuda:2', dtype=torch.float16, requires_grad=True)loss: tensor(11970.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0405, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10750.7764, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0856, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11764.3086, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0220, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11575.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4871, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12448.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6030, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11083.3311, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6738, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 11:18:19,851][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2023-09-12 11:19:13,745][train_inner][INFO] - {"epoch": 1, "update": 0.388, "loss": "9.43", "ntokens": "149652", "nsentences": "536.08", "prob_perplexity": "387.459", "code_perplexity": "371.756", "temp": "1.999", "loss_0": "6.679", "loss_1": "0.057", "loss_2": "2.694", "accuracy": "0.01201", "wps": "38042.1", "ups": "0.25", "wpb": "149652", "bsz": "536.1", "num_updates": "200", "lr": "3.125e-06", "gnorm": "1.459", "loss_scale": "32", "train_wall": "793", "gb_free": "13", "wall": "826"}
[2023-09-12 11:32:16,630][train_inner][INFO] - {"epoch": 1, "update": 0.772, "loss": "6.996", "ntokens": "149656", "nsentences": "540.92", "prob_perplexity": "541.344", "code_perplexity": "525.456", "temp": "1.997", "loss_0": "6.657", "loss_1": "0.022", "loss_2": "0.316", "accuracy": "0.01195", "wps": "38232.1", "ups": "0.26", "wpb": "149656", "bsz": "540.9", "num_updates": "400", "lr": "6.25e-06", "gnorm": "0.137", "loss_scale": "32", "train_wall": "782", "gb_free": "12.8", "wall": "1609"}

loss: tensor(12274.5947, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0781, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8492.7109, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3320, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10227.9434, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3203, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 1.6975e-03,  1.7233e-03,  5.4407e-04,  1.8072e-03,  9.0837e-04,
         1.6766e-03,  1.5001e-03,  1.6832e-03,  1.8148e-03,  1.7433e-03,
         1.7033e-03,  1.8082e-03,  1.8206e-03,  1.6832e-03,  1.6060e-03,
         1.6880e-03,  1.8311e-03,  1.8492e-03, -4.2129e-04,  1.6766e-03,
         1.6766e-03,  1.6727e-03,  1.7548e-03,  1.8396e-03,  1.7767e-03,
         1.8692e-03,  1.7834e-03,  1.6842e-03,  1.6699e-03,  1.6756e-03,
         1.6661e-03,  1.6003e-03,  1.0557e-03,  1.5621e-03,  1.8387e-03,
         1.7157e-03,  1.2980e-03,  1.6794e-03,  1.7939e-03,  1.8120e-03,
         1.6775e-03,  1.6947e-03,  1.6937e-03,  1.3447e-03,  1.7052e-03,
         1.7624e-03,  1.6174e-03,  1.6861e-03,  1.7586e-03,  1.8320e-03,
         1.6890e-03,  1.7271e-03,  1.8702e-03,  1.3390e-03,  1.6794e-03,
         9.9373e-04,  1.7662e-03,  1.3828e-03,  1.6785e-03,  1.2655e-03,
         1.7042e-03,  1.6785e-03,  1.6813e-03,  1.6766e-03,  1.7033e-03,
         1.7290e-03,  1.6880e-03,  1.6956e-03,  1.6823e-03,  1.6689e-03,
         1.6785e-03,  1.6813e-03,  1.6861e-03,  1.6842e-03,  1.6155e-03,
         1.6222e-03,  1.7080e-03, -7.9679e-04,  1.6794e-03,  1.7557e-03,
         1.6832e-03,  1.7052e-03,  1.6088e-03,  1.7071e-03,  1.6756e-03,
         1.8682e-03,  1.6479e-03,  1.3094e-03,  1.6909e-03,  1.7405e-03,
         1.4105e-03,  1.6356e-03,  1.6727e-03,  1.6804e-03,  1.7014e-03,
         1.8005e-03,  1.8301e-03,  1.7242e-03,  1.7490e-03,  1.6661e-03,
         1.8301e-03,  1.7214e-03,  1.8215e-03,  1.6556e-03,  1.6518e-03,
         1.6766e-03,  1.6918e-03,  1.8454e-03,  1.8559e-03,  1.8606e-03,
         1.7881e-03,  1.6994e-03,  7.5912e-04,  1.6842e-03,  1.6794e-03,
         1.7891e-03,  1.6756e-03,  1.6899e-03,  1.1902e-03,  1.7061e-03,
         1.6794e-03,  1.6718e-03,  1.6756e-03,  1.6785e-03, -1.0997e-04,
         1.8654e-03,  1.6851e-03,  1.6766e-03,  1.6766e-03,  1.6937e-03,
         1.7166e-03,  1.6766e-03,  1.7824e-03,  1.6613e-03,  1.3504e-03,
         1.5678e-03,  1.7014e-03,  1.6766e-03,  9.8228e-04,  1.7109e-03,
         1.0595e-03,  7.0333e-04,  1.8444e-03,  1.5087e-03,  1.7786e-03,
         1.7653e-03,  1.6756e-03,  1.8091e-03,  1.7643e-03,  1.6851e-03,
         1.8034e-03,  1.7805e-03,  1.7414e-03,  1.8167e-03,  1.6575e-03,
         1.6727e-03,  1.7586e-03,  1.7252e-03,  1.8435e-03,  1.6813e-03,
         1.5497e-03,  1.7357e-03,  1.7004e-03,  1.7118e-03,  1.6766e-03,
         1.7929e-03,  1.7958e-03,  1.4362e-03,  1.8244e-03,  1.6756e-03,
         1.6747e-03,  1.6623e-03,  1.6356e-03,  1.6756e-03,  1.6966e-03,
         1.6804e-03,  1.7109e-03,  1.7719e-03,  1.7872e-03,  1.8206e-03,
         1.7595e-03,  1.4200e-03,  1.7014e-03,  1.8129e-03,  1.8053e-03,
         1.7033e-03,  1.6775e-03,  1.6832e-03,  1.6794e-03,  1.7843e-03,
         1.8311e-03,  1.5135e-03,  1.6146e-03,  1.2770e-03,  1.6766e-03,
         1.1902e-03,  1.3647e-03,  1.7490e-03,  1.7595e-03,  1.7786e-03,
        -1.3399e-04,  1.6899e-03,  1.7862e-03,  1.6794e-03,  1.7710e-03,
         1.6785e-03,  1.6785e-03,  9.2030e-04,  1.7157e-03,  1.5917e-03,
         9.4509e-04,  1.6842e-03,  1.6880e-03,  1.6870e-03,  1.7662e-03,
         1.6794e-03,  1.6823e-03,  1.7414e-03,  1.6823e-03,  1.6317e-03,
         1.7986e-03,  1.5011e-03,  1.8730e-03,  1.6766e-03,  1.6756e-03,
         1.4105e-03,  1.6756e-03,  1.7138e-03,  1.7071e-03,  1.5564e-03,
         1.6861e-03,  1.8072e-03,  1.7538e-03,  1.7080e-03,  1.7405e-03,
         1.7366e-03,  1.7433e-03,  1.7042e-03,  1.0614e-03,  1.7529e-03,
         1.6794e-03,  1.7223e-03,  1.6909e-03,  1.8606e-03,  1.6842e-03,
         1.6909e-03,  1.6947e-03,  8.0633e-04,  1.6775e-03,  1.6766e-03,
         1.6804e-03,  1.6775e-03,  1.6813e-03,  1.6966e-03,  1.8034e-03,
         1.6785e-03,  1.6842e-03,  1.7099e-03,  1.8129e-03,  1.8272e-03,
         1.3866e-03,  1.6766e-03,  1.4868e-03,  1.8063e-03,  1.7452e-03,
         1.6899e-03,  1.7290e-03,  1.6804e-03,  1.6813e-03,  1.7681e-03,
         1.7366e-03,  1.5411e-03,  1.6823e-03,  1.2579e-03,  1.8635e-03,
         1.6775e-03,  1.6794e-03,  1.6518e-03,  1.6556e-03,  1.6794e-03,
         1.6823e-03,  1.6775e-03, -4.2677e-04,  1.6365e-03,  1.7204e-03,
         1.7691e-03,  1.7042e-03,  1.6966e-03,  1.6794e-03,  1.7176e-03,
         1.6747e-03,  1.4086e-03,  1.7681e-03,  1.8520e-03,  1.6880e-03,
         1.6794e-03,  1.6747e-03,  1.6766e-03,  1.8501e-03,  1.6756e-03,
         1.8415e-03, -5.4598e-04,  1.6775e-03,  1.3075e-03,  1.6794e-03,
         1.3647e-03,  1.7824e-03,  1.6851e-03,  1.6918e-03,  1.6756e-03,
         1.7548e-03,  1.7338e-03,  1.7328e-03,  1.6861e-03,  1.6794e-03,
         1.6775e-03,  1.6670e-03,  1.8587e-03,  1.6479e-03,  1.6804e-03,
         1.4782e-03,  1.2035e-03,  1.6766e-03,  1.6756e-03,  1.6775e-03,
         1.7538e-03,  1.7195e-03,  1.8101e-03,  1.6766e-03,  1.7147e-03,
         1.7595e-03,  1.6851e-03,  1.7023e-03,  1.6832e-03,  1.8425e-03,
         1.6623e-03,  1.8244e-03,  7.6485e-04,  1.3962e-03,  1.6756e-03,
         1.7538e-03,  1.7366e-03,  1.6670e-03,  1.6794e-03,  1.7176e-03,
        -3.1614e-04, -3.3975e-05,  1.7767e-03,  1.7366e-03,  1.6756e-03,
         1.6804e-03,  1.6785e-03,  1.7099e-03,  1.6766e-03,  1.7071e-03,
         1.5650e-03,  1.6766e-03,  1.6785e-03,  1.6699e-03,  1.6756e-03,
         1.5087e-03,  1.7385e-03,  1.6766e-03,  1.7052e-03,  1.6861e-03,
         1.8234e-03,  1.6842e-03,  1.6842e-03,  1.6785e-03,  1.6584e-03,
         1.6994e-03,  1.6861e-03,  1.8291e-03,  1.6880e-03,  1.6794e-03,
         9.8419e-04,  1.6766e-03,  1.6975e-03,  1.6823e-03, -3.2616e-04,
         1.6766e-03,  1.6775e-03,  1.6861e-03,  1.6861e-03,  1.6680e-03,
         1.6670e-03,  1.6832e-03,  1.2245e-03,  1.5163e-03,  1.6632e-03,
         1.7147e-03,  1.6756e-03,  1.7910e-03,  1.6031e-03,  1.6775e-03,
         1.6794e-03,  1.7338e-03,  1.2255e-03,  1.5993e-03,  1.7290e-03,
         1.6804e-03,  1.6775e-03,  1.8110e-03,  1.7281e-03,  1.5984e-03,
         1.6737e-03,  1.6756e-03,  1.8320e-03,  1.0796e-03,  1.7872e-03,
         1.6947e-03,  1.7204e-03,  1.6756e-03,  1.6661e-03,  1.7796e-03,
         1.7900e-03,  1.6842e-03,  1.2150e-03,  1.7090e-03,  1.6766e-03,
         1.6861e-03,  1.6785e-03,  1.5717e-03,  1.7157e-03,  1.6756e-03,
         1.7853e-03,  1.6842e-03,  1.5755e-03,  1.7757e-03,  1.6689e-03,
         1.6861e-03,  1.8339e-03,  1.7681e-03,  4.9019e-04,  1.6775e-03,
         1.8501e-03,  1.6804e-03,  1.1663e-03,  1.6756e-03,  1.8606e-03,
         1.7729e-03,  1.6794e-03,  1.8463e-03,  1.2875e-03,  1.7996e-03,
         3.7360e-04,  1.8444e-03,  1.6785e-03,  1.7242e-03,  1.0538e-03,
         1.7204e-03,  1.6756e-03,  1.6842e-03,  1.6861e-03,  1.6870e-03,
         1.6785e-03,  1.6937e-03,  1.8559e-03,  1.7767e-03,  1.6775e-03,
         1.6975e-03,  1.7786e-03,  1.4801e-03,  1.6766e-03, -2.3365e-04,
         1.7843e-03,  2.2054e-04,  1.0147e-03,  1.6766e-03,  1.8568e-03,
         1.2846e-03,  1.7891e-03,  1.8187e-03,  1.6651e-03,  1.7414e-03,
         1.7881e-03,  1.8139e-03,  1.8349e-03,  1.6747e-03,  1.5173e-03,
         1.6813e-03,  9.7656e-04,  1.7347e-03,  1.6565e-03,  1.7061e-03,
         1.7567e-03,  1.2589e-03,  1.7843e-03,  1.7576e-03,  1.7309e-03,
         1.6756e-03,  1.6670e-03,  1.6823e-03,  1.6775e-03,  1.6851e-03,
         1.6174e-03,  1.6890e-03,  1.6785e-03,  1.3838e-03,  1.6775e-03,
         1.6861e-03,  1.6527e-03,  1.7939e-03,  1.7195e-03,  1.6527e-03,
         1.6804e-03,  1.7214e-03,  1.6069e-03,  1.6956e-03, -6.8426e-04,
         1.6832e-03,  1.6747e-03,  1.6737e-03,  1.6813e-03,  1.7014e-03,
         1.7376e-03,  1.2531e-03,  1.7433e-03,  1.6575e-03,  1.6088e-03,
         1.8654e-03,  1.6766e-03,  1.6394e-03,  1.6985e-03,  1.1415e-03,
         1.6766e-03,  1.6880e-03,  1.4982e-03,  1.6994e-03,  1.8635e-03,
         1.3504e-03,  1.8797e-03,  1.6708e-03,  1.6832e-03,  1.7605e-03,
         1.7900e-03,  1.7729e-03,  1.6823e-03,  5.6267e-04,  1.8597e-03,
         1.7509e-03,  1.6785e-03,  1.6766e-03,  1.6851e-03,  1.3647e-03,
         1.6909e-03,  1.7328e-03,  1.8225e-03,  1.6880e-03, -2.5558e-04,
         1.7414e-03,  1.6823e-03,  1.6861e-03,  1.6861e-03,  1.6870e-03,
         1.8196e-03,  1.6804e-03,  1.6775e-03,  8.6594e-04,  1.6861e-03,
         1.6747e-03,  1.6022e-03,  1.6775e-03,  1.6766e-03,  1.6766e-03,
         1.8110e-03,  1.6766e-03,  1.8005e-03, -1.8239e-05,  1.8530e-03,
        -1.6499e-04,  1.7567e-03,  1.6756e-03,  1.6718e-03,  1.8377e-03,
         1.7071e-03,  1.6870e-03,  1.7452e-03,  1.7071e-03,  1.6184e-03,
         1.6766e-03,  1.4915e-03,  1.6918e-03,  1.6775e-03,  1.6756e-03,
         1.6851e-03,  1.6813e-03,  1.7061e-03,  1.6823e-03,  1.5974e-03,
         1.6794e-03,  1.6870e-03,  1.7099e-03,  1.6861e-03,  1.5068e-03,
         1.8520e-03,  1.6747e-03,  1.7118e-03,  1.8215e-03,  1.6804e-03,
        -6.0129e-04,  1.6842e-03,  1.5783e-03,  1.6699e-03,  1.8158e-03,
         1.7052e-03,  1.7338e-03,  1.8320e-03,  1.8244e-03,  1.7214e-03,
         1.5659e-03,  1.6689e-03,  1.8167e-03,  1.8387e-03,  1.6775e-03,
         1.7271e-03,  1.7748e-03,  5.9557e-04,  1.6918e-03,  1.7967e-03,
         1.6775e-03,  1.7138e-03,  1.7128e-03,  9.9373e-04,  1.6766e-03,
         1.6928e-03,  1.7052e-03,  1.7462e-03,  1.6918e-03,  1.7710e-03,
         1.8520e-03,  1.6785e-03,  1.6470e-03,  1.7023e-03,  1.6785e-03,
         1.8406e-03,  1.7395e-03,  1.6861e-03,  1.6489e-03,  1.6794e-03],
       device='cuda:2', dtype=torch.float16, requires_grad=True)loss: tensor(11593.8301, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5977, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11519.2559, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6953, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 11:40:07,614][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 11:40:07,615][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 11:40:07,738][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2023-09-12 11:40:31,368][valid][INFO] - {"epoch": 1, "valid_loss": "6.707", "valid_ntokens": "7924.15", "valid_nsentences": "55.2525", "valid_prob_perplexity": "587.776", "valid_code_perplexity": "569.734", "valid_temp": "1.995", "valid_loss_0": "6.657", "valid_loss_1": "0.011", "valid_loss_2": "0.039", "valid_accuracy": "0.01472", "valid_wps": "33437.6", "valid_wpb": "7924.2", "valid_bsz": "55.3", "valid_num_updates": "519"}
[2023-09-12 11:40:31,370][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 519 updates
[2023-09-12 11:40:31,371][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 11:40:33,246][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 11:40:34,091][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 519 updates, score 6.707) (writing took 2.7209519079187885 seconds)
[2023-09-12 11:40:34,092][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2023-09-12 11:40:34,092][train][INFO] - {"epoch": 1, "train_loss": "7.875", "train_ntokens": "149451", "train_nsentences": "538.306", "train_prob_perplexity": "492.885", "train_code_perplexity": "476.987", "train_temp": "1.997", "train_loss_0": "6.665", "train_loss_1": "0.033", "train_loss_2": "1.176", "train_accuracy": "0.01199", "train_wps": "37522.4", "train_ups": "0.25", "train_wpb": "149451", "train_bsz": "538.3", "train_num_updates": "519", "train_lr": "8.10938e-06", "train_gnorm": "0.62", "train_loss_scale": "64", "train_train_wall": "2045", "train_gb_free": "14.3", "train_wall": "2107"}
[2023-09-12 11:40:34,094][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 11:40:34,160][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2023-09-12 11:40:34,431][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 11:40:34,434][fairseq.trainer][INFO] - begin training epoch 2
[2023-09-12 11:40:34,434][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(11777.2891, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5508, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 11:45:57,176][train_inner][INFO] - {"epoch": 2, "update": 1.155, "loss": "6.718", "ntokens": "149216", "nsentences": "537.895", "prob_perplexity": "591.846", "code_perplexity": "575.581", "temp": "1.995", "loss_0": "6.656", "loss_1": "0.011", "loss_2": "0.052", "accuracy": "0.01206", "wps": "36370.1", "ups": "0.24", "wpb": "149216", "bsz": "537.9", "num_updates": "600", "lr": "9.375e-06", "gnorm": "0.018", "loss_scale": "64", "train_wall": "792", "gb_free": "13.1", "wall": "2430"}
loss: tensor(10303.2666, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7031, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12035.2578, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2578, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 11:51:34,720][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
loss: tensor(10261.5146, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4688, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 11:53:52,452][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0

loss: tensor(11188.4277, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5820, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7682.1660, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6367, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11532.1650, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.9219, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8891.0146, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0977, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10288.0596, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3398, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11354.9805, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2734, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10873.7627, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6113, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 4.6082e-03,  4.4670e-03,  2.2030e-03,  4.5853e-03, -1.6737e-04,
         4.6310e-03,  4.4212e-03,  4.6349e-03,  4.9706e-03,  4.5052e-03,
         4.6158e-03,  4.5509e-03,  4.7264e-03,  4.6310e-03,  4.5052e-03,
         4.5853e-03,  4.5471e-03,  4.5090e-03, -2.4624e-03,  4.6310e-03,
         4.6730e-03,  4.6921e-03,  4.6158e-03,  4.9515e-03,  4.7226e-03,
         4.5204e-03,  4.5815e-03,  4.6577e-03,  4.6234e-03,  4.6310e-03,
         4.6539e-03,  4.7607e-03,  3.3493e-03,  4.5052e-03,  4.6005e-03,
         4.6310e-03,  3.9253e-03,  4.6349e-03,  4.5319e-03,  4.4823e-03,
         4.6310e-03,  4.7798e-03,  4.6310e-03,  3.8586e-03,  4.6196e-03,
         4.5891e-03,  4.5509e-03,  4.6349e-03,  4.5891e-03,  5.0011e-03,
         4.6310e-03,  4.6120e-03,  4.6463e-03,  4.0016e-03,  4.6310e-03,
         3.7670e-03,  4.6158e-03,  1.3757e-04,  4.6921e-03,  3.9482e-03,
         4.6463e-03,  4.6349e-03,  4.6349e-03,  4.6310e-03,  4.6234e-03,
         4.6463e-03,  4.6310e-03,  4.7722e-03,  4.6349e-03,  4.6234e-03,
         4.6310e-03,  4.6310e-03,  4.6234e-03,  4.6310e-03,  4.1008e-03,
         4.4136e-03,  4.6272e-03, -2.7523e-03,  4.6349e-03,  4.6501e-03,
         4.6310e-03,  4.5090e-03,  4.5090e-03,  4.6120e-03,  4.6349e-03,
         4.5090e-03,  4.5815e-03,  3.9215e-03,  4.9973e-03,  4.8447e-03,
         4.3335e-03,  4.3411e-03,  4.7989e-03,  4.6272e-03,  4.4899e-03,
         4.7035e-03,  4.2648e-03,  4.4060e-03,  4.6196e-03,  4.5471e-03,
         4.5090e-03,  4.6158e-03,  4.5090e-03,  4.8637e-03,  4.6921e-03,
         4.6310e-03,  4.6387e-03,  4.6043e-03,  4.8027e-03,  4.5242e-03,
         5.2986e-03,  4.6120e-03, -2.9206e-06,  4.6005e-03,  4.6349e-03,
         4.4823e-03,  4.6349e-03,  4.6310e-03,  3.9368e-03,  5.1079e-03,
         4.6349e-03,  4.6272e-03,  4.6349e-03,  4.6158e-03,  2.0008e-03,
         4.4899e-03,  4.6349e-03,  4.6349e-03,  4.6310e-03,  4.6463e-03,
         4.5700e-03,  4.6349e-03,  8.7738e-04,  4.6234e-03,  4.3488e-03,
         4.4746e-03,  4.7607e-03,  4.8447e-03,  3.3627e-03,  4.6234e-03,
         3.7422e-03, -1.2360e-03,  4.7722e-03,  4.0321e-03,  4.3716e-03,
         4.5891e-03,  4.6349e-03,  4.4174e-03,  4.6730e-03,  4.2534e-03,
         4.6005e-03,  4.3526e-03,  4.7493e-03,  4.4060e-03,  4.6082e-03,
         4.7188e-03,  4.6082e-03,  4.6043e-03,  5.1384e-03,  4.6310e-03,
         4.4250e-03,  4.5509e-03,  4.6310e-03,  4.7569e-03,  4.6349e-03,
         4.4785e-03,  4.4861e-03,  4.0474e-03,  4.6158e-03,  4.2877e-03,
         4.6310e-03,  4.5357e-03,  4.6959e-03,  4.6196e-03,  4.6043e-03,
         4.6844e-03,  4.6310e-03,  4.3411e-03,  4.4746e-03,  2.5153e-05,
         4.4022e-03,  3.9368e-03,  4.4022e-03,  5.1460e-03,  4.5662e-03,
         4.6463e-03,  4.6349e-03,  4.6349e-03,  4.6349e-03,  4.6616e-03,
         4.9057e-03,  4.4365e-03,  4.5586e-03,  3.8548e-03,  4.6310e-03,
         4.2114e-03,  4.3106e-03,  4.6043e-03,  4.5738e-03,  4.9629e-03,
        -1.7796e-03,  4.6844e-03,  4.3907e-03,  4.6349e-03,  4.5280e-03,
         4.7607e-03,  4.6349e-03,  3.2616e-03,  4.6616e-03,  4.5471e-03,
         3.2616e-03,  4.6310e-03,  4.6959e-03,  4.6310e-03,  4.6120e-03,
         4.6310e-03,  4.7874e-03,  4.6234e-03,  4.6463e-03,  4.5967e-03,
         4.5815e-03,  4.2458e-03,  4.4441e-03,  4.6349e-03,  4.9858e-03,
         4.1618e-03,  4.6310e-03,  4.5509e-03,  4.6234e-03,  4.7150e-03,
         4.6387e-03,  4.5776e-03,  4.6196e-03,  4.6196e-03,  4.6196e-03,
         4.8904e-03,  4.6120e-03,  4.8141e-03,  4.0359e-03,  2.9621e-03,
         4.6387e-03,  4.5891e-03,  5.0926e-03,  4.2953e-03,  4.6310e-03,
         4.6349e-03,  4.7379e-03,  3.4847e-03,  4.6349e-03,  4.6310e-03,
         4.6310e-03,  4.6349e-03,  5.1727e-03,  4.6463e-03,  4.8561e-03,
         4.6349e-03,  4.6005e-03,  4.6234e-03,  4.9629e-03,  4.5357e-03,
         4.7150e-03,  4.6310e-03,  4.1656e-03,  4.5624e-03,  4.8752e-03,
         4.7531e-03,  4.6234e-03,  4.6310e-03,  4.6310e-03,  4.7340e-03,
         4.9019e-03,  4.3030e-03,  4.6349e-03,  4.0817e-03,  4.9171e-03,
         4.6349e-03,  4.6310e-03,  4.4556e-03,  4.5700e-03,  4.6349e-03,
         4.6310e-03,  4.6310e-03, -6.3372e-04,  4.5815e-03,  4.7646e-03,
         4.8904e-03,  4.6463e-03,  4.5204e-03,  4.6310e-03,  4.5509e-03,
         4.6310e-03,  4.2572e-03,  4.7112e-03,  4.5242e-03,  4.6463e-03,
         4.6310e-03,  4.6310e-03,  4.6310e-03,  3.0174e-03,  4.6539e-03,
         4.4289e-03, -1.0872e-03,  4.6425e-03,  4.1122e-03,  4.6997e-03,
         4.2801e-03,  4.5509e-03,  4.6310e-03,  4.6158e-03,  4.6272e-03,
         4.6158e-03,  4.6310e-03,  4.6959e-03,  4.9706e-03,  4.6310e-03,
         4.3755e-03,  4.5967e-03,  4.5776e-03,  4.5624e-03,  4.7035e-03,
         4.5738e-03,  4.3068e-03,  4.6349e-03,  4.6463e-03,  4.6349e-03,
         5.0850e-03,  4.5471e-03,  4.5471e-03,  4.6310e-03,  4.7340e-03,
         4.5624e-03,  4.6349e-03,  4.7455e-03,  4.6349e-03,  4.5395e-03,
         4.5357e-03,  4.6005e-03,  3.4523e-03,  4.1389e-03,  4.7684e-03,
         4.6921e-03,  4.7150e-03,  4.6043e-03,  4.6310e-03,  4.6730e-03,
         1.1740e-03, -1.0532e-04,  5.2948e-03,  5.0240e-03,  4.6310e-03,
         4.6539e-03,  4.6349e-03,  4.5204e-03,  4.6349e-03,  4.9782e-03,
         4.4670e-03,  4.6387e-03,  4.6387e-03,  4.6234e-03,  4.6349e-03,
         4.4518e-03,  4.6387e-03,  4.7989e-03,  4.6234e-03,  4.6310e-03,
         4.8256e-03,  4.6310e-03,  4.6349e-03,  4.6310e-03,  4.8447e-03,
         4.6272e-03,  4.6310e-03,  4.5700e-03,  4.6349e-03,  4.6310e-03,
         3.5973e-03,  4.6349e-03,  4.6310e-03,  4.6349e-03, -3.9458e-05,
         4.6349e-03,  5.0888e-03,  4.6310e-03,  4.6310e-03,  4.6234e-03,
         4.6082e-03,  4.6921e-03, -8.1587e-04,  4.8065e-03,  4.6158e-03,
         4.6043e-03,  4.6349e-03,  4.6272e-03,  4.5471e-03,  4.6349e-03,
         4.6349e-03,  4.6158e-03,  3.9215e-03,  4.4861e-03,  5.0392e-03,
         4.6349e-03,  4.6349e-03,  4.6387e-03,  4.5013e-03,  4.5509e-03,
         4.6349e-03,  4.6310e-03,  4.7989e-03,  3.4847e-03,  4.6196e-03,
         4.6196e-03,  4.6272e-03,  4.6349e-03,  4.6234e-03,  4.4250e-03,
         4.5738e-03,  4.6310e-03,  4.8780e-04,  4.6310e-03,  4.6425e-03,
         4.6310e-03,  4.6310e-03,  4.2610e-03,  4.5776e-03,  4.6425e-03,
         4.7379e-03,  4.6310e-03,  4.5166e-03,  4.7150e-03,  4.6577e-03,
         4.6349e-03,  4.3335e-03,  4.6120e-03,  2.7180e-03,  4.6349e-03,
         4.5204e-03,  4.6349e-03,  3.8624e-03,  5.0316e-03,  5.0621e-03,
         4.8485e-03,  4.6310e-03,  4.5738e-03,  4.2267e-03,  4.5853e-03,
         2.5997e-03,  4.9973e-03,  4.6349e-03,  4.8981e-03,  4.0245e-03,
         4.6234e-03,  4.6349e-03,  4.9934e-03,  4.6349e-03,  5.0583e-03,
         4.6349e-03,  4.6234e-03,  4.5242e-03,  4.6043e-03,  4.4594e-03,
         4.6349e-03,  4.5166e-03,  3.9787e-03,  4.6425e-03, -2.9349e-04,
         4.6005e-03,  8.2970e-04, -8.5783e-04,  4.6120e-03,  5.1498e-03,
         3.8719e-03,  5.3101e-03,  4.5509e-03,  4.6120e-03,  4.8180e-03,
         4.5891e-03,  4.5242e-03,  4.5204e-03,  4.6310e-03,  4.1809e-03,
         4.6349e-03,  1.1244e-03,  4.6196e-03,  4.6043e-03,  4.6349e-03,
         4.6120e-03,  4.0894e-03,  4.6005e-03,  4.4670e-03,  4.6463e-03,
         4.8332e-03,  4.6387e-03,  4.6310e-03,  4.6310e-03,  4.6272e-03,
         4.6120e-03,  4.6387e-03,  4.6310e-03,  4.0207e-03,  4.6349e-03,
         4.6310e-03,  4.6082e-03,  4.4479e-03,  4.6387e-03,  4.5090e-03,
         4.6349e-03,  4.8256e-03,  4.2610e-03,  5.0812e-03, -5.0831e-04,
         4.6310e-03,  4.6272e-03,  4.6196e-03,  4.6387e-03,  4.6272e-03,
         4.6539e-03,  3.7994e-03,  4.6043e-03,  4.5891e-03,  4.5547e-03,
         4.5586e-03,  4.6310e-03,  4.5891e-03,  4.6272e-03,  1.1616e-03,
         4.7951e-03,  4.6310e-03,  4.5242e-03,  4.6120e-03,  4.4708e-03,
         4.2534e-03,  4.9477e-03,  4.6082e-03,  4.6310e-03,  4.7302e-03,
         4.6234e-03,  4.5853e-03,  4.6310e-03,  2.3403e-03,  4.7150e-03,
         5.2567e-03,  4.6349e-03,  4.6310e-03,  4.6349e-03,  4.1885e-03,
         4.6158e-03,  4.5319e-03,  4.5090e-03,  4.6310e-03, -1.9970e-03,
         4.5815e-03,  4.6349e-03,  4.6349e-03,  4.6349e-03,  4.6234e-03,
         4.5891e-03,  4.6349e-03,  4.6654e-03,  3.5896e-03,  4.6349e-03,
         4.6310e-03,  4.1809e-03,  4.6349e-03,  4.6310e-03,  4.6349e-03,
         5.3253e-03,  5.0240e-03,  4.5815e-03,  2.7084e-03,  4.1313e-03,
        -1.3371e-03,  4.9629e-03,  4.6349e-03,  4.5471e-03,  4.5052e-03,
         4.6616e-03,  4.6349e-03,  4.4556e-03,  4.8218e-03,  4.4556e-03,
         4.6158e-03,  4.6463e-03,  4.6310e-03,  5.0507e-03,  4.6310e-03,
         4.7188e-03,  4.6310e-03,  5.0011e-03,  4.6310e-03,  4.2801e-03,
         4.6310e-03,  4.6349e-03,  5.0087e-03,  4.6310e-03,  4.4518e-03,
         4.5166e-03,  4.6043e-03,  4.5891e-03,  4.6730e-03,  4.9515e-03,
        -1.3781e-03,  4.6349e-03,  4.8447e-03,  4.6234e-03,  4.5471e-03,
         4.6349e-03,  5.2528e-03,  4.5357e-03,  4.5319e-03,  4.6310e-03,
         4.2877e-03,  4.6272e-03,  4.7226e-03,  4.5204e-03,  4.6310e-03,
         4.6272e-03,  4.9744e-03,  2.8477e-03,  4.6272e-03,  4.6158e-03,
         4.6349e-03,  5.0087e-03,  4.7836e-03,  3.6926e-03,  4.6349e-03,
         4.8790e-03,  4.8218e-03,  4.6310e-03,  4.5738e-03,  4.5967e-03,
         5.1384e-03,  4.9210e-03,  4.6005e-03,  4.6310e-03,  4.6043e-03,
         4.5738e-03,  4.6082e-03,  4.6310e-03,  4.9438e-03,  4.6310e-03],
       device='cuda:2', dtype=torch.float16, requires_grad=True)loss: tensor(10689.8330, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0312, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10727.0928, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6992, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 11:59:19,854][train_inner][INFO] - {"epoch": 2, "update": 1.543, "loss": "6.527", "ntokens": "149812", "nsentences": "539.575", "prob_perplexity": "479.633", "code_perplexity": "465.997", "temp": "1.993", "loss_0": "6.471", "loss_1": "0.036", "loss_2": "0.02", "accuracy": "0.02546", "wps": "37328", "ups": "0.25", "wpb": "149812", "bsz": "539.6", "num_updates": "800", "lr": "1.25e-05", "gnorm": "0.255", "loss_scale": "16", "train_wall": "801", "gb_free": "13", "wall": "3232"}
loss: tensor(10781.9297, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1543, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 12:12:30,935][train_inner][INFO] - {"epoch": 2, "update": 1.927, "loss": "6.092", "ntokens": "149669", "nsentences": "539.815", "prob_perplexity": "227.164", "code_perplexity": "222.739", "temp": "1.991", "loss_0": "5.978", "loss_1": "0.093", "loss_2": "0.021", "accuracy": "0.07094", "wps": "37839.2", "ups": "0.25", "wpb": "149669", "bsz": "539.8", "num_updates": "1000", "lr": "1.5625e-05", "gnorm": "0.567", "loss_scale": "32", "train_wall": "790", "gb_free": "12.8", "wall": "4023"}
loss: tensor(9161.3066, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0370, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 12:14:59,690][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 12:14:59,691][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 12:14:59,771][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2023-09-12 12:15:23,229][valid][INFO] - {"epoch": 2, "valid_loss": "5.708", "valid_ntokens": "7911.28", "valid_nsentences": "55.2525", "valid_prob_perplexity": "156.417", "valid_code_perplexity": "150.164", "valid_temp": "1.99", "valid_loss_0": "5.58", "valid_loss_1": "0.109", "valid_loss_2": "0.02", "valid_accuracy": "0.12429", "valid_wps": "33459.9", "valid_wpb": "7911.3", "valid_bsz": "55.3", "valid_num_updates": "1038", "valid_best_loss": "5.708"}
[2023-09-12 12:15:23,231][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 1038 updates
[2023-09-12 12:15:23,232][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 12:15:25,672][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 12:15:26,988][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 1038 updates, score 5.708) (writing took 3.7573235789313912 seconds)
[2023-09-12 12:15:26,988][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2023-09-12 12:15:26,989][train][INFO] - {"epoch": 2, "train_loss": "6.341", "train_ntokens": "149496", "train_nsentences": "538.493", "train_prob_perplexity": "378.707", "train_code_perplexity": "368.945", "train_temp": "1.992", "train_loss_0": "6.26", "train_loss_1": "0.059", "train_loss_2": "0.022", "train_accuracy": "0.04529", "train_wps": "37072.4", "train_ups": "0.25", "train_wpb": "149496", "train_bsz": "538.5", "train_num_updates": "1038", "train_lr": "1.62188e-05", "train_gnorm": "0.367", "train_loss_scale": "32", "train_train_wall": "2062", "train_gb_free": "13.1", "train_wall": "4200"}
[2023-09-12 12:15:26,991][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 12:15:27,105][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2023-09-12 12:15:27,340][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 12:15:27,343][fairseq.trainer][INFO] - begin training epoch 3
[2023-09-12 12:15:27,344][fairseq_cli.train][INFO] - Start iterating over samples

loss: tensor(11002.5664, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5508, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10932.5283, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0225, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8740.2920, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2158, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 0.0055,  0.0054,  0.0031,  0.0055,  0.0002,  0.0056,  0.0053,  0.0056,
         0.0055,  0.0053,  0.0055,  0.0054,  0.0054,  0.0056,  0.0054,  0.0055,
         0.0054,  0.0054, -0.0022,  0.0056,  0.0055,  0.0053,  0.0055,  0.0054,
         0.0051,  0.0053,  0.0055,  0.0056,  0.0055,  0.0056,  0.0056,  0.0055,
         0.0035,  0.0054,  0.0054,  0.0055,  0.0042,  0.0056,  0.0054,  0.0054,
         0.0056,  0.0055,  0.0056,  0.0048,  0.0055,  0.0055,  0.0055,  0.0056,
         0.0055,  0.0056,  0.0056,  0.0055,  0.0053,  0.0049,  0.0056,  0.0045,
         0.0055,  0.0006,  0.0055,  0.0045,  0.0054,  0.0056,  0.0056,  0.0056,
         0.0055,  0.0055,  0.0056,  0.0054,  0.0056,  0.0055,  0.0056,  0.0055,
         0.0055,  0.0055,  0.0048,  0.0052,  0.0056, -0.0026,  0.0056,  0.0055,
         0.0056,  0.0054,  0.0054,  0.0055,  0.0056,  0.0053,  0.0055,  0.0048,
         0.0055,  0.0055,  0.0053,  0.0046,  0.0055,  0.0055,  0.0049,  0.0055,
         0.0051,  0.0053,  0.0055,  0.0055,  0.0054,  0.0055,  0.0053,  0.0054,
         0.0055,  0.0056,  0.0055,  0.0053,  0.0054,  0.0054,  0.0057,  0.0055,
         0.0003,  0.0055,  0.0056,  0.0048,  0.0056,  0.0056,  0.0042,  0.0056,
         0.0056,  0.0056,  0.0056,  0.0055,  0.0024,  0.0054,  0.0056,  0.0056,
         0.0056,  0.0056,  0.0055,  0.0056,  0.0014,  0.0051,  0.0047,  0.0047,
         0.0053,  0.0052,  0.0043,  0.0055,  0.0047, -0.0010,  0.0054,  0.0049,
         0.0047,  0.0055,  0.0056,  0.0049,  0.0054,  0.0051,  0.0055,  0.0052,
         0.0052,  0.0053,  0.0055,  0.0055,  0.0055,  0.0055,  0.0055,  0.0056,
         0.0053,  0.0055,  0.0055,  0.0054,  0.0056,  0.0054,  0.0053,  0.0050,
         0.0055,  0.0052,  0.0056,  0.0055,  0.0050,  0.0055,  0.0055,  0.0056,
         0.0056,  0.0052,  0.0053,  0.0002,  0.0053,  0.0048,  0.0053,  0.0056,
         0.0054,  0.0056,  0.0056,  0.0056,  0.0056,  0.0055,  0.0052,  0.0054,
         0.0055,  0.0047,  0.0056,  0.0044,  0.0052,  0.0055,  0.0054,  0.0056,
        -0.0016,  0.0055,  0.0053,  0.0056,  0.0054,  0.0056,  0.0056,  0.0042,
         0.0055,  0.0055,  0.0041,  0.0055,  0.0055,  0.0056,  0.0055,  0.0056,
         0.0056,  0.0055,  0.0056,  0.0055,  0.0055,  0.0051,  0.0053,  0.0056,
         0.0056,  0.0051,  0.0056,  0.0055,  0.0055,  0.0054,  0.0056,  0.0055,
         0.0055,  0.0055,  0.0055,  0.0053,  0.0055,  0.0054,  0.0048,  0.0035,
         0.0056,  0.0051,  0.0057,  0.0051,  0.0056,  0.0056,  0.0056,  0.0044,
         0.0056,  0.0056,  0.0056,  0.0056,  0.0057,  0.0056,  0.0052,  0.0056,
         0.0055,  0.0055,  0.0053,  0.0054,  0.0053,  0.0056,  0.0051,  0.0056,
         0.0053,  0.0055,  0.0055,  0.0056,  0.0056,  0.0052,  0.0052,  0.0050,
         0.0056,  0.0050,  0.0052,  0.0056,  0.0056,  0.0054,  0.0055,  0.0056,
         0.0056,  0.0056, -0.0004,  0.0055,  0.0055,  0.0055,  0.0055,  0.0051,
         0.0056,  0.0054,  0.0056,  0.0051,  0.0049,  0.0054,  0.0056,  0.0056,
         0.0056,  0.0055,  0.0033,  0.0056,  0.0053, -0.0008,  0.0056,  0.0050,
         0.0056,  0.0052,  0.0054,  0.0056,  0.0055,  0.0055,  0.0055,  0.0049,
         0.0055,  0.0055,  0.0056,  0.0052,  0.0055,  0.0054,  0.0055,  0.0055,
         0.0049,  0.0056,  0.0056,  0.0056,  0.0056,  0.0055,  0.0055,  0.0054,
         0.0056,  0.0055,  0.0054,  0.0056,  0.0055,  0.0056,  0.0054,  0.0054,
         0.0052,  0.0041,  0.0051,  0.0056,  0.0055,  0.0055,  0.0055,  0.0056,
         0.0055,  0.0017,  0.0001,  0.0032,  0.0059,  0.0056,  0.0056,  0.0056,
         0.0055,  0.0056,  0.0054,  0.0054,  0.0056,  0.0056,  0.0055,  0.0056,
         0.0054,  0.0055,  0.0052,  0.0055,  0.0056,  0.0053,  0.0056,  0.0056,
         0.0056,  0.0055,  0.0056,  0.0056,  0.0054,  0.0056,  0.0056,  0.0043,
         0.0056,  0.0056,  0.0056,  0.0003,  0.0056,  0.0059,  0.0056,  0.0056,
         0.0055,  0.0055,  0.0055, -0.0009,  0.0055,  0.0055,  0.0055,  0.0056,
         0.0055,  0.0055,  0.0056,  0.0056,  0.0055,  0.0048,  0.0054,  0.0056,
         0.0056,  0.0056,  0.0055,  0.0054,  0.0055,  0.0056,  0.0056,  0.0051,
         0.0044,  0.0054,  0.0055,  0.0055,  0.0056,  0.0055,  0.0053,  0.0055,
         0.0056,  0.0011,  0.0056,  0.0055,  0.0056,  0.0056,  0.0049,  0.0055,
         0.0056,  0.0052,  0.0056,  0.0054,  0.0055,  0.0055,  0.0056,  0.0051,
         0.0055,  0.0033,  0.0056,  0.0054,  0.0056,  0.0047,  0.0059,  0.0054,
         0.0055,  0.0055,  0.0054,  0.0051,  0.0055,  0.0036,  0.0054,  0.0056,
         0.0056,  0.0048,  0.0055,  0.0056,  0.0056,  0.0056,  0.0056,  0.0056,
         0.0055,  0.0054,  0.0055,  0.0053,  0.0055,  0.0054,  0.0048,  0.0056,
         0.0001,  0.0055,  0.0014, -0.0007,  0.0055,  0.0056,  0.0047,  0.0058,
         0.0054,  0.0055,  0.0053,  0.0055,  0.0051,  0.0050,  0.0056,  0.0050,
         0.0056,  0.0009,  0.0055,  0.0055,  0.0055,  0.0055,  0.0050,  0.0055,
         0.0054,  0.0055,  0.0056,  0.0055,  0.0056,  0.0056,  0.0055,  0.0054,
         0.0055,  0.0056,  0.0048,  0.0056,  0.0056,  0.0055,  0.0052,  0.0055,
         0.0054,  0.0056,  0.0054,  0.0051,  0.0062, -0.0003,  0.0056,  0.0056,
         0.0055,  0.0056,  0.0056,  0.0055,  0.0047,  0.0055,  0.0055,  0.0055,
         0.0052,  0.0056,  0.0055,  0.0055,  0.0019,  0.0056,  0.0056,  0.0052,
         0.0055,  0.0053,  0.0056,  0.0052,  0.0055,  0.0056,  0.0055,  0.0055,
         0.0055,  0.0056,  0.0032,  0.0055,  0.0070,  0.0056,  0.0056,  0.0056,
         0.0044,  0.0055,  0.0055,  0.0053,  0.0056, -0.0019,  0.0055,  0.0056,
         0.0056,  0.0056,  0.0055,  0.0054,  0.0056,  0.0056,  0.0045,  0.0056,
         0.0056,  0.0051,  0.0056,  0.0056,  0.0056,  0.0061,  0.0056,  0.0055,
         0.0031,  0.0043, -0.0008,  0.0054,  0.0056,  0.0055,  0.0054,  0.0056,
         0.0056,  0.0053,  0.0053,  0.0054,  0.0055,  0.0055,  0.0055,  0.0054,
         0.0056,  0.0056,  0.0056,  0.0061,  0.0056,  0.0051,  0.0056,  0.0056,
         0.0061,  0.0056,  0.0054,  0.0053,  0.0055,  0.0055,  0.0054,  0.0059,
        -0.0010,  0.0056,  0.0056,  0.0055,  0.0053,  0.0056,  0.0060,  0.0053,
         0.0054,  0.0055,  0.0052,  0.0055,  0.0056,  0.0054,  0.0056,  0.0055,
         0.0054,  0.0037,  0.0055,  0.0054,  0.0056,  0.0057,  0.0055,  0.0041,
         0.0056,  0.0057,  0.0056,  0.0052,  0.0055,  0.0055,  0.0059,  0.0057,
         0.0055,  0.0056,  0.0055,  0.0053,  0.0055,  0.0056,  0.0057,  0.0056],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(9811.8926, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0526, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6629.1875, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2773, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9346.5146, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4087, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11637.3916, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6860, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8836.6553, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1434, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: [2023-09-12 12:26:12,616][train_inner][INFO] - {"epoch": 3, "update": 2.311, "loss": "5.798", "ntokens": "149121", "nsentences": "535.605", "prob_perplexity": "144.311", "code_perplexity": "141.56", "temp": "1.989", "loss_0": "5.666", "loss_1": "0.112", "loss_2": "0.02", "accuracy": "0.10211", "wps": "36296.6", "ups": "0.24", "wpb": "149121", "bsz": "535.6", "num_updates": "1200", "lr": "1.875e-05", "gnorm": "0.619", "loss_scale": "32", "train_wall": "793", "gb_free": "13", "wall": "4845"}
[2023-09-12 12:28:18,823][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
loss: tensor(9975.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3767, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 12:39:27,014][train_inner][INFO] - {"epoch": 3, "update": 2.697, "loss": "5.542", "ntokens": "149579", "nsentences": "540.68", "prob_perplexity": "67.733", "code_perplexity": "66.901", "temp": "1.987", "loss_0": "5.394", "loss_1": "0.129", "loss_2": "0.019", "accuracy": "0.15492", "wps": "37658.5", "ups": "0.25", "wpb": "149579", "bsz": "540.7", "num_updates": "1400", "lr": "2.1875e-05", "gnorm": "0.658", "loss_scale": "32", "train_wall": "793", "gb_free": "12.8", "wall": "5640"}
loss: tensor(6209.3945, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8315, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 12:45:15,912][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2023-09-12 12:45:58,716][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2023-09-12 12:49:46,684][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 12:49:46,685][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 12:49:46,757][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2023-09-12 12:50:10,597][valid][INFO] - {"epoch": 3, "valid_loss": "5.072", "valid_ntokens": "7884.95", "valid_nsentences": "55.2525", "valid_prob_perplexity": "50.445", "valid_code_perplexity": "50.113", "valid_temp": "1.985", "valid_loss_0": "4.923", "valid_loss_1": "0.133", "valid_loss_2": "0.016", "valid_accuracy": "0.23523", "valid_wps": "33038.8", "valid_wpb": "7884.9", "valid_bsz": "55.3", "valid_num_updates": "1556", "valid_best_loss": "5.072"}
[2023-09-12 12:50:10,598][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 1556 updates
[2023-09-12 12:50:10,599][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 12:50:13,050][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 12:50:14,452][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 1556 updates, score 5.072) (writing took 3.853530663996935 seconds)
[2023-09-12 12:50:14,452][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2023-09-12 12:50:14,453][train][INFO] - {"epoch": 3, "train_loss": "5.563", "train_ntokens": "149494", "train_nsentences": "538.263", "train_prob_perplexity": "85.01", "train_code_perplexity": "83.764", "train_temp": "1.987", "train_loss_0": "5.42", "train_loss_1": "0.125", "train_loss_2": "0.019", "train_accuracy": "0.14852", "train_wps": "37096.6", "train_ups": "0.25", "train_wpb": "149494", "train_bsz": "538.3", "train_num_updates": "1556", "train_lr": "2.43125e-05", "train_gnorm": "0.645", "train_loss_scale": "16", "train_train_wall": "2056", "train_gb_free": "15.6", "train_wall": "6287"}
[2023-09-12 12:50:14,455][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 12:50:14,546][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2023-09-12 12:50:14,776][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 12:50:14,779][fairseq.trainer][INFO] - begin training epoch 4
[2023-09-12 12:50:14,779][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(8619.9824, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6543, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 12:53:10,473][train_inner][INFO] - {"epoch": 4, "update": 3.084, "loss": "5.355", "ntokens": "149161", "nsentences": "535.475", "prob_perplexity": "54.216", "code_perplexity": "53.837", "temp": "1.985", "loss_0": "5.206", "loss_1": "0.132", "loss_2": "0.017", "accuracy": "0.18647", "wps": "36228", "ups": "0.24", "wpb": "149161", "bsz": "535.5", "num_updates": "1600", "lr": "2.5e-05", "gnorm": "0.691", "loss_scale": "16", "train_wall": "794", "gb_free": "12.8", "wall": "6463"}
Parameter containing:
tensor([ 0.0069,  0.0068,  0.0045,  0.0068,  0.0012,  0.0070,  0.0067,  0.0070,
         0.0063,  0.0067,  0.0069,  0.0068,  0.0067,  0.0070,  0.0068,  0.0069,
         0.0068,  0.0068, -0.0021,  0.0070,  0.0069,  0.0059,  0.0069,  0.0063,
         0.0056,  0.0067,  0.0069,  0.0070,  0.0070,  0.0070,  0.0069,  0.0064,
         0.0041,  0.0068,  0.0067,  0.0070,  0.0051,  0.0070,  0.0069,  0.0068,
         0.0070,  0.0067,  0.0070,  0.0062,  0.0070,  0.0069,  0.0069,  0.0070,
         0.0069,  0.0063,  0.0070,  0.0069,  0.0065,  0.0063,  0.0070,  0.0059,
         0.0069,  0.0008,  0.0069,  0.0056,  0.0068,  0.0070,  0.0070,  0.0070,
         0.0069,  0.0069,  0.0070,  0.0059,  0.0070,  0.0070,  0.0070,  0.0070,
         0.0070,  0.0070,  0.0061,  0.0066,  0.0070, -0.0026,  0.0070,  0.0069,
         0.0070,  0.0067,  0.0068,  0.0069,  0.0070,  0.0066,  0.0069,  0.0062,
         0.0062,  0.0063,  0.0067,  0.0053,  0.0064,  0.0069,  0.0056,  0.0065,
         0.0064,  0.0067,  0.0070,  0.0069,  0.0068,  0.0069,  0.0066,  0.0060,
         0.0067,  0.0070,  0.0070,  0.0066,  0.0065,  0.0068,  0.0061,  0.0069,
         0.0004,  0.0069,  0.0070,  0.0057,  0.0070,  0.0070,  0.0051,  0.0062,
         0.0070,  0.0070,  0.0070,  0.0070,  0.0030,  0.0067,  0.0070,  0.0070,
         0.0070,  0.0070,  0.0069,  0.0070,  0.0024,  0.0059,  0.0055,  0.0052,
         0.0064,  0.0058,  0.0057,  0.0070,  0.0061, -0.0009,  0.0066,  0.0063,
         0.0057,  0.0068,  0.0070,  0.0061,  0.0067,  0.0064,  0.0069,  0.0066,
         0.0058,  0.0066,  0.0070,  0.0067,  0.0069,  0.0069,  0.0059,  0.0070,
         0.0068,  0.0069,  0.0070,  0.0066,  0.0070,  0.0068,  0.0067,  0.0064,
         0.0068,  0.0066,  0.0070,  0.0069,  0.0056,  0.0070,  0.0069,  0.0069,
         0.0070,  0.0067,  0.0066,  0.0007,  0.0067,  0.0062,  0.0067,  0.0059,
         0.0068,  0.0070,  0.0070,  0.0070,  0.0070,  0.0068,  0.0057,  0.0068,
         0.0069,  0.0060,  0.0070,  0.0047,  0.0067,  0.0069,  0.0068,  0.0065,
        -0.0011,  0.0069,  0.0067,  0.0070,  0.0068,  0.0068,  0.0070,  0.0056,
         0.0069,  0.0069,  0.0055,  0.0070,  0.0068,  0.0070,  0.0069,  0.0070,
         0.0068,  0.0069,  0.0070,  0.0069,  0.0069,  0.0065,  0.0067,  0.0070,
         0.0061,  0.0065,  0.0070,  0.0069,  0.0069,  0.0063,  0.0070,  0.0069,
         0.0069,  0.0070,  0.0070,  0.0063,  0.0070,  0.0065,  0.0061,  0.0040,
         0.0070,  0.0062,  0.0064,  0.0065,  0.0070,  0.0070,  0.0068,  0.0058,
         0.0070,  0.0070,  0.0070,  0.0070,  0.0059,  0.0069,  0.0057,  0.0070,
         0.0069,  0.0070,  0.0056,  0.0068,  0.0054,  0.0070,  0.0065,  0.0068,
         0.0060,  0.0068,  0.0069,  0.0070,  0.0070,  0.0062,  0.0054,  0.0061,
         0.0070,  0.0064,  0.0056,  0.0070,  0.0070,  0.0068,  0.0069,  0.0070,
         0.0070,  0.0070, -0.0002,  0.0069,  0.0067,  0.0059,  0.0069,  0.0061,
         0.0070,  0.0068,  0.0070,  0.0065,  0.0055,  0.0068,  0.0070,  0.0070,
         0.0070,  0.0070,  0.0036,  0.0070,  0.0066, -0.0001,  0.0070,  0.0064,
         0.0069,  0.0066,  0.0068,  0.0070,  0.0069,  0.0069,  0.0069,  0.0056,
         0.0068,  0.0064,  0.0070,  0.0066,  0.0069,  0.0067,  0.0069,  0.0068,
         0.0055,  0.0057,  0.0070,  0.0070,  0.0070,  0.0059,  0.0069,  0.0068,
         0.0070,  0.0068,  0.0068,  0.0070,  0.0068,  0.0070,  0.0068,  0.0069,
         0.0064,  0.0053,  0.0065,  0.0068,  0.0069,  0.0068,  0.0069,  0.0070,
         0.0069,  0.0019,  0.0004,  0.0010,  0.0062,  0.0070,  0.0070,  0.0070,
         0.0066,  0.0070,  0.0058,  0.0068,  0.0070,  0.0070,  0.0070,  0.0070,
         0.0068,  0.0070,  0.0060,  0.0069,  0.0070,  0.0062,  0.0070,  0.0070,
         0.0070,  0.0057,  0.0070,  0.0070,  0.0068,  0.0070,  0.0070,  0.0044,
         0.0070,  0.0070,  0.0070,  0.0012,  0.0070,  0.0061,  0.0070,  0.0070,
         0.0070,  0.0069,  0.0069, -0.0009,  0.0057,  0.0070,  0.0069,  0.0070,
         0.0069,  0.0069,  0.0070,  0.0070,  0.0069,  0.0061,  0.0068,  0.0061,
         0.0070,  0.0070,  0.0068,  0.0068,  0.0069,  0.0070,  0.0070,  0.0054,
         0.0058,  0.0068,  0.0070,  0.0069,  0.0070,  0.0070,  0.0067,  0.0069,
         0.0070,  0.0024,  0.0070,  0.0070,  0.0070,  0.0070,  0.0059,  0.0069,
         0.0070,  0.0062,  0.0070,  0.0068,  0.0070,  0.0069,  0.0070,  0.0064,
         0.0069,  0.0038,  0.0070,  0.0067,  0.0070,  0.0060,  0.0062,  0.0061,
         0.0063,  0.0070,  0.0068,  0.0066,  0.0069,  0.0037,  0.0060,  0.0070,
         0.0067,  0.0051,  0.0070,  0.0070,  0.0064,  0.0070,  0.0059,  0.0070,
         0.0069,  0.0068,  0.0069,  0.0067,  0.0070,  0.0068,  0.0062,  0.0070,
         0.0006,  0.0069,  0.0027, -0.0002,  0.0070,  0.0063,  0.0060,  0.0060,
         0.0068,  0.0070,  0.0064,  0.0069,  0.0063,  0.0059,  0.0070,  0.0064,
         0.0070,  0.0009,  0.0070,  0.0069,  0.0069,  0.0069,  0.0064,  0.0069,
         0.0068,  0.0069,  0.0066,  0.0069,  0.0070,  0.0070,  0.0070,  0.0068,
         0.0070,  0.0070,  0.0060,  0.0070,  0.0070,  0.0069,  0.0065,  0.0070,
         0.0068,  0.0070,  0.0066,  0.0065,  0.0062,  0.0004,  0.0070,  0.0070,
         0.0069,  0.0070,  0.0070,  0.0069,  0.0061,  0.0069,  0.0069,  0.0069,
         0.0065,  0.0070,  0.0069,  0.0070,  0.0032,  0.0069,  0.0070,  0.0065,
         0.0069,  0.0067,  0.0062,  0.0056,  0.0069,  0.0070,  0.0068,  0.0068,
         0.0069,  0.0070,  0.0046,  0.0064,  0.0071,  0.0070,  0.0070,  0.0070,
         0.0048,  0.0069,  0.0069,  0.0067,  0.0070, -0.0017,  0.0069,  0.0070,
         0.0070,  0.0070,  0.0070,  0.0066,  0.0070,  0.0070,  0.0059,  0.0070,
         0.0070,  0.0065,  0.0070,  0.0070,  0.0070,  0.0062,  0.0062,  0.0069,
         0.0033,  0.0063, -0.0004,  0.0064,  0.0070,  0.0069,  0.0068,  0.0069,
         0.0070,  0.0065,  0.0063,  0.0068,  0.0069,  0.0056,  0.0070,  0.0057,
         0.0070,  0.0069,  0.0070,  0.0062,  0.0070,  0.0065,  0.0070,  0.0070,
         0.0063,  0.0070,  0.0068,  0.0066,  0.0069,  0.0069,  0.0067,  0.0085,
         0.0012,  0.0070,  0.0062,  0.0070,  0.0067,  0.0070,  0.0061,  0.0067,
         0.0068,  0.0070,  0.0066,  0.0070,  0.0067,  0.0068,  0.0070,  0.0069,
         0.0063,  0.0050,  0.0070,  0.0068,  0.0070,  0.0062,  0.0066,  0.0048,
         0.0070,  0.0064,  0.0066,  0.0060,  0.0069,  0.0069,  0.0059,  0.0064,
         0.0069,  0.0070,  0.0069,  0.0066,  0.0069,  0.0070,  0.0059,  0.0070],
       device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7009.8765, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7437, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9989.9561, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3911, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8659.6855, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1553, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7629.2500, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6138, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8575.2373, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8691, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8251.8730, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1919, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8817.6064, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1748, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9258.5352, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8994, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6622.8589, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7930, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9128.1055, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3877, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8723.2861, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(9146.5156, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1924, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8333.6182, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8076, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 13:06:18,902][train_inner][INFO] - {"epoch": 4, "update": 3.468, "loss": "5.212", "ntokens": "149807", "nsentences": "540.115", "prob_perplexity": "53.077", "code_perplexity": "52.825", "temp": "1.983", "loss_0": "5.064", "loss_1": "0.132", "loss_2": "0.015", "accuracy": "0.20236", "wps": "38001.5", "ups": "0.25", "wpb": "149807", "bsz": "540.1", "num_updates": "1800", "lr": "2.8125e-05", "gnorm": "0.686", "loss_scale": "32", "train_wall": "787", "gb_free": "12.9", "wall": "7251"}
[2023-09-12 13:07:06,716][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
loss: tensor(12013.8945, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0213, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11124.5283, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2137, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8833.7715, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2177, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11046.5508, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2217, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10782.6162, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4785, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12443.1289, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2148, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10823.6348, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8281, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11048.7910, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3945, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12050.7109, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8516, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10264.7021, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11342.6299, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6914, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9175.2793, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6348, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10018.1436, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.3691, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10491.1963, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0329, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6060.7271, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0323, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10521.6680, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6919, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9807.9912, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.7803, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5239.2358, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0732, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7035.3984, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1807, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8895.3105, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9321, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8197.9668, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5352, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5596.5190, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6582, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8241.0303, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0566, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8408.9775, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9419, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8535.8965, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8271, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9762.2012, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9248, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8565.5244, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6514, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7342.3662, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5840, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8119.1514, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7061, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8704.8096, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5049, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6931.6841, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6924, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([ 2.5665e-02,  2.5528e-02,  2.3239e-02,  2.5833e-02,  1.9760e-02,
         2.5696e-02,  2.5452e-02,  2.5696e-02,  2.4780e-02,  2.5436e-02,
         2.5650e-02,  2.5543e-02,  2.5406e-02,  2.5696e-02,  2.5558e-02,
         2.5650e-02,  2.5574e-02,  2.5558e-02,  1.2917e-02,  2.5696e-02,
         2.5665e-02,  2.5864e-02,  2.7802e-02,  2.7542e-02,  2.3788e-02,
         2.5406e-02,  2.5589e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,
         2.5742e-02,  2.4994e-02,  2.2430e-02,  2.5574e-02,  2.5421e-02,
         2.5696e-02,  2.3666e-02,  2.5696e-02,  2.5574e-02,  2.5513e-02,
         2.5696e-02,  2.5787e-02,  2.5696e-02,  2.4918e-02,  2.5681e-02,
         2.5879e-02,  2.5650e-02,  2.5696e-02,  2.5635e-02,  2.4673e-02,
         2.5696e-02,  2.5620e-02,  2.5177e-02,  2.5024e-02,  2.5711e-02,
         2.4582e-02,  2.5620e-02,  1.8173e-02,  2.5620e-02,  2.4292e-02,
         2.5482e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,  2.6031e-02,
         2.5635e-02,  2.5696e-02,  2.4170e-02,  2.5696e-02,  2.5696e-02,
         2.5696e-02,  2.5681e-02,  2.5696e-02,  2.5681e-02,  2.4841e-02,
         2.5330e-02,  2.5696e-02,  1.2695e-02,  2.5696e-02,  2.7023e-02,
         2.5696e-02,  2.5452e-02,  2.6672e-02,  2.5650e-02,  2.5009e-02,
         2.5391e-02,  2.5650e-02,  2.4979e-02,  2.4673e-02,  2.4979e-02,
         2.5406e-02,  2.3743e-02,  2.5070e-02,  2.5681e-02,  2.4292e-02,
         2.5574e-02,  2.5116e-02,  2.5452e-02,  2.6291e-02,  2.5620e-02,
         2.5528e-02,  2.5665e-02,  2.5299e-02,  2.4414e-02,  2.5467e-02,
         2.5696e-02,  2.5681e-02,  2.5955e-02,  2.5192e-02,  2.5558e-02,
         2.4078e-02,  2.5696e-02,  1.7303e-02,  2.5696e-02,  2.5696e-02,
         2.4292e-02,  2.5696e-02,  2.5574e-02,  2.3666e-02,  2.3865e-02,
         2.5696e-02,  2.5696e-02,  2.5696e-02,  2.5681e-02,  2.1408e-02,
         2.5436e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,
         2.5620e-02,  2.5696e-02,  2.1057e-02,  2.4460e-02,  2.3987e-02,
         2.3529e-02,  2.5101e-02,  2.4155e-02,  2.4414e-02,  2.5681e-02,
         2.4811e-02,  1.5488e-02,  2.5253e-02,  2.5055e-02,  2.4277e-02,
         2.5574e-02,  2.5757e-02,  2.4780e-02,  2.5421e-02,  2.5146e-02,
         2.5620e-02,  2.5314e-02,  2.4277e-02,  2.3499e-02,  2.5681e-02,
         2.5391e-02,  2.5604e-02,  2.5665e-02,  2.3972e-02,  2.5696e-02,
         2.5497e-02,  2.5620e-02,  2.5696e-02,  2.5314e-02,  2.5696e-02,
         2.5513e-02,  2.5391e-02,  2.5101e-02,  2.5543e-02,  2.5284e-02,
         2.5696e-02,  2.5604e-02,  2.4063e-02,  2.5696e-02,  2.5650e-02,
         2.5620e-02,  2.5696e-02,  2.5375e-02,  2.2339e-02,  1.9058e-02,
         2.5452e-02,  2.4963e-02,  2.5421e-02,  2.3453e-02,  2.5543e-02,
         2.5681e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,  2.5497e-02,
         2.3972e-02,  2.5497e-02,  2.5620e-02,  2.4750e-02,  2.5803e-02,
         2.2491e-02,  2.5467e-02,  2.5650e-02,  2.5558e-02,  2.5040e-02,
         1.7166e-02,  2.5574e-02,  2.5406e-02,  2.5696e-02,  2.5574e-02,
         2.5452e-02,  2.5742e-02,  2.4323e-02,  2.5650e-02,  2.5620e-02,
         2.4231e-02,  2.5711e-02,  2.5558e-02,  2.5696e-02,  2.5665e-02,
         2.5696e-02,  2.4094e-02,  2.5681e-02,  2.5681e-02,  2.5620e-02,
         2.5589e-02,  2.5192e-02,  2.5467e-02,  2.5696e-02,  2.4200e-02,
         2.5238e-02,  2.5696e-02,  2.5620e-02,  2.5681e-02,  2.5421e-02,
         2.5696e-02,  2.5620e-02,  2.5681e-02,  2.5681e-02,  2.5681e-02,
         2.4857e-02,  2.5681e-02,  2.5192e-02,  2.4857e-02,  2.2141e-02,
         2.5696e-02,  2.4918e-02,  2.5085e-02,  2.5192e-02,  2.5696e-02,
         2.5696e-02,  2.5574e-02,  2.4475e-02,  2.5696e-02,  2.5696e-02,
         2.5696e-02,  2.5696e-02,  2.3071e-02,  2.6184e-02,  2.4002e-02,
         2.5696e-02,  2.5635e-02,  2.5696e-02,  2.2934e-02,  2.2446e-02,
         2.1942e-02,  2.5711e-02,  2.5223e-02,  2.2934e-02,  2.4490e-02,
         2.5528e-02,  2.5635e-02,  2.5696e-02,  2.5696e-02,  2.4902e-02,
         2.2736e-02,  2.4780e-02,  2.5696e-02,  2.5146e-02,  2.3773e-02,
         2.5696e-02,  2.5696e-02,  2.5528e-02,  2.5635e-02,  2.5909e-02,
         2.5696e-02,  2.5696e-02,  1.7181e-02,  2.5650e-02,  2.5452e-02,
         2.4017e-02,  2.5665e-02,  2.4689e-02,  2.5696e-02,  2.5543e-02,
         2.5696e-02,  2.5269e-02,  2.3773e-02,  2.5513e-02,  2.5681e-02,
         2.4658e-02,  2.5696e-02,  2.6550e-02,  2.1240e-02,  2.5681e-02,
         2.5360e-02,  1.8280e-02,  2.5711e-02,  2.5146e-02,  2.5589e-02,
         2.5345e-02,  2.5543e-02,  2.5696e-02,  2.5665e-02,  2.5665e-02,
         2.5665e-02,  2.4063e-02,  2.5574e-02,  2.4979e-02,  2.5696e-02,
         2.5330e-02,  2.5665e-02,  2.5467e-02,  2.6123e-02,  2.5543e-02,
         2.3880e-02,  2.0950e-02,  2.5696e-02,  2.5681e-02,  2.5696e-02,
         2.3941e-02,  2.5589e-02,  2.6291e-02,  2.5696e-02,  2.5513e-02,
         2.5497e-02,  2.5696e-02,  2.5513e-02,  2.5894e-02,  2.5543e-02,
         2.5589e-02,  2.5085e-02,  2.4048e-02,  2.5208e-02,  2.5558e-02,
         2.5635e-02,  2.5467e-02,  2.5665e-02,  2.5711e-02,  2.5604e-02,
         1.8311e-02,  1.8036e-02,  1.3489e-02,  2.3956e-02,  2.5696e-02,
         2.5681e-02,  2.5757e-02,  2.5269e-02,  2.5696e-02,  2.3666e-02,
         2.5528e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,
         2.5513e-02,  2.5681e-02,  2.4551e-02,  2.5650e-02,  2.6321e-02,
         2.4796e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,  2.3178e-02,
         2.5696e-02,  2.5696e-02,  2.5497e-02,  2.5696e-02,  2.4277e-02,
         2.1347e-02,  2.5696e-02,  2.5711e-02,  2.5696e-02,  1.9714e-02,
         2.5696e-02,  2.3178e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,
         2.5665e-02,  2.5604e-02,  9.8038e-03,  2.2949e-02,  2.5681e-02,
         2.5650e-02,  2.5696e-02,  2.5604e-02,  2.5620e-02,  2.5726e-02,
         2.5696e-02,  2.5681e-02,  2.4857e-02,  2.5558e-02,  2.4216e-02,
         2.5696e-02,  2.5711e-02,  2.5467e-02,  2.5513e-02,  2.5620e-02,
         2.5681e-02,  2.5696e-02,  2.3331e-02,  2.4551e-02,  2.5497e-02,
         2.5681e-02,  2.5665e-02,  2.6840e-02,  2.5696e-02,  2.5391e-02,
         2.5635e-02,  2.5696e-02,  2.1149e-02,  2.5696e-02,  2.5681e-02,
         2.5696e-02,  2.5696e-02,  2.4521e-02,  2.5635e-02,  2.5696e-02,
         2.4765e-02,  2.5696e-02,  2.5574e-02,  2.2583e-02,  2.5604e-02,
         2.5696e-02,  2.5116e-02,  2.5665e-02,  2.2125e-02,  2.5696e-02,
         2.5452e-02,  2.5696e-02,  2.4704e-02,  2.4338e-02,  2.4582e-02,
         2.4933e-02,  2.5681e-02,  2.5558e-02,  2.5299e-02,  2.5650e-02,
         1.9363e-02,  2.4353e-02,  2.5696e-02,  9.9792e-03,  2.2385e-02,
         2.5681e-02,  2.5696e-02,  2.4933e-02,  2.5696e-02,  2.3300e-02,
         2.5696e-02,  2.5681e-02,  2.5482e-02,  2.5635e-02,  2.6428e-02,
         2.5696e-02,  2.5528e-02,  2.4918e-02,  2.5696e-02,  1.8677e-02,
         2.5650e-02,  2.1362e-02,  1.7914e-02,  2.5681e-02,  2.4658e-02,
         2.4765e-02,  2.2644e-02,  2.5543e-02,  2.5681e-02,  2.4994e-02,
         2.5650e-02,  2.5040e-02,  2.4475e-02,  2.5696e-02,  2.5162e-02,
         2.5696e-02,  1.3863e-02,  2.5696e-02,  2.5665e-02,  2.5589e-02,
         2.5635e-02,  2.5162e-02,  2.5650e-02,  2.5528e-02,  2.5650e-02,
         2.5253e-02,  2.5589e-02,  2.5696e-02,  2.5711e-02,  2.5696e-02,
         2.5528e-02,  2.5681e-02,  2.5696e-02,  2.5604e-02,  2.5696e-02,
         2.5696e-02,  2.5665e-02,  2.5253e-02,  2.5681e-02,  2.5574e-02,
         2.5696e-02,  2.5284e-02,  2.5269e-02,  2.1622e-02,  1.8799e-02,
         2.5238e-02,  2.5696e-02,  2.5681e-02,  2.5696e-02,  2.5696e-02,
         2.5635e-02,  2.4857e-02,  2.5665e-02,  2.5650e-02,  2.5620e-02,
         2.5208e-02,  2.5696e-02,  2.5665e-02,  2.5696e-02,  2.2034e-02,
         2.5894e-02,  2.5696e-02,  2.5162e-02,  2.5650e-02,  2.5467e-02,
         2.4506e-02,  2.3407e-02,  2.5681e-02,  2.5696e-02,  2.5513e-02,
         2.8397e-02,  2.5650e-02,  2.5696e-02,  2.3392e-02,  2.5024e-02,
         1.7105e-02,  2.5696e-02,  2.5894e-02,  2.5696e-02,  2.2781e-02,
         2.5681e-02,  2.5589e-02,  2.5406e-02,  2.5711e-02,  1.5327e-02,
         2.5589e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,
         2.5299e-02,  2.5696e-02,  2.5757e-02,  2.4658e-02,  2.5696e-02,
         2.5696e-02,  2.5253e-02,  2.5696e-02,  2.5696e-02,  2.5696e-02,
         2.2980e-02,  2.4414e-02,  2.5589e-02,  2.0386e-02,  6.7902e-04,
         1.7456e-02,  2.4979e-02,  2.5696e-02,  2.5620e-02,  2.5497e-02,
         2.5650e-02,  2.5696e-02,  2.5238e-02,  2.5009e-02,  2.5528e-02,
         2.5665e-02,  2.1912e-02,  2.5696e-02,  2.4460e-02,  2.5696e-02,
         2.5574e-02,  2.5696e-02,  2.5253e-02,  2.5726e-02,  2.5177e-02,
         2.5696e-02,  2.5696e-02,  2.3758e-02,  2.5696e-02,  2.5528e-02,
         2.5284e-02,  2.5604e-02,  2.5635e-02,  2.5406e-02, -1.7881e-06,
         1.6708e-02,  2.5696e-02,  2.4536e-02,  2.5742e-02,  2.5391e-02,
         2.5696e-02,  2.3346e-02,  2.5406e-02,  2.6001e-02,  2.5681e-02,
         2.5345e-02,  2.5696e-02,  2.5406e-02,  2.5543e-02,  2.5696e-02,
         2.5665e-02,  2.4918e-02,  2.3712e-02,  2.5696e-02,  2.5528e-02,
         1.6129e-02,  2.4292e-02,  2.5345e-02,  2.3178e-02,  2.5833e-02,
         2.4918e-02,  2.5253e-02,  2.4506e-02,  2.5650e-02,  2.5635e-02,
         2.2049e-02,  2.4872e-02,  2.5665e-02,  2.5696e-02,  2.5665e-02,
         2.5360e-02,  2.5665e-02,  2.5696e-02,  2.2629e-02,  2.5787e-02],
       device='cuda:1', dtype=torch.float16, requires_grad=True)loss: tensor(8381.3037, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2., device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7628.6963, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1367, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8663.2168, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5156, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 13:19:36,085][train_inner][INFO] - {"epoch": 4, "update": 3.854, "loss": "5.08", "ntokens": "149643", "nsentences": "540.825", "prob_perplexity": "52.963", "code_perplexity": "52.75", "temp": "1.981", "loss_0": "4.933", "loss_1": "0.132", "loss_2": "0.014", "accuracy": "0.22119", "wps": "37542.9", "ups": "0.25", "wpb": "149643", "bsz": "540.8", "num_updates": "2000", "lr": "3.125e-05", "gnorm": "0.707", "loss_scale": "16", "train_wall": "796", "gb_free": "12.6", "wall": "8049"}
loss: tensor(8064.2866, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5391, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 13:24:13,557][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2023-09-12 13:24:33,279][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 13:24:33,280][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 13:24:33,357][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2023-09-12 13:24:56,838][valid][INFO] - {"epoch": 4, "valid_loss": "4.697", "valid_ntokens": "7917.79", "valid_nsentences": "55.2525", "valid_prob_perplexity": "51.875", "valid_code_perplexity": "51.664", "valid_temp": "1.979", "valid_loss_0": "4.552", "valid_loss_1": "0.133", "valid_loss_2": "0.013", "valid_accuracy": "0.28534", "valid_wps": "33459.7", "valid_wpb": "7917.8", "valid_bsz": "55.3", "valid_num_updates": "2075", "valid_best_loss": "4.697"}
[2023-09-12 13:24:56,840][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 2075 updates
[2023-09-12 13:24:56,841][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 13:24:59,270][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 13:25:00,616][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 2075 updates, score 4.697) (writing took 3.77565784601029 seconds)
[2023-09-12 13:25:00,617][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2023-09-12 13:25:00,617][train][INFO] - {"epoch": 4, "train_loss": "5.138", "train_ntokens": "149466", "train_nsentences": "538.432", "train_prob_perplexity": "53.158", "train_code_perplexity": "52.923", "train_temp": "1.982", "train_loss_0": "4.991", "train_loss_1": "0.132", "train_loss_2": "0.015", "train_accuracy": "0.21289", "train_wps": "37184.4", "train_ups": "0.25", "train_wpb": "149466", "train_bsz": "538.4", "train_num_updates": "2075", "train_lr": "3.24219e-05", "train_gnorm": "0.716", "train_loss_scale": "16", "train_train_wall": "2055", "train_gb_free": "14", "train_wall": "8373"}
[2023-09-12 13:25:00,621][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 13:25:00,708][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2023-09-12 13:25:00,934][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 13:25:00,937][fairseq.trainer][INFO] - begin training epoch 5
[2023-09-12 13:25:00,937][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(8831.8730, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1816, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 13:33:12,174][train_inner][INFO] - {"epoch": 5, "update": 4.24, "loss": "4.952", "ntokens": "149059", "nsentences": "535.415", "prob_perplexity": "53.979", "code_perplexity": "53.777", "temp": "1.979", "loss_0": "4.806", "loss_1": "0.132", "loss_2": "0.013", "accuracy": "0.23801", "wps": "36530.2", "ups": "0.25", "wpb": "149059", "bsz": "535.4", "num_updates": "2200", "lr": "3.4375e-05", "gnorm": "0.775", "loss_scale": "16", "train_wall": "787", "gb_free": "12.6", "wall": "8865"}
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0339, 0.0338, 0.0315, 0.0343, 0.0280, 0.0339, 0.0337, 0.0339, 0.0330,
        0.0337, 0.0339, 0.0339, 0.0337, 0.0339, 0.0338, 0.0339, 0.0338, 0.0338,
        0.0207, 0.0339, 0.0339, 0.0331, 0.0309, 0.0256, 0.0320, 0.0336, 0.0338,
        0.0339, 0.0339, 0.0339, 0.0340, 0.0332, 0.0307, 0.0338, 0.0337, 0.0339,
        0.0319, 0.0339, 0.0338, 0.0338, 0.0339, 0.0341, 0.0339, 0.0332, 0.0339,
        0.0347, 0.0352, 0.0339, 0.0339, 0.0329, 0.0339, 0.0345, 0.0334, 0.0333,
        0.0342, 0.0328, 0.0339, 0.0265, 0.0338, 0.0325, 0.0337, 0.0339, 0.0339,
        0.0339, 0.0341, 0.0339, 0.0339, 0.0324, 0.0339, 0.0339, 0.0339, 0.0339,
        0.0339, 0.0339, 0.0331, 0.0336, 0.0339, 0.0209, 0.0339, 0.0337, 0.0339,
        0.0337, 0.0317, 0.0339, 0.0332, 0.0336, 0.0339, 0.0332, 0.0329, 0.0333,
        0.0336, 0.0318, 0.0333, 0.0339, 0.0323, 0.0332, 0.0334, 0.0337, 0.0297,
        0.0338, 0.0338, 0.0339, 0.0335, 0.0327, 0.0337, 0.0339, 0.0339, 0.0307,
        0.0334, 0.0338, 0.0323, 0.0340, 0.0255, 0.0339, 0.0339, 0.0325, 0.0339,
        0.0338, 0.0319, 0.0300, 0.0339, 0.0339, 0.0340, 0.0339, 0.0296, 0.0337,
        0.0339, 0.0339, 0.0339, 0.0339, 0.0338, 0.0339, 0.0293, 0.0327, 0.0322,
        0.0318, 0.0333, 0.0324, 0.0327, 0.0339, 0.0331, 0.0237, 0.0335, 0.0333,
        0.0325, 0.0338, 0.0340, 0.0330, 0.0337, 0.0334, 0.0338, 0.0335, 0.0327,
        0.0305, 0.0339, 0.0336, 0.0338, 0.0339, 0.0322, 0.0339, 0.0337, 0.0338,
        0.0339, 0.0335, 0.0339, 0.0338, 0.0336, 0.0333, 0.0338, 0.0335, 0.0339,
        0.0338, 0.0323, 0.0339, 0.0339, 0.0339, 0.0339, 0.0336, 0.0302, 0.0272,
        0.0337, 0.0332, 0.0337, 0.0321, 0.0338, 0.0340, 0.0339, 0.0339, 0.0339,
        0.0337, 0.0322, 0.0337, 0.0339, 0.0330, 0.0341, 0.0307, 0.0339, 0.0339,
        0.0338, 0.0333, 0.0254, 0.0338, 0.0337, 0.0339, 0.0338, 0.0337, 0.0341,
        0.0326, 0.0339, 0.0338, 0.0325, 0.0343, 0.0338, 0.0339, 0.0339, 0.0339,
        0.0325, 0.0339, 0.0339, 0.0338, 0.0338, 0.0334, 0.0337, 0.0339, 0.0324,
        0.0335, 0.0339, 0.0338, 0.0339, 0.0340, 0.0339, 0.0339, 0.0339, 0.0339,
        0.0339, 0.0331, 0.0339, 0.0334, 0.0331, 0.0304, 0.0339, 0.0331, 0.0347,
        0.0334, 0.0339, 0.0339, 0.0338, 0.0327, 0.0339, 0.0339, 0.0339, 0.0339,
        0.0313, 0.0342, 0.0323, 0.0339, 0.0339, 0.0339, 0.0312, 0.0285, 0.0302,
        0.0339, 0.0334, 0.0299, 0.0327, 0.0338, 0.0339, 0.0339, 0.0339, 0.0331,
        0.0311, 0.0330, 0.0339, 0.0334, 0.0320, 0.0339, 0.0339, 0.0338, 0.0339,
        0.0337, 0.0339, 0.0339, 0.0255, 0.0339, 0.0337, 0.0323, 0.0339, 0.0329,
        0.0339, 0.0338, 0.0339, 0.0335, 0.0320, 0.0338, 0.0339, 0.0348, 0.0339,
        0.0359, 0.0295, 0.0339, 0.0336, 0.0265, 0.0351, 0.0334, 0.0338, 0.0336,
        0.0338, 0.0339, 0.0339, 0.0341, 0.0339, 0.0323, 0.0339, 0.0332, 0.0339,
        0.0336, 0.0339, 0.0338, 0.0358, 0.0338, 0.0321, 0.0299, 0.0339, 0.0339,
        0.0339, 0.0322, 0.0338, 0.0336, 0.0339, 0.0338, 0.0338, 0.0339, 0.0338,
        0.0348, 0.0338, 0.0338, 0.0333, 0.0323, 0.0334, 0.0338, 0.0337, 0.0337,
        0.0339, 0.0339, 0.0338, 0.0266, 0.0263, 0.0173, 0.0322, 0.0339, 0.0339,
        0.0346, 0.0335, 0.0339, 0.0319, 0.0338, 0.0339, 0.0339, 0.0339, 0.0339,
        0.0338, 0.0339, 0.0328, 0.0339, 0.0276, 0.0330, 0.0339, 0.0339, 0.0339,
        0.0314, 0.0339, 0.0339, 0.0338, 0.0339, 0.0325, 0.0296, 0.0339, 0.0339,
        0.0339, 0.0279, 0.0339, 0.0316, 0.0339, 0.0339, 0.0339, 0.0339, 0.0339,
        0.0180, 0.0312, 0.0339, 0.0340, 0.0339, 0.0338, 0.0338, 0.0341, 0.0339,
        0.0339, 0.0331, 0.0338, 0.0324, 0.0339, 0.0342, 0.0337, 0.0338, 0.0338,
        0.0339, 0.0339, 0.0317, 0.0328, 0.0337, 0.0339, 0.0339, 0.0362, 0.0339,
        0.0336, 0.0339, 0.0339, 0.0296, 0.0339, 0.0339, 0.0339, 0.0339, 0.0327,
        0.0339, 0.0339, 0.0330, 0.0339, 0.0338, 0.0308, 0.0338, 0.0340, 0.0334,
        0.0339, 0.0304, 0.0339, 0.0337, 0.0339, 0.0329, 0.0326, 0.0328, 0.0332,
        0.0339, 0.0338, 0.0335, 0.0339, 0.0276, 0.0326, 0.0339, 0.0036, 0.0306,
        0.0339, 0.0341, 0.0332, 0.0339, 0.0315, 0.0339, 0.0339, 0.0337, 0.0339,
        0.0356, 0.0339, 0.0338, 0.0331, 0.0339, 0.0269, 0.0339, 0.0296, 0.0262,
        0.0339, 0.0329, 0.0330, 0.0309, 0.0338, 0.0339, 0.0332, 0.0339, 0.0333,
        0.0327, 0.0339, 0.0334, 0.0339, 0.0221, 0.0339, 0.0339, 0.0338, 0.0339,
        0.0334, 0.0339, 0.0338, 0.0339, 0.0335, 0.0339, 0.0339, 0.0339, 0.0339,
        0.0338, 0.0339, 0.0339, 0.0353, 0.0339, 0.0339, 0.0339, 0.0335, 0.0339,
        0.0338, 0.0340, 0.0335, 0.0335, 0.0299, 0.0270, 0.0325, 0.0339, 0.0350,
        0.0339, 0.0339, 0.0339, 0.0331, 0.0339, 0.0339, 0.0339, 0.0334, 0.0339,
        0.0339, 0.0339, 0.0306, 0.0343, 0.0339, 0.0334, 0.0339, 0.0337, 0.0326,
        0.0317, 0.0339, 0.0339, 0.0338, 0.0355, 0.0339, 0.0339, 0.0320, 0.0333,
        0.0120, 0.0339, 0.0341, 0.0339, 0.0310, 0.0339, 0.0338, 0.0336, 0.0339,
        0.0236, 0.0338, 0.0339, 0.0339, 0.0339, 0.0339, 0.0335, 0.0339, 0.0320,
        0.0329, 0.0339, 0.0339, 0.0335, 0.0339, 0.0339, 0.0339, 0.0312, 0.0327,
        0.0338, 0.0286, 0.0036, 0.0257, 0.0332, 0.0339, 0.0338, 0.0337, 0.0339,
        0.0339, 0.0335, 0.0332, 0.0338, 0.0339, 0.0298, 0.0339, 0.0331, 0.0339,
        0.0338, 0.0339, 0.0324, 0.0341, 0.0334, 0.0339, 0.0339, 0.0320, 0.0339,
        0.0338, 0.0335, 0.0338, 0.0339, 0.0336, 0.0076, 0.0238, 0.0339, 0.0328,
        0.0341, 0.0336, 0.0339, 0.0316, 0.0336, 0.0349, 0.0339, 0.0336, 0.0339,
        0.0336, 0.0338, 0.0339, 0.0339, 0.0331, 0.0320, 0.0339, 0.0338, 0.0188,
        0.0325, 0.0336, 0.0314, 0.0354, 0.0331, 0.0335, 0.0327, 0.0339, 0.0339,
        0.0303, 0.0331, 0.0339, 0.0339, 0.0339, 0.0336, 0.0339, 0.0339, 0.0309,
        0.0341], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(8045.8286, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6074, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8284.7910, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9170, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8269.2051, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5859, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 13:41:42,145][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
loss: tensor(9226.5137, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.8779, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 13:46:25,850][train_inner][INFO] - {"epoch": 5, "update": 4.626, "loss": "4.818", "ntokens": "149960", "nsentences": "536.82", "prob_perplexity": "54.801", "code_perplexity": "54.622", "temp": "1.977", "loss_0": "4.673", "loss_1": "0.132", "loss_2": "0.013", "accuracy": "0.25492", "wps": "37788.7", "ups": "0.25", "wpb": "149960", "bsz": "536.8", "num_updates": "2400", "lr": "3.75e-05", "gnorm": "0.784", "loss_scale": "16", "train_wall": "792", "gb_free": "12.4", "wall": "9658"}
tensor(-0.3040, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8820.9180, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6533, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8637.3789, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6299, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7855.5581, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4453, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7456.3320, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6855, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0353, 0.0352, 0.0329, 0.0361, 0.0294, 0.0353, 0.0351, 0.0353, 0.0344,
        0.0351, 0.0353, 0.0354, 0.0350, 0.0353, 0.0352, 0.0353, 0.0352, 0.0352,
        0.0223, 0.0353, 0.0353, 0.0343, 0.0318, 0.0246, 0.0334, 0.0350, 0.0352,
        0.0353, 0.0353, 0.0353, 0.0354, 0.0346, 0.0320, 0.0352, 0.0350, 0.0353,
        0.0333, 0.0353, 0.0352, 0.0351, 0.0353, 0.0356, 0.0353, 0.0345, 0.0353,
        0.0358, 0.0370, 0.0353, 0.0352, 0.0343, 0.0353, 0.0360, 0.0348, 0.0346,
        0.0363, 0.0342, 0.0352, 0.0280, 0.0352, 0.0339, 0.0351, 0.0353, 0.0353,
        0.0353, 0.0356, 0.0352, 0.0353, 0.0338, 0.0353, 0.0353, 0.0353, 0.0353,
        0.0353, 0.0353, 0.0345, 0.0349, 0.0353, 0.0223, 0.0353, 0.0349, 0.0353,
        0.0351, 0.0298, 0.0353, 0.0346, 0.0350, 0.0353, 0.0346, 0.0343, 0.0346,
        0.0350, 0.0332, 0.0347, 0.0353, 0.0338, 0.0345, 0.0348, 0.0352, 0.0289,
        0.0352, 0.0352, 0.0353, 0.0349, 0.0340, 0.0351, 0.0353, 0.0353, 0.0303,
        0.0348, 0.0352, 0.0337, 0.0354, 0.0269, 0.0353, 0.0353, 0.0339, 0.0353,
        0.0352, 0.0333, 0.0310, 0.0353, 0.0353, 0.0353, 0.0353, 0.0311, 0.0351,
        0.0353, 0.0353, 0.0353, 0.0353, 0.0352, 0.0353, 0.0307, 0.0341, 0.0336,
        0.0331, 0.0347, 0.0338, 0.0340, 0.0353, 0.0344, 0.0251, 0.0349, 0.0347,
        0.0339, 0.0352, 0.0354, 0.0344, 0.0350, 0.0348, 0.0352, 0.0349, 0.0345,
        0.0315, 0.0353, 0.0350, 0.0352, 0.0353, 0.0336, 0.0353, 0.0351, 0.0352,
        0.0353, 0.0349, 0.0353, 0.0351, 0.0350, 0.0347, 0.0352, 0.0349, 0.0353,
        0.0352, 0.0337, 0.0353, 0.0353, 0.0352, 0.0353, 0.0350, 0.0315, 0.0286,
        0.0351, 0.0346, 0.0350, 0.0333, 0.0352, 0.0353, 0.0353, 0.0353, 0.0353,
        0.0351, 0.0336, 0.0351, 0.0352, 0.0344, 0.0356, 0.0321, 0.0355, 0.0353,
        0.0352, 0.0347, 0.0268, 0.0352, 0.0350, 0.0353, 0.0352, 0.0351, 0.0355,
        0.0339, 0.0353, 0.0352, 0.0340, 0.0359, 0.0352, 0.0353, 0.0353, 0.0353,
        0.0337, 0.0353, 0.0353, 0.0352, 0.0352, 0.0348, 0.0351, 0.0353, 0.0338,
        0.0349, 0.0353, 0.0352, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,
        0.0353, 0.0345, 0.0353, 0.0348, 0.0345, 0.0318, 0.0353, 0.0345, 0.0360,
        0.0349, 0.0353, 0.0353, 0.0352, 0.0341, 0.0353, 0.0353, 0.0353, 0.0353,
        0.0327, 0.0355, 0.0336, 0.0353, 0.0353, 0.0353, 0.0326, 0.0292, 0.0316,
        0.0353, 0.0349, 0.0313, 0.0341, 0.0352, 0.0352, 0.0353, 0.0353, 0.0345,
        0.0326, 0.0344, 0.0353, 0.0348, 0.0334, 0.0353, 0.0353, 0.0352, 0.0353,
        0.0349, 0.0353, 0.0353, 0.0271, 0.0353, 0.0351, 0.0336, 0.0353, 0.0343,
        0.0353, 0.0352, 0.0353, 0.0349, 0.0334, 0.0351, 0.0353, 0.0365, 0.0353,
        0.0379, 0.0309, 0.0353, 0.0350, 0.0279, 0.0366, 0.0348, 0.0352, 0.0350,
        0.0352, 0.0353, 0.0353, 0.0360, 0.0353, 0.0337, 0.0353, 0.0346, 0.0353,
        0.0349, 0.0353, 0.0353, 0.0369, 0.0352, 0.0335, 0.0315, 0.0353, 0.0353,
        0.0353, 0.0336, 0.0352, 0.0356, 0.0353, 0.0351, 0.0353, 0.0353, 0.0351,
        0.0360, 0.0352, 0.0352, 0.0347, 0.0337, 0.0348, 0.0352, 0.0350, 0.0351,
        0.0353, 0.0353, 0.0352, 0.0279, 0.0276, 0.0167, 0.0336, 0.0353, 0.0353,
        0.0360, 0.0349, 0.0353, 0.0333, 0.0352, 0.0353, 0.0353, 0.0353, 0.0353,
        0.0351, 0.0353, 0.0342, 0.0353, 0.0283, 0.0344, 0.0353, 0.0353, 0.0353,
        0.0328, 0.0353, 0.0353, 0.0351, 0.0353, 0.0339, 0.0310, 0.0353, 0.0353,
        0.0353, 0.0293, 0.0353, 0.0331, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,
        0.0194, 0.0326, 0.0353, 0.0354, 0.0353, 0.0352, 0.0352, 0.0354, 0.0353,
        0.0353, 0.0345, 0.0352, 0.0338, 0.0353, 0.0355, 0.0351, 0.0351, 0.0352,
        0.0353, 0.0353, 0.0331, 0.0342, 0.0351, 0.0353, 0.0353, 0.0377, 0.0353,
        0.0350, 0.0352, 0.0353, 0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0341,
        0.0352, 0.0353, 0.0344, 0.0353, 0.0352, 0.0322, 0.0352, 0.0354, 0.0347,
        0.0353, 0.0317, 0.0353, 0.0351, 0.0353, 0.0343, 0.0340, 0.0342, 0.0345,
        0.0353, 0.0352, 0.0349, 0.0353, 0.0290, 0.0340, 0.0353, 0.0034, 0.0320,
        0.0353, 0.0358, 0.0345, 0.0353, 0.0329, 0.0353, 0.0353, 0.0351, 0.0352,
        0.0373, 0.0353, 0.0352, 0.0345, 0.0353, 0.0283, 0.0353, 0.0310, 0.0275,
        0.0353, 0.0343, 0.0344, 0.0323, 0.0352, 0.0353, 0.0346, 0.0353, 0.0347,
        0.0341, 0.0353, 0.0348, 0.0353, 0.0235, 0.0353, 0.0353, 0.0352, 0.0353,
        0.0348, 0.0353, 0.0352, 0.0353, 0.0349, 0.0353, 0.0353, 0.0353, 0.0353,
        0.0352, 0.0353, 0.0353, 0.0377, 0.0353, 0.0353, 0.0353, 0.0349, 0.0353,
        0.0352, 0.0353, 0.0349, 0.0349, 0.0312, 0.0284, 0.0339, 0.0353, 0.0370,
        0.0353, 0.0353, 0.0352, 0.0345, 0.0353, 0.0353, 0.0352, 0.0348, 0.0353,
        0.0353, 0.0353, 0.0320, 0.0357, 0.0353, 0.0348, 0.0353, 0.0351, 0.0341,
        0.0331, 0.0354, 0.0353, 0.0351, 0.0349, 0.0353, 0.0353, 0.0333, 0.0346,
        0.0117, 0.0353, 0.0353, 0.0353, 0.0324, 0.0354, 0.0352, 0.0350, 0.0353,
        0.0250, 0.0352, 0.0353, 0.0353, 0.0353, 0.0353, 0.0349, 0.0353, 0.0330,
        0.0343, 0.0353, 0.0353, 0.0349, 0.0353, 0.0353, 0.0353, 0.0326, 0.0340,
        0.0352, 0.0300, 0.0032, 0.0271, 0.0346, 0.0353, 0.0352, 0.0351, 0.0353,
        0.0353, 0.0349, 0.0346, 0.0351, 0.0353, 0.0316, 0.0353, 0.0348, 0.0353,
        0.0352, 0.0353, 0.0341, 0.0356, 0.0348, 0.0353, 0.0353, 0.0334, 0.0353,
        0.0351, 0.0349, 0.0352, 0.0352, 0.0350, 0.0092, 0.0253, 0.0353, 0.0342,
        0.0355, 0.0350, 0.0353, 0.0330, 0.0350, 0.0366, 0.0353, 0.0349, 0.0353,
        0.0350, 0.0352, 0.0353, 0.0353, 0.0345, 0.0333, 0.0353, 0.0352, 0.0195,
        0.0339, 0.0349, 0.0328, 0.0373, 0.0345, 0.0349, 0.0341, 0.0352, 0.0352,
        0.0317, 0.0345, 0.0353, 0.0353, 0.0353, 0.0350, 0.0353, 0.0353, 0.0323,
        0.0355], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8716.7412, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8482.1133, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9277, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5992.3032, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4199, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7744.8413, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3633, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7673.0112, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6094, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7729.3677, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5195, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7324.2388, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8164, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(12265.3164, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0490, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10890.6885, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0289, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(12833.5596, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0414, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11538.8467, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0349, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9107.9893, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1164, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11187.8340, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2208, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11393.1660, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2042, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11535.1807, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6006, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9753.1992, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2050, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11891.4609, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5679, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11628.8008, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5312, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10596.3232, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5820, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7866.2471, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.7227, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11596.5508, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2109, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11596.3906, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6094, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10633.1797, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4062, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11274.0059, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.9023, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11502.6064, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5859, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10450.0596, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6094, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10262.1318, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-5.0234, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11705.1045, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.8867, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11677.2793, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3125, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(11221.5225, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7637, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10655.5723, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3652, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10541.0332, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.5430, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10453.2354, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.2435, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10417.4463, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.0935, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10190.8604, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.1076, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10375.3584, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.4849, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9168.6621, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8101, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9704.6719, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6260, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10522.4395, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0039, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9070.4023, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.3181, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(10517.7041, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9507, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9152.3818, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.8745, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8725.8379, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.6216, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8164.7778, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1455, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8549.3408, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.0947, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8516.7783, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1133, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6533.0913, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2363, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8690.6855, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2539, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6513.8701, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5488, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9123.7529, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.9541, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8328.8018, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.9761, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8080.2837, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.1416, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7018.5161, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-0.5835, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8156.4443, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.2324, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7794.3135, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7578, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7806.5024, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4863, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7335.2998, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3379, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7330.4009, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(5945.7588, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4219, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7666.5830, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0664, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7846.9912, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7109, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0436, 0.0435, 0.0412, 0.0444, 0.0377, 0.0437, 0.0434, 0.0437, 0.0427,
        0.0434, 0.0439, 0.0442, 0.0439, 0.0437, 0.0435, 0.0436, 0.0436, 0.0435,
        0.0307, 0.0437, 0.0436, 0.0389, 0.0341, 0.0202, 0.0418, 0.0434, 0.0435,
        0.0436, 0.0436, 0.0437, 0.0437, 0.0430, 0.0404, 0.0435, 0.0434, 0.0436,
        0.0416, 0.0437, 0.0435, 0.0435, 0.0437, 0.0446, 0.0437, 0.0429, 0.0436,
        0.0452, 0.0459, 0.0437, 0.0436, 0.0426, 0.0437, 0.0456, 0.0432, 0.0430,
        0.0447, 0.0425, 0.0436, 0.0365, 0.0436, 0.0422, 0.0434, 0.0437, 0.0437,
        0.0437, 0.0442, 0.0436, 0.0437, 0.0421, 0.0437, 0.0436, 0.0437, 0.0436,
        0.0436, 0.0436, 0.0428, 0.0433, 0.0436, 0.0307, 0.0436, 0.0412, 0.0437,
        0.0434, 0.0239, 0.0436, 0.0429, 0.0433, 0.0436, 0.0429, 0.0426, 0.0429,
        0.0434, 0.0418, 0.0430, 0.0436, 0.0431, 0.0421, 0.0441, 0.0435, 0.0258,
        0.0436, 0.0435, 0.0436, 0.0432, 0.0424, 0.0435, 0.0437, 0.0436, 0.0245,
        0.0432, 0.0435, 0.0421, 0.0439, 0.0352, 0.0437, 0.0437, 0.0422, 0.0437,
        0.0423, 0.0416, 0.0385, 0.0437, 0.0436, 0.0437, 0.0436, 0.0396, 0.0436,
        0.0436, 0.0437, 0.0437, 0.0436, 0.0436, 0.0437, 0.0391, 0.0424, 0.0420,
        0.0415, 0.0431, 0.0421, 0.0424, 0.0436, 0.0428, 0.0334, 0.0432, 0.0430,
        0.0422, 0.0435, 0.0438, 0.0427, 0.0434, 0.0431, 0.0436, 0.0433, 0.0428,
        0.0370, 0.0436, 0.0434, 0.0435, 0.0436, 0.0419, 0.0437, 0.0435, 0.0436,
        0.0436, 0.0433, 0.0437, 0.0435, 0.0433, 0.0431, 0.0435, 0.0432, 0.0437,
        0.0435, 0.0420, 0.0436, 0.0436, 0.0436, 0.0436, 0.0433, 0.0385, 0.0369,
        0.0434, 0.0429, 0.0434, 0.0422, 0.0435, 0.0439, 0.0437, 0.0437, 0.0437,
        0.0435, 0.0419, 0.0435, 0.0436, 0.0427, 0.0448, 0.0405, 0.0436, 0.0436,
        0.0435, 0.0430, 0.0351, 0.0435, 0.0434, 0.0436, 0.0435, 0.0434, 0.0439,
        0.0423, 0.0436, 0.0436, 0.0423, 0.0453, 0.0435, 0.0437, 0.0436, 0.0437,
        0.0418, 0.0436, 0.0436, 0.0436, 0.0435, 0.0432, 0.0434, 0.0437, 0.0422,
        0.0432, 0.0437, 0.0436, 0.0436, 0.0428, 0.0437, 0.0445, 0.0436, 0.0436,
        0.0436, 0.0428, 0.0436, 0.0432, 0.0428, 0.0401, 0.0437, 0.0429, 0.0442,
        0.0432, 0.0437, 0.0436, 0.0435, 0.0424, 0.0437, 0.0437, 0.0437, 0.0437,
        0.0412, 0.0445, 0.0420, 0.0437, 0.0436, 0.0436, 0.0409, 0.0356, 0.0400,
        0.0437, 0.0432, 0.0388, 0.0424, 0.0435, 0.0436, 0.0437, 0.0437, 0.0428,
        0.0417, 0.0428, 0.0437, 0.0431, 0.0417, 0.0437, 0.0437, 0.0435, 0.0442,
        0.0418, 0.0437, 0.0437, 0.0359, 0.0436, 0.0434, 0.0420, 0.0436, 0.0426,
        0.0437, 0.0435, 0.0437, 0.0432, 0.0417, 0.0435, 0.0436, 0.0407, 0.0437,
        0.0434, 0.0392, 0.0436, 0.0433, 0.0364, 0.0455, 0.0431, 0.0435, 0.0433,
        0.0435, 0.0437, 0.0436, 0.0464, 0.0436, 0.0420, 0.0438, 0.0430, 0.0436,
        0.0433, 0.0436, 0.0453, 0.0442, 0.0435, 0.0418, 0.0424, 0.0437, 0.0436,
        0.0437, 0.0419, 0.0435, 0.0451, 0.0437, 0.0435, 0.0429, 0.0437, 0.0435,
        0.0432, 0.0435, 0.0435, 0.0431, 0.0420, 0.0432, 0.0435, 0.0433, 0.0434,
        0.0436, 0.0437, 0.0435, 0.0363, 0.0360, 0.0121, 0.0420, 0.0437, 0.0436,
        0.0454, 0.0432, 0.0437, 0.0416, 0.0435, 0.0436, 0.0436, 0.0436, 0.0437,
        0.0435, 0.0436, 0.0425, 0.0436, 0.0331, 0.0428, 0.0437, 0.0437, 0.0437,
        0.0411, 0.0436, 0.0437, 0.0435, 0.0437, 0.0423, 0.0393, 0.0436, 0.0437,
        0.0437, 0.0377, 0.0437, 0.0437, 0.0437, 0.0437, 0.0436, 0.0436, 0.0439,
        0.0279, 0.0409, 0.0436, 0.0439, 0.0437, 0.0435, 0.0436, 0.0439, 0.0437,
        0.0436, 0.0429, 0.0435, 0.0422, 0.0437, 0.0439, 0.0434, 0.0435, 0.0436,
        0.0436, 0.0437, 0.0424, 0.0425, 0.0435, 0.0436, 0.0436, 0.0439, 0.0436,
        0.0434, 0.0436, 0.0437, 0.0403, 0.0436, 0.0436, 0.0437, 0.0437, 0.0425,
        0.0436, 0.0436, 0.0427, 0.0437, 0.0435, 0.0405, 0.0436, 0.0438, 0.0431,
        0.0436, 0.0401, 0.0437, 0.0434, 0.0437, 0.0427, 0.0423, 0.0425, 0.0429,
        0.0436, 0.0435, 0.0432, 0.0436, 0.0373, 0.0423, 0.0437, 0.0053, 0.0403,
        0.0436, 0.0450, 0.0429, 0.0437, 0.0413, 0.0437, 0.0436, 0.0434, 0.0439,
        0.0455, 0.0440, 0.0435, 0.0429, 0.0436, 0.0367, 0.0436, 0.0393, 0.0361,
        0.0436, 0.0426, 0.0428, 0.0406, 0.0435, 0.0436, 0.0430, 0.0436, 0.0430,
        0.0424, 0.0437, 0.0431, 0.0437, 0.0318, 0.0436, 0.0436, 0.0435, 0.0443,
        0.0431, 0.0437, 0.0435, 0.0436, 0.0432, 0.0436, 0.0437, 0.0437, 0.0436,
        0.0435, 0.0436, 0.0437, 0.0383, 0.0437, 0.0437, 0.0436, 0.0432, 0.0436,
        0.0435, 0.0437, 0.0432, 0.0432, 0.0396, 0.0368, 0.0417, 0.0437, 0.0452,
        0.0436, 0.0436, 0.0436, 0.0428, 0.0436, 0.0436, 0.0436, 0.0432, 0.0437,
        0.0436, 0.0436, 0.0406, 0.0417, 0.0437, 0.0431, 0.0436, 0.0434, 0.0427,
        0.0417, 0.0444, 0.0437, 0.0435, 0.0366, 0.0436, 0.0437, 0.0413, 0.0430,
        0.0108, 0.0442, 0.0426, 0.0437, 0.0408, 0.0439, 0.0435, 0.0434, 0.0437,
        0.0336, 0.0435, 0.0437, 0.0437, 0.0437, 0.0437, 0.0433, 0.0437, 0.0412,
        0.0426, 0.0437, 0.0437, 0.0432, 0.0437, 0.0437, 0.0437, 0.0409, 0.0424,
        0.0435, 0.0383, 0.0037, 0.0354, 0.0429, 0.0437, 0.0436, 0.0435, 0.0436,
        0.0437, 0.0432, 0.0430, 0.0435, 0.0436, 0.0402, 0.0436, 0.0347, 0.0437,
        0.0435, 0.0437, 0.0375, 0.0453, 0.0431, 0.0437, 0.0437, 0.0417, 0.0437,
        0.0435, 0.0432, 0.0435, 0.0436, 0.0434, 0.0108, 0.0326, 0.0437, 0.0425,
        0.0438, 0.0433, 0.0436, 0.0414, 0.0434, 0.0449, 0.0436, 0.0433, 0.0436,
        0.0434, 0.0435, 0.0437, 0.0436, 0.0429, 0.0417, 0.0436, 0.0435, 0.0246,
        0.0423, 0.0433, 0.0411, 0.0470, 0.0429, 0.0432, 0.0425, 0.0436, 0.0436,
        0.0400, 0.0428, 0.0436, 0.0437, 0.0436, 0.0434, 0.0436, 0.0437, 0.0406,
        0.0439], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 13:58:54,544][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2023-09-12 13:59:11,409][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 13:59:11,410][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 13:59:11,606][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2023-09-12 13:59:35,230][valid][INFO] - {"epoch": 5, "valid_loss": "4.513", "valid_ntokens": "7873.81", "valid_nsentences": "55.2525", "valid_prob_perplexity": "56.149", "valid_code_perplexity": "55.979", "valid_temp": "1.974", "valid_loss_0": "4.369", "valid_loss_1": "0.132", "valid_loss_2": "0.012", "valid_accuracy": "0.30106", "valid_wps": "33074.8", "valid_wpb": "7873.8", "valid_bsz": "55.3", "valid_num_updates": "2594", "valid_best_loss": "4.513"}
[2023-09-12 13:59:35,231][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 2594 updates
[2023-09-12 13:59:35,232][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 13:59:37,671][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 13:59:38,975][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 2594 updates, score 4.513) (writing took 3.743601585039869 seconds)
[2023-09-12 13:59:38,976][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2023-09-12 13:59:38,976][train][INFO] - {"epoch": 5, "train_loss": "4.864", "train_ntokens": "149476", "train_nsentences": "538.472", "train_prob_perplexity": "55.468", "train_code_perplexity": "55.282", "train_temp": "1.977", "train_loss_0": "4.72", "train_loss_1": "0.132", "train_loss_2": "0.013", "train_accuracy": "0.2453", "train_wps": "37326.6", "train_ups": "0.25", "train_wpb": "149476", "train_bsz": "538.5", "train_num_updates": "2594", "train_lr": "4.05312e-05", "train_gnorm": "0.753", "train_loss_scale": "16", "train_train_wall": "2047", "train_gb_free": "13.9", "train_wall": "10452"}
[2023-09-12 13:59:38,979][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 13:59:39,077][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2023-09-12 13:59:39,407][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 13:59:39,410][fairseq.trainer][INFO] - begin training epoch 6
[2023-09-12 13:59:39,410][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(7802.7915, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3848, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 14:00:02,709][train_inner][INFO] - {"epoch": 6, "update": 5.012, "loss": "4.876", "ntokens": "148868", "nsentences": "539.54", "prob_perplexity": "57.038", "code_perplexity": "56.853", "temp": "1.975", "loss_0": "4.732", "loss_1": "0.131", "loss_2": "0.012", "accuracy": "0.23751", "wps": "36449", "ups": "0.24", "wpb": "148868", "bsz": "539.5", "num_updates": "2600", "lr": "4.0625e-05", "gnorm": "0.722", "loss_scale": "16", "train_wall": "787", "gb_free": "13.1", "wall": "10475"}
loss: tensor(7551.8872, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6348, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7104.4497, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3535, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 14:13:12,690][train_inner][INFO] - {"epoch": 6, "update": 5.395, "loss": "4.824", "ntokens": "149698", "nsentences": "543.265", "prob_perplexity": "58.432", "code_perplexity": "58.238", "temp": "1.973", "loss_0": "4.682", "loss_1": "0.131", "loss_2": "0.012", "accuracy": "0.24205", "wps": "37899.1", "ups": "0.25", "wpb": "149698", "bsz": "543.3", "num_updates": "2800", "lr": "4.375e-05", "gnorm": "0.666", "loss_scale": "16", "train_wall": "789", "gb_free": "13", "wall": "11265"}
loss: tensor(7501.0835, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0098, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 14:16:19,289][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0585, 0.0583, 0.0560, 0.0566, 0.0526, 0.0585, 0.0583, 0.0585, 0.0576,
        0.0583, 0.0603, 0.0587, 0.0586, 0.0585, 0.0583, 0.0585, 0.0591, 0.0583,
        0.0443, 0.0585, 0.0585, 0.0505, 0.0332, 0.0158, 0.0568, 0.0582, 0.0584,
        0.0585, 0.0585, 0.0585, 0.0591, 0.0578, 0.0554, 0.0584, 0.0582, 0.0585,
        0.0565, 0.0585, 0.0584, 0.0583, 0.0585, 0.0499, 0.0585, 0.0577, 0.0585,
        0.0572, 0.0541, 0.0585, 0.0584, 0.0575, 0.0585, 0.0584, 0.0587, 0.0578,
        0.0579, 0.0574, 0.0595, 0.0514, 0.0584, 0.0571, 0.0583, 0.0585, 0.0585,
        0.0585, 0.0609, 0.0584, 0.0585, 0.0570, 0.0585, 0.0585, 0.0587, 0.0585,
        0.0585, 0.0585, 0.0576, 0.0581, 0.0585, 0.0460, 0.0585, 0.0522, 0.0585,
        0.0583, 0.0199, 0.0585, 0.0578, 0.0582, 0.0585, 0.0578, 0.0575, 0.0578,
        0.0586, 0.0559, 0.0579, 0.0585, 0.0560, 0.0540, 0.0573, 0.0585, 0.0296,
        0.0584, 0.0583, 0.0585, 0.0581, 0.0573, 0.0585, 0.0585, 0.0585, 0.0235,
        0.0585, 0.0584, 0.0573, 0.0591, 0.0501, 0.0585, 0.0585, 0.0571, 0.0585,
        0.0431, 0.0565, 0.0519, 0.0585, 0.0585, 0.0585, 0.0585, 0.0563, 0.0593,
        0.0585, 0.0585, 0.0585, 0.0585, 0.0584, 0.0585, 0.0540, 0.0573, 0.0577,
        0.0564, 0.0579, 0.0570, 0.0572, 0.0585, 0.0576, 0.0483, 0.0580, 0.0579,
        0.0571, 0.0584, 0.0586, 0.0576, 0.0582, 0.0580, 0.0586, 0.0582, 0.0577,
        0.0490, 0.0585, 0.0586, 0.0584, 0.0585, 0.0568, 0.0585, 0.0583, 0.0585,
        0.0585, 0.0581, 0.0585, 0.0583, 0.0582, 0.0579, 0.0584, 0.0581, 0.0585,
        0.0584, 0.0569, 0.0585, 0.0585, 0.0586, 0.0585, 0.0582, 0.0512, 0.0520,
        0.0583, 0.0578, 0.0582, 0.0563, 0.0583, 0.0586, 0.0585, 0.0585, 0.0585,
        0.0583, 0.0569, 0.0583, 0.0584, 0.0576, 0.0570, 0.0553, 0.0573, 0.0585,
        0.0583, 0.0579, 0.0500, 0.0584, 0.0582, 0.0585, 0.0584, 0.0596, 0.0580,
        0.0578, 0.0585, 0.0584, 0.0572, 0.0599, 0.0584, 0.0585, 0.0585, 0.0585,
        0.0539, 0.0585, 0.0587, 0.0584, 0.0584, 0.0581, 0.0583, 0.0585, 0.0570,
        0.0580, 0.0585, 0.0584, 0.0585, 0.0543, 0.0587, 0.0614, 0.0585, 0.0586,
        0.0585, 0.0577, 0.0587, 0.0580, 0.0577, 0.0549, 0.0585, 0.0580, 0.0568,
        0.0580, 0.0585, 0.0585, 0.0584, 0.0573, 0.0585, 0.0585, 0.0585, 0.0585,
        0.0563, 0.0597, 0.0570, 0.0585, 0.0585, 0.0585, 0.0558, 0.0466, 0.0549,
        0.0586, 0.0580, 0.0497, 0.0573, 0.0583, 0.0584, 0.0585, 0.0585, 0.0577,
        0.0555, 0.0576, 0.0585, 0.0580, 0.0566, 0.0585, 0.0585, 0.0583, 0.0602,
        0.0543, 0.0585, 0.0585, 0.0510, 0.0585, 0.0583, 0.0568, 0.0585, 0.0575,
        0.0585, 0.0583, 0.0585, 0.0581, 0.0566, 0.0583, 0.0585, 0.0511, 0.0585,
        0.0378, 0.0540, 0.0585, 0.0582, 0.0551, 0.0622, 0.0581, 0.0584, 0.0582,
        0.0583, 0.0585, 0.0585, 0.0620, 0.0585, 0.0569, 0.0590, 0.0579, 0.0585,
        0.0584, 0.0585, 0.0606, 0.0579, 0.0583, 0.0567, 0.0561, 0.0585, 0.0585,
        0.0586, 0.0568, 0.0584, 0.0596, 0.0585, 0.0586, 0.0572, 0.0585, 0.0583,
        0.0578, 0.0584, 0.0584, 0.0579, 0.0571, 0.0580, 0.0584, 0.0582, 0.0583,
        0.0585, 0.0585, 0.0584, 0.0512, 0.0508, 0.0139, 0.0573, 0.0585, 0.0585,
        0.0600, 0.0581, 0.0585, 0.0565, 0.0583, 0.0585, 0.0585, 0.0585, 0.0585,
        0.0583, 0.0585, 0.0574, 0.0585, 0.0441, 0.0576, 0.0584, 0.0585, 0.0585,
        0.0560, 0.0585, 0.0585, 0.0583, 0.0585, 0.0571, 0.0541, 0.0585, 0.0596,
        0.0585, 0.0525, 0.0588, 0.0583, 0.0585, 0.0585, 0.0585, 0.0585, 0.0589,
        0.0426, 0.0558, 0.0585, 0.0596, 0.0585, 0.0584, 0.0584, 0.0616, 0.0586,
        0.0585, 0.0582, 0.0583, 0.0585, 0.0585, 0.0592, 0.0585, 0.0583, 0.0584,
        0.0585, 0.0585, 0.0585, 0.0574, 0.0583, 0.0585, 0.0585, 0.0557, 0.0585,
        0.0582, 0.0584, 0.0585, 0.0546, 0.0585, 0.0585, 0.0585, 0.0585, 0.0573,
        0.0584, 0.0585, 0.0576, 0.0585, 0.0584, 0.0554, 0.0584, 0.0586, 0.0579,
        0.0595, 0.0549, 0.0585, 0.0583, 0.0585, 0.0575, 0.0572, 0.0574, 0.0577,
        0.0585, 0.0584, 0.0581, 0.0585, 0.0522, 0.0572, 0.0585, 0.0185, 0.0552,
        0.0585, 0.0568, 0.0577, 0.0585, 0.0561, 0.0585, 0.0585, 0.0583, 0.0595,
        0.0471, 0.0610, 0.0583, 0.0577, 0.0585, 0.0515, 0.0585, 0.0542, 0.0514,
        0.0585, 0.0576, 0.0586, 0.0555, 0.0583, 0.0585, 0.0578, 0.0585, 0.0578,
        0.0573, 0.0585, 0.0580, 0.0585, 0.0467, 0.0585, 0.0585, 0.0584, 0.0576,
        0.0580, 0.0584, 0.0584, 0.0585, 0.0581, 0.0585, 0.0587, 0.0585, 0.0585,
        0.0583, 0.0585, 0.0585, 0.0320, 0.0585, 0.0585, 0.0585, 0.0581, 0.0585,
        0.0584, 0.0585, 0.0581, 0.0583, 0.0544, 0.0516, 0.0577, 0.0585, 0.0533,
        0.0585, 0.0585, 0.0584, 0.0577, 0.0585, 0.0585, 0.0587, 0.0582, 0.0585,
        0.0585, 0.0585, 0.0551, 0.0338, 0.0585, 0.0580, 0.0585, 0.0583, 0.0573,
        0.0570, 0.0562, 0.0585, 0.0584, 0.0471, 0.0585, 0.0585, 0.0561, 0.0578,
        0.0159, 0.0572, 0.0579, 0.0585, 0.0556, 0.0586, 0.0584, 0.0584, 0.0585,
        0.0493, 0.0584, 0.0586, 0.0585, 0.0586, 0.0591, 0.0581, 0.0585, 0.0532,
        0.0575, 0.0585, 0.0585, 0.0580, 0.0585, 0.0585, 0.0585, 0.0558, 0.0572,
        0.0584, 0.0532, 0.0126, 0.0503, 0.0578, 0.0585, 0.0584, 0.0583, 0.0585,
        0.0585, 0.0580, 0.0578, 0.0583, 0.0585, 0.0504, 0.0585, 0.0259, 0.0585,
        0.0584, 0.0585, 0.0404, 0.0605, 0.0580, 0.0585, 0.0585, 0.0566, 0.0585,
        0.0583, 0.0581, 0.0584, 0.0588, 0.0584, 0.0067, 0.0437, 0.0593, 0.0574,
        0.0587, 0.0582, 0.0585, 0.0565, 0.0582, 0.0572, 0.0585, 0.0581, 0.0585,
        0.0582, 0.0583, 0.0587, 0.0585, 0.0579, 0.0565, 0.0585, 0.0583, 0.0374,
        0.0571, 0.0581, 0.0560, 0.0458, 0.0577, 0.0581, 0.0573, 0.0584, 0.0584,
        0.0549, 0.0577, 0.0585, 0.0585, 0.0585, 0.0582, 0.0585, 0.0585, 0.0555,
        0.0595], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7811.2441, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2852, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7198.3521, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1680, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 14:26:31,631][train_inner][INFO] - {"epoch": 6, "update": 5.781, "loss": "4.78", "ntokens": "149952", "nsentences": "537.435", "prob_perplexity": "59.819", "code_perplexity": "59.606", "temp": "1.971", "loss_0": "4.638", "loss_1": "0.131", "loss_2": "0.011", "accuracy": "0.24592", "wps": "37537.8", "ups": "0.25", "wpb": "149952", "bsz": "537.4", "num_updates": "3000", "lr": "4.6875e-05", "gnorm": "0.685", "loss_scale": "16", "train_wall": "798", "gb_free": "12.9", "wall": "12064"}
loss: tensor(8544.9922, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3906, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8758.3643, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0996, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 14:31:57,900][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
loss: tensor(6903.4067, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4473, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 14:33:59,338][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 14:33:59,339][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 14:33:59,525][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2023-09-12 14:34:23,292][valid][INFO] - {"epoch": 6, "valid_loss": "4.451", "valid_ntokens": "7895.09", "valid_nsentences": "55.2525", "valid_prob_perplexity": "59.569", "valid_code_perplexity": "59.405", "valid_temp": "1.969", "valid_loss_0": "4.309", "valid_loss_1": "0.131", "valid_loss_2": "0.011", "valid_accuracy": "0.30179", "valid_wps": "33123.7", "valid_wpb": "7895.1", "valid_bsz": "55.3", "valid_num_updates": "3113", "valid_best_loss": "4.451"}
[2023-09-12 14:34:23,293][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 3113 updates
[2023-09-12 14:34:23,294][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 14:34:25,813][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 14:34:27,168][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 3113 updates, score 4.451) (writing took 3.8745582629926503 seconds)
[2023-09-12 14:34:27,168][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2023-09-12 14:34:27,169][train][INFO] - {"epoch": 6, "train_loss": "4.791", "train_ntokens": "149505", "train_nsentences": "538.476", "train_prob_perplexity": "59.479", "train_code_perplexity": "59.27", "train_temp": "1.972", "train_loss_0": "4.649", "train_loss_1": "0.131", "train_loss_2": "0.011", "train_accuracy": "0.24507", "train_wps": "37157.9", "train_ups": "0.25", "train_wpb": "149505", "train_bsz": "538.5", "train_num_updates": "3113", "train_lr": "4.86406e-05", "train_gnorm": "0.691", "train_loss_scale": "8", "train_train_wall": "2057", "train_gb_free": "16", "train_wall": "12540"}
[2023-09-12 14:34:27,170][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 14:34:27,248][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2023-09-12 14:34:27,473][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 14:34:27,476][fairseq.trainer][INFO] - begin training epoch 7
[2023-09-12 14:34:27,476][fairseq_cli.train][INFO] - Start iterating over samples
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0679, 0.0678, 0.0655, 0.0643, 0.0620, 0.0680, 0.0677, 0.0680, 0.0671,
        0.0677, 0.0671, 0.0673, 0.0680, 0.0680, 0.0678, 0.0679, 0.0689, 0.0678,
        0.0538, 0.0680, 0.0679, 0.0568, 0.0314, 0.0152, 0.0666, 0.0677, 0.0679,
        0.0680, 0.0680, 0.0680, 0.0693, 0.0673, 0.0649, 0.0679, 0.0677, 0.0680,
        0.0659, 0.0680, 0.0679, 0.0678, 0.0680, 0.0482, 0.0680, 0.0673, 0.0680,
        0.0636, 0.0605, 0.0680, 0.0679, 0.0670, 0.0680, 0.0686, 0.0677, 0.0673,
        0.0677, 0.0668, 0.0680, 0.0609, 0.0679, 0.0666, 0.0677, 0.0680, 0.0685,
        0.0680, 0.0692, 0.0679, 0.0680, 0.0665, 0.0680, 0.0680, 0.0683, 0.0679,
        0.0680, 0.0679, 0.0671, 0.0676, 0.0680, 0.0566, 0.0680, 0.0602, 0.0682,
        0.0677, 0.0261, 0.0679, 0.0673, 0.0677, 0.0679, 0.0673, 0.0670, 0.0673,
        0.0685, 0.0652, 0.0673, 0.0679, 0.0634, 0.0610, 0.0660, 0.0679, 0.0305,
        0.0679, 0.0678, 0.0679, 0.0676, 0.0668, 0.0682, 0.0680, 0.0679, 0.0301,
        0.0685, 0.0682, 0.0665, 0.0689, 0.0596, 0.0680, 0.0680, 0.0666, 0.0680,
        0.0348, 0.0659, 0.0612, 0.0680, 0.0680, 0.0680, 0.0679, 0.0640, 0.0686,
        0.0680, 0.0680, 0.0681, 0.0680, 0.0679, 0.0680, 0.0634, 0.0668, 0.0675,
        0.0659, 0.0674, 0.0670, 0.0667, 0.0680, 0.0671, 0.0578, 0.0675, 0.0673,
        0.0665, 0.0679, 0.0680, 0.0671, 0.0677, 0.0674, 0.0681, 0.0678, 0.0665,
        0.0582, 0.0679, 0.0678, 0.0679, 0.0679, 0.0662, 0.0680, 0.0677, 0.0679,
        0.0680, 0.0676, 0.0680, 0.0678, 0.0677, 0.0674, 0.0679, 0.0676, 0.0680,
        0.0679, 0.0663, 0.0680, 0.0679, 0.0682, 0.0680, 0.0676, 0.0551, 0.0614,
        0.0677, 0.0673, 0.0677, 0.0662, 0.0678, 0.0681, 0.0680, 0.0680, 0.0680,
        0.0677, 0.0665, 0.0677, 0.0679, 0.0670, 0.0662, 0.0648, 0.0667, 0.0679,
        0.0678, 0.0674, 0.0594, 0.0679, 0.0677, 0.0680, 0.0679, 0.0699, 0.0659,
        0.0676, 0.0679, 0.0679, 0.0668, 0.0696, 0.0678, 0.0680, 0.0679, 0.0680,
        0.0603, 0.0679, 0.0681, 0.0679, 0.0679, 0.0674, 0.0677, 0.0680, 0.0665,
        0.0676, 0.0680, 0.0679, 0.0679, 0.0598, 0.0682, 0.0686, 0.0679, 0.0681,
        0.0679, 0.0671, 0.0681, 0.0675, 0.0671, 0.0644, 0.0680, 0.0676, 0.0654,
        0.0676, 0.0680, 0.0680, 0.0679, 0.0668, 0.0680, 0.0680, 0.0680, 0.0680,
        0.0659, 0.0665, 0.0667, 0.0680, 0.0679, 0.0680, 0.0652, 0.0551, 0.0645,
        0.0688, 0.0675, 0.0585, 0.0668, 0.0679, 0.0679, 0.0680, 0.0680, 0.0672,
        0.0654, 0.0671, 0.0680, 0.0674, 0.0660, 0.0681, 0.0680, 0.0678, 0.0692,
        0.0552, 0.0680, 0.0680, 0.0603, 0.0681, 0.0677, 0.0663, 0.0679, 0.0670,
        0.0680, 0.0678, 0.0680, 0.0676, 0.0660, 0.0678, 0.0680, 0.0562, 0.0680,
        0.0355, 0.0635, 0.0679, 0.0676, 0.0640, 0.0659, 0.0676, 0.0679, 0.0676,
        0.0678, 0.0680, 0.0679, 0.0641, 0.0679, 0.0663, 0.0681, 0.0673, 0.0680,
        0.0685, 0.0679, 0.0647, 0.0669, 0.0678, 0.0662, 0.0617, 0.0680, 0.0680,
        0.0681, 0.0662, 0.0679, 0.0675, 0.0680, 0.0681, 0.0664, 0.0680, 0.0678,
        0.0676, 0.0683, 0.0679, 0.0674, 0.0673, 0.0675, 0.0678, 0.0677, 0.0677,
        0.0679, 0.0680, 0.0679, 0.0608, 0.0603, 0.0212, 0.0664, 0.0680, 0.0680,
        0.0657, 0.0676, 0.0680, 0.0659, 0.0678, 0.0680, 0.0681, 0.0680, 0.0680,
        0.0678, 0.0679, 0.0668, 0.0679, 0.0519, 0.0671, 0.0677, 0.0680, 0.0680,
        0.0654, 0.0680, 0.0680, 0.0677, 0.0680, 0.0666, 0.0636, 0.0680, 0.0702,
        0.0680, 0.0620, 0.0682, 0.0617, 0.0680, 0.0680, 0.0680, 0.0679, 0.0694,
        0.0521, 0.0652, 0.0679, 0.0678, 0.0680, 0.0679, 0.0679, 0.0637, 0.0681,
        0.0680, 0.0676, 0.0678, 0.0673, 0.0680, 0.0671, 0.0687, 0.0678, 0.0679,
        0.0680, 0.0680, 0.0649, 0.0668, 0.0677, 0.0679, 0.0679, 0.0586, 0.0680,
        0.0677, 0.0679, 0.0680, 0.0634, 0.0680, 0.0683, 0.0680, 0.0680, 0.0668,
        0.0679, 0.0680, 0.0670, 0.0680, 0.0679, 0.0649, 0.0679, 0.0682, 0.0674,
        0.0708, 0.0644, 0.0680, 0.0677, 0.0680, 0.0671, 0.0667, 0.0668, 0.0672,
        0.0680, 0.0679, 0.0676, 0.0679, 0.0616, 0.0667, 0.0680, 0.0244, 0.0646,
        0.0679, 0.0565, 0.0672, 0.0680, 0.0656, 0.0680, 0.0679, 0.0677, 0.0684,
        0.0516, 0.0695, 0.0678, 0.0672, 0.0680, 0.0610, 0.0679, 0.0637, 0.0612,
        0.0681, 0.0680, 0.0682, 0.0649, 0.0678, 0.0679, 0.0673, 0.0680, 0.0673,
        0.0668, 0.0680, 0.0674, 0.0680, 0.0562, 0.0680, 0.0679, 0.0679, 0.0681,
        0.0674, 0.0691, 0.0684, 0.0679, 0.0675, 0.0680, 0.0682, 0.0680, 0.0680,
        0.0678, 0.0679, 0.0680, 0.0333, 0.0680, 0.0680, 0.0679, 0.0675, 0.0679,
        0.0679, 0.0680, 0.0676, 0.0681, 0.0639, 0.0611, 0.0663, 0.0680, 0.0488,
        0.0680, 0.0680, 0.0679, 0.0671, 0.0679, 0.0679, 0.0687, 0.0677, 0.0680,
        0.0679, 0.0680, 0.0646, 0.0255, 0.0680, 0.0674, 0.0679, 0.0677, 0.0669,
        0.0650, 0.0629, 0.0680, 0.0679, 0.0536, 0.0679, 0.0680, 0.0653, 0.0673,
        0.0206, 0.0670, 0.0644, 0.0680, 0.0651, 0.0681, 0.0679, 0.0677, 0.0680,
        0.0581, 0.0679, 0.0681, 0.0680, 0.0682, 0.0688, 0.0676, 0.0680, 0.0600,
        0.0670, 0.0680, 0.0680, 0.0675, 0.0680, 0.0680, 0.0680, 0.0652, 0.0667,
        0.0679, 0.0627, 0.0193, 0.0598, 0.0674, 0.0680, 0.0679, 0.0677, 0.0679,
        0.0680, 0.0675, 0.0673, 0.0678, 0.0679, 0.0575, 0.0680, 0.0249, 0.0680,
        0.0679, 0.0680, 0.0430, 0.0686, 0.0674, 0.0680, 0.0680, 0.0660, 0.0680,
        0.0678, 0.0676, 0.0679, 0.0686, 0.0683, 0.0124, 0.0511, 0.0689, 0.0668,
        0.0683, 0.0677, 0.0680, 0.0658, 0.0677, 0.0652, 0.0679, 0.0676, 0.0680,
        0.0677, 0.0678, 0.0685, 0.0682, 0.0676, 0.0660, 0.0680, 0.0678, 0.0458,
        0.0666, 0.0676, 0.0654, 0.0438, 0.0673, 0.0675, 0.0668, 0.0679, 0.0679,
        0.0643, 0.0671, 0.0679, 0.0680, 0.0679, 0.0678, 0.0679, 0.0680, 0.0649,
        0.0685], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(6814.4580, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2949, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8032.2939, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1094, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 14:40:09,270][train_inner][INFO] - {"epoch": 7, "update": 6.167, "loss": "4.737", "ntokens": "148862", "nsentences": "537.06", "prob_perplexity": "61.044", "code_perplexity": "60.813", "temp": "1.969", "loss_0": "4.595", "loss_1": "0.131", "loss_2": "0.011", "accuracy": "0.25041", "wps": "36412.7", "ups": "0.24", "wpb": "148862", "bsz": "537.1", "num_updates": "3200", "lr": "5e-05", "gnorm": "0.705", "loss_scale": "8", "train_wall": "788", "gb_free": "12.7", "wall": "12882"}
tensor(-2.7949, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8240.0791, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.1855, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7504.6226, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3125, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8237.4570, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4844, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6648.8716, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7500, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7724.4150, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8281, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8472.0195, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0742, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5902.2188, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.5986, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9028.7939, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8301, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8626.0732, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3145, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8582.8271, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8047, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7512.4580, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5332, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5626.8862, device='cuda:3')
loss_ent_max: tensor(-2.1309, device='cuda:3', dtype=torch.float16)
loss: tensor(8356.2803, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9707, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0705, 0.0704, 0.0681, 0.0666, 0.0646, 0.0706, 0.0703, 0.0706, 0.0696,
        0.0703, 0.0697, 0.0686, 0.0705, 0.0706, 0.0704, 0.0705, 0.0715, 0.0704,
        0.0564, 0.0706, 0.0705, 0.0587, 0.0319, 0.0169, 0.0685, 0.0703, 0.0704,
        0.0709, 0.0706, 0.0706, 0.0715, 0.0698, 0.0675, 0.0704, 0.0703, 0.0706,
        0.0685, 0.0706, 0.0704, 0.0704, 0.0706, 0.0503, 0.0706, 0.0698, 0.0706,
        0.0639, 0.0624, 0.0706, 0.0705, 0.0695, 0.0706, 0.0715, 0.0696, 0.0699,
        0.0698, 0.0695, 0.0705, 0.0642, 0.0705, 0.0692, 0.0704, 0.0706, 0.0706,
        0.0706, 0.0717, 0.0705, 0.0706, 0.0691, 0.0706, 0.0706, 0.0710, 0.0706,
        0.0706, 0.0706, 0.0697, 0.0702, 0.0706, 0.0597, 0.0706, 0.0626, 0.0707,
        0.0703, 0.0282, 0.0705, 0.0698, 0.0703, 0.0705, 0.0698, 0.0695, 0.0698,
        0.0715, 0.0675, 0.0699, 0.0706, 0.0654, 0.0635, 0.0688, 0.0704, 0.0308,
        0.0705, 0.0704, 0.0705, 0.0701, 0.0696, 0.0711, 0.0706, 0.0706, 0.0301,
        0.0699, 0.0710, 0.0691, 0.0711, 0.0622, 0.0706, 0.0706, 0.0692, 0.0706,
        0.0332, 0.0685, 0.0638, 0.0706, 0.0706, 0.0706, 0.0706, 0.0669, 0.0714,
        0.0706, 0.0706, 0.0707, 0.0706, 0.0705, 0.0706, 0.0662, 0.0694, 0.0669,
        0.0684, 0.0699, 0.0699, 0.0693, 0.0706, 0.0696, 0.0604, 0.0701, 0.0699,
        0.0692, 0.0704, 0.0706, 0.0696, 0.0703, 0.0700, 0.0708, 0.0703, 0.0688,
        0.0608, 0.0706, 0.0708, 0.0704, 0.0706, 0.0688, 0.0706, 0.0704, 0.0706,
        0.0706, 0.0702, 0.0706, 0.0704, 0.0703, 0.0699, 0.0704, 0.0701, 0.0706,
        0.0704, 0.0689, 0.0706, 0.0705, 0.0712, 0.0706, 0.0703, 0.0553, 0.0639,
        0.0703, 0.0698, 0.0703, 0.0696, 0.0704, 0.0707, 0.0706, 0.0706, 0.0706,
        0.0704, 0.0691, 0.0704, 0.0705, 0.0696, 0.0690, 0.0674, 0.0697, 0.0705,
        0.0704, 0.0700, 0.0620, 0.0704, 0.0703, 0.0706, 0.0704, 0.0734, 0.0684,
        0.0692, 0.0705, 0.0705, 0.0692, 0.0724, 0.0704, 0.0706, 0.0705, 0.0706,
        0.0625, 0.0706, 0.0706, 0.0705, 0.0704, 0.0703, 0.0703, 0.0706, 0.0691,
        0.0702, 0.0706, 0.0705, 0.0706, 0.0596, 0.0709, 0.0700, 0.0706, 0.0706,
        0.0706, 0.0697, 0.0707, 0.0701, 0.0697, 0.0670, 0.0706, 0.0703, 0.0673,
        0.0707, 0.0706, 0.0706, 0.0706, 0.0693, 0.0706, 0.0706, 0.0706, 0.0708,
        0.0684, 0.0687, 0.0693, 0.0706, 0.0705, 0.0706, 0.0678, 0.0572, 0.0671,
        0.0713, 0.0701, 0.0610, 0.0693, 0.0704, 0.0705, 0.0706, 0.0706, 0.0698,
        0.0683, 0.0696, 0.0706, 0.0700, 0.0686, 0.0707, 0.0707, 0.0704, 0.0723,
        0.0555, 0.0706, 0.0706, 0.0640, 0.0706, 0.0703, 0.0688, 0.0705, 0.0696,
        0.0706, 0.0704, 0.0706, 0.0701, 0.0686, 0.0704, 0.0707, 0.0581, 0.0706,
        0.0374, 0.0661, 0.0706, 0.0703, 0.0673, 0.0651, 0.0701, 0.0706, 0.0703,
        0.0704, 0.0706, 0.0706, 0.0615, 0.0706, 0.0689, 0.0709, 0.0700, 0.0706,
        0.0717, 0.0706, 0.0661, 0.0688, 0.0704, 0.0687, 0.0608, 0.0706, 0.0706,
        0.0707, 0.0688, 0.0704, 0.0691, 0.0706, 0.0708, 0.0689, 0.0706, 0.0704,
        0.0699, 0.0710, 0.0704, 0.0699, 0.0707, 0.0701, 0.0704, 0.0700, 0.0703,
        0.0706, 0.0706, 0.0704, 0.0634, 0.0629, 0.0237, 0.0695, 0.0706, 0.0706,
        0.0657, 0.0701, 0.0706, 0.0685, 0.0704, 0.0706, 0.0707, 0.0706, 0.0706,
        0.0704, 0.0706, 0.0694, 0.0705, 0.0545, 0.0696, 0.0703, 0.0706, 0.0706,
        0.0681, 0.0706, 0.0706, 0.0704, 0.0706, 0.0692, 0.0662, 0.0706, 0.0735,
        0.0706, 0.0646, 0.0707, 0.0614, 0.0706, 0.0706, 0.0706, 0.0706, 0.0718,
        0.0547, 0.0678, 0.0706, 0.0693, 0.0706, 0.0704, 0.0705, 0.0610, 0.0709,
        0.0706, 0.0708, 0.0704, 0.0699, 0.0706, 0.0692, 0.0714, 0.0704, 0.0705,
        0.0706, 0.0706, 0.0651, 0.0694, 0.0704, 0.0706, 0.0705, 0.0574, 0.0706,
        0.0703, 0.0705, 0.0706, 0.0653, 0.0706, 0.0707, 0.0706, 0.0706, 0.0694,
        0.0705, 0.0706, 0.0696, 0.0706, 0.0704, 0.0674, 0.0705, 0.0707, 0.0700,
        0.0731, 0.0670, 0.0706, 0.0703, 0.0706, 0.0696, 0.0693, 0.0695, 0.0698,
        0.0707, 0.0704, 0.0701, 0.0705, 0.0642, 0.0692, 0.0706, 0.0270, 0.0673,
        0.0706, 0.0548, 0.0698, 0.0707, 0.0682, 0.0706, 0.0706, 0.0703, 0.0710,
        0.0515, 0.0727, 0.0704, 0.0698, 0.0706, 0.0635, 0.0705, 0.0662, 0.0632,
        0.0706, 0.0696, 0.0710, 0.0675, 0.0704, 0.0706, 0.0699, 0.0706, 0.0699,
        0.0693, 0.0706, 0.0700, 0.0706, 0.0587, 0.0706, 0.0706, 0.0704, 0.0712,
        0.0700, 0.0712, 0.0711, 0.0705, 0.0701, 0.0706, 0.0710, 0.0706, 0.0706,
        0.0704, 0.0706, 0.0706, 0.0326, 0.0706, 0.0706, 0.0706, 0.0701, 0.0706,
        0.0704, 0.0706, 0.0701, 0.0707, 0.0665, 0.0637, 0.0696, 0.0706, 0.0468,
        0.0706, 0.0706, 0.0705, 0.0697, 0.0706, 0.0705, 0.0709, 0.0703, 0.0706,
        0.0705, 0.0706, 0.0685, 0.0240, 0.0706, 0.0700, 0.0706, 0.0703, 0.0703,
        0.0673, 0.0642, 0.0706, 0.0705, 0.0557, 0.0705, 0.0706, 0.0679, 0.0699,
        0.0229, 0.0701, 0.0659, 0.0706, 0.0677, 0.0706, 0.0704, 0.0703, 0.0706,
        0.0606, 0.0704, 0.0707, 0.0706, 0.0709, 0.0710, 0.0702, 0.0706, 0.0623,
        0.0695, 0.0706, 0.0706, 0.0701, 0.0706, 0.0706, 0.0706, 0.0678, 0.0693,
        0.0704, 0.0652, 0.0222, 0.0623, 0.0702, 0.0706, 0.0705, 0.0704, 0.0705,
        0.0706, 0.0701, 0.0699, 0.0704, 0.0705, 0.0605, 0.0706, 0.0257, 0.0706,
        0.0704, 0.0707, 0.0446, 0.0717, 0.0700, 0.0706, 0.0706, 0.0686, 0.0706,
        0.0704, 0.0701, 0.0704, 0.0709, 0.0707, 0.0136, 0.0520, 0.0721, 0.0695,
        0.0706, 0.0703, 0.0706, 0.0688, 0.0703, 0.0674, 0.0706, 0.0702, 0.0706,
        0.0703, 0.0704, 0.0709, 0.0709, 0.0706, 0.0685, 0.0706, 0.0704, 0.0483,
        0.0692, 0.0702, 0.0681, 0.0453, 0.0699, 0.0701, 0.0694, 0.0705, 0.0705,
        0.0669, 0.0698, 0.0706, 0.0706, 0.0706, 0.0705, 0.0706, 0.0706, 0.0675,
        0.0709], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(8134.7563, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4922, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6197.6191, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: loss: tensor(7951.8672, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8574, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8976.7754, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6270, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0757, 0.0756, 0.0732, 0.0715, 0.0698, 0.0757, 0.0755, 0.0759, 0.0748,
        0.0754, 0.0750, 0.0729, 0.0767, 0.0757, 0.0756, 0.0757, 0.0760, 0.0756,
        0.0616, 0.0757, 0.0757, 0.0627, 0.0307, 0.0190, 0.0730, 0.0754, 0.0756,
        0.0764, 0.0757, 0.0757, 0.0775, 0.0750, 0.0731, 0.0756, 0.0754, 0.0757,
        0.0737, 0.0757, 0.0756, 0.0756, 0.0757, 0.0518, 0.0757, 0.0750, 0.0757,
        0.0662, 0.0671, 0.0757, 0.0757, 0.0747, 0.0757, 0.0735, 0.0754, 0.0751,
        0.0754, 0.0746, 0.0757, 0.0681, 0.0756, 0.0743, 0.0755, 0.0757, 0.0743,
        0.0757, 0.0753, 0.0757, 0.0757, 0.0743, 0.0757, 0.0757, 0.0766, 0.0757,
        0.0757, 0.0757, 0.0749, 0.0754, 0.0757, 0.0652, 0.0757, 0.0673, 0.0757,
        0.0755, 0.0309, 0.0757, 0.0750, 0.0754, 0.0757, 0.0750, 0.0747, 0.0750,
        0.0775, 0.0714, 0.0751, 0.0757, 0.0690, 0.0676, 0.0732, 0.0756, 0.0340,
        0.0756, 0.0756, 0.0757, 0.0753, 0.0746, 0.0770, 0.0757, 0.0757, 0.0320,
        0.0751, 0.0769, 0.0746, 0.0773, 0.0673, 0.0757, 0.0757, 0.0743, 0.0757,
        0.0305, 0.0737, 0.0690, 0.0757, 0.0757, 0.0757, 0.0757, 0.0730, 0.0749,
        0.0757, 0.0757, 0.0759, 0.0757, 0.0756, 0.0757, 0.0713, 0.0748, 0.0664,
        0.0736, 0.0751, 0.0751, 0.0745, 0.0758, 0.0748, 0.0655, 0.0753, 0.0751,
        0.0743, 0.0756, 0.0757, 0.0748, 0.0754, 0.0752, 0.0764, 0.0754, 0.0750,
        0.0660, 0.0757, 0.0767, 0.0756, 0.0757, 0.0740, 0.0757, 0.0755, 0.0757,
        0.0757, 0.0753, 0.0757, 0.0756, 0.0754, 0.0751, 0.0756, 0.0753, 0.0757,
        0.0756, 0.0741, 0.0757, 0.0757, 0.0764, 0.0757, 0.0754, 0.0514, 0.0692,
        0.0755, 0.0750, 0.0754, 0.0739, 0.0756, 0.0758, 0.0757, 0.0757, 0.0757,
        0.0755, 0.0742, 0.0755, 0.0756, 0.0748, 0.0741, 0.0725, 0.0741, 0.0757,
        0.0756, 0.0758, 0.0672, 0.0756, 0.0754, 0.0757, 0.0756, 0.0784, 0.0734,
        0.0748, 0.0757, 0.0756, 0.0745, 0.0779, 0.0756, 0.0757, 0.0757, 0.0757,
        0.0666, 0.0757, 0.0757, 0.0756, 0.0756, 0.0754, 0.0755, 0.0757, 0.0742,
        0.0757, 0.0757, 0.0756, 0.0757, 0.0568, 0.0760, 0.0754, 0.0757, 0.0761,
        0.0757, 0.0749, 0.0759, 0.0753, 0.0749, 0.0722, 0.0757, 0.0756, 0.0725,
        0.0759, 0.0758, 0.0757, 0.0762, 0.0745, 0.0757, 0.0757, 0.0757, 0.0759,
        0.0741, 0.0746, 0.0745, 0.0757, 0.0757, 0.0757, 0.0729, 0.0618, 0.0722,
        0.0760, 0.0754, 0.0659, 0.0745, 0.0758, 0.0757, 0.0757, 0.0757, 0.0750,
        0.0734, 0.0748, 0.0757, 0.0752, 0.0738, 0.0759, 0.0760, 0.0756, 0.0767,
        0.0534, 0.0757, 0.0757, 0.0690, 0.0759, 0.0755, 0.0740, 0.0757, 0.0747,
        0.0757, 0.0756, 0.0757, 0.0759, 0.0738, 0.0756, 0.0762, 0.0620, 0.0757,
        0.0397, 0.0713, 0.0757, 0.0754, 0.0713, 0.0640, 0.0758, 0.0760, 0.0754,
        0.0756, 0.0757, 0.0757, 0.0604, 0.0757, 0.0741, 0.0746, 0.0752, 0.0757,
        0.0757, 0.0757, 0.0662, 0.0743, 0.0756, 0.0739, 0.0626, 0.0757, 0.0757,
        0.0760, 0.0740, 0.0756, 0.0723, 0.0757, 0.0759, 0.0741, 0.0757, 0.0756,
        0.0743, 0.0765, 0.0756, 0.0751, 0.0773, 0.0753, 0.0756, 0.0748, 0.0755,
        0.0757, 0.0757, 0.0756, 0.0688, 0.0681, 0.0269, 0.0751, 0.0757, 0.0757,
        0.0611, 0.0753, 0.0757, 0.0737, 0.0756, 0.0757, 0.0761, 0.0757, 0.0757,
        0.0756, 0.0757, 0.0746, 0.0757, 0.0594, 0.0748, 0.0754, 0.0757, 0.0757,
        0.0733, 0.0757, 0.0757, 0.0755, 0.0757, 0.0743, 0.0714, 0.0757, 0.0764,
        0.0757, 0.0698, 0.0759, 0.0589, 0.0757, 0.0757, 0.0757, 0.0757, 0.0774,
        0.0598, 0.0730, 0.0757, 0.0755, 0.0757, 0.0756, 0.0756, 0.0577, 0.0760,
        0.0759, 0.0773, 0.0756, 0.0757, 0.0757, 0.0750, 0.0779, 0.0756, 0.0756,
        0.0760, 0.0757, 0.0659, 0.0746, 0.0755, 0.0757, 0.0757, 0.0573, 0.0757,
        0.0754, 0.0757, 0.0757, 0.0701, 0.0757, 0.0758, 0.0757, 0.0757, 0.0745,
        0.0757, 0.0758, 0.0748, 0.0757, 0.0756, 0.0726, 0.0756, 0.0760, 0.0751,
        0.0792, 0.0721, 0.0757, 0.0755, 0.0757, 0.0749, 0.0745, 0.0746, 0.0750,
        0.0758, 0.0756, 0.0753, 0.0757, 0.0694, 0.0744, 0.0757, 0.0296, 0.0724,
        0.0757, 0.0500, 0.0750, 0.0758, 0.0733, 0.0757, 0.0757, 0.0755, 0.0771,
        0.0547, 0.0775, 0.0756, 0.0750, 0.0757, 0.0688, 0.0757, 0.0714, 0.0689,
        0.0759, 0.0746, 0.0760, 0.0727, 0.0756, 0.0757, 0.0750, 0.0760, 0.0751,
        0.0745, 0.0757, 0.0752, 0.0757, 0.0639, 0.0757, 0.0757, 0.0756, 0.0759,
        0.0752, 0.0767, 0.0763, 0.0757, 0.0753, 0.0757, 0.0768, 0.0757, 0.0757,
        0.0756, 0.0757, 0.0757, 0.0353, 0.0757, 0.0757, 0.0757, 0.0753, 0.0757,
        0.0756, 0.0757, 0.0753, 0.0759, 0.0717, 0.0688, 0.0748, 0.0757, 0.0439,
        0.0757, 0.0757, 0.0757, 0.0749, 0.0757, 0.0757, 0.0759, 0.0754, 0.0757,
        0.0757, 0.0757, 0.0721, 0.0221, 0.0757, 0.0752, 0.0758, 0.0755, 0.0761,
        0.0731, 0.0674, 0.0757, 0.0757, 0.0618, 0.0757, 0.0757, 0.0731, 0.0751,
        0.0246, 0.0749, 0.0653, 0.0757, 0.0728, 0.0757, 0.0756, 0.0754, 0.0757,
        0.0663, 0.0756, 0.0762, 0.0757, 0.0760, 0.0754, 0.0754, 0.0757, 0.0663,
        0.0748, 0.0757, 0.0757, 0.0753, 0.0757, 0.0757, 0.0757, 0.0730, 0.0745,
        0.0756, 0.0704, 0.0257, 0.0675, 0.0764, 0.0757, 0.0757, 0.0755, 0.0757,
        0.0757, 0.0753, 0.0750, 0.0756, 0.0757, 0.0642, 0.0757, 0.0261, 0.0757,
        0.0756, 0.0759, 0.0445, 0.0766, 0.0752, 0.0757, 0.0757, 0.0738, 0.0757,
        0.0756, 0.0753, 0.0756, 0.0768, 0.0758, 0.0141, 0.0548, 0.0768, 0.0746,
        0.0756, 0.0754, 0.0757, 0.0746, 0.0754, 0.0718, 0.0757, 0.0754, 0.0757,
        0.0754, 0.0756, 0.0761, 0.0758, 0.0762, 0.0737, 0.0757, 0.0756, 0.0530,
        0.0743, 0.0754, 0.0732, 0.0448, 0.0753, 0.0753, 0.0745, 0.0757, 0.0757,
        0.0721, 0.0749, 0.0757, 0.0757, 0.0757, 0.0762, 0.0757, 0.0757, 0.0727,
        0.0745], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 14:53:23,527][train_inner][INFO] - {"epoch": 7, "update": 6.551, "loss": "4.688", "ntokens": "149875", "nsentences": "539.575", "prob_perplexity": "62.41", "code_perplexity": "62.152", "temp": "1.967", "loss_0": "4.547", "loss_1": "0.13", "loss_2": "0.011", "accuracy": "0.25523", "wps": "37739.6", "ups": "0.25", "wpb": "149875", "bsz": "539.6", "num_updates": "3400", "lr": "5.3125e-05", "gnorm": "0.679", "loss_scale": "16", "train_wall": "793", "gb_free": "12.6", "wall": "13676"}
loss: tensor(7720.3735, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9277, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7608.9683, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3691, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 15:01:27,304][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
loss: tensor(9065.4355, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8398, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 15:06:38,154][train_inner][INFO] - {"epoch": 7, "update": 6.937, "loss": "4.658", "ntokens": "149656", "nsentences": "538.47", "prob_perplexity": "63.816", "code_perplexity": "63.533", "temp": "1.965", "loss_0": "4.518", "loss_1": "0.13", "loss_2": "0.011", "accuracy": "0.25743", "wps": "37667", "ups": "0.25", "wpb": "149656", "bsz": "538.5", "num_updates": "3600", "lr": "5.625e-05", "gnorm": "0.667", "loss_scale": "8", "train_wall": "793", "gb_free": "12.9", "wall": "14471"}
[2023-09-12 15:08:43,070][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 15:08:43,070][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 15:08:43,171][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2023-09-12 15:09:06,870][valid][INFO] - {"epoch": 7, "valid_loss": "4.336", "valid_ntokens": "7904.1", "valid_nsentences": "55.2525", "valid_prob_perplexity": "62.128", "valid_code_perplexity": "61.938", "valid_temp": "1.964", "valid_loss_0": "4.195", "valid_loss_1": "0.13", "valid_loss_2": "0.01", "valid_accuracy": "0.31846", "valid_wps": "33246.6", "valid_wpb": "7904.1", "valid_bsz": "55.3", "valid_num_updates": "3633", "valid_best_loss": "4.336"}
[2023-09-12 15:09:06,872][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 3633 updates
[2023-09-12 15:09:06,873][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 15:09:09,330][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 15:09:10,659][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 3633 updates, score 4.336) (writing took 3.7871546359965578 seconds)
[2023-09-12 15:09:10,659][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2023-09-12 15:09:10,660][train][INFO] - {"epoch": 7, "train_loss": "4.68", "train_ntokens": "149494", "train_nsentences": "538.41", "train_prob_perplexity": "62.906", "train_code_perplexity": "62.64", "train_temp": "1.967", "train_loss_0": "4.539", "train_loss_1": "0.13", "train_loss_2": "0.011", "train_accuracy": "0.25572", "train_wps": "37310.9", "train_ups": "0.25", "train_wpb": "149494", "train_bsz": "538.4", "train_num_updates": "3633", "train_lr": "5.67656e-05", "train_gnorm": "0.677", "train_loss_scale": "8", "train_train_wall": "2052", "train_gb_free": "13.4", "train_wall": "14623"}
[2023-09-12 15:09:10,661][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 15:09:10,755][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2023-09-12 15:09:11,002][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 15:09:11,007][fairseq.trainer][INFO] - begin training epoch 8
[2023-09-12 15:09:11,008][fairseq_cli.train][INFO] - Start iterating over samples
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0998, 0.0997, 0.0974, 0.0955, 0.0939, 0.1000, 0.0996, 0.1003, 0.0989,
        0.0996, 0.0964, 0.0922, 0.1010, 0.0999, 0.0997, 0.0998, 0.0968, 0.0997,
        0.0773, 0.0999, 0.0999, 0.0848, 0.0415, 0.0334, 0.0966, 0.0995, 0.0997,
        0.0991, 0.0999, 0.0999, 0.0824, 0.0992, 0.0967, 0.0997, 0.0996, 0.0999,
        0.0978, 0.0999, 0.0997, 0.0999, 0.0999, 0.0565, 0.0999, 0.0991, 0.0999,
        0.0765, 0.0880, 0.0999, 0.0998, 0.0988, 0.0999, 0.0945, 0.0989, 0.0992,
        0.0989, 0.0988, 0.0975, 0.0895, 0.0998, 0.0984, 0.1013, 0.1000, 0.0978,
        0.0999, 0.0963, 0.0998, 0.0999, 0.0985, 0.0999, 0.0999, 0.0984, 0.0999,
        0.1006, 0.0999, 0.0990, 0.0995, 0.0999, 0.0824, 0.0999, 0.0906, 0.1002,
        0.0996, 0.0463, 0.0998, 0.0991, 0.0989, 0.0998, 0.0991, 0.0988, 0.0991,
        0.0989, 0.0535, 0.0992, 0.0999, 0.0893, 0.0892, 0.0942, 0.0998, 0.0505,
        0.0998, 0.0997, 0.0998, 0.0995, 0.0987, 0.0982, 0.0999, 0.0999, 0.0488,
        0.0974, 0.0956, 0.0983, 0.1009, 0.0915, 0.0998, 0.0999, 0.0984, 0.1000,
        0.0449, 0.0978, 0.0931, 0.0999, 0.0999, 0.0999, 0.0999, 0.0948, 0.0975,
        0.0999, 0.0999, 0.1000, 0.0999, 0.0998, 0.0999, 0.0958, 0.0986, 0.0641,
        0.0978, 0.0992, 0.0994, 0.0986, 0.0999, 0.0989, 0.0897, 0.0994, 0.0992,
        0.0986, 0.0997, 0.1000, 0.0989, 0.0996, 0.1011, 0.0997, 0.0995, 0.0984,
        0.0901, 0.0999, 0.1006, 0.0997, 0.0999, 0.0981, 0.0999, 0.0997, 0.0998,
        0.0999, 0.0995, 0.0999, 0.0997, 0.0995, 0.0992, 0.0997, 0.0994, 0.0999,
        0.0997, 0.0982, 0.0999, 0.0998, 0.1002, 0.0999, 0.0995, 0.0421, 0.0944,
        0.0996, 0.0991, 0.0995, 0.0983, 0.0997, 0.1006, 0.0999, 0.0999, 0.0999,
        0.0997, 0.0983, 0.0997, 0.0998, 0.1017, 0.0981, 0.0967, 0.0973, 0.0998,
        0.0997, 0.0989, 0.0913, 0.0997, 0.0995, 0.0999, 0.0997, 0.0728, 0.0928,
        0.0980, 0.0998, 0.0998, 0.0983, 0.0889, 0.0997, 0.0999, 0.1002, 0.0999,
        0.0851, 0.0999, 0.0999, 0.0998, 0.0997, 0.0970, 0.0996, 0.0999, 0.0984,
        0.0982, 0.0999, 0.0998, 0.0999, 0.0404, 0.1000, 0.0981, 0.0999, 0.0999,
        0.0999, 0.0990, 0.0984, 0.0975, 0.0990, 0.0973, 0.0999, 0.0986, 0.0938,
        0.1003, 0.1009, 0.0999, 0.1005, 0.0983, 0.0999, 0.0999, 0.1003, 0.1001,
        0.0995, 0.0951, 0.0986, 0.0999, 0.0998, 0.0999, 0.0971, 0.0857, 0.0964,
        0.0995, 0.0994, 0.0889, 0.0986, 0.0999, 0.0998, 0.0999, 0.1003, 0.0991,
        0.0934, 0.0989, 0.0999, 0.0993, 0.0979, 0.0999, 0.1017, 0.0997, 0.0984,
        0.0630, 0.0999, 0.0999, 0.0908, 0.1001, 0.0996, 0.0981, 0.0999, 0.0989,
        0.0999, 0.0997, 0.0999, 0.1021, 0.0980, 0.0997, 0.1003, 0.0613, 0.1000,
        0.0450, 0.0954, 0.0999, 0.0995, 0.0830, 0.0721, 0.0996, 0.0995, 0.1000,
        0.0997, 0.1000, 0.0999, 0.0688, 0.0999, 0.0982, 0.0966, 0.0995, 0.0999,
        0.0966, 0.0999, 0.0729, 0.0850, 0.0997, 0.0980, 0.0730, 0.0999, 0.0999,
        0.0999, 0.0981, 0.0997, 0.0934, 0.0999, 0.0991, 0.0981, 0.0999, 0.0997,
        0.0800, 0.0992, 0.0997, 0.0992, 0.0963, 0.0994, 0.0997, 0.0989, 0.0996,
        0.0999, 0.0999, 0.0997, 0.0933, 0.0922, 0.0434, 0.0963, 0.0999, 0.1003,
        0.0598, 0.0994, 0.0999, 0.0978, 0.0997, 0.0999, 0.1000, 0.0999, 0.1008,
        0.0997, 0.0999, 0.0989, 0.0998, 0.0774, 0.0989, 0.0995, 0.0999, 0.0999,
        0.0975, 0.0999, 0.0999, 0.0997, 0.0999, 0.0984, 0.0955, 0.0999, 0.0728,
        0.1005, 0.0939, 0.1000, 0.0577, 0.0999, 0.0999, 0.1010, 0.0999, 0.0958,
        0.0840, 0.0971, 0.0999, 0.0953, 0.0999, 0.0997, 0.0998, 0.0556, 0.1000,
        0.1010, 0.0778, 0.0997, 0.0781, 0.0999, 0.0979, 0.1002, 0.0997, 0.0998,
        0.1010, 0.0999, 0.0779, 0.0987, 0.0997, 0.0999, 0.0998, 0.0739, 0.0999,
        0.0995, 0.0998, 0.0999, 0.0940, 0.0999, 0.1000, 0.0999, 0.0999, 0.0987,
        0.0998, 0.1000, 0.0989, 0.0999, 0.0997, 0.0967, 0.0998, 0.0999, 0.0993,
        0.0930, 0.0963, 0.0999, 0.0996, 0.0999, 0.0989, 0.0985, 0.0988, 0.0991,
        0.0999, 0.0997, 0.0994, 0.0998, 0.0936, 0.0986, 0.0999, 0.0488, 0.0966,
        0.0999, 0.0365, 0.0992, 0.0999, 0.0975, 0.0999, 0.0999, 0.0996, 0.0972,
        0.0766, 0.0944, 0.0997, 0.0991, 0.0999, 0.0926, 0.0998, 0.0955, 0.0928,
        0.0977, 0.0994, 0.0958, 0.0969, 0.0997, 0.0999, 0.0992, 0.1021, 0.0992,
        0.0986, 0.0999, 0.0993, 0.0999, 0.0880, 0.1003, 0.0999, 0.0997, 0.0879,
        0.0993, 0.0969, 0.0991, 0.0998, 0.0994, 0.0999, 0.1002, 0.0999, 0.0999,
        0.0997, 0.0999, 0.0999, 0.0505, 0.0999, 0.0999, 0.0999, 0.0994, 0.0999,
        0.0997, 0.0999, 0.0994, 0.0992, 0.0959, 0.0930, 0.0742, 0.0999, 0.0581,
        0.0999, 0.0999, 0.0998, 0.0990, 0.0999, 0.0998, 0.0976, 0.1000, 0.0999,
        0.0998, 0.0999, 0.0923, 0.0349, 0.0999, 0.0993, 0.0999, 0.0996, 0.0977,
        0.0959, 0.0901, 0.0999, 0.0998, 0.0863, 0.0998, 0.0999, 0.0971, 0.0992,
        0.0403, 0.0958, 0.0511, 0.0999, 0.0970, 0.0999, 0.0997, 0.0996, 0.0999,
        0.0895, 0.0997, 0.1008, 0.0999, 0.0990, 0.0995, 0.0995, 0.0999, 0.0876,
        0.0988, 0.0999, 0.0999, 0.0994, 0.0999, 0.0999, 0.0999, 0.0972, 0.0986,
        0.0997, 0.0945, 0.0424, 0.0916, 0.0981, 0.0999, 0.0998, 0.0997, 0.0998,
        0.0999, 0.0994, 0.0995, 0.0997, 0.0998, 0.0858, 0.0999, 0.0361, 0.0999,
        0.0997, 0.1000, 0.0546, 0.0966, 0.0993, 0.0999, 0.0999, 0.0980, 0.0999,
        0.0997, 0.0994, 0.0997, 0.1016, 0.1005, 0.0299, 0.0765, 0.0995, 0.0988,
        0.0997, 0.0995, 0.0999, 0.0970, 0.0995, 0.0947, 0.0999, 0.0995, 0.0999,
        0.0995, 0.0997, 0.0997, 0.0999, 0.0989, 0.0978, 0.0999, 0.0997, 0.0541,
        0.0984, 0.0995, 0.0974, 0.0530, 0.0993, 0.0994, 0.0987, 0.0998, 0.0998,
        0.0962, 0.0991, 0.0999, 0.0999, 0.0999, 0.0989, 0.0999, 0.0999, 0.0968,
        0.0975], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 15:20:13,003][train_inner][INFO] - {"epoch": 8, "update": 7.321, "loss": "4.634", "ntokens": "149016", "nsentences": "537.505", "prob_perplexity": "65.234", "code_perplexity": "64.917", "temp": "1.963", "loss_0": "4.494", "loss_1": "0.13", "loss_2": "0.01", "accuracy": "0.25871", "wps": "36575.2", "ups": "0.25", "wpb": "149016", "bsz": "537.5", "num_updates": "3800", "lr": "5.9375e-05", "gnorm": "0.668", "loss_scale": "16", "train_wall": "786", "gb_free": "12.6", "wall": "15286"}
[2023-09-12 15:20:44,733][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Parameter containing:
tensor([0.0430, 0.0428, 0.0406, 0.0438, 0.0371, 0.0430, 0.0428, 0.0430, 0.0421,
        0.0428, 0.0432, 0.0436, 0.0431, 0.0430, 0.0429, 0.0430, 0.0430, 0.0429,
        0.0301, 0.0430, 0.0430, 0.0384, 0.0341, 0.0205, 0.0412, 0.0427, 0.0429,
        0.0430, 0.0430, 0.0430, 0.0431, 0.0423, 0.0398, 0.0429, 0.0428, 0.0430,
        0.0410, 0.0430, 0.0429, 0.0428, 0.0430, 0.0440, 0.0430, 0.0423, 0.0430,
        0.0446, 0.0454, 0.0430, 0.0430, 0.0420, 0.0430, 0.0448, 0.0425, 0.0424,
        0.0444, 0.0419, 0.0430, 0.0359, 0.0429, 0.0416, 0.0428, 0.0430, 0.0430,
        0.0430, 0.0437, 0.0430, 0.0430, 0.0415, 0.0430, 0.0430, 0.0431, 0.0430,
        0.0430, 0.0430, 0.0422, 0.0427, 0.0430, 0.0300, 0.0430, 0.0407, 0.0430,
        0.0428, 0.0243, 0.0430, 0.0423, 0.0427, 0.0430, 0.0423, 0.0420, 0.0423,
        0.0428, 0.0412, 0.0424, 0.0430, 0.0424, 0.0415, 0.0433, 0.0429, 0.0259,
        0.0429, 0.0428, 0.0430, 0.0426, 0.0417, 0.0429, 0.0430, 0.0430, 0.0250,
        0.0426, 0.0429, 0.0414, 0.0432, 0.0346, 0.0430, 0.0430, 0.0416, 0.0430,
        0.0417, 0.0410, 0.0381, 0.0430, 0.0430, 0.0431, 0.0430, 0.0389, 0.0430,
        0.0430, 0.0430, 0.0430, 0.0430, 0.0429, 0.0430, 0.0384, 0.0418, 0.0413,
        0.0409, 0.0424, 0.0415, 0.0417, 0.0430, 0.0421, 0.0328, 0.0426, 0.0424,
        0.0416, 0.0429, 0.0431, 0.0421, 0.0428, 0.0425, 0.0430, 0.0427, 0.0424,
        0.0367, 0.0430, 0.0428, 0.0429, 0.0430, 0.0413, 0.0430, 0.0428, 0.0429,
        0.0430, 0.0426, 0.0430, 0.0428, 0.0427, 0.0424, 0.0429, 0.0426, 0.0430,
        0.0429, 0.0414, 0.0430, 0.0430, 0.0430, 0.0430, 0.0427, 0.0378, 0.0363,
        0.0428, 0.0423, 0.0427, 0.0416, 0.0429, 0.0433, 0.0430, 0.0430, 0.0430,
        0.0428, 0.0413, 0.0428, 0.0430, 0.0421, 0.0441, 0.0398, 0.0431, 0.0430,
        0.0429, 0.0424, 0.0345, 0.0429, 0.0428, 0.0430, 0.0429, 0.0428, 0.0433,
        0.0417, 0.0430, 0.0429, 0.0417, 0.0448, 0.0429, 0.0430, 0.0430, 0.0430,
        0.0413, 0.0430, 0.0430, 0.0429, 0.0429, 0.0425, 0.0428, 0.0430, 0.0415,
        0.0426, 0.0430, 0.0429, 0.0430, 0.0422, 0.0431, 0.0439, 0.0430, 0.0430,
        0.0430, 0.0422, 0.0430, 0.0425, 0.0422, 0.0395, 0.0430, 0.0423, 0.0438,
        0.0426, 0.0430, 0.0430, 0.0429, 0.0418, 0.0430, 0.0430, 0.0430, 0.0430,
        0.0405, 0.0439, 0.0414, 0.0430, 0.0430, 0.0430, 0.0403, 0.0353, 0.0393,
        0.0430, 0.0425, 0.0383, 0.0418, 0.0428, 0.0430, 0.0430, 0.0430, 0.0422,
        0.0412, 0.0421, 0.0430, 0.0425, 0.0411, 0.0430, 0.0430, 0.0428, 0.0434,
        0.0415, 0.0430, 0.0430, 0.0354, 0.0430, 0.0428, 0.0414, 0.0430, 0.0420,
        0.0430, 0.0429, 0.0430, 0.0426, 0.0411, 0.0428, 0.0430, 0.0406, 0.0430,
        0.0440, 0.0386, 0.0430, 0.0427, 0.0358, 0.0451, 0.0425, 0.0429, 0.0427,
        0.0429, 0.0430, 0.0430, 0.0456, 0.0430, 0.0414, 0.0431, 0.0424, 0.0430,
        0.0427, 0.0430, 0.0447, 0.0437, 0.0429, 0.0412, 0.0420, 0.0430, 0.0430,
        0.0430, 0.0413, 0.0429, 0.0444, 0.0430, 0.0428, 0.0421, 0.0430, 0.0428,
        0.0427, 0.0429, 0.0429, 0.0424, 0.0414, 0.0425, 0.0429, 0.0427, 0.0428,
        0.0430, 0.0430, 0.0429, 0.0356, 0.0354, 0.0125, 0.0414, 0.0430, 0.0430,
        0.0448, 0.0426, 0.0430, 0.0410, 0.0428, 0.0430, 0.0430, 0.0430, 0.0430,
        0.0428, 0.0430, 0.0419, 0.0430, 0.0329, 0.0421, 0.0430, 0.0430, 0.0430,
        0.0405, 0.0430, 0.0430, 0.0428, 0.0430, 0.0416, 0.0387, 0.0430, 0.0431,
        0.0430, 0.0370, 0.0431, 0.0428, 0.0430, 0.0430, 0.0430, 0.0430, 0.0433,
        0.0272, 0.0403, 0.0430, 0.0433, 0.0430, 0.0429, 0.0429, 0.0432, 0.0430,
        0.0430, 0.0423, 0.0429, 0.0416, 0.0430, 0.0433, 0.0428, 0.0428, 0.0429,
        0.0430, 0.0430, 0.0418, 0.0419, 0.0428, 0.0430, 0.0430, 0.0437, 0.0430,
        0.0427, 0.0430, 0.0430, 0.0395, 0.0430, 0.0430, 0.0430, 0.0430, 0.0418,
        0.0430, 0.0430, 0.0421, 0.0430, 0.0429, 0.0399, 0.0429, 0.0431, 0.0424,
        0.0430, 0.0395, 0.0430, 0.0428, 0.0430, 0.0420, 0.0417, 0.0419, 0.0423,
        0.0430, 0.0429, 0.0426, 0.0430, 0.0367, 0.0417, 0.0430, 0.0049, 0.0397,
        0.0430, 0.0442, 0.0423, 0.0430, 0.0406, 0.0430, 0.0430, 0.0428, 0.0431,
        0.0452, 0.0433, 0.0428, 0.0422, 0.0430, 0.0360, 0.0430, 0.0387, 0.0354,
        0.0430, 0.0420, 0.0422, 0.0400, 0.0429, 0.0430, 0.0423, 0.0430, 0.0424,
        0.0418, 0.0430, 0.0425, 0.0430, 0.0312, 0.0430, 0.0430, 0.0429, 0.0436,
        0.0425, 0.0431, 0.0429, 0.0430, 0.0426, 0.0430, 0.0431, 0.0430, 0.0430,
        0.0428, 0.0430, 0.0431, 0.0385, 0.0430, 0.0430, 0.0430, 0.0426, 0.0430,
        0.0429, 0.0431, 0.0426, 0.0426, 0.0389, 0.0361, 0.0410, 0.0430, 0.0446,
        0.0430, 0.0430, 0.0430, 0.0422, 0.0430, 0.0430, 0.0430, 0.0425, 0.0430,
        0.0430, 0.0430, 0.0399, 0.0414, 0.0430, 0.0425, 0.0430, 0.0428, 0.0420,
        0.0410, 0.0437, 0.0430, 0.0428, 0.0362, 0.0430, 0.0430, 0.0407, 0.0424,
        0.0106, 0.0435, 0.0420, 0.0430, 0.0402, 0.0432, 0.0429, 0.0427, 0.0431,
        0.0330, 0.0429, 0.0430, 0.0430, 0.0430, 0.0430, 0.0427, 0.0430, 0.0405,
        0.0420, 0.0430, 0.0430, 0.0426, 0.0430, 0.0430, 0.0430, 0.0403, 0.0417,
        0.0429, 0.0377, 0.0034, 0.0348, 0.0423, 0.0430, 0.0430, 0.0428, 0.0430,
        0.0430, 0.0426, 0.0423, 0.0428, 0.0430, 0.0396, 0.0430, 0.0351, 0.0430,
        0.0429, 0.0430, 0.0374, 0.0444, 0.0425, 0.0430, 0.0430, 0.0411, 0.0430,
        0.0428, 0.0426, 0.0429, 0.0430, 0.0428, 0.0109, 0.0321, 0.0431, 0.0419,
        0.0432, 0.0427, 0.0430, 0.0407, 0.0427, 0.0443, 0.0430, 0.0427, 0.0430,
        0.0427, 0.0429, 0.0430, 0.0430, 0.0422, 0.0410, 0.0430, 0.0428, 0.0241,
        0.0416, 0.0427, 0.0405, 0.0463, 0.0422, 0.0426, 0.0418, 0.0430, 0.0430,
        0.0394, 0.0422, 0.0430, 0.0430, 0.0430, 0.0428, 0.0430, 0.0430, 0.0400,
        0.0432], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7942.6284, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.7559, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6944.1147, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2832, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7001.0190, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2520, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7738.0356, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3555, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5172.7324, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4121, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6701.1045, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6426, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7877.1216, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7899.7241, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6641, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7889.0908, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8594, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7527.3652, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7306.5430, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8105, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8335.3428, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5195, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5556.8213, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1270, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8282.4014, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9453, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6008.5728, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8809, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
tensor(-3.3750, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8208.2861, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8398, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8159.5210, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.9746, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0805, 0.0804, 0.0781, 0.0762, 0.0746, 0.0806, 0.0803, 0.0806, 0.0796,
        0.0803, 0.0782, 0.0767, 0.0806, 0.0806, 0.0804, 0.0805, 0.0804, 0.0804,
        0.0656, 0.0806, 0.0805, 0.0667, 0.0322, 0.0220, 0.0780, 0.0803, 0.0804,
        0.0808, 0.0806, 0.0806, 0.0836, 0.0798, 0.0782, 0.0804, 0.0803, 0.0805,
        0.0785, 0.0806, 0.0804, 0.0804, 0.0806, 0.0517, 0.0806, 0.0798, 0.0805,
        0.0694, 0.0707, 0.0806, 0.0804, 0.0795, 0.0806, 0.0770, 0.0811, 0.0798,
        0.0806, 0.0794, 0.0817, 0.0720, 0.0804, 0.0791, 0.0804, 0.0806, 0.0793,
        0.0806, 0.0791, 0.0804, 0.0806, 0.0792, 0.0806, 0.0805, 0.0819, 0.0805,
        0.0806, 0.0805, 0.0797, 0.0801, 0.0805, 0.0706, 0.0805, 0.0718, 0.0806,
        0.0803, 0.0327, 0.0805, 0.0798, 0.0803, 0.0805, 0.0798, 0.0795, 0.0798,
        0.0803, 0.0703, 0.0799, 0.0805, 0.0731, 0.0721, 0.0768, 0.0804, 0.0380,
        0.0804, 0.0804, 0.0805, 0.0801, 0.0794, 0.0814, 0.0806, 0.0805, 0.0353,
        0.0794, 0.0823, 0.0791, 0.0817, 0.0721, 0.0805, 0.0806, 0.0792, 0.0806,
        0.0321, 0.0785, 0.0738, 0.0806, 0.0805, 0.0806, 0.0805, 0.0771, 0.0797,
        0.0805, 0.0806, 0.0806, 0.0805, 0.0804, 0.0806, 0.0761, 0.0794, 0.0654,
        0.0784, 0.0800, 0.0795, 0.0793, 0.0806, 0.0797, 0.0703, 0.0801, 0.0799,
        0.0792, 0.0804, 0.0806, 0.0796, 0.0803, 0.0800, 0.0817, 0.0803, 0.0787,
        0.0708, 0.0805, 0.0806, 0.0804, 0.0805, 0.0788, 0.0806, 0.0803, 0.0805,
        0.0805, 0.0801, 0.0806, 0.0804, 0.0802, 0.0800, 0.0804, 0.0801, 0.0805,
        0.0804, 0.0789, 0.0805, 0.0805, 0.0811, 0.0805, 0.0802, 0.0479, 0.0746,
        0.0803, 0.0798, 0.0803, 0.0789, 0.0804, 0.0807, 0.0806, 0.0806, 0.0806,
        0.0803, 0.0790, 0.0803, 0.0804, 0.0796, 0.0788, 0.0773, 0.0786, 0.0805,
        0.0804, 0.0806, 0.0720, 0.0804, 0.0803, 0.0805, 0.0804, 0.0818, 0.0775,
        0.0804, 0.0805, 0.0804, 0.0795, 0.0814, 0.0804, 0.0806, 0.0805, 0.0806,
        0.0705, 0.0805, 0.0806, 0.0804, 0.0804, 0.0802, 0.0803, 0.0806, 0.0790,
        0.0811, 0.0806, 0.0804, 0.0805, 0.0539, 0.0811, 0.0798, 0.0805, 0.0812,
        0.0805, 0.0797, 0.0809, 0.0804, 0.0797, 0.0770, 0.0805, 0.0801, 0.0762,
        0.0801, 0.0807, 0.0805, 0.0813, 0.0793, 0.0806, 0.0806, 0.0806, 0.0807,
        0.0797, 0.0778, 0.0793, 0.0806, 0.0805, 0.0805, 0.0778, 0.0665, 0.0770,
        0.0806, 0.0803, 0.0701, 0.0793, 0.0807, 0.0804, 0.0806, 0.0806, 0.0798,
        0.0775, 0.0797, 0.0806, 0.0800, 0.0786, 0.0806, 0.0815, 0.0804, 0.0818,
        0.0525, 0.0806, 0.0806, 0.0726, 0.0807, 0.0803, 0.0789, 0.0805, 0.0795,
        0.0805, 0.0804, 0.0806, 0.0808, 0.0786, 0.0804, 0.0810, 0.0656, 0.0806,
        0.0404, 0.0761, 0.0805, 0.0802, 0.0700, 0.0660, 0.0811, 0.0811, 0.0803,
        0.0804, 0.0806, 0.0805, 0.0600, 0.0805, 0.0789, 0.0779, 0.0800, 0.0805,
        0.0806, 0.0805, 0.0678, 0.0786, 0.0804, 0.0787, 0.0635, 0.0806, 0.0805,
        0.0811, 0.0788, 0.0804, 0.0766, 0.0806, 0.0803, 0.0789, 0.0806, 0.0804,
        0.0781, 0.0811, 0.0804, 0.0800, 0.0818, 0.0801, 0.0804, 0.0797, 0.0803,
        0.0805, 0.0806, 0.0804, 0.0737, 0.0729, 0.0297, 0.0804, 0.0806, 0.0806,
        0.0591, 0.0801, 0.0806, 0.0785, 0.0804, 0.0805, 0.0812, 0.0805, 0.0807,
        0.0804, 0.0805, 0.0794, 0.0805, 0.0632, 0.0797, 0.0803, 0.0806, 0.0806,
        0.0781, 0.0805, 0.0806, 0.0803, 0.0806, 0.0792, 0.0762, 0.0805, 0.0756,
        0.0806, 0.0745, 0.0807, 0.0556, 0.0806, 0.0806, 0.0806, 0.0805, 0.0809,
        0.0646, 0.0778, 0.0805, 0.0787, 0.0806, 0.0804, 0.0804, 0.0557, 0.0808,
        0.0808, 0.0831, 0.0804, 0.0791, 0.0806, 0.0800, 0.0825, 0.0803, 0.0804,
        0.0806, 0.0806, 0.0659, 0.0794, 0.0803, 0.0805, 0.0805, 0.0595, 0.0806,
        0.0803, 0.0805, 0.0806, 0.0751, 0.0805, 0.0806, 0.0806, 0.0806, 0.0793,
        0.0804, 0.0807, 0.0796, 0.0806, 0.0804, 0.0774, 0.0804, 0.0806, 0.0800,
        0.0838, 0.0770, 0.0805, 0.0803, 0.0806, 0.0797, 0.0793, 0.0794, 0.0798,
        0.0807, 0.0804, 0.0801, 0.0805, 0.0742, 0.0792, 0.0806, 0.0328, 0.0772,
        0.0805, 0.0447, 0.0798, 0.0806, 0.0781, 0.0806, 0.0805, 0.0803, 0.0808,
        0.0585, 0.0807, 0.0804, 0.0798, 0.0805, 0.0739, 0.0805, 0.0762, 0.0742,
        0.0808, 0.0793, 0.0811, 0.0775, 0.0804, 0.0805, 0.0798, 0.0810, 0.0799,
        0.0793, 0.0806, 0.0800, 0.0806, 0.0687, 0.0806, 0.0805, 0.0804, 0.0796,
        0.0800, 0.0811, 0.0805, 0.0805, 0.0801, 0.0806, 0.0807, 0.0806, 0.0805,
        0.0804, 0.0805, 0.0806, 0.0391, 0.0806, 0.0806, 0.0805, 0.0801, 0.0805,
        0.0804, 0.0806, 0.0801, 0.0803, 0.0765, 0.0737, 0.0788, 0.0805, 0.0452,
        0.0805, 0.0805, 0.0804, 0.0797, 0.0805, 0.0805, 0.0796, 0.0804, 0.0806,
        0.0805, 0.0805, 0.0772, 0.0230, 0.0805, 0.0800, 0.0806, 0.0803, 0.0792,
        0.0770, 0.0715, 0.0806, 0.0805, 0.0662, 0.0805, 0.0806, 0.0778, 0.0798,
        0.0275, 0.0788, 0.0623, 0.0806, 0.0776, 0.0806, 0.0804, 0.0803, 0.0806,
        0.0717, 0.0804, 0.0811, 0.0806, 0.0809, 0.0803, 0.0801, 0.0806, 0.0701,
        0.0795, 0.0805, 0.0806, 0.0801, 0.0806, 0.0806, 0.0806, 0.0778, 0.0792,
        0.0804, 0.0752, 0.0287, 0.0723, 0.0800, 0.0806, 0.0805, 0.0803, 0.0805,
        0.0806, 0.0801, 0.0798, 0.0804, 0.0805, 0.0685, 0.0805, 0.0295, 0.0806,
        0.0804, 0.0806, 0.0478, 0.0804, 0.0800, 0.0806, 0.0806, 0.0786, 0.0806,
        0.0804, 0.0801, 0.0804, 0.0809, 0.0811, 0.0177, 0.0586, 0.0814, 0.0794,
        0.0803, 0.0802, 0.0805, 0.0789, 0.0803, 0.0766, 0.0805, 0.0802, 0.0805,
        0.0803, 0.0804, 0.0809, 0.0806, 0.0805, 0.0786, 0.0805, 0.0804, 0.0577,
        0.0792, 0.0802, 0.0780, 0.0448, 0.0799, 0.0801, 0.0793, 0.0805, 0.0804,
        0.0769, 0.0797, 0.0805, 0.0805, 0.0805, 0.0806, 0.0805, 0.0806, 0.0775,
        0.0790], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(8124.4829, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8633, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7602.4438, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9160, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7007.6479, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8828, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8175.0068, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5039, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8001.7158, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7656, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8005.4893, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8809, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8441.3486, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8926, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8577.8359, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8691, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9283.0850, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9180, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7339.6670, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9902, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8752.2793, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1914, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7413.1636, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0781, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: loss: tensor(7965.4111, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0039, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6615.0688, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0547, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 15:33:26,719][train_inner][INFO] - {"epoch": 8, "update": 7.706, "loss": "4.606", "ntokens": "149659", "nsentences": "538.91", "prob_perplexity": "66.862", "code_perplexity": "66.499", "temp": "1.961", "loss_0": "4.466", "loss_1": "0.129", "loss_2": "0.01", "accuracy": "0.26067", "wps": "37710.9", "ups": "0.25", "wpb": "149659", "bsz": "538.9", "num_updates": "4000", "lr": "6.25e-05", "gnorm": "0.653", "loss_scale": "8", "train_wall": "793", "gb_free": "12.6", "wall": "16079"}
[2023-09-12 15:37:53,671][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
loss: tensor(5769.8467, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 15:43:36,677][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 15:43:36,678][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 15:43:36,869][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2023-09-12 15:44:00,221][valid][INFO] - {"epoch": 8, "valid_loss": "4.292", "valid_ntokens": "7894.81", "valid_nsentences": "55.2525", "valid_prob_perplexity": "64.003", "valid_code_perplexity": "63.799", "valid_temp": "1.959", "valid_loss_0": "4.152", "valid_loss_1": "0.13", "valid_loss_2": "0.01", "valid_accuracy": "0.31985", "valid_wps": "33641.6", "valid_wpb": "7894.8", "valid_bsz": "55.3", "valid_num_updates": "4152", "valid_best_loss": "4.292"}
[2023-09-12 15:44:00,222][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 4152 updates
[2023-09-12 15:44:00,223][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 15:44:02,670][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 15:44:04,041][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 4152 updates, score 4.292) (writing took 3.8191495009232312 seconds)
[2023-09-12 15:44:04,042][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2023-09-12 15:44:04,042][train][INFO] - {"epoch": 8, "train_loss": "4.609", "train_ntokens": "149492", "train_nsentences": "538.414", "train_prob_perplexity": "66.771", "train_code_perplexity": "66.419", "train_temp": "1.961", "train_loss_0": "4.469", "train_loss_1": "0.129", "train_loss_2": "0.01", "train_accuracy": "0.26059", "train_wps": "37062.7", "train_ups": "0.25", "train_wpb": "149492", "train_bsz": "538.4", "train_num_updates": "4152", "train_lr": "6.4875e-05", "train_gnorm": "0.66", "train_loss_scale": "8", "train_train_wall": "2062", "train_gb_free": "16", "train_wall": "16717"}
[2023-09-12 15:44:04,045][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 15:44:04,133][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2023-09-12 15:44:04,375][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 15:44:04,378][fairseq.trainer][INFO] - begin training epoch 9
[2023-09-12 15:44:04,379][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(7863.1792, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3398, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5812.6938, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9102, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 15:47:12,713][train_inner][INFO] - {"epoch": 9, "update": 8.092, "loss": "4.586", "ntokens": "149271", "nsentences": "535.595", "prob_perplexity": "68.255", "code_perplexity": "67.882", "temp": "1.959", "loss_0": "4.447", "loss_1": "0.129", "loss_2": "0.01", "accuracy": "0.26253", "wps": "36143.4", "ups": "0.24", "wpb": "149271", "bsz": "535.6", "num_updates": "4200", "lr": "6.5625e-05", "gnorm": "0.663", "loss_scale": "8", "train_wall": "797", "gb_free": "12.4", "wall": "16905"}

loss: tensor(7807.0610, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.4951, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9040.1289, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-1.6338, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8412.0303, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8555, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6621.4126, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4551, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7831.9814, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0020, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8545.4600, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.0234, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7147.5601, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.0820, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5630.7896, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8008, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7528.1948, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.4648, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5039.3154, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8203, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7391.1030, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.3691, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7851.8354, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.2480, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8160.2441, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.1191, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8304.2549, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.7305, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7788.4678, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.3145, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7004.5586, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5234, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8450.4297, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.4199, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5465.0684, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.5684, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6873.6553, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6641, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6782.5308, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8770, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4726.6172, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-2.8516, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.0880, 0.0879, 0.0856, 0.0837, 0.0822, 0.0881, 0.0878, 0.0883, 0.0872,
        0.0878, 0.0847, 0.0822, 0.0884, 0.0881, 0.0879, 0.0880, 0.0863, 0.0879,
        0.0718, 0.0881, 0.0880, 0.0731, 0.0365, 0.0265, 0.0854, 0.0878, 0.0880,
        0.0881, 0.0881, 0.0881, 0.0861, 0.0873, 0.0851, 0.0880, 0.0878, 0.0881,
        0.0860, 0.0881, 0.0880, 0.0879, 0.0881, 0.0553, 0.0881, 0.0873, 0.0881,
        0.0748, 0.0775, 0.0881, 0.0880, 0.0870, 0.0881, 0.0844, 0.0875, 0.0874,
        0.0874, 0.0869, 0.0873, 0.0792, 0.0880, 0.0867, 0.0881, 0.0881, 0.0871,
        0.0881, 0.0856, 0.0880, 0.0881, 0.0867, 0.0881, 0.0881, 0.0873, 0.0880,
        0.0883, 0.0881, 0.0872, 0.0877, 0.0881, 0.0775, 0.0881, 0.0790, 0.0880,
        0.0878, 0.0374, 0.0880, 0.0873, 0.0883, 0.0880, 0.0873, 0.0870, 0.0873,
        0.0865, 0.0638, 0.0874, 0.0880, 0.0796, 0.0781, 0.0833, 0.0880, 0.0410,
        0.0880, 0.0879, 0.0880, 0.0876, 0.0869, 0.0898, 0.0881, 0.0880, 0.0418,
        0.0850, 0.0899, 0.0865, 0.0887, 0.0797, 0.0880, 0.0881, 0.0868, 0.0881,
        0.0370, 0.0860, 0.0813, 0.0881, 0.0881, 0.0881, 0.0880, 0.0840, 0.0865,
        0.0881, 0.0881, 0.0882, 0.0881, 0.0880, 0.0881, 0.0837, 0.0869, 0.0634,
        0.0860, 0.0875, 0.0869, 0.0868, 0.0881, 0.0872, 0.0779, 0.0876, 0.0874,
        0.0867, 0.0880, 0.0881, 0.0872, 0.0878, 0.0876, 0.0886, 0.0877, 0.0861,
        0.0783, 0.0880, 0.0887, 0.0880, 0.0880, 0.0863, 0.0881, 0.0878, 0.0880,
        0.0881, 0.0877, 0.0881, 0.0879, 0.0878, 0.0875, 0.0880, 0.0876, 0.0881,
        0.0880, 0.0864, 0.0881, 0.0881, 0.0891, 0.0881, 0.0877, 0.0433, 0.0816,
        0.0878, 0.0873, 0.0878, 0.0854, 0.0879, 0.0884, 0.0881, 0.0881, 0.0881,
        0.0878, 0.0865, 0.0878, 0.0880, 0.0874, 0.0862, 0.0848, 0.0858, 0.0880,
        0.0879, 0.0874, 0.0795, 0.0880, 0.0878, 0.0881, 0.0880, 0.0813, 0.0828,
        0.0873, 0.0880, 0.0880, 0.0867, 0.0870, 0.0880, 0.0881, 0.0880, 0.0881,
        0.0770, 0.0880, 0.0881, 0.0880, 0.0880, 0.0878, 0.0878, 0.0881, 0.0865,
        0.0894, 0.0881, 0.0880, 0.0880, 0.0480, 0.0889, 0.0861, 0.0880, 0.0882,
        0.0880, 0.0872, 0.0883, 0.0889, 0.0872, 0.0854, 0.0881, 0.0871, 0.0828,
        0.0880, 0.0883, 0.0881, 0.0892, 0.0869, 0.0881, 0.0881, 0.0881, 0.0883,
        0.0866, 0.0858, 0.0866, 0.0881, 0.0880, 0.0881, 0.0853, 0.0740, 0.0845,
        0.0880, 0.0877, 0.0772, 0.0869, 0.0882, 0.0880, 0.0881, 0.0881, 0.0873,
        0.0841, 0.0872, 0.0881, 0.0875, 0.0861, 0.0881, 0.0886, 0.0879, 0.0892,
        0.0558, 0.0881, 0.0881, 0.0790, 0.0882, 0.0878, 0.0864, 0.0881, 0.0870,
        0.0881, 0.0879, 0.0881, 0.0893, 0.0861, 0.0879, 0.0889, 0.0653, 0.0881,
        0.0412, 0.0836, 0.0880, 0.0877, 0.0747, 0.0692, 0.0898, 0.0882, 0.0878,
        0.0879, 0.0881, 0.0880, 0.0623, 0.0880, 0.0864, 0.0852, 0.0875, 0.0881,
        0.0873, 0.0880, 0.0712, 0.0858, 0.0879, 0.0862, 0.0655, 0.0881, 0.0881,
        0.0887, 0.0863, 0.0880, 0.0827, 0.0881, 0.0881, 0.0864, 0.0881, 0.0879,
        0.0800, 0.0879, 0.0880, 0.0875, 0.0895, 0.0876, 0.0879, 0.0872, 0.0878,
        0.0880, 0.0881, 0.0880, 0.0815, 0.0804, 0.0350, 0.0866, 0.0881, 0.0881,
        0.0578, 0.0876, 0.0881, 0.0860, 0.0879, 0.0881, 0.0896, 0.0881, 0.0890,
        0.0879, 0.0881, 0.0872, 0.0880, 0.0691, 0.0872, 0.0878, 0.0881, 0.0881,
        0.0856, 0.0881, 0.0881, 0.0878, 0.0881, 0.0867, 0.0837, 0.0881, 0.0745,
        0.0881, 0.0821, 0.0883, 0.0546, 0.0881, 0.0881, 0.0884, 0.0880, 0.0869,
        0.0722, 0.0853, 0.0880, 0.0854, 0.0881, 0.0880, 0.0880, 0.0539, 0.0883,
        0.0885, 0.0818, 0.0879, 0.0762, 0.0881, 0.0869, 0.0901, 0.0879, 0.0880,
        0.0884, 0.0881, 0.0676, 0.0869, 0.0878, 0.0880, 0.0880, 0.0641, 0.0881,
        0.0878, 0.0880, 0.0881, 0.0825, 0.0881, 0.0881, 0.0881, 0.0881, 0.0869,
        0.0880, 0.0881, 0.0872, 0.0881, 0.0880, 0.0850, 0.0880, 0.0881, 0.0875,
        0.0894, 0.0845, 0.0881, 0.0878, 0.0881, 0.0872, 0.0869, 0.0870, 0.0873,
        0.0883, 0.0880, 0.0876, 0.0880, 0.0817, 0.0867, 0.0881, 0.0386, 0.0847,
        0.0881, 0.0391, 0.0873, 0.0881, 0.0856, 0.0881, 0.0880, 0.0878, 0.0866,
        0.0651, 0.0863, 0.0879, 0.0873, 0.0881, 0.0820, 0.0880, 0.0837, 0.0819,
        0.0885, 0.0870, 0.0876, 0.0850, 0.0879, 0.0880, 0.0873, 0.0882, 0.0874,
        0.0869, 0.0881, 0.0875, 0.0881, 0.0762, 0.0881, 0.0880, 0.0880, 0.0875,
        0.0875, 0.0885, 0.0881, 0.0880, 0.0876, 0.0883, 0.0880, 0.0881, 0.0881,
        0.0879, 0.0881, 0.0881, 0.0446, 0.0881, 0.0881, 0.0880, 0.0876, 0.0880,
        0.0880, 0.0881, 0.0876, 0.0876, 0.0841, 0.0812, 0.0788, 0.0881, 0.0506,
        0.0881, 0.0881, 0.0880, 0.0872, 0.0880, 0.0880, 0.0865, 0.0880, 0.0881,
        0.0880, 0.0881, 0.0845, 0.0277, 0.0881, 0.0875, 0.0884, 0.0878, 0.0870,
        0.0845, 0.0786, 0.0881, 0.0881, 0.0739, 0.0880, 0.0881, 0.0853, 0.0874,
        0.0331, 0.0861, 0.0554, 0.0881, 0.0851, 0.0881, 0.0880, 0.0878, 0.0881,
        0.0782, 0.0880, 0.0886, 0.0881, 0.0887, 0.0873, 0.0877, 0.0881, 0.0764,
        0.0870, 0.0881, 0.0881, 0.0876, 0.0881, 0.0881, 0.0881, 0.0853, 0.0868,
        0.0880, 0.0828, 0.0352, 0.0798, 0.0869, 0.0881, 0.0880, 0.0878, 0.0880,
        0.0881, 0.0876, 0.0874, 0.0879, 0.0880, 0.0748, 0.0881, 0.0313, 0.0881,
        0.0880, 0.0881, 0.0511, 0.0869, 0.0875, 0.0881, 0.0881, 0.0861, 0.0881,
        0.0879, 0.0876, 0.0880, 0.0894, 0.0898, 0.0230, 0.0653, 0.0897, 0.0869,
        0.0879, 0.0878, 0.0881, 0.0859, 0.0878, 0.0856, 0.0880, 0.0877, 0.0881,
        0.0878, 0.0879, 0.0892, 0.0882, 0.0874, 0.0861, 0.0881, 0.0879, 0.0620,
        0.0867, 0.0877, 0.0855, 0.0481, 0.0875, 0.0876, 0.0869, 0.0880, 0.0880,
        0.0844, 0.0872, 0.0880, 0.0881, 0.0880, 0.0882, 0.0880, 0.0881, 0.0850,
        0.0859], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(9215.4258, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.7832, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8580.0039, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9336, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7656.9624, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9980, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8611.9082, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6738, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9193.5332, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1328, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8082.4990, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8261.0635, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0977, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7969.6855, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0195, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4999.4268, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9199, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7393.1641, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6942.2793, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.8066, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7313.2437, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1836, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6028.9590, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0195, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8036.4370, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2578, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1243, 0.1243, 0.1219, 0.1201, 0.1185, 0.1254, 0.1241, 0.1240, 0.1235,
        0.1241, 0.1191, 0.1160, 0.1245, 0.1244, 0.1242, 0.1243, 0.1210, 0.1242,
        0.0685, 0.1244, 0.1243, 0.1026, 0.0570, 0.0514, 0.1213, 0.1241, 0.1243,
        0.1231, 0.1246, 0.1244, 0.0817, 0.1237, 0.1209, 0.1243, 0.1241, 0.1244,
        0.1223, 0.1244, 0.1243, 0.1263, 0.1244, 0.0660, 0.1244, 0.1236, 0.1243,
        0.0854, 0.1109, 0.1244, 0.1243, 0.1234, 0.1244, 0.1085, 0.1194, 0.1237,
        0.1214, 0.1232, 0.1185, 0.1138, 0.1243, 0.1230, 0.1243, 0.1245, 0.1204,
        0.1244, 0.1196, 0.1243, 0.1244, 0.1229, 0.1244, 0.1244, 0.0890, 0.1243,
        0.1246, 0.1243, 0.1235, 0.1247, 0.1248, 0.0951, 0.1244, 0.1151, 0.1250,
        0.1241, 0.0701, 0.1243, 0.1237, 0.1190, 0.1243, 0.1237, 0.1234, 0.1238,
        0.1179, 0.0586, 0.1237, 0.1243, 0.1127, 0.1121, 0.1098, 0.1244, 0.0634,
        0.1243, 0.1242, 0.1243, 0.1240, 0.1231, 0.0900, 0.1244, 0.1243, 0.0650,
        0.1206, 0.0941, 0.1228, 0.1221, 0.1160, 0.1243, 0.1245, 0.1229, 0.1245,
        0.0583, 0.1223, 0.1176, 0.1244, 0.1244, 0.1244, 0.1243, 0.1130, 0.1207,
        0.1244, 0.1244, 0.1244, 0.1243, 0.1243, 0.1244, 0.1202, 0.1231, 0.0801,
        0.1221, 0.1238, 0.1247, 0.1232, 0.1244, 0.1236, 0.1142, 0.1239, 0.1237,
        0.1230, 0.1243, 0.1240, 0.1235, 0.1241, 0.1199, 0.1230, 0.1240, 0.1215,
        0.1146, 0.1243, 0.1224, 0.1243, 0.1243, 0.1226, 0.1244, 0.1241, 0.1243,
        0.1243, 0.1242, 0.1244, 0.1242, 0.1241, 0.1238, 0.1241, 0.1240, 0.1244,
        0.1243, 0.1227, 0.1243, 0.1243, 0.1163, 0.1244, 0.1240, 0.0592, 0.1171,
        0.1241, 0.1237, 0.1241, 0.1193, 0.1242, 0.1268, 0.1244, 0.1244, 0.1244,
        0.1251, 0.1227, 0.1241, 0.1243, 0.1166, 0.1226, 0.1212, 0.1217, 0.1243,
        0.1242, 0.1234, 0.1158, 0.1243, 0.1241, 0.1244, 0.1243, 0.0744, 0.1160,
        0.1192, 0.1243, 0.1245, 0.1229, 0.0806, 0.1242, 0.1244, 0.1246, 0.1244,
        0.1035, 0.1244, 0.1244, 0.1243, 0.1243, 0.1186, 0.1241, 0.1244, 0.1229,
        0.1241, 0.1244, 0.1243, 0.1244, 0.0559, 0.1244, 0.1192, 0.1243, 0.1244,
        0.1243, 0.1235, 0.1225, 0.1091, 0.1235, 0.1227, 0.1244, 0.1222, 0.1177,
        0.1240, 0.1224, 0.1244, 0.1229, 0.1221, 0.1244, 0.1244, 0.1249, 0.1246,
        0.0900, 0.1188, 0.1225, 0.1244, 0.1245, 0.1244, 0.1216, 0.1102, 0.1203,
        0.1240, 0.1240, 0.1077, 0.1232, 0.1219, 0.1243, 0.1244, 0.1252, 0.1237,
        0.1154, 0.1235, 0.1244, 0.1238, 0.1224, 0.1245, 0.1029, 0.1242, 0.0975,
        0.0839, 0.1244, 0.1244, 0.1173, 0.1246, 0.1241, 0.1227, 0.1246, 0.1234,
        0.1244, 0.1242, 0.1244, 0.1143, 0.1224, 0.1242, 0.1238, 0.0632, 0.1244,
        0.0600, 0.1199, 0.1243, 0.1240, 0.1037, 0.0790, 0.1226, 0.1245, 0.1254,
        0.1242, 0.1245, 0.1243, 0.0867, 0.1243, 0.1227, 0.1211, 0.1241, 0.1244,
        0.1210, 0.1243, 0.0865, 0.1043, 0.1242, 0.1226, 0.0750, 0.1244, 0.1243,
        0.1227, 0.1226, 0.1243, 0.1143, 0.1244, 0.1233, 0.1226, 0.1244, 0.1242,
        0.0785, 0.1231, 0.1243, 0.1238, 0.0889, 0.1238, 0.1242, 0.1234, 0.1241,
        0.1243, 0.1244, 0.1243, 0.1199, 0.1167, 0.0600, 0.1196, 0.1244, 0.1250,
        0.0693, 0.1240, 0.1244, 0.1223, 0.1242, 0.1244, 0.1239, 0.1244, 0.1254,
        0.1242, 0.1244, 0.1234, 0.1243, 0.0994, 0.1235, 0.1241, 0.1244, 0.1244,
        0.1219, 0.1244, 0.1244, 0.1241, 0.1244, 0.1230, 0.1200, 0.1244, 0.0773,
        0.1245, 0.1184, 0.1245, 0.0711, 0.1244, 0.1244, 0.1263, 0.1243, 0.1182,
        0.1085, 0.1216, 0.1243, 0.1186, 0.1244, 0.1243, 0.1243, 0.0688, 0.1245,
        0.1227, 0.0818, 0.1242, 0.0846, 0.1244, 0.1212, 0.1236, 0.1242, 0.1243,
        0.1248, 0.1244, 0.1018, 0.1232, 0.1241, 0.1243, 0.1243, 0.0966, 0.1249,
        0.1241, 0.1243, 0.1244, 0.1183, 0.1244, 0.1246, 0.1244, 0.1244, 0.1232,
        0.1243, 0.1246, 0.1235, 0.1244, 0.1243, 0.1213, 0.1243, 0.1245, 0.1238,
        0.0939, 0.1208, 0.1244, 0.1241, 0.1244, 0.1234, 0.1230, 0.1232, 0.1236,
        0.1243, 0.1242, 0.1240, 0.1243, 0.1181, 0.1230, 0.1244, 0.0702, 0.1210,
        0.1243, 0.0507, 0.1237, 0.1244, 0.1219, 0.1244, 0.1243, 0.1241, 0.1217,
        0.1002, 0.1180, 0.1242, 0.1236, 0.1244, 0.1151, 0.1243, 0.1201, 0.1132,
        0.1196, 0.0992, 0.1024, 0.1216, 0.1242, 0.1243, 0.1237, 0.1246, 0.1240,
        0.1233, 0.1244, 0.1238, 0.1244, 0.1125, 0.1250, 0.1243, 0.1243, 0.0974,
        0.1238, 0.0994, 0.1235, 0.1243, 0.1239, 0.1242, 0.1234, 0.1244, 0.1244,
        0.1243, 0.1243, 0.1244, 0.0652, 0.1244, 0.1244, 0.1247, 0.1239, 0.1247,
        0.1243, 0.1244, 0.1240, 0.1237, 0.1204, 0.1175, 0.0765, 0.1244, 0.0716,
        0.1244, 0.1244, 0.1243, 0.1235, 0.1243, 0.1243, 0.1213, 0.1240, 0.1244,
        0.1243, 0.1244, 0.0964, 0.0508, 0.1244, 0.1238, 0.1242, 0.1241, 0.1213,
        0.1203, 0.1146, 0.1243, 0.1243, 0.1120, 0.1243, 0.1244, 0.1216, 0.1237,
        0.0572, 0.1198, 0.0625, 0.1244, 0.1216, 0.1244, 0.1243, 0.1241, 0.1244,
        0.1108, 0.1243, 0.1255, 0.1244, 0.0763, 0.1215, 0.1240, 0.1244, 0.1105,
        0.1234, 0.1244, 0.1244, 0.1239, 0.1244, 0.1244, 0.1246, 0.1216, 0.1231,
        0.1243, 0.1191, 0.0571, 0.1161, 0.1223, 0.1244, 0.1243, 0.1241, 0.1243,
        0.1244, 0.1239, 0.1240, 0.1242, 0.1243, 0.1098, 0.1244, 0.0516, 0.1244,
        0.1243, 0.1245, 0.0697, 0.1176, 0.1238, 0.1244, 0.1244, 0.1226, 0.1244,
        0.1242, 0.1240, 0.1243, 0.1249, 0.1234, 0.0472, 0.1001, 0.0964, 0.1232,
        0.1243, 0.1241, 0.1244, 0.1214, 0.1241, 0.1137, 0.1244, 0.1240, 0.1244,
        0.1241, 0.1248, 0.1216, 0.1244, 0.1232, 0.1224, 0.1244, 0.1242, 0.0641,
        0.1230, 0.1240, 0.1218, 0.0665, 0.1238, 0.1239, 0.1232, 0.1243, 0.1243,
        0.1207, 0.1235, 0.1243, 0.1244, 0.1243, 0.1227, 0.1243, 0.1244, 0.1213,
        0.0943], device='cuda:1', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(7376.9102, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9688, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 15:55:25,437][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2023-09-12 16:00:28,137][train_inner][INFO] - {"epoch": 9, "update": 8.478, "loss": "4.555", "ntokens": "149914", "nsentences": "539.28", "prob_perplexity": "69.841", "code_perplexity": "69.395", "temp": "1.957", "loss_0": "4.416", "loss_1": "0.129", "loss_2": "0.01", "accuracy": "0.26579", "wps": "37694.1", "ups": "0.25", "wpb": "149914", "bsz": "539.3", "num_updates": "4400", "lr": "6.875e-05", "gnorm": "0.653", "loss_scale": "8", "train_wall": "794", "gb_free": "13", "wall": "17701"}
Parameter containing:
tensor([0.1261, 0.1262, 0.1237, 0.1218, 0.1202, 0.1272, 0.1259, 0.1259, 0.1252,
        0.1259, 0.1208, 0.1178, 0.1256, 0.1262, 0.1260, 0.1261, 0.1227, 0.1260,
        0.0688, 0.1262, 0.1261, 0.1028, 0.0573, 0.0526, 0.1227, 0.1259, 0.1261,
        0.1249, 0.1265, 0.1262, 0.0815, 0.1255, 0.1226, 0.1260, 0.1259, 0.1261,
        0.1241, 0.1262, 0.1261, 0.1277, 0.1262, 0.0663, 0.1262, 0.1254, 0.1261,
        0.0853, 0.1127, 0.1261, 0.1261, 0.1251, 0.1262, 0.1104, 0.1212, 0.1255,
        0.1231, 0.1250, 0.1200, 0.1156, 0.1261, 0.1248, 0.1259, 0.1263, 0.1219,
        0.1261, 0.1209, 0.1261, 0.1262, 0.1248, 0.1262, 0.1261, 0.0876, 0.1261,
        0.1261, 0.1261, 0.1252, 0.1267, 0.1266, 0.0944, 0.1262, 0.1169, 0.1265,
        0.1259, 0.0717, 0.1261, 0.1255, 0.1190, 0.1261, 0.1255, 0.1251, 0.1255,
        0.1180, 0.0599, 0.1255, 0.1261, 0.1145, 0.1139, 0.1099, 0.1262, 0.0646,
        0.1261, 0.1260, 0.1261, 0.1257, 0.1249, 0.0904, 0.1262, 0.1261, 0.0648,
        0.1219, 0.0942, 0.1246, 0.1235, 0.1178, 0.1261, 0.1262, 0.1246, 0.1263,
        0.0589, 0.1241, 0.1193, 0.1262, 0.1261, 0.1262, 0.1261, 0.1146, 0.1223,
        0.1261, 0.1262, 0.1262, 0.1261, 0.1261, 0.1262, 0.1219, 0.1249, 0.0811,
        0.1238, 0.1256, 0.1262, 0.1249, 0.1262, 0.1254, 0.1160, 0.1257, 0.1255,
        0.1248, 0.1260, 0.1255, 0.1252, 0.1259, 0.1199, 0.1247, 0.1257, 0.1235,
        0.1164, 0.1261, 0.1240, 0.1261, 0.1261, 0.1245, 0.1262, 0.1260, 0.1261,
        0.1261, 0.1260, 0.1262, 0.1260, 0.1259, 0.1257, 0.1259, 0.1257, 0.1262,
        0.1261, 0.1245, 0.1261, 0.1261, 0.1180, 0.1261, 0.1259, 0.0601, 0.1187,
        0.1259, 0.1254, 0.1259, 0.1205, 0.1260, 0.1282, 0.1262, 0.1261, 0.1262,
        0.1271, 0.1245, 0.1260, 0.1261, 0.1179, 0.1244, 0.1230, 0.1235, 0.1261,
        0.1260, 0.1251, 0.1176, 0.1261, 0.1259, 0.1262, 0.1261, 0.0756, 0.1175,
        0.1217, 0.1261, 0.1263, 0.1246, 0.0803, 0.1260, 0.1262, 0.1263, 0.1262,
        0.1047, 0.1262, 0.1262, 0.1261, 0.1261, 0.1204, 0.1260, 0.1262, 0.1247,
        0.1252, 0.1262, 0.1261, 0.1262, 0.0571, 0.1261, 0.1203, 0.1261, 0.1262,
        0.1261, 0.1254, 0.1243, 0.1112, 0.1252, 0.1233, 0.1262, 0.1240, 0.1195,
        0.1255, 0.1238, 0.1261, 0.1241, 0.1237, 0.1262, 0.1262, 0.1261, 0.1262,
        0.0882, 0.1209, 0.1243, 0.1262, 0.1262, 0.1261, 0.1234, 0.1120, 0.1220,
        0.1259, 0.1257, 0.1078, 0.1249, 0.1237, 0.1261, 0.1262, 0.1262, 0.1255,
        0.1169, 0.1252, 0.1262, 0.1256, 0.1242, 0.1262, 0.1031, 0.1260, 0.0961,
        0.0835, 0.1262, 0.1262, 0.1188, 0.1263, 0.1259, 0.1245, 0.1263, 0.1251,
        0.1262, 0.1260, 0.1262, 0.1161, 0.1243, 0.1260, 0.1256, 0.0623, 0.1261,
        0.0609, 0.1217, 0.1261, 0.1259, 0.1054, 0.0804, 0.1242, 0.1265, 0.1268,
        0.1260, 0.1262, 0.1261, 0.0875, 0.1261, 0.1245, 0.1229, 0.1259, 0.1261,
        0.1228, 0.1261, 0.0866, 0.1061, 0.1260, 0.1243, 0.0733, 0.1262, 0.1261,
        0.1245, 0.1244, 0.1261, 0.1154, 0.1261, 0.1249, 0.1245, 0.1262, 0.1260,
        0.0781, 0.1248, 0.1261, 0.1256, 0.0874, 0.1256, 0.1260, 0.1252, 0.1260,
        0.1261, 0.1262, 0.1261, 0.1218, 0.1185, 0.0613, 0.1207, 0.1262, 0.1271,
        0.0701, 0.1257, 0.1262, 0.1241, 0.1260, 0.1261, 0.1252, 0.1261, 0.1278,
        0.1260, 0.1262, 0.1249, 0.1261, 0.1011, 0.1252, 0.1259, 0.1262, 0.1262,
        0.1238, 0.1262, 0.1262, 0.1260, 0.1262, 0.1248, 0.1218, 0.1261, 0.0782,
        0.1262, 0.1202, 0.1263, 0.0718, 0.1262, 0.1262, 0.1278, 0.1261, 0.1196,
        0.1103, 0.1234, 0.1261, 0.1204, 0.1262, 0.1261, 0.1261, 0.0685, 0.1263,
        0.1243, 0.0832, 0.1260, 0.0855, 0.1262, 0.1231, 0.1249, 0.1260, 0.1261,
        0.1268, 0.1262, 0.1038, 0.1250, 0.1260, 0.1261, 0.1261, 0.0980, 0.1267,
        0.1259, 0.1261, 0.1262, 0.1202, 0.1261, 0.1262, 0.1262, 0.1262, 0.1250,
        0.1261, 0.1263, 0.1252, 0.1262, 0.1260, 0.1230, 0.1261, 0.1262, 0.1256,
        0.0927, 0.1226, 0.1262, 0.1260, 0.1262, 0.1252, 0.1248, 0.1250, 0.1254,
        0.1261, 0.1260, 0.1257, 0.1261, 0.1199, 0.1249, 0.1262, 0.0717, 0.1229,
        0.1261, 0.0513, 0.1255, 0.1262, 0.1238, 0.1262, 0.1261, 0.1260, 0.1229,
        0.1021, 0.1198, 0.1260, 0.1254, 0.1262, 0.1169, 0.1261, 0.1218, 0.1147,
        0.1214, 0.0980, 0.1040, 0.1232, 0.1260, 0.1261, 0.1255, 0.1262, 0.1259,
        0.1250, 0.1262, 0.1256, 0.1262, 0.1143, 0.1267, 0.1261, 0.1261, 0.0972,
        0.1256, 0.0995, 0.1254, 0.1261, 0.1257, 0.1260, 0.1249, 0.1262, 0.1261,
        0.1261, 0.1261, 0.1262, 0.0662, 0.1262, 0.1262, 0.1265, 0.1257, 0.1265,
        0.1261, 0.1262, 0.1257, 0.1255, 0.1221, 0.1193, 0.0773, 0.1262, 0.0712,
        0.1261, 0.1261, 0.1261, 0.1254, 0.1261, 0.1261, 0.1232, 0.1255, 0.1262,
        0.1261, 0.1261, 0.0985, 0.0518, 0.1262, 0.1256, 0.1256, 0.1260, 0.1230,
        0.1223, 0.1164, 0.1261, 0.1261, 0.1138, 0.1261, 0.1262, 0.1234, 0.1255,
        0.0583, 0.1213, 0.0625, 0.1262, 0.1235, 0.1262, 0.1261, 0.1259, 0.1262,
        0.1125, 0.1261, 0.1260, 0.1262, 0.0751, 0.1234, 0.1257, 0.1262, 0.1122,
        0.1251, 0.1262, 0.1262, 0.1257, 0.1262, 0.1262, 0.1263, 0.1235, 0.1249,
        0.1261, 0.1208, 0.0574, 0.1179, 0.1241, 0.1262, 0.1261, 0.1260, 0.1261,
        0.1262, 0.1257, 0.1256, 0.1260, 0.1261, 0.1116, 0.1261, 0.0526, 0.1262,
        0.1261, 0.1263, 0.0696, 0.1193, 0.1256, 0.1262, 0.1261, 0.1244, 0.1262,
        0.1260, 0.1257, 0.1261, 0.1251, 0.1249, 0.0485, 0.1017, 0.0962, 0.1249,
        0.1261, 0.1259, 0.1261, 0.1232, 0.1259, 0.1146, 0.1262, 0.1259, 0.1261,
        0.1259, 0.1266, 0.1231, 0.1262, 0.1249, 0.1241, 0.1261, 0.1260, 0.0652,
        0.1248, 0.1257, 0.1237, 0.0667, 0.1255, 0.1257, 0.1250, 0.1261, 0.1261,
        0.1225, 0.1254, 0.1261, 0.1262, 0.1261, 0.1245, 0.1261, 0.1262, 0.1231,
        0.0945], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(5528.6558, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2852, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8274.7061, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2109, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1392, 0.1392, 0.1368, 0.1350, 0.1334, 0.1409, 0.1390, 0.1388, 0.1383,
        0.1390, 0.1317, 0.1302, 0.1370, 0.1393, 0.1392, 0.1393, 0.1357, 0.1392,
        0.0688, 0.1393, 0.1393, 0.1104, 0.0624, 0.0606, 0.1332, 0.1390, 0.1392,
        0.1375, 0.1394, 0.1393, 0.0806, 0.1385, 0.1353, 0.1392, 0.1389, 0.1393,
        0.1372, 0.1393, 0.1392, 0.1407, 0.1393, 0.0706, 0.1393, 0.1385, 0.1392,
        0.0870, 0.1243, 0.1393, 0.1393, 0.1383, 0.1393, 0.1229, 0.1344, 0.1385,
        0.1344, 0.1382, 0.1323, 0.1283, 0.1392, 0.1379, 0.1385, 0.1395, 0.1345,
        0.1392, 0.1331, 0.1392, 0.1393, 0.1378, 0.1393, 0.1393, 0.0771, 0.1393,
        0.1371, 0.1393, 0.1384, 0.1405, 0.1394, 0.1014, 0.1393, 0.1300, 0.1392,
        0.1390, 0.0835, 0.1393, 0.1385, 0.1278, 0.1392, 0.1385, 0.1383, 0.1385,
        0.1203, 0.0649, 0.1387, 0.1393, 0.1273, 0.1232, 0.1021, 0.1393, 0.0684,
        0.1392, 0.1392, 0.1393, 0.1389, 0.1381, 0.0860, 0.1393, 0.1393, 0.0670,
        0.1313, 0.0910, 0.1376, 0.1375, 0.1309, 0.1393, 0.1390, 0.1372, 0.1393,
        0.0648, 0.1372, 0.1324, 0.1393, 0.1393, 0.1393, 0.1393, 0.1270, 0.1313,
        0.1393, 0.1393, 0.1392, 0.1393, 0.1392, 0.1393, 0.1349, 0.1379, 0.0869,
        0.1370, 0.1387, 0.1383, 0.1381, 0.1393, 0.1384, 0.1292, 0.1388, 0.1387,
        0.1378, 0.1392, 0.1384, 0.1384, 0.1390, 0.1088, 0.1372, 0.1389, 0.1367,
        0.1295, 0.1393, 0.1364, 0.1392, 0.1393, 0.1376, 0.1393, 0.1390, 0.1393,
        0.1393, 0.1390, 0.1393, 0.1392, 0.1389, 0.1388, 0.1394, 0.1387, 0.1393,
        0.1392, 0.1377, 0.1393, 0.1390, 0.1312, 0.1393, 0.1389, 0.0687, 0.1316,
        0.1390, 0.1385, 0.1390, 0.1311, 0.1392, 0.1425, 0.1393, 0.1393, 0.1393,
        0.1404, 0.1377, 0.1390, 0.1392, 0.1313, 0.1375, 0.1361, 0.1366, 0.1392,
        0.1392, 0.1383, 0.1307, 0.1392, 0.1390, 0.1393, 0.1392, 0.0767, 0.1296,
        0.1327, 0.1393, 0.1393, 0.1377, 0.0758, 0.1392, 0.1393, 0.1394, 0.1393,
        0.1145, 0.1393, 0.1393, 0.1392, 0.1392, 0.1326, 0.1392, 0.1393, 0.1378,
        0.1270, 0.1393, 0.1392, 0.1393, 0.0633, 0.1393, 0.1320, 0.1393, 0.1393,
        0.1393, 0.1384, 0.1375, 0.1075, 0.1384, 0.1342, 0.1393, 0.1373, 0.1326,
        0.1372, 0.1351, 0.1393, 0.1348, 0.1329, 0.1393, 0.1393, 0.1388, 0.1400,
        0.0829, 0.1346, 0.1373, 0.1393, 0.1395, 0.1393, 0.1365, 0.1251, 0.1344,
        0.1389, 0.1389, 0.1127, 0.1381, 0.1366, 0.1392, 0.1393, 0.1342, 0.1385,
        0.1210, 0.1384, 0.1393, 0.1388, 0.1373, 0.1394, 0.0972, 0.1392, 0.0891,
        0.0782, 0.1393, 0.1393, 0.1310, 0.1394, 0.1390, 0.1376, 0.1394, 0.1383,
        0.1393, 0.1392, 0.1393, 0.1282, 0.1373, 0.1392, 0.1385, 0.0660, 0.1389,
        0.0642, 0.1349, 0.1393, 0.1389, 0.1182, 0.0839, 0.1357, 0.1392, 0.1400,
        0.1392, 0.1393, 0.1393, 0.0891, 0.1393, 0.1377, 0.1360, 0.1388, 0.1393,
        0.1344, 0.1393, 0.0945, 0.1192, 0.1392, 0.1375, 0.0753, 0.1393, 0.1393,
        0.1376, 0.1376, 0.1392, 0.1271, 0.1393, 0.1375, 0.1376, 0.1393, 0.1392,
        0.0781, 0.1378, 0.1390, 0.1387, 0.0850, 0.1388, 0.1392, 0.1383, 0.1390,
        0.1393, 0.1393, 0.1392, 0.1349, 0.1316, 0.0642, 0.1337, 0.1393, 0.1396,
        0.0739, 0.1389, 0.1393, 0.1372, 0.1392, 0.1393, 0.1368, 0.1393, 0.1388,
        0.1392, 0.1393, 0.1371, 0.1393, 0.1139, 0.1384, 0.1390, 0.1393, 0.1393,
        0.1368, 0.1393, 0.1393, 0.1390, 0.1393, 0.1379, 0.1349, 0.1393, 0.0789,
        0.1405, 0.1333, 0.1394, 0.0748, 0.1393, 0.1393, 0.1388, 0.1393, 0.1320,
        0.1234, 0.1366, 0.1393, 0.1334, 0.1393, 0.1392, 0.1392, 0.0729, 0.1394,
        0.1371, 0.0815, 0.1392, 0.0832, 0.1393, 0.1357, 0.1362, 0.1392, 0.1392,
        0.1366, 0.1393, 0.1180, 0.1382, 0.1390, 0.1393, 0.1393, 0.1106, 0.1392,
        0.1389, 0.1393, 0.1393, 0.1333, 0.1393, 0.1392, 0.1393, 0.1393, 0.1381,
        0.1393, 0.1393, 0.1384, 0.1393, 0.1392, 0.1362, 0.1392, 0.1393, 0.1387,
        0.0891, 0.1357, 0.1393, 0.1390, 0.1393, 0.1383, 0.1378, 0.1382, 0.1385,
        0.1392, 0.1392, 0.1389, 0.1393, 0.1331, 0.1367, 0.1393, 0.0826, 0.1360,
        0.1393, 0.0583, 0.1385, 0.1393, 0.1368, 0.1393, 0.1393, 0.1390, 0.1357,
        0.1159, 0.1327, 0.1392, 0.1385, 0.1393, 0.1300, 0.1393, 0.1350, 0.1276,
        0.1345, 0.0902, 0.0995, 0.1361, 0.1392, 0.1393, 0.1385, 0.1368, 0.1389,
        0.1379, 0.1393, 0.1388, 0.1393, 0.1274, 0.1389, 0.1393, 0.1392, 0.0955,
        0.1388, 0.1058, 0.1384, 0.1393, 0.1388, 0.1390, 0.1387, 0.1393, 0.1393,
        0.1392, 0.1393, 0.1393, 0.0694, 0.1393, 0.1393, 0.1392, 0.1388, 0.1393,
        0.1392, 0.1393, 0.1389, 0.1385, 0.1350, 0.1324, 0.0798, 0.1393, 0.0743,
        0.1393, 0.1393, 0.1393, 0.1384, 0.1393, 0.1393, 0.1360, 0.1379, 0.1393,
        0.1393, 0.1392, 0.1131, 0.0591, 0.1393, 0.1388, 0.1377, 0.1390, 0.1333,
        0.1348, 0.1295, 0.1393, 0.1392, 0.1263, 0.1393, 0.1393, 0.1366, 0.1385,
        0.0664, 0.1331, 0.0652, 0.1393, 0.1360, 0.1393, 0.1392, 0.1390, 0.1393,
        0.1255, 0.1392, 0.1274, 0.1393, 0.0668, 0.1356, 0.1389, 0.1393, 0.1247,
        0.1376, 0.1393, 0.1393, 0.1388, 0.1393, 0.1393, 0.1395, 0.1366, 0.1381,
        0.1392, 0.1340, 0.0623, 0.1311, 0.1368, 0.1393, 0.1393, 0.1390, 0.1392,
        0.1393, 0.1388, 0.1393, 0.1392, 0.1393, 0.1245, 0.1393, 0.0595, 0.1393,
        0.1392, 0.1394, 0.0740, 0.1321, 0.1388, 0.1393, 0.1393, 0.1375, 0.1393,
        0.1392, 0.1389, 0.1392, 0.1340, 0.1355, 0.0556, 0.1144, 0.0894, 0.1379,
        0.1392, 0.1389, 0.1393, 0.1364, 0.1390, 0.1249, 0.1393, 0.1389, 0.1393,
        0.1390, 0.1393, 0.1356, 0.1393, 0.1381, 0.1373, 0.1393, 0.1392, 0.0666,
        0.1379, 0.1389, 0.1367, 0.0695, 0.1387, 0.1388, 0.1381, 0.1393, 0.1392,
        0.1356, 0.1384, 0.1393, 0.1393, 0.1393, 0.1377, 0.1393, 0.1393, 0.1362,
        0.0995], device='cuda:1', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(6686.8481, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3516, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 16:12:25,946][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2023-09-12 16:13:43,558][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2023-09-12 16:13:55,924][train_inner][INFO] - {"epoch": 9, "update": 8.866, "loss": "4.539", "ntokens": "149816", "nsentences": "541.475", "prob_perplexity": "71.516", "code_perplexity": "70.918", "temp": "1.956", "loss_0": "4.4", "loss_1": "0.128", "loss_2": "0.01", "accuracy": "0.26736", "wps": "37092.9", "ups": "0.25", "wpb": "149816", "bsz": "541.5", "num_updates": "4600", "lr": "7.1875e-05", "gnorm": "0.638", "loss_scale": "4", "train_wall": "807", "gb_free": "13.2", "wall": "18508"}
[2023-09-12 16:18:27,279][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 16:18:27,280][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 16:18:27,493][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2023-09-12 16:18:51,246][valid][INFO] - {"epoch": 9, "valid_loss": "4.277", "valid_ntokens": "7940.39", "valid_nsentences": "55.2525", "valid_prob_perplexity": "68.451", "valid_code_perplexity": "68.195", "valid_temp": "1.954", "valid_loss_0": "4.137", "valid_loss_1": "0.129", "valid_loss_2": "0.011", "valid_accuracy": "0.32013", "valid_wps": "33322.9", "valid_wpb": "7940.4", "valid_bsz": "55.3", "valid_num_updates": "4670", "valid_best_loss": "4.277"}
[2023-09-12 16:18:51,247][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 4670 updates
[2023-09-12 16:18:51,248][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 16:18:53,699][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 16:18:55,067][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 4670 updates, score 4.277) (writing took 3.8200766310328618 seconds)
[2023-09-12 16:18:55,068][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2023-09-12 16:18:55,068][train][INFO] - {"epoch": 9, "train_loss": "4.548", "train_ntokens": "149522", "train_nsentences": "538.411", "train_prob_perplexity": "70.764", "train_code_perplexity": "70.228", "train_temp": "1.956", "train_loss_0": "4.41", "train_loss_1": "0.128", "train_loss_2": "0.01", "train_accuracy": "0.26634", "train_wps": "37040.4", "train_ups": "0.25", "train_wpb": "149522", "train_bsz": "538.4", "train_num_updates": "4670", "train_lr": "7.29687e-05", "train_gnorm": "0.646", "train_loss_scale": "4", "train_train_wall": "2060", "train_gb_free": "13.5", "train_wall": "18808"}
[2023-09-12 16:18:55,070][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 16:18:55,166][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2023-09-12 16:18:55,408][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 16:18:55,411][fairseq.trainer][INFO] - begin training epoch 10
[2023-09-12 16:18:55,412][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([0.1394, 0.1394, 0.1371, 0.1351, 0.1335, 0.1411, 0.1393, 0.1390, 0.1385,
        0.1393, 0.1320, 0.1305, 0.1372, 0.1395, 0.1394, 0.1395, 0.1360, 0.1394,
        0.0686, 0.1395, 0.1395, 0.1106, 0.0624, 0.0605, 0.1333, 0.1393, 0.1394,
        0.1377, 0.1396, 0.1395, 0.0806, 0.1388, 0.1354, 0.1394, 0.1392, 0.1395,
        0.1375, 0.1395, 0.1394, 0.1410, 0.1395, 0.0706, 0.1395, 0.1388, 0.1394,
        0.0871, 0.1246, 0.1395, 0.1394, 0.1385, 0.1395, 0.1231, 0.1346, 0.1388,
        0.1345, 0.1384, 0.1324, 0.1285, 0.1394, 0.1381, 0.1388, 0.1396, 0.1348,
        0.1394, 0.1332, 0.1394, 0.1395, 0.1381, 0.1395, 0.1395, 0.0770, 0.1395,
        0.1373, 0.1395, 0.1387, 0.1407, 0.1396, 0.1016, 0.1395, 0.1302, 0.1394,
        0.1393, 0.0837, 0.1395, 0.1388, 0.1281, 0.1394, 0.1388, 0.1385, 0.1388,
        0.1202, 0.0649, 0.1389, 0.1395, 0.1276, 0.1231, 0.1017, 0.1395, 0.0683,
        0.1394, 0.1394, 0.1395, 0.1392, 0.1383, 0.0861, 0.1395, 0.1395, 0.0668,
        0.1316, 0.0909, 0.1378, 0.1377, 0.1311, 0.1395, 0.1393, 0.1375, 0.1395,
        0.0646, 0.1375, 0.1327, 0.1395, 0.1394, 0.1395, 0.1395, 0.1272, 0.1313,
        0.1395, 0.1395, 0.1394, 0.1395, 0.1394, 0.1395, 0.1351, 0.1382, 0.0871,
        0.1372, 0.1389, 0.1385, 0.1383, 0.1395, 0.1387, 0.1293, 0.1390, 0.1389,
        0.1381, 0.1394, 0.1387, 0.1385, 0.1393, 0.1087, 0.1373, 0.1392, 0.1370,
        0.1298, 0.1395, 0.1366, 0.1394, 0.1395, 0.1378, 0.1395, 0.1393, 0.1395,
        0.1395, 0.1393, 0.1395, 0.1394, 0.1392, 0.1390, 0.1395, 0.1389, 0.1395,
        0.1394, 0.1379, 0.1395, 0.1393, 0.1315, 0.1395, 0.1392, 0.0687, 0.1318,
        0.1393, 0.1388, 0.1393, 0.1312, 0.1394, 0.1427, 0.1395, 0.1395, 0.1395,
        0.1406, 0.1379, 0.1393, 0.1394, 0.1316, 0.1377, 0.1364, 0.1368, 0.1394,
        0.1394, 0.1385, 0.1310, 0.1394, 0.1393, 0.1395, 0.1394, 0.0767, 0.1299,
        0.1328, 0.1395, 0.1395, 0.1379, 0.0759, 0.1394, 0.1395, 0.1396, 0.1395,
        0.1147, 0.1395, 0.1395, 0.1394, 0.1394, 0.1328, 0.1393, 0.1395, 0.1381,
        0.1268, 0.1395, 0.1394, 0.1395, 0.0633, 0.1395, 0.1321, 0.1395, 0.1395,
        0.1395, 0.1387, 0.1377, 0.1075, 0.1387, 0.1344, 0.1395, 0.1376, 0.1328,
        0.1375, 0.1354, 0.1395, 0.1350, 0.1331, 0.1395, 0.1395, 0.1390, 0.1403,
        0.0829, 0.1348, 0.1376, 0.1395, 0.1398, 0.1395, 0.1367, 0.1254, 0.1346,
        0.1392, 0.1390, 0.1129, 0.1383, 0.1367, 0.1394, 0.1395, 0.1343, 0.1388,
        0.1207, 0.1387, 0.1395, 0.1390, 0.1376, 0.1395, 0.0971, 0.1394, 0.0889,
        0.0783, 0.1395, 0.1394, 0.1311, 0.1396, 0.1393, 0.1378, 0.1396, 0.1385,
        0.1395, 0.1394, 0.1395, 0.1284, 0.1376, 0.1394, 0.1387, 0.0662, 0.1390,
        0.0641, 0.1351, 0.1395, 0.1392, 0.1185, 0.0839, 0.1359, 0.1393, 0.1401,
        0.1394, 0.1395, 0.1395, 0.0892, 0.1395, 0.1379, 0.1362, 0.1389, 0.1395,
        0.1345, 0.1395, 0.0942, 0.1194, 0.1394, 0.1377, 0.0755, 0.1395, 0.1395,
        0.1378, 0.1378, 0.1394, 0.1273, 0.1395, 0.1377, 0.1378, 0.1395, 0.1394,
        0.0781, 0.1381, 0.1393, 0.1388, 0.0851, 0.1390, 0.1394, 0.1385, 0.1393,
        0.1395, 0.1395, 0.1394, 0.1351, 0.1318, 0.0642, 0.1338, 0.1395, 0.1399,
        0.0738, 0.1390, 0.1395, 0.1375, 0.1394, 0.1395, 0.1370, 0.1395, 0.1390,
        0.1394, 0.1395, 0.1373, 0.1395, 0.1141, 0.1387, 0.1392, 0.1395, 0.1395,
        0.1371, 0.1395, 0.1395, 0.1393, 0.1395, 0.1382, 0.1351, 0.1395, 0.0789,
        0.1407, 0.1335, 0.1396, 0.0748, 0.1395, 0.1395, 0.1389, 0.1395, 0.1321,
        0.1237, 0.1368, 0.1395, 0.1337, 0.1395, 0.1394, 0.1394, 0.0729, 0.1396,
        0.1373, 0.0815, 0.1394, 0.0833, 0.1395, 0.1360, 0.1364, 0.1393, 0.1394,
        0.1367, 0.1395, 0.1183, 0.1384, 0.1393, 0.1395, 0.1395, 0.1108, 0.1394,
        0.1392, 0.1395, 0.1395, 0.1335, 0.1395, 0.1394, 0.1395, 0.1395, 0.1383,
        0.1394, 0.1395, 0.1387, 0.1395, 0.1394, 0.1364, 0.1394, 0.1395, 0.1389,
        0.0888, 0.1360, 0.1395, 0.1393, 0.1395, 0.1385, 0.1381, 0.1384, 0.1388,
        0.1394, 0.1394, 0.1392, 0.1395, 0.1332, 0.1370, 0.1395, 0.0828, 0.1362,
        0.1395, 0.0582, 0.1388, 0.1395, 0.1371, 0.1395, 0.1395, 0.1393, 0.1360,
        0.1161, 0.1329, 0.1394, 0.1387, 0.1395, 0.1302, 0.1395, 0.1353, 0.1277,
        0.1348, 0.0898, 0.0994, 0.1364, 0.1394, 0.1395, 0.1388, 0.1371, 0.1392,
        0.1382, 0.1395, 0.1390, 0.1395, 0.1277, 0.1392, 0.1395, 0.1394, 0.0953,
        0.1390, 0.1060, 0.1387, 0.1395, 0.1390, 0.1393, 0.1388, 0.1395, 0.1395,
        0.1394, 0.1395, 0.1395, 0.0693, 0.1395, 0.1395, 0.1394, 0.1390, 0.1395,
        0.1394, 0.1395, 0.1392, 0.1388, 0.1353, 0.1327, 0.0797, 0.1395, 0.0743,
        0.1395, 0.1395, 0.1394, 0.1387, 0.1395, 0.1395, 0.1361, 0.1382, 0.1395,
        0.1395, 0.1394, 0.1134, 0.0592, 0.1395, 0.1390, 0.1379, 0.1393, 0.1334,
        0.1350, 0.1298, 0.1395, 0.1394, 0.1266, 0.1395, 0.1395, 0.1368, 0.1388,
        0.0665, 0.1333, 0.0654, 0.1395, 0.1362, 0.1395, 0.1394, 0.1393, 0.1395,
        0.1257, 0.1394, 0.1273, 0.1395, 0.0668, 0.1359, 0.1392, 0.1395, 0.1249,
        0.1376, 0.1395, 0.1395, 0.1390, 0.1395, 0.1395, 0.1398, 0.1368, 0.1382,
        0.1394, 0.1342, 0.0623, 0.1313, 0.1371, 0.1395, 0.1394, 0.1393, 0.1394,
        0.1395, 0.1390, 0.1394, 0.1394, 0.1395, 0.1247, 0.1395, 0.0596, 0.1395,
        0.1394, 0.1396, 0.0738, 0.1323, 0.1390, 0.1395, 0.1395, 0.1377, 0.1395,
        0.1394, 0.1392, 0.1394, 0.1343, 0.1356, 0.0556, 0.1147, 0.0892, 0.1382,
        0.1394, 0.1392, 0.1395, 0.1366, 0.1393, 0.1251, 0.1395, 0.1392, 0.1395,
        0.1393, 0.1395, 0.1359, 0.1395, 0.1383, 0.1376, 0.1395, 0.1394, 0.0667,
        0.1382, 0.1392, 0.1370, 0.0695, 0.1389, 0.1390, 0.1383, 0.1395, 0.1394,
        0.1359, 0.1387, 0.1395, 0.1395, 0.1395, 0.1379, 0.1395, 0.1395, 0.1365,
        0.0996], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(9135.8281, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3320, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8903.2188, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1641, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8068.7559, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2852, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6516.5454, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2891, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(8683.9922, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7079.2456, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2656, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 16:27:26,666][train_inner][INFO] - {"epoch": 10, "update": 9.25, "loss": "4.527", "ntokens": "149024", "nsentences": "537.845", "prob_perplexity": "73.11", "code_perplexity": "72.383", "temp": "1.954", "loss_0": "4.388", "loss_1": "0.128", "loss_2": "0.011", "accuracy": "0.26812", "wps": "36762.4", "ups": "0.25", "wpb": "149024", "bsz": "537.8", "num_updates": "4800", "lr": "7.5e-05", "gnorm": "0.66", "loss_scale": "4", "train_wall": "781", "gb_free": "13.4", "wall": "19319"}
loss: tensor(7770.7480, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2266, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 16:31:09,749][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
tensor(8944.4512, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0820, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7142.9663, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9453, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6201.8062, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2070, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6917.2236, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2422, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6310.4883, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1094, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4532.0684, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2734, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1392, 0.1392, 0.1368, 0.1350, 0.1334, 0.1409, 0.1390, 0.1388, 0.1383,
        0.1390, 0.1317, 0.1302, 0.1370, 0.1393, 0.1392, 0.1393, 0.1357, 0.1392,
        0.0688, 0.1393, 0.1393, 0.1104, 0.0624, 0.0606, 0.1332, 0.1390, 0.1392,
        0.1375, 0.1394, 0.1393, 0.0806, 0.1385, 0.1353, 0.1392, 0.1389, 0.1393,
        0.1372, 0.1393, 0.1392, 0.1407, 0.1393, 0.0706, 0.1393, 0.1385, 0.1392,
        0.0870, 0.1243, 0.1393, 0.1393, 0.1383, 0.1393, 0.1229, 0.1344, 0.1385,
        0.1344, 0.1382, 0.1323, 0.1283, 0.1392, 0.1379, 0.1385, 0.1395, 0.1345,
        0.1392, 0.1331, 0.1392, 0.1393, 0.1378, 0.1393, 0.1393, 0.0771, 0.1393,
        0.1371, 0.1393, 0.1384, 0.1405, 0.1394, 0.1014, 0.1393, 0.1300, 0.1392,
        0.1390, 0.0835, 0.1393, 0.1385, 0.1278, 0.1392, 0.1385, 0.1383, 0.1385,
        0.1203, 0.0649, 0.1387, 0.1393, 0.1273, 0.1232, 0.1021, 0.1393, 0.0684,
        0.1392, 0.1392, 0.1393, 0.1389, 0.1381, 0.0860, 0.1393, 0.1393, 0.0670,
        0.1313, 0.0910, 0.1376, 0.1375, 0.1309, 0.1393, 0.1390, 0.1372, 0.1393,
        0.0648, 0.1372, 0.1324, 0.1393, 0.1393, 0.1393, 0.1393, 0.1270, 0.1313,
        0.1393, 0.1393, 0.1392, 0.1393, 0.1392, 0.1393, 0.1349, 0.1379, 0.0869,
        0.1370, 0.1387, 0.1383, 0.1381, 0.1393, 0.1384, 0.1292, 0.1388, 0.1387,
        0.1378, 0.1392, 0.1384, 0.1384, 0.1390, 0.1088, 0.1372, 0.1389, 0.1367,
        0.1295, 0.1393, 0.1364, 0.1392, 0.1393, 0.1376, 0.1393, 0.1390, 0.1393,
        0.1393, 0.1390, 0.1393, 0.1392, 0.1389, 0.1388, 0.1394, 0.1387, 0.1393,
        0.1392, 0.1377, 0.1393, 0.1390, 0.1312, 0.1393, 0.1389, 0.0687, 0.1316,
        0.1390, 0.1385, 0.1390, 0.1311, 0.1392, 0.1425, 0.1393, 0.1393, 0.1393,
        0.1404, 0.1377, 0.1390, 0.1392, 0.1313, 0.1375, 0.1361, 0.1366, 0.1392,
        0.1392, 0.1383, 0.1307, 0.1392, 0.1390, 0.1393, 0.1392, 0.0767, 0.1296,
        0.1327, 0.1393, 0.1393, 0.1377, 0.0758, 0.1392, 0.1393, 0.1394, 0.1393,
        0.1145, 0.1393, 0.1393, 0.1392, 0.1392, 0.1326, 0.1392, 0.1393, 0.1378,
        0.1270, 0.1393, 0.1392, 0.1393, 0.0633, 0.1393, 0.1320, 0.1393, 0.1393,
        0.1393, 0.1384, 0.1375, 0.1075, 0.1384, 0.1342, 0.1393, 0.1373, 0.1326,
        0.1372, 0.1351, 0.1393, 0.1348, 0.1329, 0.1393, 0.1393, 0.1388, 0.1400,
        0.0829, 0.1346, 0.1373, 0.1393, 0.1395, 0.1393, 0.1365, 0.1251, 0.1344,
        0.1389, 0.1389, 0.1127, 0.1381, 0.1366, 0.1392, 0.1393, 0.1342, 0.1385,
        0.1210, 0.1384, 0.1393, 0.1388, 0.1373, 0.1394, 0.0972, 0.1392, 0.0891,
        0.0782, 0.1393, 0.1393, 0.1310, 0.1394, 0.1390, 0.1376, 0.1394, 0.1383,
        0.1393, 0.1392, 0.1393, 0.1282, 0.1373, 0.1392, 0.1385, 0.0660, 0.1389,
        0.0642, 0.1349, 0.1393, 0.1389, 0.1182, 0.0839, 0.1357, 0.1392, 0.1400,
        0.1392, 0.1393, 0.1393, 0.0891, 0.1393, 0.1377, 0.1360, 0.1388, 0.1393,
        0.1344, 0.1393, 0.0945, 0.1192, 0.1392, 0.1375, 0.0753, 0.1393, 0.1393,
        0.1376, 0.1376, 0.1392, 0.1271, 0.1393, 0.1375, 0.1376, 0.1393, 0.1392,
        0.0781, 0.1378, 0.1390, 0.1387, 0.0850, 0.1388, 0.1392, 0.1383, 0.1390,
        0.1393, 0.1393, 0.1392, 0.1349, 0.1316, 0.0642, 0.1337, 0.1393, 0.1396,
        0.0739, 0.1389, 0.1393, 0.1372, 0.1392, 0.1393, 0.1368, 0.1393, 0.1388,
        0.1392, 0.1393, 0.1371, 0.1393, 0.1139, 0.1384, 0.1390, 0.1393, 0.1393,
        0.1368, 0.1393, 0.1393, 0.1390, 0.1393, 0.1379, 0.1349, 0.1393, 0.0789,
        0.1405, 0.1333, 0.1394, 0.0748, 0.1393, 0.1393, 0.1388, 0.1393, 0.1320,
        0.1234, 0.1366, 0.1393, 0.1334, 0.1393, 0.1392, 0.1392, 0.0729, 0.1394,
        0.1371, 0.0815, 0.1392, 0.0832, 0.1393, 0.1357, 0.1362, 0.1392, 0.1392,
        0.1366, 0.1393, 0.1180, 0.1382, 0.1390, 0.1393, 0.1393, 0.1106, 0.1392,
        0.1389, 0.1393, 0.1393, 0.1333, 0.1393, 0.1392, 0.1393, 0.1393, 0.1381,
        0.1393, 0.1393, 0.1384, 0.1393, 0.1392, 0.1362, 0.1392, 0.1393, 0.1387,
        0.0891, 0.1357, 0.1393, 0.1390, 0.1393, 0.1383, 0.1378, 0.1382, 0.1385,
        0.1392, 0.1392, 0.1389, 0.1393, 0.1331, 0.1367, 0.1393, 0.0826, 0.1360,
        0.1393, 0.0583, 0.1385, 0.1393, 0.1368, 0.1393, 0.1393, 0.1390, 0.1357,
        0.1159, 0.1327, 0.1392, 0.1385, 0.1393, 0.1300, 0.1393, 0.1350, 0.1276,
        0.1345, 0.0902, 0.0995, 0.1361, 0.1392, 0.1393, 0.1385, 0.1368, 0.1389,
        0.1379, 0.1393, 0.1388, 0.1393, 0.1274, 0.1389, 0.1393, 0.1392, 0.0955,
        0.1388, 0.1058, 0.1384, 0.1393, 0.1388, 0.1390, 0.1387, 0.1393, 0.1393,
        0.1392, 0.1393, 0.1393, 0.0694, 0.1393, 0.1393, 0.1392, 0.1388, 0.1393,
        0.1392, 0.1393, 0.1389, 0.1385, 0.1350, 0.1324, 0.0798, 0.1393, 0.0743,
        0.1393, 0.1393, 0.1393, 0.1384, 0.1393, 0.1393, 0.1360, 0.1379, 0.1393,
        0.1393, 0.1392, 0.1131, 0.0591, 0.1393, 0.1388, 0.1377, 0.1390, 0.1333,
        0.1348, 0.1295, 0.1393, 0.1392, 0.1263, 0.1393, 0.1393, 0.1366, 0.1385,
        0.0664, 0.1331, 0.0652, 0.1393, 0.1360, 0.1393, 0.1392, 0.1390, 0.1393,
        0.1255, 0.1392, 0.1274, 0.1393, 0.0668, 0.1356, 0.1389, 0.1393, 0.1247,
        0.1376, 0.1393, 0.1393, 0.1388, 0.1393, 0.1393, 0.1395, 0.1366, 0.1381,
        0.1392, 0.1340, 0.0623, 0.1311, 0.1368, 0.1393, 0.1393, 0.1390, 0.1392,
        0.1393, 0.1388, 0.1393, 0.1392, 0.1393, 0.1245, 0.1393, 0.0595, 0.1393,
        0.1392, 0.1394, 0.0740, 0.1321, 0.1388, 0.1393, 0.1393, 0.1375, 0.1393,
        0.1392, 0.1389, 0.1392, 0.1340, 0.1355, 0.0556, 0.1144, 0.0894, 0.1379,
        0.1392, 0.1389, 0.1393, 0.1364, 0.1390, 0.1249, 0.1393, 0.1389, 0.1393,
        0.1390, 0.1393, 0.1356, 0.1393, 0.1381, 0.1373, 0.1393, 0.1392, 0.0666,
        0.1379, 0.1389, 0.1367, 0.0695, 0.1387, 0.1388, 0.1381, 0.1393, 0.1392,
        0.1356, 0.1384, 0.1393, 0.1393, 0.1393, 0.1377, 0.1393, 0.1393, 0.1362,
        0.0995], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6764.6343, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3281, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8374.3984, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3828, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7502.7930, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3398, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8546.6152, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2930, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7560.7520, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3047, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8546.3398, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2461, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6703.4194, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2266, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1586, 0.1587, 0.1562, 0.1544, 0.1528, 0.1561, 0.1584, 0.1582, 0.1578,
        0.1584, 0.1465, 0.1492, 0.1511, 0.1588, 0.1586, 0.1587, 0.1553, 0.1586,
        0.0765, 0.1588, 0.1587, 0.1288, 0.0736, 0.0725, 0.1470, 0.1584, 0.1587,
        0.1554, 0.1569, 0.1587, 0.0870, 0.1581, 0.1537, 0.1586, 0.1583, 0.1583,
        0.1567, 0.1588, 0.1587, 0.1542, 0.1588, 0.0802, 0.1588, 0.1580, 0.1573,
        0.0899, 0.1400, 0.1587, 0.1587, 0.1577, 0.1588, 0.1422, 0.1499, 0.1576,
        0.1471, 0.1576, 0.1471, 0.1470, 0.1587, 0.1573, 0.1554, 0.1580, 0.1509,
        0.1580, 0.1462, 0.1587, 0.1588, 0.1573, 0.1588, 0.1587, 0.0822, 0.1587,
        0.1561, 0.1587, 0.1578, 0.1549, 0.1576, 0.1058, 0.1588, 0.1492, 0.1565,
        0.1584, 0.1056, 0.1587, 0.1581, 0.1444, 0.1586, 0.1581, 0.1577, 0.1577,
        0.1155, 0.0753, 0.1581, 0.1587, 0.1465, 0.1130, 0.0828, 0.1588, 0.0787,
        0.1587, 0.1586, 0.1587, 0.1583, 0.1575, 0.0901, 0.1588, 0.1587, 0.0750,
        0.1450, 0.0944, 0.1571, 0.1493, 0.1504, 0.1587, 0.1565, 0.1554, 0.1584,
        0.0757, 0.1567, 0.1520, 0.1588, 0.1587, 0.1588, 0.1587, 0.1444, 0.1131,
        0.1587, 0.1588, 0.1583, 0.1587, 0.1587, 0.1588, 0.1541, 0.1571, 0.1002,
        0.1562, 0.1580, 0.1497, 0.1548, 0.1588, 0.1577, 0.1486, 0.1583, 0.1581,
        0.1573, 0.1586, 0.1542, 0.1578, 0.1584, 0.1066, 0.1562, 0.1583, 0.1527,
        0.1490, 0.1587, 0.1489, 0.1587, 0.1587, 0.1570, 0.1588, 0.1586, 0.1587,
        0.1587, 0.1571, 0.1588, 0.1586, 0.1584, 0.1571, 0.1555, 0.1580, 0.1587,
        0.1587, 0.1571, 0.1587, 0.1580, 0.1449, 0.1587, 0.1583, 0.0813, 0.1506,
        0.1584, 0.1581, 0.1584, 0.1498, 0.1586, 0.1573, 0.1588, 0.1584, 0.1588,
        0.1547, 0.1571, 0.1586, 0.1586, 0.1466, 0.1570, 0.1555, 0.1560, 0.1583,
        0.1586, 0.1577, 0.1503, 0.1587, 0.1584, 0.1588, 0.1586, 0.0859, 0.1478,
        0.1482, 0.1587, 0.1587, 0.1571, 0.0827, 0.1586, 0.1588, 0.1588, 0.1586,
        0.1274, 0.1588, 0.1587, 0.1587, 0.1587, 0.1508, 0.1571, 0.1588, 0.1572,
        0.1172, 0.1588, 0.1587, 0.1588, 0.0749, 0.1569, 0.1449, 0.1587, 0.1584,
        0.1587, 0.1580, 0.1569, 0.1180, 0.1577, 0.1492, 0.1587, 0.1543, 0.1521,
        0.1504, 0.1487, 0.1587, 0.1512, 0.1387, 0.1588, 0.1588, 0.1543, 0.1553,
        0.0909, 0.1467, 0.1567, 0.1588, 0.1581, 0.1587, 0.1560, 0.1447, 0.1511,
        0.1584, 0.1583, 0.1279, 0.1576, 0.1532, 0.1587, 0.1588, 0.1321, 0.1581,
        0.1042, 0.1578, 0.1588, 0.1582, 0.1569, 0.1588, 0.0956, 0.1586, 0.0927,
        0.0856, 0.1588, 0.1582, 0.1477, 0.1589, 0.1584, 0.1570, 0.1588, 0.1577,
        0.1588, 0.1586, 0.1588, 0.1414, 0.1569, 0.1586, 0.1578, 0.0749, 0.1559,
        0.0741, 0.1543, 0.1587, 0.1584, 0.1353, 0.0881, 0.1520, 0.1554, 0.1549,
        0.1586, 0.1588, 0.1587, 0.0975, 0.1587, 0.1571, 0.1554, 0.1577, 0.1587,
        0.1489, 0.1587, 0.0975, 0.1335, 0.1586, 0.1570, 0.0829, 0.1588, 0.1587,
        0.1571, 0.1570, 0.1586, 0.1464, 0.1587, 0.1565, 0.1570, 0.1588, 0.1586,
        0.0833, 0.1572, 0.1556, 0.1570, 0.0922, 0.1582, 0.1586, 0.1578, 0.1586,
        0.1587, 0.1588, 0.1587, 0.1510, 0.1511, 0.0750, 0.1466, 0.1588, 0.1543,
        0.0838, 0.1583, 0.1588, 0.1567, 0.1586, 0.1588, 0.1495, 0.1588, 0.1521,
        0.1586, 0.1587, 0.1550, 0.1587, 0.1322, 0.1578, 0.1584, 0.1588, 0.1588,
        0.1564, 0.1588, 0.1588, 0.1586, 0.1588, 0.1573, 0.1544, 0.1588, 0.0877,
        0.1580, 0.1528, 0.1589, 0.0846, 0.1588, 0.1588, 0.1526, 0.1587, 0.1448,
        0.1429, 0.1560, 0.1587, 0.1527, 0.1588, 0.1587, 0.1584, 0.0824, 0.1589,
        0.1566, 0.0881, 0.1586, 0.0869, 0.1588, 0.1498, 0.1515, 0.1586, 0.1587,
        0.1506, 0.1588, 0.1379, 0.1576, 0.1586, 0.1587, 0.1587, 0.1250, 0.1562,
        0.1583, 0.1587, 0.1588, 0.1527, 0.1588, 0.1586, 0.1588, 0.1588, 0.1576,
        0.1587, 0.1586, 0.1578, 0.1588, 0.1586, 0.1556, 0.1587, 0.1584, 0.1582,
        0.0936, 0.1552, 0.1588, 0.1586, 0.1588, 0.1578, 0.1567, 0.1576, 0.1580,
        0.1587, 0.1586, 0.1583, 0.1587, 0.1525, 0.1560, 0.1588, 0.1038, 0.1554,
        0.1587, 0.0704, 0.1581, 0.1588, 0.1564, 0.1588, 0.1587, 0.1586, 0.1481,
        0.1367, 0.1511, 0.1586, 0.1580, 0.1588, 0.1494, 0.1587, 0.1544, 0.1436,
        0.1541, 0.0920, 0.1016, 0.1554, 0.1586, 0.1587, 0.1581, 0.1503, 0.1583,
        0.1572, 0.1588, 0.1582, 0.1588, 0.1470, 0.1583, 0.1587, 0.1587, 0.1008,
        0.1582, 0.1246, 0.1580, 0.1587, 0.1583, 0.1586, 0.1522, 0.1588, 0.1587,
        0.1582, 0.1587, 0.1588, 0.0800, 0.1588, 0.1588, 0.1586, 0.1583, 0.1587,
        0.1587, 0.1588, 0.1583, 0.1581, 0.1527, 0.1519, 0.0911, 0.1588, 0.0844,
        0.1587, 0.1588, 0.1587, 0.1580, 0.1587, 0.1587, 0.1481, 0.1567, 0.1587,
        0.1587, 0.1587, 0.1313, 0.0721, 0.1588, 0.1582, 0.1503, 0.1586, 0.1467,
        0.1517, 0.1489, 0.1587, 0.1587, 0.1400, 0.1587, 0.1588, 0.1559, 0.1581,
        0.0771, 0.1515, 0.0748, 0.1588, 0.1508, 0.1588, 0.1587, 0.1584, 0.1588,
        0.1450, 0.1587, 0.1215, 0.1588, 0.0736, 0.1549, 0.1583, 0.1588, 0.1434,
        0.1442, 0.1588, 0.1588, 0.1583, 0.1588, 0.1588, 0.1589, 0.1560, 0.1575,
        0.1587, 0.1534, 0.0729, 0.1505, 0.1560, 0.1588, 0.1587, 0.1586, 0.1587,
        0.1588, 0.1583, 0.1538, 0.1586, 0.1587, 0.1437, 0.1587, 0.0713, 0.1588,
        0.1587, 0.1587, 0.0822, 0.1516, 0.1582, 0.1588, 0.1588, 0.1569, 0.1588,
        0.1586, 0.1583, 0.1587, 0.1471, 0.1510, 0.0678, 0.1323, 0.0894, 0.1572,
        0.1586, 0.1584, 0.1587, 0.1558, 0.1584, 0.1440, 0.1588, 0.1584, 0.1587,
        0.1584, 0.1560, 0.1550, 0.1588, 0.1575, 0.1567, 0.1587, 0.1586, 0.0768,
        0.1573, 0.1583, 0.1562, 0.0804, 0.1581, 0.1583, 0.1576, 0.1587, 0.1587,
        0.1552, 0.1580, 0.1587, 0.1588, 0.1587, 0.1571, 0.1587, 0.1588, 0.1556,
        0.0979], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(8332.0215, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.6055, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7977.5615, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2305, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: [2023-09-12 16:40:45,459][train_inner][INFO] - {"epoch": 10, "update": 9.635, "loss": "4.48", "ntokens": "149685", "nsentences": "537.675", "prob_perplexity": "73.968", "code_perplexity": "73.279", "temp": "1.952", "loss_0": "4.342", "loss_1": "0.128", "loss_2": "0.01", "accuracy": "0.27419", "wps": "37477.8", "ups": "0.25", "wpb": "149685", "bsz": "537.7", "num_updates": "5000", "lr": "7.8125e-05", "gnorm": "0.633", "loss_scale": "4", "train_wall": "798", "gb_free": "12.9", "wall": "20118"}
loss: tensor(6990.6318, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1914, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8334.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7942.1118, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2188, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7136.3911, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2656, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3925.5623, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2344, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 16:49:17,657][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
Parameter containing:
tensor([0.1637, 0.1637, 0.1613, 0.1594, 0.1578, 0.1600, 0.1636, 0.1633, 0.1628,
        0.1636, 0.1510, 0.1542, 0.1555, 0.1638, 0.1637, 0.1638, 0.1603, 0.1637,
        0.0813, 0.1638, 0.1638, 0.1334, 0.0784, 0.0776, 0.1515, 0.1636, 0.1637,
        0.1604, 0.1615, 0.1637, 0.0926, 0.1631, 0.1587, 0.1637, 0.1633, 0.1633,
        0.1617, 0.1638, 0.1637, 0.1571, 0.1638, 0.0847, 0.1638, 0.1631, 0.1625,
        0.0948, 0.1447, 0.1638, 0.1638, 0.1628, 0.1638, 0.1461, 0.1545, 0.1626,
        0.1512, 0.1627, 0.1505, 0.1520, 0.1637, 0.1624, 0.1603, 0.1631, 0.1556,
        0.1630, 0.1495, 0.1637, 0.1638, 0.1624, 0.1638, 0.1638, 0.0871, 0.1638,
        0.1611, 0.1638, 0.1630, 0.1593, 0.1625, 0.1072, 0.1638, 0.1543, 0.1610,
        0.1636, 0.1107, 0.1638, 0.1631, 0.1493, 0.1637, 0.1631, 0.1628, 0.1628,
        0.1191, 0.0797, 0.1632, 0.1638, 0.1516, 0.1065, 0.0831, 0.1638, 0.0833,
        0.1637, 0.1637, 0.1638, 0.1635, 0.1626, 0.0947, 0.1638, 0.1638, 0.0795,
        0.1486, 0.1004, 0.1621, 0.1522, 0.1554, 0.1638, 0.1613, 0.1604, 0.1636,
        0.0803, 0.1617, 0.1570, 0.1638, 0.1637, 0.1638, 0.1638, 0.1492, 0.1116,
        0.1638, 0.1638, 0.1632, 0.1638, 0.1637, 0.1638, 0.1591, 0.1622, 0.1056,
        0.1614, 0.1631, 0.1527, 0.1595, 0.1638, 0.1628, 0.1537, 0.1633, 0.1632,
        0.1624, 0.1637, 0.1586, 0.1628, 0.1635, 0.1122, 0.1614, 0.1635, 0.1556,
        0.1541, 0.1638, 0.1526, 0.1637, 0.1638, 0.1621, 0.1638, 0.1636, 0.1638,
        0.1638, 0.1620, 0.1638, 0.1637, 0.1635, 0.1622, 0.1605, 0.1631, 0.1638,
        0.1637, 0.1622, 0.1638, 0.1631, 0.1482, 0.1638, 0.1633, 0.0865, 0.1558,
        0.1636, 0.1631, 0.1636, 0.1548, 0.1637, 0.1604, 0.1638, 0.1635, 0.1638,
        0.1570, 0.1622, 0.1636, 0.1637, 0.1510, 0.1620, 0.1606, 0.1611, 0.1633,
        0.1637, 0.1628, 0.1553, 0.1637, 0.1636, 0.1638, 0.1637, 0.0895, 0.1530,
        0.1528, 0.1638, 0.1638, 0.1621, 0.0869, 0.1637, 0.1638, 0.1638, 0.1637,
        0.1311, 0.1638, 0.1638, 0.1637, 0.1637, 0.1558, 0.1620, 0.1638, 0.1624,
        0.1163, 0.1638, 0.1637, 0.1638, 0.0801, 0.1619, 0.1489, 0.1638, 0.1636,
        0.1638, 0.1630, 0.1620, 0.1217, 0.1628, 0.1523, 0.1638, 0.1593, 0.1572,
        0.1545, 0.1522, 0.1638, 0.1560, 0.1353, 0.1638, 0.1638, 0.1581, 0.1598,
        0.0963, 0.1504, 0.1619, 0.1638, 0.1631, 0.1638, 0.1610, 0.1497, 0.1559,
        0.1635, 0.1635, 0.1322, 0.1626, 0.1583, 0.1637, 0.1638, 0.1317, 0.1631,
        0.1017, 0.1630, 0.1638, 0.1633, 0.1619, 0.1639, 0.1015, 0.1637, 0.0975,
        0.0912, 0.1638, 0.1633, 0.1506, 0.1639, 0.1636, 0.1621, 0.1639, 0.1628,
        0.1638, 0.1636, 0.1638, 0.1440, 0.1619, 0.1637, 0.1630, 0.0794, 0.1610,
        0.0784, 0.1594, 0.1638, 0.1635, 0.1398, 0.0931, 0.1545, 0.1600, 0.1600,
        0.1637, 0.1638, 0.1638, 0.1021, 0.1638, 0.1622, 0.1605, 0.1628, 0.1638,
        0.1511, 0.1638, 0.1006, 0.1361, 0.1637, 0.1620, 0.0880, 0.1638, 0.1638,
        0.1621, 0.1621, 0.1636, 0.1514, 0.1638, 0.1615, 0.1621, 0.1638, 0.1637,
        0.0883, 0.1624, 0.1602, 0.1620, 0.0956, 0.1633, 0.1637, 0.1628, 0.1636,
        0.1638, 0.1638, 0.1637, 0.1543, 0.1561, 0.0797, 0.1501, 0.1638, 0.1589,
        0.0881, 0.1633, 0.1638, 0.1617, 0.1637, 0.1638, 0.1534, 0.1638, 0.1558,
        0.1637, 0.1638, 0.1599, 0.1638, 0.1371, 0.1630, 0.1635, 0.1638, 0.1638,
        0.1614, 0.1638, 0.1638, 0.1636, 0.1638, 0.1625, 0.1594, 0.1638, 0.0922,
        0.1609, 0.1578, 0.1639, 0.0893, 0.1638, 0.1638, 0.1576, 0.1638, 0.1486,
        0.1479, 0.1611, 0.1638, 0.1577, 0.1638, 0.1637, 0.1636, 0.0870, 0.1639,
        0.1616, 0.0931, 0.1637, 0.0916, 0.1638, 0.1537, 0.1558, 0.1637, 0.1637,
        0.1552, 0.1638, 0.1426, 0.1627, 0.1636, 0.1638, 0.1638, 0.1285, 0.1610,
        0.1633, 0.1638, 0.1638, 0.1578, 0.1638, 0.1636, 0.1638, 0.1638, 0.1626,
        0.1637, 0.1637, 0.1630, 0.1638, 0.1637, 0.1606, 0.1637, 0.1635, 0.1632,
        0.0991, 0.1602, 0.1638, 0.1636, 0.1638, 0.1628, 0.1617, 0.1627, 0.1631,
        0.1637, 0.1637, 0.1635, 0.1638, 0.1576, 0.1611, 0.1638, 0.1090, 0.1605,
        0.1638, 0.0750, 0.1631, 0.1638, 0.1614, 0.1638, 0.1638, 0.1636, 0.1514,
        0.1399, 0.1562, 0.1637, 0.1631, 0.1638, 0.1545, 0.1638, 0.1595, 0.1486,
        0.1591, 0.0972, 0.1053, 0.1604, 0.1637, 0.1638, 0.1631, 0.1539, 0.1635,
        0.1622, 0.1638, 0.1633, 0.1638, 0.1520, 0.1635, 0.1638, 0.1637, 0.1011,
        0.1633, 0.1301, 0.1630, 0.1637, 0.1633, 0.1636, 0.1564, 0.1638, 0.1638,
        0.1632, 0.1638, 0.1638, 0.0849, 0.1638, 0.1638, 0.1636, 0.1633, 0.1638,
        0.1637, 0.1638, 0.1635, 0.1631, 0.1577, 0.1570, 0.0957, 0.1638, 0.0887,
        0.1638, 0.1638, 0.1637, 0.1630, 0.1638, 0.1638, 0.1517, 0.1619, 0.1638,
        0.1638, 0.1638, 0.1348, 0.0770, 0.1638, 0.1633, 0.1538, 0.1636, 0.1510,
        0.1564, 0.1541, 0.1638, 0.1637, 0.1427, 0.1638, 0.1638, 0.1610, 0.1631,
        0.0812, 0.1565, 0.0793, 0.1638, 0.1549, 0.1638, 0.1637, 0.1636, 0.1638,
        0.1500, 0.1637, 0.1196, 0.1638, 0.0781, 0.1599, 0.1635, 0.1638, 0.1484,
        0.1469, 0.1638, 0.1638, 0.1633, 0.1638, 0.1638, 0.1641, 0.1611, 0.1626,
        0.1637, 0.1586, 0.0770, 0.1556, 0.1611, 0.1638, 0.1638, 0.1636, 0.1638,
        0.1638, 0.1633, 0.1584, 0.1637, 0.1638, 0.1487, 0.1638, 0.0757, 0.1638,
        0.1637, 0.1638, 0.0875, 0.1567, 0.1633, 0.1638, 0.1638, 0.1620, 0.1638,
        0.1637, 0.1635, 0.1637, 0.1494, 0.1556, 0.0726, 0.1372, 0.0933, 0.1624,
        0.1637, 0.1635, 0.1638, 0.1609, 0.1636, 0.1492, 0.1638, 0.1635, 0.1638,
        0.1636, 0.1600, 0.1602, 0.1638, 0.1626, 0.1619, 0.1638, 0.1637, 0.0819,
        0.1625, 0.1635, 0.1613, 0.0851, 0.1632, 0.1633, 0.1626, 0.1638, 0.1637,
        0.1602, 0.1630, 0.1638, 0.1638, 0.1638, 0.1622, 0.1638, 0.1638, 0.1608,
        0.1027], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7412.5532, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0859, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8686.5088, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1836, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7374.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2266, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7838.0137, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2227, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 16:53:20,469][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 16:53:20,470][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 16:53:20,624][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2023-09-12 16:53:44,373][valid][INFO] - {"epoch": 10, "valid_loss": "4.161", "valid_ntokens": "7878.08", "valid_nsentences": "55.2525", "valid_prob_perplexity": "71.707", "valid_code_perplexity": "71.33", "valid_temp": "1.949", "valid_loss_0": "4.021", "valid_loss_1": "0.128", "valid_loss_2": "0.011", "valid_accuracy": "0.33599", "valid_wps": "33034.7", "valid_wpb": "7878.1", "valid_bsz": "55.3", "valid_num_updates": "5189", "valid_best_loss": "4.161"}
[2023-09-12 16:53:44,374][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 5189 updates
[2023-09-12 16:53:44,375][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 16:53:46,812][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 16:53:48,193][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 5189 updates, score 4.161) (writing took 3.8186499250587076 seconds)
[2023-09-12 16:53:48,193][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2023-09-12 16:53:48,193][train][INFO] - {"epoch": 10, "train_loss": "4.487", "train_ntokens": "149484", "train_nsentences": "538.437", "train_prob_perplexity": "74.566", "train_code_perplexity": "73.796", "train_temp": "1.951", "train_loss_0": "4.349", "train_loss_1": "0.127", "train_loss_2": "0.011", "train_accuracy": "0.27291", "train_wps": "37065.2", "train_ups": "0.25", "train_wpb": "149484", "train_bsz": "538.4", "train_num_updates": "5189", "train_lr": "8.10781e-05", "train_gnorm": "0.65", "train_loss_scale": "4", "train_train_wall": "2062", "train_gb_free": "13.4", "train_wall": "20901"}
[2023-09-12 16:53:48,195][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 16:53:48,275][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2023-09-12 16:53:48,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 16:53:48,510][fairseq.trainer][INFO] - begin training epoch 11
[2023-09-12 16:53:48,510][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 16:54:30,772][train_inner][INFO] - {"epoch": 11, "update": 10.021, "loss": "4.471", "ntokens": "149070", "nsentences": "536.265", "prob_perplexity": "76.191", "code_perplexity": "75.288", "temp": "1.95", "loss_0": "4.334", "loss_1": "0.127", "loss_2": "0.011", "accuracy": "0.27422", "wps": "36124.5", "ups": "0.24", "wpb": "149070", "bsz": "536.3", "num_updates": "5200", "lr": "8.125e-05", "gnorm": "0.667", "loss_scale": "4", "train_wall": "796", "gb_free": "12.5", "wall": "20943"}
Parameter containing:
tensor([0.1493, 0.1494, 0.1470, 0.1451, 0.1436, 0.1489, 0.1493, 0.1489, 0.1486,
        0.1493, 0.1401, 0.1403, 0.1455, 0.1495, 0.1494, 0.1494, 0.1460, 0.1494,
        0.0703, 0.1495, 0.1495, 0.1208, 0.0676, 0.0663, 0.1401, 0.1492, 0.1494,
        0.1469, 0.1492, 0.1494, 0.0815, 0.1488, 0.1450, 0.1494, 0.1490, 0.1493,
        0.1475, 0.1495, 0.1494, 0.1489, 0.1495, 0.0747, 0.1495, 0.1488, 0.1489,
        0.0857, 0.1326, 0.1494, 0.1494, 0.1486, 0.1495, 0.1332, 0.1429, 0.1484,
        0.1427, 0.1484, 0.1411, 0.1379, 0.1494, 0.1481, 0.1471, 0.1489, 0.1434,
        0.1489, 0.1398, 0.1494, 0.1495, 0.1481, 0.1495, 0.1495, 0.0757, 0.1495,
        0.1469, 0.1495, 0.1486, 0.1494, 0.1490, 0.1017, 0.1495, 0.1401, 0.1492,
        0.1493, 0.0950, 0.1495, 0.1488, 0.1381, 0.1493, 0.1488, 0.1484, 0.1486,
        0.1172, 0.0690, 0.1489, 0.1495, 0.1373, 0.1204, 0.0898, 0.1495, 0.0740,
        0.1494, 0.1494, 0.1495, 0.1492, 0.1483, 0.0854, 0.1495, 0.1495, 0.0699,
        0.1395, 0.0897, 0.1478, 0.1462, 0.1411, 0.1495, 0.1486, 0.1469, 0.1492,
        0.0698, 0.1475, 0.1427, 0.1495, 0.1494, 0.1495, 0.1494, 0.1364, 0.1226,
        0.1495, 0.1495, 0.1492, 0.1495, 0.1494, 0.1495, 0.1449, 0.1479, 0.0927,
        0.1471, 0.1487, 0.1455, 0.1483, 0.1495, 0.1484, 0.1393, 0.1490, 0.1489,
        0.1481, 0.1494, 0.1475, 0.1486, 0.1492, 0.1029, 0.1471, 0.1490, 0.1464,
        0.1398, 0.1495, 0.1440, 0.1494, 0.1495, 0.1478, 0.1495, 0.1493, 0.1495,
        0.1495, 0.1486, 0.1495, 0.1493, 0.1492, 0.1482, 0.1473, 0.1488, 0.1495,
        0.1494, 0.1478, 0.1495, 0.1490, 0.1405, 0.1495, 0.1492, 0.0747, 0.1415,
        0.1493, 0.1488, 0.1492, 0.1407, 0.1494, 0.1514, 0.1495, 0.1492, 0.1495,
        0.1500, 0.1479, 0.1493, 0.1493, 0.1401, 0.1477, 0.1464, 0.1469, 0.1492,
        0.1494, 0.1484, 0.1410, 0.1494, 0.1493, 0.1495, 0.1494, 0.0804, 0.1392,
        0.1405, 0.1494, 0.1494, 0.1478, 0.0777, 0.1494, 0.1495, 0.1495, 0.1494,
        0.1198, 0.1495, 0.1495, 0.1494, 0.1494, 0.1420, 0.1487, 0.1495, 0.1481,
        0.1213, 0.1495, 0.1494, 0.1495, 0.0682, 0.1487, 0.1398, 0.1495, 0.1493,
        0.1495, 0.1487, 0.1476, 0.1093, 0.1486, 0.1432, 0.1495, 0.1467, 0.1428,
        0.1440, 0.1434, 0.1495, 0.1429, 0.1399, 0.1495, 0.1495, 0.1484, 0.1487,
        0.0850, 0.1428, 0.1475, 0.1495, 0.1489, 0.1495, 0.1467, 0.1354, 0.1440,
        0.1492, 0.1490, 0.1208, 0.1483, 0.1453, 0.1494, 0.1495, 0.1324, 0.1488,
        0.1129, 0.1486, 0.1495, 0.1489, 0.1476, 0.1495, 0.0939, 0.1493, 0.0885,
        0.0796, 0.1495, 0.1492, 0.1401, 0.1497, 0.1493, 0.1478, 0.1497, 0.1486,
        0.1495, 0.1493, 0.1495, 0.1368, 0.1476, 0.1493, 0.1486, 0.0693, 0.1470,
        0.0689, 0.1450, 0.1495, 0.1492, 0.1277, 0.0843, 0.1460, 0.1477, 0.1470,
        0.1494, 0.1495, 0.1495, 0.0927, 0.1495, 0.1478, 0.1462, 0.1486, 0.1495,
        0.1434, 0.1495, 0.0949, 0.1267, 0.1494, 0.1477, 0.0775, 0.1495, 0.1495,
        0.1478, 0.1478, 0.1493, 0.1371, 0.1495, 0.1473, 0.1478, 0.1495, 0.1494,
        0.0817, 0.1479, 0.1486, 0.1484, 0.0880, 0.1490, 0.1494, 0.1486, 0.1493,
        0.1495, 0.1495, 0.1494, 0.1442, 0.1418, 0.0688, 0.1414, 0.1495, 0.1478,
        0.0768, 0.1490, 0.1495, 0.1475, 0.1493, 0.1495, 0.1456, 0.1495, 0.1469,
        0.1493, 0.1494, 0.1469, 0.1494, 0.1235, 0.1486, 0.1492, 0.1495, 0.1495,
        0.1471, 0.1495, 0.1495, 0.1493, 0.1495, 0.1481, 0.1451, 0.1495, 0.0809,
        0.1504, 0.1436, 0.1497, 0.0800, 0.1495, 0.1495, 0.1462, 0.1495, 0.1404,
        0.1337, 0.1467, 0.1495, 0.1436, 0.1495, 0.1494, 0.1493, 0.0769, 0.1497,
        0.1473, 0.0818, 0.1494, 0.0845, 0.1495, 0.1447, 0.1448, 0.1493, 0.1494,
        0.1440, 0.1495, 0.1294, 0.1483, 0.1493, 0.1495, 0.1495, 0.1193, 0.1488,
        0.1490, 0.1494, 0.1495, 0.1436, 0.1495, 0.1493, 0.1495, 0.1495, 0.1483,
        0.1494, 0.1493, 0.1486, 0.1495, 0.1494, 0.1464, 0.1494, 0.1493, 0.1489,
        0.0896, 0.1459, 0.1495, 0.1493, 0.1495, 0.1486, 0.1476, 0.1484, 0.1488,
        0.1494, 0.1494, 0.1492, 0.1494, 0.1432, 0.1467, 0.1495, 0.0934, 0.1462,
        0.1495, 0.0641, 0.1488, 0.1495, 0.1471, 0.1495, 0.1495, 0.1493, 0.1439,
        0.1289, 0.1422, 0.1493, 0.1487, 0.1495, 0.1401, 0.1494, 0.1451, 0.1355,
        0.1448, 0.0878, 0.0980, 0.1462, 0.1494, 0.1495, 0.1488, 0.1445, 0.1492,
        0.1479, 0.1495, 0.1489, 0.1495, 0.1377, 0.1492, 0.1495, 0.1494, 0.0951,
        0.1489, 0.1135, 0.1487, 0.1494, 0.1490, 0.1493, 0.1461, 0.1495, 0.1495,
        0.1490, 0.1495, 0.1495, 0.0735, 0.1495, 0.1495, 0.1493, 0.1490, 0.1494,
        0.1494, 0.1495, 0.1490, 0.1488, 0.1444, 0.1426, 0.0820, 0.1495, 0.0784,
        0.1495, 0.1495, 0.1494, 0.1487, 0.1495, 0.1495, 0.1436, 0.1476, 0.1495,
        0.1494, 0.1494, 0.1250, 0.0653, 0.1495, 0.1489, 0.1447, 0.1493, 0.1407,
        0.1440, 0.1398, 0.1495, 0.1494, 0.1356, 0.1494, 0.1495, 0.1467, 0.1488,
        0.0707, 0.1425, 0.0693, 0.1495, 0.1445, 0.1495, 0.1494, 0.1493, 0.1495,
        0.1357, 0.1494, 0.1230, 0.1495, 0.0671, 0.1456, 0.1492, 0.1495, 0.1344,
        0.1425, 0.1495, 0.1495, 0.1490, 0.1495, 0.1495, 0.1497, 0.1469, 0.1482,
        0.1494, 0.1442, 0.0671, 0.1412, 0.1467, 0.1495, 0.1494, 0.1493, 0.1494,
        0.1495, 0.1490, 0.1471, 0.1493, 0.1495, 0.1345, 0.1495, 0.0654, 0.1495,
        0.1494, 0.1495, 0.0776, 0.1423, 0.1490, 0.1495, 0.1495, 0.1477, 0.1495,
        0.1493, 0.1490, 0.1494, 0.1421, 0.1434, 0.0611, 0.1242, 0.0853, 0.1481,
        0.1493, 0.1492, 0.1495, 0.1465, 0.1492, 0.1349, 0.1495, 0.1492, 0.1495,
        0.1492, 0.1492, 0.1459, 0.1495, 0.1483, 0.1476, 0.1495, 0.1493, 0.0715,
        0.1481, 0.1490, 0.1470, 0.0742, 0.1488, 0.1490, 0.1483, 0.1494, 0.1494,
        0.1459, 0.1487, 0.1495, 0.1495, 0.1495, 0.1478, 0.1495, 0.1495, 0.1465,
        0.0978], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7788.1245, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3320, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6807.0273, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1211, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7352.0952, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6581.9038, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1680, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7782.3975, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2305, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5483.3433, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2617, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7006.7769, device='cuda:1')
loss_ent_max: tensor(-4.2344, device='cuda:1', dtype=torch.float16)
self.scaling_factor_for_vector: loss: tensor(7641.9707, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9453, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5837.2993, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1914, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6982.7998, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9531, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3909.0454, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0703, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6918.1934, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1992, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7281.5591, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2188, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8696.5742, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9180, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8121.8657, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2344, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7184.0674, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3906, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7350.0659, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2812, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7162.6035, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3008, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6816.1074, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.1211, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7464.1831, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.0352, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5787.0396, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-3.9648, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5341.1494, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2266, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6982.9536, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2422, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7382.8848, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2773, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1749, 0.1749, 0.1725, 0.1707, 0.1691, 0.1666, 0.1748, 0.1746, 0.1741,
        0.1748, 0.1617, 0.1655, 0.1658, 0.1750, 0.1749, 0.1750, 0.1715, 0.1749,
        0.0877, 0.1750, 0.1750, 0.1421, 0.0862, 0.0854, 0.1627, 0.1748, 0.1749,
        0.1716, 0.1725, 0.1749, 0.1001, 0.1743, 0.1700, 0.1749, 0.1746, 0.1744,
        0.1730, 0.1750, 0.1749, 0.1654, 0.1750, 0.0922, 0.1750, 0.1743, 0.1736,
        0.0997, 0.1559, 0.1750, 0.1749, 0.1741, 0.1750, 0.1541, 0.1656, 0.1738,
        0.1602, 0.1740, 0.1595, 0.1631, 0.1749, 0.1736, 0.1716, 0.1743, 0.1666,
        0.1743, 0.1586, 0.1749, 0.1750, 0.1736, 0.1750, 0.1750, 0.0917, 0.1750,
        0.1724, 0.1750, 0.1742, 0.1670, 0.1736, 0.1133, 0.1750, 0.1655, 0.1722,
        0.1748, 0.1202, 0.1750, 0.1743, 0.1549, 0.1749, 0.1743, 0.1741, 0.1741,
        0.1240, 0.0869, 0.1744, 0.1750, 0.1628, 0.0961, 0.0909, 0.1750, 0.0899,
        0.1749, 0.1749, 0.1750, 0.1747, 0.1738, 0.1001, 0.1750, 0.1750, 0.0864,
        0.1586, 0.1030, 0.1733, 0.1595, 0.1666, 0.1750, 0.1724, 0.1718, 0.1748,
        0.0876, 0.1730, 0.1682, 0.1750, 0.1749, 0.1750, 0.1750, 0.1605, 0.1108,
        0.1750, 0.1750, 0.1746, 0.1750, 0.1749, 0.1750, 0.1703, 0.1735, 0.1124,
        0.1726, 0.1743, 0.1630, 0.1709, 0.1750, 0.1741, 0.1648, 0.1746, 0.1744,
        0.1736, 0.1749, 0.1689, 0.1741, 0.1747, 0.1238, 0.1726, 0.1747, 0.1637,
        0.1653, 0.1750, 0.1633, 0.1749, 0.1750, 0.1733, 0.1750, 0.1748, 0.1750,
        0.1750, 0.1731, 0.1750, 0.1749, 0.1747, 0.1735, 0.1719, 0.1743, 0.1750,
        0.1749, 0.1735, 0.1750, 0.1743, 0.1562, 0.1750, 0.1746, 0.0925, 0.1670,
        0.1748, 0.1743, 0.1748, 0.1660, 0.1749, 0.1687, 0.1750, 0.1747, 0.1750,
        0.1638, 0.1735, 0.1748, 0.1749, 0.1594, 0.1732, 0.1719, 0.1724, 0.1746,
        0.1749, 0.1741, 0.1665, 0.1749, 0.1748, 0.1750, 0.1749, 0.0968, 0.1642,
        0.1638, 0.1750, 0.1750, 0.1733, 0.0923, 0.1749, 0.1750, 0.1750, 0.1749,
        0.1385, 0.1750, 0.1750, 0.1749, 0.1749, 0.1670, 0.1732, 0.1750, 0.1736,
        0.1183, 0.1750, 0.1749, 0.1750, 0.0874, 0.1732, 0.1577, 0.1750, 0.1748,
        0.1750, 0.1742, 0.1732, 0.1283, 0.1741, 0.1593, 0.1750, 0.1705, 0.1685,
        0.1648, 0.1608, 0.1750, 0.1674, 0.1304, 0.1750, 0.1750, 0.1683, 0.1709,
        0.1021, 0.1598, 0.1731, 0.1750, 0.1743, 0.1750, 0.1722, 0.1609, 0.1672,
        0.1747, 0.1747, 0.1416, 0.1738, 0.1696, 0.1749, 0.1750, 0.1342, 0.1743,
        0.1008, 0.1742, 0.1750, 0.1746, 0.1731, 0.1752, 0.1054, 0.1749, 0.1033,
        0.0950, 0.1750, 0.1746, 0.1572, 0.1752, 0.1748, 0.1733, 0.1752, 0.1741,
        0.1750, 0.1748, 0.1750, 0.1520, 0.1731, 0.1749, 0.1742, 0.0863, 0.1722,
        0.0851, 0.1707, 0.1750, 0.1747, 0.1503, 0.0988, 0.1635, 0.1707, 0.1713,
        0.1749, 0.1750, 0.1750, 0.1080, 0.1750, 0.1735, 0.1718, 0.1741, 0.1750,
        0.1592, 0.1750, 0.1081, 0.1427, 0.1749, 0.1732, 0.0927, 0.1750, 0.1750,
        0.1733, 0.1733, 0.1748, 0.1626, 0.1750, 0.1729, 0.1733, 0.1750, 0.1749,
        0.0955, 0.1736, 0.1709, 0.1732, 0.1018, 0.1746, 0.1749, 0.1741, 0.1748,
        0.1750, 0.1750, 0.1749, 0.1624, 0.1674, 0.0872, 0.1610, 0.1750, 0.1698,
        0.0952, 0.1746, 0.1750, 0.1730, 0.1749, 0.1750, 0.1625, 0.1750, 0.1649,
        0.1749, 0.1750, 0.1704, 0.1750, 0.1482, 0.1742, 0.1747, 0.1750, 0.1750,
        0.1726, 0.1750, 0.1750, 0.1748, 0.1750, 0.1737, 0.1707, 0.1750, 0.0991,
        0.1691, 0.1691, 0.1752, 0.0963, 0.1750, 0.1750, 0.1683, 0.1750, 0.1591,
        0.1592, 0.1724, 0.1750, 0.1689, 0.1750, 0.1749, 0.1748, 0.0933, 0.1752,
        0.1729, 0.0981, 0.1749, 0.0960, 0.1750, 0.1605, 0.1656, 0.1748, 0.1749,
        0.1627, 0.1750, 0.1514, 0.1740, 0.1748, 0.1750, 0.1750, 0.1368, 0.1722,
        0.1746, 0.1750, 0.1750, 0.1691, 0.1750, 0.1748, 0.1750, 0.1750, 0.1738,
        0.1749, 0.1749, 0.1742, 0.1750, 0.1749, 0.1719, 0.1749, 0.1747, 0.1744,
        0.1071, 0.1714, 0.1750, 0.1748, 0.1750, 0.1741, 0.1731, 0.1740, 0.1743,
        0.1749, 0.1749, 0.1747, 0.1750, 0.1688, 0.1724, 0.1750, 0.1174, 0.1718,
        0.1750, 0.0825, 0.1743, 0.1750, 0.1726, 0.1750, 0.1750, 0.1748, 0.1597,
        0.1461, 0.1676, 0.1749, 0.1743, 0.1750, 0.1656, 0.1750, 0.1708, 0.1599,
        0.1703, 0.1019, 0.1125, 0.1716, 0.1749, 0.1750, 0.1743, 0.1630, 0.1747,
        0.1735, 0.1750, 0.1746, 0.1750, 0.1632, 0.1747, 0.1750, 0.1749, 0.1079,
        0.1746, 0.1390, 0.1742, 0.1749, 0.1746, 0.1748, 0.1667, 0.1750, 0.1750,
        0.1744, 0.1750, 0.1750, 0.0930, 0.1750, 0.1750, 0.1748, 0.1746, 0.1750,
        0.1749, 0.1750, 0.1747, 0.1743, 0.1688, 0.1682, 0.1017, 0.1750, 0.0950,
        0.1750, 0.1750, 0.1749, 0.1742, 0.1750, 0.1750, 0.1622, 0.1731, 0.1750,
        0.1750, 0.1750, 0.1436, 0.0848, 0.1750, 0.1746, 0.1614, 0.1748, 0.1621,
        0.1675, 0.1653, 0.1750, 0.1749, 0.1508, 0.1750, 0.1750, 0.1722, 0.1743,
        0.0881, 0.1678, 0.0865, 0.1750, 0.1652, 0.1750, 0.1749, 0.1748, 0.1750,
        0.1613, 0.1749, 0.1185, 0.1750, 0.0848, 0.1711, 0.1747, 0.1750, 0.1598,
        0.1550, 0.1750, 0.1750, 0.1746, 0.1750, 0.1750, 0.1753, 0.1724, 0.1737,
        0.1749, 0.1698, 0.0840, 0.1669, 0.1724, 0.1750, 0.1750, 0.1748, 0.1750,
        0.1750, 0.1746, 0.1696, 0.1749, 0.1750, 0.1599, 0.1750, 0.0836, 0.1750,
        0.1749, 0.1750, 0.0925, 0.1680, 0.1746, 0.1750, 0.1750, 0.1732, 0.1750,
        0.1749, 0.1747, 0.1749, 0.1578, 0.1664, 0.0811, 0.1482, 0.0971, 0.1736,
        0.1749, 0.1747, 0.1750, 0.1721, 0.1748, 0.1604, 0.1750, 0.1747, 0.1750,
        0.1748, 0.1698, 0.1714, 0.1750, 0.1738, 0.1731, 0.1750, 0.1749, 0.0875,
        0.1737, 0.1747, 0.1725, 0.0910, 0.1744, 0.1746, 0.1738, 0.1750, 0.1749,
        0.1714, 0.1742, 0.1750, 0.1750, 0.1750, 0.1735, 0.1750, 0.1750, 0.1720,
        0.1141], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6023.6240, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2695, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6416.8589, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2852, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5038.2998, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3516, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7116.3477, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2812, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5758.7734, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2734, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8151.7520, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2773, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7674.2783, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3086, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7525.4453, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3008, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7853.3633, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3008, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8310.4502, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3125, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7781.2729, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3047, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6589.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3203, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7744.1660, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3281, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7921.2471, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3398, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1913, 0.1913, 0.1890, 0.1871, 0.1855, 0.1523, 0.1912, 0.1909, 0.1906,
        0.1912, 0.1774, 0.1819, 0.1820, 0.1914, 0.1913, 0.1914, 0.1880, 0.1913,
        0.0846, 0.1914, 0.1914, 0.1257, 0.0844, 0.0834, 0.1794, 0.1912, 0.1914,
        0.1881, 0.1885, 0.1914, 0.0920, 0.1908, 0.1865, 0.1913, 0.1910, 0.1907,
        0.1895, 0.1914, 0.1913, 0.1787, 0.1914, 0.0883, 0.1914, 0.1907, 0.1901,
        0.0914, 0.1724, 0.1914, 0.1914, 0.1904, 0.1914, 0.1409, 0.1823, 0.1903,
        0.1744, 0.1903, 0.1498, 0.1796, 0.1914, 0.1901, 0.1881, 0.1908, 0.1832,
        0.1908, 0.1442, 0.1914, 0.1914, 0.1901, 0.1914, 0.1914, 0.0865, 0.1914,
        0.1888, 0.1914, 0.1906, 0.1670, 0.1901, 0.1022, 0.1914, 0.1819, 0.1888,
        0.1912, 0.1095, 0.1914, 0.1908, 0.1354, 0.1913, 0.1908, 0.1904, 0.1906,
        0.1267, 0.0842, 0.1908, 0.1914, 0.1792, 0.0884, 0.0862, 0.1915, 0.0850,
        0.1914, 0.1913, 0.1914, 0.1910, 0.1902, 0.0941, 0.1914, 0.1914, 0.0849,
        0.1733, 0.0972, 0.1898, 0.1450, 0.1831, 0.1914, 0.1890, 0.1882, 0.1912,
        0.0847, 0.1895, 0.1847, 0.1915, 0.1914, 0.1915, 0.1914, 0.1770, 0.0966,
        0.1914, 0.1914, 0.1909, 0.1914, 0.1914, 0.1914, 0.1868, 0.1898, 0.1165,
        0.1890, 0.1907, 0.1796, 0.1874, 0.1914, 0.1904, 0.1813, 0.1910, 0.1908,
        0.1901, 0.1913, 0.1852, 0.1906, 0.1912, 0.1304, 0.1890, 0.1910, 0.1479,
        0.1818, 0.1914, 0.1802, 0.1914, 0.1914, 0.1897, 0.1914, 0.1913, 0.1914,
        0.1914, 0.1893, 0.1914, 0.1913, 0.1912, 0.1898, 0.1884, 0.1907, 0.1914,
        0.1914, 0.1898, 0.1914, 0.1907, 0.1388, 0.1914, 0.1910, 0.0859, 0.1835,
        0.1912, 0.1907, 0.1912, 0.1825, 0.1913, 0.1559, 0.1914, 0.1912, 0.1914,
        0.1455, 0.1898, 0.1913, 0.1913, 0.1427, 0.1897, 0.1882, 0.1887, 0.1910,
        0.1913, 0.1904, 0.1830, 0.1913, 0.1912, 0.1914, 0.1913, 0.0911, 0.1807,
        0.1803, 0.1914, 0.1914, 0.1898, 0.0887, 0.1913, 0.1914, 0.1915, 0.1913,
        0.1213, 0.1915, 0.1914, 0.1914, 0.1914, 0.1833, 0.1898, 0.1914, 0.1899,
        0.1050, 0.1914, 0.1914, 0.1914, 0.0848, 0.1897, 0.1710, 0.1914, 0.1912,
        0.1914, 0.1907, 0.1896, 0.1335, 0.1904, 0.1415, 0.1914, 0.1870, 0.1848,
        0.1802, 0.1682, 0.1914, 0.1838, 0.1199, 0.1914, 0.1914, 0.1838, 0.1876,
        0.0920, 0.1688, 0.1895, 0.1914, 0.1908, 0.1914, 0.1887, 0.1772, 0.1837,
        0.1912, 0.1910, 0.1299, 0.1903, 0.1860, 0.1914, 0.1914, 0.1323, 0.1907,
        0.0912, 0.1906, 0.1914, 0.1909, 0.1896, 0.1915, 0.0931, 0.1913, 0.0922,
        0.0860, 0.1914, 0.1909, 0.1398, 0.1915, 0.1912, 0.1897, 0.1915, 0.1904,
        0.1914, 0.1912, 0.1914, 0.1306, 0.1896, 0.1913, 0.1906, 0.0844, 0.1886,
        0.0837, 0.1870, 0.1914, 0.1912, 0.1663, 0.0908, 0.1782, 0.1871, 0.1877,
        0.1913, 0.1915, 0.1914, 0.1030, 0.1914, 0.1898, 0.1881, 0.1906, 0.1914,
        0.1393, 0.1914, 0.1034, 0.1205, 0.1913, 0.1897, 0.0872, 0.1914, 0.1914,
        0.1898, 0.1897, 0.1913, 0.1791, 0.1914, 0.1892, 0.1897, 0.1914, 0.1913,
        0.0867, 0.1899, 0.1876, 0.1897, 0.0924, 0.1909, 0.1913, 0.1906, 0.1913,
        0.1914, 0.1914, 0.1914, 0.1433, 0.1838, 0.0844, 0.1774, 0.1914, 0.1865,
        0.0909, 0.1910, 0.1914, 0.1895, 0.1913, 0.1914, 0.1653, 0.1914, 0.1808,
        0.1913, 0.1914, 0.1866, 0.1914, 0.1646, 0.1906, 0.1912, 0.1914, 0.1914,
        0.1891, 0.1914, 0.1914, 0.1913, 0.1914, 0.1901, 0.1871, 0.1914, 0.0911,
        0.1573, 0.1854, 0.1917, 0.0895, 0.1914, 0.1914, 0.1847, 0.1914, 0.1753,
        0.1757, 0.1887, 0.1914, 0.1854, 0.1914, 0.1914, 0.1912, 0.0880, 0.1915,
        0.1893, 0.0898, 0.1913, 0.0884, 0.1915, 0.1437, 0.1783, 0.1913, 0.1914,
        0.1453, 0.1914, 0.1422, 0.1903, 0.1913, 0.1914, 0.1914, 0.1171, 0.1888,
        0.1910, 0.1914, 0.1914, 0.1854, 0.1914, 0.1913, 0.1914, 0.1914, 0.1903,
        0.1914, 0.1913, 0.1906, 0.1914, 0.1913, 0.1884, 0.1914, 0.1912, 0.1909,
        0.0984, 0.1879, 0.1914, 0.1912, 0.1914, 0.1906, 0.1895, 0.1903, 0.1907,
        0.1914, 0.1913, 0.1910, 0.1914, 0.1852, 0.1887, 0.1914, 0.1217, 0.1881,
        0.1914, 0.0828, 0.1908, 0.1915, 0.1891, 0.1914, 0.1914, 0.1913, 0.1517,
        0.1249, 0.1840, 0.1913, 0.1907, 0.1914, 0.1821, 0.1914, 0.1871, 0.1764,
        0.1868, 0.0915, 0.1029, 0.1881, 0.1913, 0.1914, 0.1908, 0.1787, 0.1910,
        0.1899, 0.1914, 0.1909, 0.1914, 0.1797, 0.1910, 0.1914, 0.1914, 0.0988,
        0.1909, 0.1274, 0.1906, 0.1914, 0.1910, 0.1913, 0.1827, 0.1914, 0.1914,
        0.1909, 0.1914, 0.1915, 0.0905, 0.1914, 0.1914, 0.1913, 0.1910, 0.1914,
        0.1913, 0.1915, 0.1910, 0.1907, 0.1852, 0.1846, 0.0966, 0.1914, 0.0901,
        0.1914, 0.1914, 0.1914, 0.1907, 0.1914, 0.1914, 0.1788, 0.1896, 0.1914,
        0.1914, 0.1914, 0.1281, 0.0834, 0.1914, 0.1909, 0.1504, 0.1913, 0.1785,
        0.1842, 0.1816, 0.1914, 0.1914, 0.1299, 0.1914, 0.1914, 0.1886, 0.1908,
        0.0850, 0.1842, 0.0843, 0.1914, 0.1792, 0.1915, 0.1914, 0.1912, 0.1915,
        0.1777, 0.1914, 0.1109, 0.1914, 0.0837, 0.1876, 0.1910, 0.1914, 0.1761,
        0.1456, 0.1914, 0.1914, 0.1910, 0.1914, 0.1914, 0.1917, 0.1887, 0.1902,
        0.1914, 0.1862, 0.0835, 0.1832, 0.1887, 0.1914, 0.1914, 0.1913, 0.1914,
        0.1914, 0.1910, 0.1863, 0.1913, 0.1914, 0.1764, 0.1914, 0.0832, 0.1914,
        0.1913, 0.1913, 0.0885, 0.1843, 0.1909, 0.1914, 0.1914, 0.1896, 0.1914,
        0.1913, 0.1910, 0.1914, 0.1447, 0.1830, 0.0825, 0.1647, 0.0920, 0.1899,
        0.1913, 0.1912, 0.1914, 0.1885, 0.1912, 0.1768, 0.1915, 0.1910, 0.1914,
        0.1912, 0.1833, 0.1877, 0.1914, 0.1902, 0.1895, 0.1914, 0.1913, 0.0842,
        0.1901, 0.1910, 0.1890, 0.0862, 0.1908, 0.1910, 0.1903, 0.1914, 0.1914,
        0.1879, 0.1907, 0.1914, 0.1914, 0.1914, 0.1898, 0.1914, 0.1914, 0.1884,
        0.1105], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 17:06:41,765][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
loss: tensor(6188.3687, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3672, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 17:07:45,730][train_inner][INFO] - {"epoch": 11, "update": 10.407, "loss": "4.505", "ntokens": "149659", "nsentences": "540.185", "prob_perplexity": "84.476", "code_perplexity": "81.864", "temp": "1.948", "loss_0": "4.367", "loss_1": "0.125", "loss_2": "0.013", "accuracy": "0.26827", "wps": "37652.2", "ups": "0.25", "wpb": "149659", "bsz": "540.2", "num_updates": "5400", "lr": "8.4375e-05", "gnorm": "0.643", "loss_scale": "4", "train_wall": "794", "gb_free": "12.9", "wall": "21738"}
Parameter containing:
tensor([0.1821, 0.1821, 0.1797, 0.1779, 0.1763, 0.1653, 0.1820, 0.1818, 0.1813,
        0.1820, 0.1677, 0.1727, 0.1722, 0.1823, 0.1821, 0.1821, 0.1787, 0.1821,
        0.0880, 0.1823, 0.1823, 0.1394, 0.0874, 0.0864, 0.1702, 0.1819, 0.1821,
        0.1788, 0.1791, 0.1821, 0.0996, 0.1815, 0.1772, 0.1821, 0.1818, 0.1814,
        0.1802, 0.1823, 0.1821, 0.1691, 0.1823, 0.0927, 0.1823, 0.1815, 0.1808,
        0.0982, 0.1631, 0.1823, 0.1821, 0.1813, 0.1823, 0.1536, 0.1730, 0.1810,
        0.1648, 0.1812, 0.1589, 0.1703, 0.1821, 0.1808, 0.1788, 0.1815, 0.1740,
        0.1815, 0.1577, 0.1821, 0.1823, 0.1808, 0.1823, 0.1823, 0.0918, 0.1823,
        0.1796, 0.1823, 0.1814, 0.1671, 0.1807, 0.1107, 0.1823, 0.1727, 0.1796,
        0.1820, 0.1202, 0.1823, 0.1815, 0.1495, 0.1820, 0.1815, 0.1812, 0.1813,
        0.1252, 0.0876, 0.1816, 0.1823, 0.1700, 0.0918, 0.0917, 0.1823, 0.0898,
        0.1821, 0.1821, 0.1823, 0.1819, 0.1810, 0.1004, 0.1823, 0.1823, 0.0875,
        0.1636, 0.1032, 0.1805, 0.1580, 0.1738, 0.1823, 0.1796, 0.1790, 0.1820,
        0.0881, 0.1802, 0.1754, 0.1823, 0.1821, 0.1823, 0.1823, 0.1678, 0.1049,
        0.1823, 0.1823, 0.1818, 0.1823, 0.1821, 0.1823, 0.1775, 0.1807, 0.1132,
        0.1798, 0.1815, 0.1699, 0.1781, 0.1823, 0.1813, 0.1720, 0.1818, 0.1816,
        0.1808, 0.1821, 0.1757, 0.1813, 0.1819, 0.1282, 0.1798, 0.1818, 0.1605,
        0.1725, 0.1823, 0.1708, 0.1821, 0.1823, 0.1805, 0.1823, 0.1820, 0.1823,
        0.1823, 0.1799, 0.1823, 0.1820, 0.1819, 0.1807, 0.1791, 0.1815, 0.1823,
        0.1821, 0.1805, 0.1823, 0.1815, 0.1530, 0.1823, 0.1818, 0.0913, 0.1742,
        0.1820, 0.1815, 0.1820, 0.1732, 0.1821, 0.1671, 0.1823, 0.1819, 0.1823,
        0.1594, 0.1807, 0.1820, 0.1821, 0.1561, 0.1804, 0.1791, 0.1796, 0.1818,
        0.1821, 0.1812, 0.1737, 0.1821, 0.1820, 0.1823, 0.1821, 0.0966, 0.1714,
        0.1709, 0.1821, 0.1821, 0.1805, 0.0923, 0.1821, 0.1823, 0.1823, 0.1821,
        0.1349, 0.1823, 0.1821, 0.1821, 0.1821, 0.1741, 0.1805, 0.1823, 0.1808,
        0.1143, 0.1823, 0.1821, 0.1823, 0.0887, 0.1804, 0.1615, 0.1823, 0.1820,
        0.1823, 0.1814, 0.1803, 0.1332, 0.1813, 0.1547, 0.1823, 0.1777, 0.1755,
        0.1705, 0.1622, 0.1823, 0.1747, 0.1251, 0.1823, 0.1823, 0.1744, 0.1782,
        0.0992, 0.1617, 0.1803, 0.1823, 0.1815, 0.1823, 0.1794, 0.1681, 0.1744,
        0.1819, 0.1818, 0.1428, 0.1810, 0.1768, 0.1821, 0.1823, 0.1356, 0.1815,
        0.0989, 0.1814, 0.1823, 0.1816, 0.1803, 0.1823, 0.1030, 0.1820, 0.0991,
        0.0928, 0.1823, 0.1818, 0.1537, 0.1824, 0.1820, 0.1805, 0.1824, 0.1813,
        0.1823, 0.1819, 0.1823, 0.1447, 0.1803, 0.1820, 0.1814, 0.0870, 0.1794,
        0.0859, 0.1777, 0.1823, 0.1819, 0.1567, 0.0975, 0.1688, 0.1776, 0.1785,
        0.1821, 0.1823, 0.1823, 0.1092, 0.1823, 0.1805, 0.1790, 0.1813, 0.1823,
        0.1530, 0.1823, 0.1057, 0.1349, 0.1821, 0.1804, 0.0919, 0.1823, 0.1823,
        0.1805, 0.1805, 0.1820, 0.1698, 0.1823, 0.1799, 0.1805, 0.1823, 0.1821,
        0.0923, 0.1808, 0.1781, 0.1804, 0.1001, 0.1818, 0.1821, 0.1813, 0.1820,
        0.1823, 0.1823, 0.1821, 0.1567, 0.1746, 0.0880, 0.1678, 0.1823, 0.1771,
        0.0958, 0.1818, 0.1823, 0.1802, 0.1821, 0.1823, 0.1637, 0.1823, 0.1709,
        0.1820, 0.1823, 0.1771, 0.1823, 0.1552, 0.1813, 0.1819, 0.1823, 0.1823,
        0.1798, 0.1823, 0.1823, 0.1820, 0.1823, 0.1808, 0.1779, 0.1823, 0.0980,
        0.1685, 0.1763, 0.1824, 0.0955, 0.1823, 0.1823, 0.1752, 0.1823, 0.1656,
        0.1664, 0.1794, 0.1823, 0.1761, 0.1823, 0.1821, 0.1819, 0.0926, 0.1824,
        0.1801, 0.0961, 0.1821, 0.0942, 0.1823, 0.1569, 0.1696, 0.1820, 0.1821,
        0.1586, 0.1823, 0.1543, 0.1810, 0.1820, 0.1823, 0.1823, 0.1294, 0.1796,
        0.1818, 0.1821, 0.1823, 0.1763, 0.1823, 0.1820, 0.1823, 0.1823, 0.1810,
        0.1821, 0.1821, 0.1814, 0.1823, 0.1821, 0.1791, 0.1821, 0.1819, 0.1816,
        0.1062, 0.1786, 0.1823, 0.1820, 0.1823, 0.1813, 0.1803, 0.1812, 0.1815,
        0.1821, 0.1821, 0.1819, 0.1821, 0.1759, 0.1796, 0.1823, 0.1210, 0.1790,
        0.1823, 0.0844, 0.1815, 0.1823, 0.1798, 0.1823, 0.1823, 0.1820, 0.1595,
        0.1393, 0.1747, 0.1820, 0.1814, 0.1823, 0.1729, 0.1823, 0.1779, 0.1671,
        0.1775, 0.0993, 0.1094, 0.1788, 0.1821, 0.1823, 0.1815, 0.1687, 0.1819,
        0.1807, 0.1823, 0.1818, 0.1823, 0.1704, 0.1819, 0.1823, 0.1821, 0.1052,
        0.1816, 0.1395, 0.1814, 0.1821, 0.1818, 0.1820, 0.1731, 0.1823, 0.1821,
        0.1816, 0.1823, 0.1823, 0.0936, 0.1823, 0.1823, 0.1820, 0.1818, 0.1823,
        0.1821, 0.1823, 0.1818, 0.1815, 0.1758, 0.1753, 0.1011, 0.1823, 0.0953,
        0.1823, 0.1823, 0.1821, 0.1814, 0.1823, 0.1823, 0.1693, 0.1803, 0.1823,
        0.1821, 0.1821, 0.1412, 0.0860, 0.1823, 0.1816, 0.1603, 0.1820, 0.1688,
        0.1749, 0.1725, 0.1823, 0.1821, 0.1440, 0.1821, 0.1823, 0.1794, 0.1815,
        0.0891, 0.1750, 0.0875, 0.1823, 0.1697, 0.1823, 0.1821, 0.1820, 0.1823,
        0.1685, 0.1821, 0.1147, 0.1823, 0.0859, 0.1783, 0.1819, 0.1823, 0.1670,
        0.1539, 0.1823, 0.1823, 0.1818, 0.1823, 0.1823, 0.1824, 0.1796, 0.1809,
        0.1821, 0.1769, 0.0854, 0.1741, 0.1796, 0.1823, 0.1821, 0.1820, 0.1821,
        0.1823, 0.1818, 0.1769, 0.1820, 0.1823, 0.1671, 0.1823, 0.0850, 0.1823,
        0.1821, 0.1820, 0.0933, 0.1750, 0.1818, 0.1823, 0.1823, 0.1804, 0.1823,
        0.1820, 0.1818, 0.1821, 0.1560, 0.1735, 0.0831, 0.1553, 0.0975, 0.1808,
        0.1821, 0.1819, 0.1823, 0.1792, 0.1819, 0.1676, 0.1823, 0.1819, 0.1823,
        0.1819, 0.1738, 0.1786, 0.1823, 0.1810, 0.1803, 0.1823, 0.1820, 0.0879,
        0.1808, 0.1818, 0.1797, 0.0909, 0.1815, 0.1818, 0.1810, 0.1821, 0.1821,
        0.1786, 0.1814, 0.1823, 0.1823, 0.1823, 0.1807, 0.1823, 0.1823, 0.1792,
        0.1190], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7009.2969, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3047, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1901, 0.1901, 0.1876, 0.1858, 0.1842, 0.1542, 0.1899, 0.1897, 0.1892,
        0.1899, 0.1760, 0.1807, 0.1807, 0.1902, 0.1901, 0.1901, 0.1866, 0.1901,
        0.0850, 0.1902, 0.1902, 0.1277, 0.0850, 0.0840, 0.1781, 0.1898, 0.1901,
        0.1868, 0.1873, 0.1901, 0.0931, 0.1895, 0.1852, 0.1901, 0.1897, 0.1895,
        0.1881, 0.1902, 0.1901, 0.1774, 0.1902, 0.0892, 0.1902, 0.1895, 0.1888,
        0.0925, 0.1711, 0.1902, 0.1901, 0.1892, 0.1902, 0.1428, 0.1809, 0.1890,
        0.1732, 0.1891, 0.1516, 0.1783, 0.1901, 0.1887, 0.1868, 0.1895, 0.1820,
        0.1895, 0.1461, 0.1901, 0.1902, 0.1887, 0.1902, 0.1902, 0.0873, 0.1902,
        0.1875, 0.1902, 0.1893, 0.1659, 0.1887, 0.1033, 0.1902, 0.1807, 0.1875,
        0.1899, 0.1110, 0.1902, 0.1895, 0.1373, 0.1899, 0.1895, 0.1891, 0.1892,
        0.1263, 0.0848, 0.1896, 0.1902, 0.1780, 0.0889, 0.0871, 0.1902, 0.0859,
        0.1901, 0.1901, 0.1902, 0.1898, 0.1890, 0.0954, 0.1902, 0.1902, 0.0854,
        0.1720, 0.0981, 0.1885, 0.1469, 0.1818, 0.1902, 0.1876, 0.1869, 0.1899,
        0.0853, 0.1881, 0.1833, 0.1902, 0.1901, 0.1902, 0.1902, 0.1758, 0.0978,
        0.1902, 0.1902, 0.1897, 0.1902, 0.1901, 0.1902, 0.1854, 0.1886, 0.1160,
        0.1877, 0.1895, 0.1782, 0.1860, 0.1902, 0.1892, 0.1799, 0.1897, 0.1896,
        0.1887, 0.1901, 0.1838, 0.1892, 0.1898, 0.1313, 0.1877, 0.1897, 0.1498,
        0.1804, 0.1902, 0.1788, 0.1901, 0.1902, 0.1885, 0.1902, 0.1899, 0.1902,
        0.1902, 0.1881, 0.1902, 0.1899, 0.1898, 0.1886, 0.1870, 0.1895, 0.1902,
        0.1901, 0.1885, 0.1902, 0.1895, 0.1407, 0.1902, 0.1897, 0.0869, 0.1821,
        0.1899, 0.1895, 0.1899, 0.1812, 0.1901, 0.1575, 0.1902, 0.1898, 0.1902,
        0.1475, 0.1886, 0.1899, 0.1901, 0.1445, 0.1884, 0.1870, 0.1875, 0.1897,
        0.1901, 0.1891, 0.1816, 0.1901, 0.1899, 0.1902, 0.1901, 0.0920, 0.1793,
        0.1791, 0.1901, 0.1902, 0.1885, 0.0894, 0.1901, 0.1902, 0.1902, 0.1901,
        0.1232, 0.1902, 0.1901, 0.1901, 0.1901, 0.1821, 0.1885, 0.1902, 0.1887,
        0.1063, 0.1902, 0.1901, 0.1902, 0.0854, 0.1884, 0.1697, 0.1902, 0.1899,
        0.1902, 0.1893, 0.1882, 0.1349, 0.1892, 0.1433, 0.1902, 0.1857, 0.1835,
        0.1788, 0.1667, 0.1902, 0.1826, 0.1205, 0.1902, 0.1902, 0.1826, 0.1863,
        0.0930, 0.1674, 0.1882, 0.1902, 0.1895, 0.1902, 0.1874, 0.1760, 0.1825,
        0.1898, 0.1897, 0.1320, 0.1890, 0.1847, 0.1901, 0.1902, 0.1340, 0.1895,
        0.0922, 0.1893, 0.1902, 0.1896, 0.1882, 0.1902, 0.0945, 0.1899, 0.0933,
        0.0869, 0.1902, 0.1897, 0.1416, 0.1903, 0.1899, 0.1885, 0.1903, 0.1892,
        0.1902, 0.1899, 0.1902, 0.1326, 0.1882, 0.1899, 0.1893, 0.0850, 0.1874,
        0.0841, 0.1857, 0.1902, 0.1898, 0.1649, 0.0923, 0.1769, 0.1858, 0.1864,
        0.1901, 0.1902, 0.1902, 0.1038, 0.1902, 0.1885, 0.1869, 0.1892, 0.1902,
        0.1411, 0.1902, 0.1042, 0.1224, 0.1901, 0.1884, 0.0881, 0.1902, 0.1902,
        0.1885, 0.1885, 0.1899, 0.1777, 0.1902, 0.1879, 0.1885, 0.1902, 0.1901,
        0.0876, 0.1887, 0.1863, 0.1884, 0.0934, 0.1897, 0.1901, 0.1892, 0.1899,
        0.1902, 0.1902, 0.1901, 0.1450, 0.1825, 0.0850, 0.1761, 0.1902, 0.1852,
        0.0920, 0.1897, 0.1902, 0.1881, 0.1901, 0.1902, 0.1642, 0.1902, 0.1794,
        0.1899, 0.1902, 0.1854, 0.1902, 0.1632, 0.1892, 0.1898, 0.1902, 0.1902,
        0.1877, 0.1902, 0.1902, 0.1899, 0.1902, 0.1887, 0.1858, 0.1902, 0.0920,
        0.1589, 0.1842, 0.1903, 0.0906, 0.1902, 0.1902, 0.1833, 0.1902, 0.1740,
        0.1743, 0.1874, 0.1902, 0.1841, 0.1902, 0.1901, 0.1898, 0.0889, 0.1903,
        0.1880, 0.0909, 0.1901, 0.0895, 0.1902, 0.1455, 0.1770, 0.1899, 0.1901,
        0.1473, 0.1902, 0.1442, 0.1890, 0.1899, 0.1902, 0.1902, 0.1187, 0.1875,
        0.1897, 0.1901, 0.1902, 0.1842, 0.1902, 0.1899, 0.1902, 0.1902, 0.1890,
        0.1901, 0.1901, 0.1893, 0.1902, 0.1901, 0.1870, 0.1901, 0.1898, 0.1896,
        0.0997, 0.1865, 0.1902, 0.1899, 0.1902, 0.1892, 0.1882, 0.1891, 0.1895,
        0.1901, 0.1901, 0.1898, 0.1901, 0.1838, 0.1875, 0.1902, 0.1218, 0.1869,
        0.1902, 0.0832, 0.1895, 0.1902, 0.1877, 0.1902, 0.1902, 0.1899, 0.1530,
        0.1268, 0.1826, 0.1899, 0.1893, 0.1902, 0.1808, 0.1902, 0.1858, 0.1752,
        0.1854, 0.0927, 0.1036, 0.1868, 0.1901, 0.1902, 0.1895, 0.1774, 0.1898,
        0.1886, 0.1902, 0.1897, 0.1902, 0.1783, 0.1898, 0.1902, 0.1901, 0.0998,
        0.1896, 0.1293, 0.1893, 0.1901, 0.1897, 0.1899, 0.1815, 0.1902, 0.1901,
        0.1896, 0.1902, 0.1902, 0.0911, 0.1902, 0.1902, 0.1899, 0.1897, 0.1902,
        0.1901, 0.1902, 0.1897, 0.1895, 0.1838, 0.1832, 0.0974, 0.1902, 0.0909,
        0.1902, 0.1902, 0.1901, 0.1893, 0.1902, 0.1902, 0.1775, 0.1882, 0.1902,
        0.1901, 0.1901, 0.1299, 0.0839, 0.1902, 0.1897, 0.1519, 0.1899, 0.1771,
        0.1830, 0.1804, 0.1902, 0.1901, 0.1318, 0.1901, 0.1902, 0.1874, 0.1895,
        0.0857, 0.1830, 0.0850, 0.1902, 0.1777, 0.1902, 0.1901, 0.1899, 0.1902,
        0.1764, 0.1901, 0.1115, 0.1902, 0.0842, 0.1863, 0.1898, 0.1902, 0.1749,
        0.1470, 0.1902, 0.1902, 0.1897, 0.1902, 0.1902, 0.1903, 0.1875, 0.1888,
        0.1901, 0.1848, 0.0840, 0.1820, 0.1875, 0.1902, 0.1901, 0.1899, 0.1901,
        0.1902, 0.1897, 0.1849, 0.1899, 0.1902, 0.1750, 0.1902, 0.0836, 0.1902,
        0.1901, 0.1901, 0.0893, 0.1830, 0.1897, 0.1902, 0.1902, 0.1884, 0.1902,
        0.1899, 0.1897, 0.1901, 0.1464, 0.1816, 0.0828, 0.1633, 0.0928, 0.1887,
        0.1901, 0.1898, 0.1902, 0.1871, 0.1898, 0.1755, 0.1902, 0.1898, 0.1902,
        0.1898, 0.1820, 0.1865, 0.1902, 0.1890, 0.1882, 0.1902, 0.1899, 0.0847,
        0.1887, 0.1897, 0.1876, 0.0870, 0.1895, 0.1897, 0.1890, 0.1901, 0.1901,
        0.1865, 0.1893, 0.1902, 0.1902, 0.1902, 0.1886, 0.1902, 0.1902, 0.1871,
        0.1124], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6430.1035, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3750, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7716.7925, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3906, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1949, 0.1949, 0.1925, 0.1907, 0.1891, 0.1476, 0.1948, 0.1946, 0.1941,
        0.1948, 0.1810, 0.1855, 0.1857, 0.1951, 0.1949, 0.1949, 0.1915, 0.1949,
        0.0833, 0.1951, 0.1951, 0.1203, 0.0829, 0.0823, 0.1830, 0.1947, 0.1949,
        0.1917, 0.1921, 0.1949, 0.0891, 0.1943, 0.1901, 0.1949, 0.1946, 0.1943,
        0.1930, 0.1951, 0.1949, 0.1824, 0.1951, 0.0859, 0.1951, 0.1943, 0.1937,
        0.0884, 0.1760, 0.1951, 0.1949, 0.1941, 0.1951, 0.1355, 0.1858, 0.1938,
        0.1781, 0.1940, 0.1439, 0.1832, 0.1949, 0.1936, 0.1917, 0.1943, 0.1869,
        0.1943, 0.1387, 0.1949, 0.1951, 0.1936, 0.1951, 0.1951, 0.0845, 0.1951,
        0.1924, 0.1951, 0.1942, 0.1709, 0.1936, 0.0992, 0.1951, 0.1855, 0.1924,
        0.1948, 0.1061, 0.1951, 0.1943, 0.1298, 0.1948, 0.1943, 0.1940, 0.1941,
        0.1243, 0.0829, 0.1945, 0.1951, 0.1829, 0.0871, 0.0842, 0.1951, 0.0832,
        0.1949, 0.1949, 0.1951, 0.1947, 0.1938, 0.0908, 0.1951, 0.1951, 0.0834,
        0.1770, 0.0942, 0.1934, 0.1396, 0.1866, 0.1951, 0.1925, 0.1918, 0.1948,
        0.0830, 0.1930, 0.1882, 0.1951, 0.1949, 0.1951, 0.1951, 0.1807, 0.0934,
        0.1951, 0.1951, 0.1946, 0.1951, 0.1949, 0.1951, 0.1903, 0.1935, 0.1145,
        0.1926, 0.1943, 0.1831, 0.1909, 0.1951, 0.1941, 0.1848, 0.1946, 0.1945,
        0.1936, 0.1949, 0.1888, 0.1941, 0.1947, 0.1255, 0.1926, 0.1946, 0.1426,
        0.1853, 0.1951, 0.1837, 0.1949, 0.1951, 0.1934, 0.1951, 0.1948, 0.1951,
        0.1951, 0.1930, 0.1951, 0.1948, 0.1947, 0.1935, 0.1919, 0.1943, 0.1951,
        0.1949, 0.1934, 0.1951, 0.1943, 0.1333, 0.1951, 0.1946, 0.0836, 0.1870,
        0.1948, 0.1943, 0.1948, 0.1860, 0.1949, 0.1511, 0.1951, 0.1947, 0.1951,
        0.1401, 0.1935, 0.1948, 0.1949, 0.1373, 0.1932, 0.1919, 0.1924, 0.1946,
        0.1949, 0.1940, 0.1865, 0.1949, 0.1948, 0.1951, 0.1949, 0.0882, 0.1842,
        0.1840, 0.1949, 0.1951, 0.1934, 0.0862, 0.1949, 0.1951, 0.1951, 0.1949,
        0.1165, 0.1951, 0.1949, 0.1949, 0.1949, 0.1870, 0.1934, 0.1951, 0.1936,
        0.1017, 0.1951, 0.1949, 0.1951, 0.0830, 0.1932, 0.1748, 0.1951, 0.1948,
        0.1951, 0.1942, 0.1931, 0.1288, 0.1941, 0.1360, 0.1951, 0.1906, 0.1884,
        0.1838, 0.1721, 0.1951, 0.1875, 0.1183, 0.1951, 0.1951, 0.1875, 0.1912,
        0.0891, 0.1727, 0.1931, 0.1951, 0.1943, 0.1951, 0.1923, 0.1809, 0.1874,
        0.1947, 0.1946, 0.1241, 0.1938, 0.1896, 0.1949, 0.1951, 0.1274, 0.1943,
        0.0878, 0.1942, 0.1951, 0.1945, 0.1931, 0.1951, 0.0898, 0.1948, 0.0894,
        0.0841, 0.1951, 0.1946, 0.1344, 0.1952, 0.1948, 0.1934, 0.1952, 0.1941,
        0.1951, 0.1948, 0.1951, 0.1251, 0.1931, 0.1948, 0.1942, 0.0831, 0.1923,
        0.0824, 0.1906, 0.1951, 0.1947, 0.1699, 0.0876, 0.1819, 0.1908, 0.1913,
        0.1949, 0.1951, 0.1951, 0.1014, 0.1951, 0.1934, 0.1918, 0.1941, 0.1951,
        0.1338, 0.1951, 0.1005, 0.1166, 0.1949, 0.1932, 0.0847, 0.1951, 0.1951,
        0.1934, 0.1934, 0.1948, 0.1826, 0.1951, 0.1927, 0.1934, 0.1951, 0.1949,
        0.0842, 0.1936, 0.1912, 0.1932, 0.0897, 0.1946, 0.1949, 0.1941, 0.1948,
        0.1951, 0.1951, 0.1949, 0.1379, 0.1874, 0.0829, 0.1810, 0.1951, 0.1902,
        0.0880, 0.1946, 0.1951, 0.1930, 0.1949, 0.1951, 0.1693, 0.1951, 0.1843,
        0.1948, 0.1951, 0.1903, 0.1951, 0.1681, 0.1941, 0.1947, 0.1951, 0.1951,
        0.1926, 0.1951, 0.1951, 0.1948, 0.1951, 0.1936, 0.1907, 0.1951, 0.0883,
        0.1525, 0.1891, 0.1952, 0.0865, 0.1951, 0.1951, 0.1882, 0.1951, 0.1788,
        0.1792, 0.1923, 0.1951, 0.1890, 0.1951, 0.1949, 0.1947, 0.0858, 0.1952,
        0.1929, 0.0870, 0.1949, 0.0857, 0.1951, 0.1384, 0.1821, 0.1948, 0.1949,
        0.1399, 0.1951, 0.1370, 0.1940, 0.1948, 0.1951, 0.1951, 0.1133, 0.1924,
        0.1946, 0.1949, 0.1951, 0.1891, 0.1951, 0.1948, 0.1951, 0.1951, 0.1938,
        0.1949, 0.1949, 0.1942, 0.1951, 0.1949, 0.1919, 0.1949, 0.1947, 0.1945,
        0.0952, 0.1914, 0.1951, 0.1948, 0.1951, 0.1941, 0.1931, 0.1940, 0.1943,
        0.1949, 0.1949, 0.1947, 0.1951, 0.1887, 0.1924, 0.1951, 0.1210, 0.1918,
        0.1951, 0.0819, 0.1943, 0.1951, 0.1926, 0.1951, 0.1951, 0.1948, 0.1473,
        0.1194, 0.1875, 0.1949, 0.1942, 0.1951, 0.1857, 0.1951, 0.1908, 0.1801,
        0.1903, 0.0883, 0.1013, 0.1917, 0.1949, 0.1951, 0.1943, 0.1823, 0.1947,
        0.1935, 0.1951, 0.1946, 0.1951, 0.1832, 0.1947, 0.1951, 0.1949, 0.0967,
        0.1945, 0.1222, 0.1942, 0.1949, 0.1946, 0.1948, 0.1864, 0.1951, 0.1949,
        0.1945, 0.1951, 0.1951, 0.0881, 0.1951, 0.1951, 0.1948, 0.1946, 0.1951,
        0.1949, 0.1951, 0.1946, 0.1943, 0.1887, 0.1881, 0.0942, 0.1951, 0.0872,
        0.1951, 0.1951, 0.1949, 0.1942, 0.1951, 0.1951, 0.1825, 0.1931, 0.1951,
        0.1949, 0.1949, 0.1227, 0.0824, 0.1951, 0.1946, 0.1459, 0.1948, 0.1820,
        0.1879, 0.1853, 0.1951, 0.1949, 0.1243, 0.1951, 0.1951, 0.1923, 0.1943,
        0.0831, 0.1879, 0.0829, 0.1951, 0.1829, 0.1951, 0.1949, 0.1948, 0.1951,
        0.1813, 0.1949, 0.1096, 0.1951, 0.0826, 0.1912, 0.1947, 0.1951, 0.1798,
        0.1407, 0.1951, 0.1951, 0.1946, 0.1951, 0.1951, 0.1952, 0.1924, 0.1937,
        0.1949, 0.1897, 0.0825, 0.1869, 0.1924, 0.1951, 0.1949, 0.1948, 0.1949,
        0.1951, 0.1946, 0.1898, 0.1948, 0.1951, 0.1799, 0.1951, 0.0822, 0.1951,
        0.1949, 0.1949, 0.0858, 0.1879, 0.1946, 0.1951, 0.1951, 0.1932, 0.1951,
        0.1948, 0.1946, 0.1949, 0.1395, 0.1865, 0.0818, 0.1683, 0.0897, 0.1936,
        0.1949, 0.1947, 0.1951, 0.1920, 0.1947, 0.1804, 0.1951, 0.1947, 0.1951,
        0.1947, 0.1870, 0.1914, 0.1951, 0.1938, 0.1931, 0.1951, 0.1949, 0.0828,
        0.1936, 0.1946, 0.1925, 0.0841, 0.1943, 0.1946, 0.1938, 0.1949, 0.1949,
        0.1914, 0.1942, 0.1951, 0.1951, 0.1951, 0.1935, 0.1951, 0.1951, 0.1920,
        0.1055], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(5813.7080, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4570, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6216.2964, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4648, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(8521.7881, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4883, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1997, 0.1997, 0.1973, 0.1956, 0.1940, 0.1410, 0.1996, 0.1993, 0.1990,
        0.1996, 0.1858, 0.1903, 0.1904, 0.1998, 0.1997, 0.1998, 0.1963, 0.1997,
        0.0858, 0.1998, 0.1998, 0.1143, 0.0851, 0.0850, 0.1879, 0.1996, 0.1997,
        0.1965, 0.1969, 0.1998, 0.0888, 0.1991, 0.1948, 0.1997, 0.1993, 0.1991,
        0.1979, 0.1998, 0.1997, 0.1873, 0.1998, 0.0871, 0.1998, 0.1991, 0.1985,
        0.0875, 0.1808, 0.1998, 0.1998, 0.1989, 0.1998, 0.1290, 0.1907, 0.1986,
        0.1830, 0.1987, 0.1362, 0.1880, 0.1998, 0.1985, 0.1964, 0.1992, 0.1917,
        0.1991, 0.1331, 0.1998, 0.1998, 0.1985, 0.1998, 0.1998, 0.0859, 0.1998,
        0.1971, 0.1998, 0.1990, 0.1760, 0.1985, 0.0961, 0.1998, 0.1903, 0.1971,
        0.1996, 0.1047, 0.1998, 0.1991, 0.1259, 0.1997, 0.1991, 0.1989, 0.1989,
        0.1210, 0.0854, 0.1992, 0.1998, 0.1876, 0.0861, 0.0855, 0.1998, 0.0851,
        0.1997, 0.1997, 0.1998, 0.1995, 0.1986, 0.0895, 0.1998, 0.1998, 0.0857,
        0.1818, 0.0929, 0.1981, 0.1324, 0.1915, 0.1998, 0.1974, 0.1965, 0.1996,
        0.0857, 0.1978, 0.1930, 0.1998, 0.1998, 0.1998, 0.1998, 0.1854, 0.0914,
        0.1998, 0.1998, 0.1993, 0.1998, 0.1998, 0.1998, 0.1952, 0.1982, 0.1088,
        0.1974, 0.1991, 0.1880, 0.1957, 0.1998, 0.1989, 0.1897, 0.1995, 0.1992,
        0.1984, 0.1997, 0.1936, 0.1990, 0.1996, 0.1208, 0.1974, 0.1995, 0.1353,
        0.1901, 0.1998, 0.1886, 0.1997, 0.1998, 0.1981, 0.1998, 0.1997, 0.1998,
        0.1998, 0.1978, 0.1998, 0.1997, 0.1996, 0.1982, 0.1967, 0.1991, 0.1998,
        0.1997, 0.1982, 0.1998, 0.1991, 0.1287, 0.1998, 0.1995, 0.0848, 0.1918,
        0.1996, 0.1991, 0.1996, 0.1908, 0.1997, 0.1433, 0.1998, 0.1996, 0.1998,
        0.1344, 0.1982, 0.1997, 0.1997, 0.1302, 0.1980, 0.1967, 0.1971, 0.1993,
        0.1997, 0.1989, 0.1913, 0.1997, 0.1996, 0.1998, 0.1997, 0.0883, 0.1891,
        0.1887, 0.1998, 0.1998, 0.1982, 0.0872, 0.1997, 0.1998, 0.1998, 0.1997,
        0.1128, 0.1998, 0.1998, 0.1998, 0.1997, 0.1918, 0.1982, 0.1998, 0.1984,
        0.0982, 0.1998, 0.1997, 0.1998, 0.0851, 0.1981, 0.1796, 0.1998, 0.1996,
        0.1998, 0.1990, 0.1980, 0.1262, 0.1989, 0.1288, 0.1998, 0.1954, 0.1932,
        0.1886, 0.1770, 0.1998, 0.1923, 0.1130, 0.1998, 0.1998, 0.1924, 0.1959,
        0.0881, 0.1776, 0.1979, 0.1998, 0.1992, 0.1998, 0.1971, 0.1857, 0.1921,
        0.1995, 0.1995, 0.1172, 0.1986, 0.1945, 0.1998, 0.1998, 0.1225, 0.1991,
        0.0871, 0.1990, 0.1998, 0.1993, 0.1979, 0.2000, 0.0884, 0.1997, 0.0891,
        0.0854, 0.1998, 0.1993, 0.1274, 0.2000, 0.1996, 0.1981, 0.2000, 0.1989,
        0.1998, 0.1996, 0.1998, 0.1182, 0.1979, 0.1997, 0.1990, 0.0859, 0.1970,
        0.0854, 0.1954, 0.1998, 0.1996, 0.1747, 0.0871, 0.1868, 0.1956, 0.1962,
        0.1997, 0.2000, 0.1998, 0.1018, 0.1998, 0.1982, 0.1965, 0.1989, 0.1998,
        0.1267, 0.1998, 0.0972, 0.1130, 0.1997, 0.1980, 0.0859, 0.1998, 0.1998,
        0.1981, 0.1981, 0.1997, 0.1875, 0.1998, 0.1976, 0.1981, 0.1998, 0.1997,
        0.0854, 0.1984, 0.1960, 0.1981, 0.0891, 0.1993, 0.1997, 0.1990, 0.1996,
        0.1998, 0.1998, 0.1997, 0.1327, 0.1921, 0.0851, 0.1858, 0.1998, 0.1949,
        0.0877, 0.1995, 0.1998, 0.1979, 0.1997, 0.1998, 0.1746, 0.1998, 0.1892,
        0.1997, 0.1998, 0.1951, 0.1998, 0.1729, 0.1990, 0.1996, 0.1998, 0.1998,
        0.1974, 0.1998, 0.1998, 0.1997, 0.1998, 0.1985, 0.1956, 0.1998, 0.0884,
        0.1449, 0.1938, 0.2001, 0.0867, 0.1998, 0.1998, 0.1931, 0.1998, 0.1837,
        0.1840, 0.1971, 0.1998, 0.1937, 0.1998, 0.1997, 0.1996, 0.0869, 0.2000,
        0.1978, 0.0869, 0.1997, 0.0859, 0.1998, 0.1316, 0.1870, 0.1997, 0.1998,
        0.1354, 0.1998, 0.1307, 0.1987, 0.1997, 0.1998, 0.1998, 0.1093, 0.1973,
        0.1993, 0.1998, 0.1998, 0.1938, 0.1998, 0.1996, 0.1998, 0.1998, 0.1987,
        0.1998, 0.1997, 0.1990, 0.1998, 0.1997, 0.1968, 0.1997, 0.1996, 0.1992,
        0.0930, 0.1962, 0.1998, 0.1996, 0.1998, 0.1989, 0.1979, 0.1987, 0.1991,
        0.1997, 0.1997, 0.1995, 0.1998, 0.1936, 0.1971, 0.1998, 0.1132, 0.1965,
        0.1998, 0.0850, 0.1991, 0.2000, 0.1975, 0.1998, 0.1998, 0.1996, 0.1398,
        0.1126, 0.1924, 0.1997, 0.1991, 0.1998, 0.1906, 0.1998, 0.1956, 0.1848,
        0.1951, 0.0881, 0.0962, 0.1965, 0.1997, 0.1998, 0.1991, 0.1871, 0.1995,
        0.1982, 0.1998, 0.1993, 0.1998, 0.1880, 0.1995, 0.1998, 0.1997, 0.0961,
        0.1993, 0.1179, 0.1990, 0.1998, 0.1995, 0.1996, 0.1913, 0.1998, 0.1998,
        0.1993, 0.1998, 0.1998, 0.0890, 0.1998, 0.1998, 0.1997, 0.1995, 0.1998,
        0.1997, 0.1998, 0.1995, 0.1991, 0.1936, 0.1930, 0.0929, 0.1998, 0.0870,
        0.1998, 0.1998, 0.1998, 0.1990, 0.1998, 0.1998, 0.1873, 0.1979, 0.1998,
        0.1998, 0.1998, 0.1158, 0.0852, 0.1998, 0.1993, 0.1385, 0.1996, 0.1868,
        0.1926, 0.1901, 0.1998, 0.1997, 0.1177, 0.1998, 0.1998, 0.1970, 0.1992,
        0.0854, 0.1926, 0.0853, 0.1998, 0.1876, 0.2000, 0.1997, 0.1996, 0.1998,
        0.1862, 0.1997, 0.1028, 0.1998, 0.0854, 0.1960, 0.1995, 0.1998, 0.1846,
        0.1326, 0.1998, 0.1998, 0.1995, 0.1998, 0.1998, 0.2001, 0.1971, 0.1986,
        0.1997, 0.1946, 0.0855, 0.1917, 0.1971, 0.1998, 0.1998, 0.1997, 0.1998,
        0.1998, 0.1993, 0.1946, 0.1997, 0.1998, 0.1847, 0.1998, 0.0849, 0.1998,
        0.1997, 0.1997, 0.0864, 0.1927, 0.1993, 0.1998, 0.1998, 0.1980, 0.1998,
        0.1997, 0.1995, 0.1997, 0.1316, 0.1914, 0.0849, 0.1731, 0.0894, 0.1984,
        0.1997, 0.1996, 0.1998, 0.1969, 0.1996, 0.1852, 0.1998, 0.1995, 0.1998,
        0.1996, 0.1919, 0.1962, 0.1998, 0.1986, 0.1979, 0.1998, 0.1997, 0.0855,
        0.1985, 0.1995, 0.1974, 0.0856, 0.1992, 0.1995, 0.1986, 0.1998, 0.1997,
        0.1962, 0.1990, 0.1998, 0.1998, 0.1998, 0.1982, 0.1998, 0.1998, 0.1968,
        0.0987], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7239.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5391, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8599.2607, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5586, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 17:21:03,106][train_inner][INFO] - {"epoch": 11, "update": 10.791, "loss": "4.534", "ntokens": "149716", "nsentences": "538.855", "prob_perplexity": "85.301", "code_perplexity": "79.613", "temp": "1.946", "loss_0": "4.397", "loss_1": "0.125", "loss_2": "0.012", "accuracy": "0.26684", "wps": "37552.1", "ups": "0.25", "wpb": "149716", "bsz": "538.9", "num_updates": "5600", "lr": "8.75e-05", "gnorm": "0.605", "loss_scale": "4", "train_wall": "796", "gb_free": "12.6", "wall": "22536"}
Parameter containing:
tensor([0.2056, 0.2057, 0.2032, 0.2014, 0.1998, 0.1321, 0.2056, 0.2052, 0.2048,
        0.2054, 0.1918, 0.1963, 0.1963, 0.2058, 0.2056, 0.2057, 0.2023, 0.2056,
        0.0920, 0.2058, 0.2057, 0.1122, 0.0917, 0.0912, 0.1937, 0.2054, 0.2057,
        0.2024, 0.2029, 0.2057, 0.0941, 0.2051, 0.2008, 0.2057, 0.2053, 0.2051,
        0.2037, 0.2058, 0.2057, 0.1931, 0.2058, 0.0935, 0.2058, 0.2050, 0.2043,
        0.0929, 0.1868, 0.2057, 0.2057, 0.2047, 0.2058, 0.1217, 0.1965, 0.2046,
        0.1888, 0.2046, 0.1288, 0.1938, 0.2057, 0.2043, 0.2024, 0.2051, 0.1976,
        0.2051, 0.1265, 0.2057, 0.2058, 0.2043, 0.2058, 0.2057, 0.0925, 0.2057,
        0.2031, 0.2057, 0.2048, 0.1820, 0.2043, 0.0986, 0.2058, 0.1963, 0.2031,
        0.2056, 0.1017, 0.2057, 0.2051, 0.1256, 0.2056, 0.2051, 0.2047, 0.2048,
        0.1186, 0.0919, 0.2051, 0.2057, 0.1935, 0.0868, 0.0920, 0.2058, 0.0916,
        0.2057, 0.2056, 0.2057, 0.2053, 0.2045, 0.0947, 0.2058, 0.2057, 0.0922,
        0.1877, 0.0975, 0.2041, 0.1259, 0.1974, 0.2057, 0.2032, 0.2025, 0.2054,
        0.0916, 0.2037, 0.1990, 0.2058, 0.2057, 0.2058, 0.2057, 0.1913, 0.0942,
        0.2058, 0.2058, 0.2053, 0.2057, 0.2057, 0.2058, 0.2010, 0.2041, 0.1057,
        0.2032, 0.2050, 0.1938, 0.2017, 0.2058, 0.2048, 0.1956, 0.2053, 0.2051,
        0.2043, 0.2056, 0.1996, 0.2048, 0.2054, 0.1196, 0.2032, 0.2053, 0.1318,
        0.1960, 0.2057, 0.1945, 0.2057, 0.2057, 0.2040, 0.2058, 0.2056, 0.2057,
        0.2057, 0.2037, 0.2058, 0.2056, 0.2054, 0.2041, 0.2026, 0.2050, 0.2057,
        0.2057, 0.2041, 0.2057, 0.2050, 0.1282, 0.2058, 0.2053, 0.0912, 0.1978,
        0.2054, 0.2051, 0.2054, 0.1968, 0.2056, 0.1359, 0.2058, 0.2054, 0.2058,
        0.1332, 0.2041, 0.2056, 0.2056, 0.1245, 0.2040, 0.2025, 0.2031, 0.2053,
        0.2056, 0.2047, 0.1973, 0.2057, 0.2054, 0.2058, 0.2056, 0.0942, 0.1949,
        0.1946, 0.2057, 0.2057, 0.2041, 0.0937, 0.2056, 0.2058, 0.2058, 0.2057,
        0.1105, 0.2058, 0.2057, 0.2057, 0.2057, 0.1976, 0.2041, 0.2058, 0.2042,
        0.0992, 0.2058, 0.2057, 0.2058, 0.0915, 0.2040, 0.1855, 0.2057, 0.2056,
        0.2057, 0.2050, 0.2039, 0.1208, 0.2048, 0.1228, 0.2057, 0.2013, 0.1991,
        0.1946, 0.1830, 0.2057, 0.1981, 0.1059, 0.2058, 0.2058, 0.1982, 0.2019,
        0.0933, 0.1833, 0.2037, 0.2058, 0.2051, 0.2057, 0.2030, 0.1917, 0.1980,
        0.2054, 0.2053, 0.1121, 0.2046, 0.2003, 0.2057, 0.2058, 0.1155, 0.2051,
        0.0927, 0.2048, 0.2058, 0.2052, 0.2039, 0.2058, 0.0927, 0.2056, 0.0945,
        0.0919, 0.2058, 0.2052, 0.1196, 0.2059, 0.2056, 0.2040, 0.2058, 0.2047,
        0.2058, 0.2054, 0.2058, 0.1119, 0.2039, 0.2056, 0.2048, 0.0921, 0.2030,
        0.0912, 0.2013, 0.2057, 0.2054, 0.1807, 0.0928, 0.1926, 0.2014, 0.2020,
        0.2056, 0.2058, 0.2057, 0.1063, 0.2057, 0.2041, 0.2025, 0.2048, 0.2057,
        0.1194, 0.2057, 0.0972, 0.1048, 0.2056, 0.2040, 0.0924, 0.2058, 0.2057,
        0.2041, 0.2040, 0.2056, 0.1934, 0.2058, 0.2035, 0.2040, 0.2058, 0.2056,
        0.0920, 0.2042, 0.2019, 0.2040, 0.0944, 0.2052, 0.2056, 0.2048, 0.2056,
        0.2057, 0.2058, 0.2057, 0.1282, 0.1981, 0.0917, 0.1918, 0.2058, 0.2008,
        0.0938, 0.2053, 0.2058, 0.2037, 0.2056, 0.2058, 0.1805, 0.2058, 0.1951,
        0.2056, 0.2057, 0.2010, 0.2057, 0.1788, 0.2048, 0.2054, 0.2058, 0.2058,
        0.2034, 0.2058, 0.2058, 0.2056, 0.2058, 0.2043, 0.2014, 0.2058, 0.0943,
        0.1394, 0.1998, 0.2059, 0.0928, 0.2058, 0.2058, 0.1990, 0.2057, 0.1896,
        0.1899, 0.2030, 0.2057, 0.1997, 0.2058, 0.2057, 0.2054, 0.0934, 0.2059,
        0.2036, 0.0928, 0.2056, 0.0920, 0.2058, 0.1268, 0.1929, 0.2056, 0.2057,
        0.1295, 0.2058, 0.1237, 0.2046, 0.2056, 0.2057, 0.2057, 0.1013, 0.2031,
        0.2053, 0.2057, 0.2058, 0.1997, 0.2058, 0.2056, 0.2058, 0.2058, 0.2046,
        0.2057, 0.2056, 0.2048, 0.2058, 0.2056, 0.2026, 0.2057, 0.2054, 0.2052,
        0.0966, 0.2021, 0.2058, 0.2056, 0.2058, 0.2048, 0.2037, 0.2046, 0.2050,
        0.2057, 0.2056, 0.2053, 0.2057, 0.1995, 0.2030, 0.2058, 0.1060, 0.2024,
        0.2057, 0.0909, 0.2051, 0.2058, 0.2034, 0.2058, 0.2057, 0.2056, 0.1344,
        0.1064, 0.1982, 0.2056, 0.2050, 0.2058, 0.1964, 0.2057, 0.2014, 0.1907,
        0.2010, 0.0933, 0.0967, 0.2024, 0.2056, 0.2057, 0.2051, 0.1930, 0.2053,
        0.2042, 0.2058, 0.2052, 0.2058, 0.1940, 0.2053, 0.2057, 0.2057, 0.1000,
        0.2052, 0.1121, 0.2050, 0.2057, 0.2053, 0.2056, 0.1971, 0.2058, 0.2057,
        0.2052, 0.2057, 0.2058, 0.0951, 0.2058, 0.2058, 0.2056, 0.2053, 0.2057,
        0.2057, 0.2058, 0.2053, 0.2051, 0.1995, 0.1989, 0.0984, 0.2058, 0.0933,
        0.2057, 0.2058, 0.2057, 0.2050, 0.2057, 0.2057, 0.1931, 0.2039, 0.2057,
        0.2057, 0.2057, 0.1097, 0.0912, 0.2058, 0.2052, 0.1292, 0.2056, 0.1927,
        0.1986, 0.1959, 0.2057, 0.2057, 0.1113, 0.2057, 0.2058, 0.2029, 0.2051,
        0.0920, 0.1985, 0.0917, 0.2058, 0.1936, 0.2058, 0.2057, 0.2054, 0.2058,
        0.1920, 0.2057, 0.0980, 0.2058, 0.0917, 0.2019, 0.2053, 0.2058, 0.1906,
        0.1249, 0.2058, 0.2058, 0.2053, 0.2058, 0.2058, 0.2059, 0.2030, 0.2045,
        0.2057, 0.2004, 0.0917, 0.1975, 0.2030, 0.2058, 0.2057, 0.2056, 0.2057,
        0.2058, 0.2053, 0.2006, 0.2056, 0.2057, 0.1907, 0.2057, 0.0909, 0.2058,
        0.2057, 0.2056, 0.0928, 0.1986, 0.2052, 0.2058, 0.2058, 0.2039, 0.2058,
        0.2056, 0.2053, 0.2057, 0.1232, 0.1973, 0.0906, 0.1790, 0.0955, 0.2043,
        0.2056, 0.2054, 0.2057, 0.2028, 0.2054, 0.1910, 0.2058, 0.2054, 0.2058,
        0.2054, 0.1978, 0.2020, 0.2058, 0.2045, 0.2037, 0.2058, 0.2056, 0.0916,
        0.2043, 0.2053, 0.2032, 0.0920, 0.2051, 0.2053, 0.2046, 0.2057, 0.2057,
        0.2021, 0.2050, 0.2057, 0.2058, 0.2057, 0.2041, 0.2057, 0.2058, 0.2028,
        0.0961], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(5476.2622, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5859, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1779, 0.1779, 0.1755, 0.1737, 0.1721, 0.1675, 0.1777, 0.1775, 0.1771,
        0.1777, 0.1643, 0.1685, 0.1680, 0.1780, 0.1779, 0.1780, 0.1746, 0.1779,
        0.0880, 0.1780, 0.1780, 0.1425, 0.0870, 0.0860, 0.1659, 0.1777, 0.1779,
        0.1747, 0.1752, 0.1780, 0.0995, 0.1774, 0.1731, 0.1779, 0.1776, 0.1771,
        0.1760, 0.1780, 0.1779, 0.1667, 0.1780, 0.0930, 0.1780, 0.1772, 0.1766,
        0.0998, 0.1588, 0.1780, 0.1780, 0.1770, 0.1780, 0.1545, 0.1687, 0.1769,
        0.1615, 0.1769, 0.1598, 0.1660, 0.1780, 0.1766, 0.1746, 0.1774, 0.1697,
        0.1772, 0.1598, 0.1780, 0.1780, 0.1766, 0.1780, 0.1780, 0.0925, 0.1780,
        0.1754, 0.1780, 0.1771, 0.1674, 0.1765, 0.1115, 0.1780, 0.1685, 0.1753,
        0.1777, 0.1213, 0.1780, 0.1774, 0.1547, 0.1779, 0.1774, 0.1770, 0.1771,
        0.1241, 0.0874, 0.1774, 0.1780, 0.1658, 0.0941, 0.0925, 0.1781, 0.0898,
        0.1780, 0.1779, 0.1780, 0.1776, 0.1768, 0.1014, 0.1780, 0.1780, 0.0873,
        0.1603, 0.1052, 0.1764, 0.1605, 0.1697, 0.1780, 0.1754, 0.1747, 0.1777,
        0.0876, 0.1760, 0.1711, 0.1781, 0.1780, 0.1781, 0.1780, 0.1636, 0.1080,
        0.1780, 0.1780, 0.1775, 0.1780, 0.1780, 0.1780, 0.1733, 0.1764, 0.1130,
        0.1755, 0.1772, 0.1656, 0.1738, 0.1780, 0.1770, 0.1678, 0.1776, 0.1774,
        0.1766, 0.1779, 0.1714, 0.1771, 0.1777, 0.1272, 0.1755, 0.1776, 0.1636,
        0.1682, 0.1780, 0.1663, 0.1780, 0.1780, 0.1763, 0.1780, 0.1779, 0.1780,
        0.1780, 0.1759, 0.1780, 0.1779, 0.1777, 0.1764, 0.1749, 0.1772, 0.1780,
        0.1780, 0.1764, 0.1780, 0.1772, 0.1559, 0.1780, 0.1776, 0.0930, 0.1700,
        0.1777, 0.1772, 0.1777, 0.1689, 0.1779, 0.1693, 0.1780, 0.1777, 0.1780,
        0.1641, 0.1764, 0.1779, 0.1779, 0.1594, 0.1763, 0.1748, 0.1753, 0.1775,
        0.1779, 0.1770, 0.1696, 0.1779, 0.1777, 0.1780, 0.1779, 0.0968, 0.1672,
        0.1666, 0.1780, 0.1780, 0.1764, 0.0927, 0.1779, 0.1780, 0.1781, 0.1779,
        0.1383, 0.1781, 0.1780, 0.1780, 0.1779, 0.1699, 0.1763, 0.1780, 0.1765,
        0.1165, 0.1780, 0.1780, 0.1780, 0.0884, 0.1763, 0.1591, 0.1780, 0.1777,
        0.1780, 0.1772, 0.1761, 0.1299, 0.1770, 0.1591, 0.1780, 0.1735, 0.1714,
        0.1674, 0.1616, 0.1780, 0.1704, 0.1284, 0.1780, 0.1780, 0.1710, 0.1738,
        0.1005, 0.1609, 0.1760, 0.1780, 0.1774, 0.1780, 0.1753, 0.1638, 0.1702,
        0.1777, 0.1776, 0.1429, 0.1769, 0.1726, 0.1780, 0.1780, 0.1344, 0.1772,
        0.0998, 0.1771, 0.1780, 0.1775, 0.1761, 0.1781, 0.1044, 0.1779, 0.1024,
        0.0934, 0.1780, 0.1775, 0.1576, 0.1781, 0.1777, 0.1763, 0.1781, 0.1770,
        0.1780, 0.1777, 0.1780, 0.1506, 0.1761, 0.1779, 0.1771, 0.0868, 0.1752,
        0.0856, 0.1736, 0.1780, 0.1777, 0.1525, 0.0991, 0.1654, 0.1733, 0.1743,
        0.1779, 0.1781, 0.1780, 0.1088, 0.1780, 0.1764, 0.1747, 0.1770, 0.1780,
        0.1584, 0.1780, 0.1075, 0.1411, 0.1779, 0.1763, 0.0931, 0.1780, 0.1780,
        0.1764, 0.1763, 0.1779, 0.1656, 0.1780, 0.1758, 0.1763, 0.1780, 0.1779,
        0.0948, 0.1765, 0.1737, 0.1761, 0.1007, 0.1775, 0.1779, 0.1771, 0.1779,
        0.1780, 0.1780, 0.1780, 0.1616, 0.1704, 0.0881, 0.1635, 0.1780, 0.1727,
        0.0959, 0.1776, 0.1780, 0.1760, 0.1779, 0.1780, 0.1638, 0.1780, 0.1674,
        0.1779, 0.1780, 0.1731, 0.1780, 0.1511, 0.1771, 0.1777, 0.1780, 0.1780,
        0.1757, 0.1780, 0.1780, 0.1779, 0.1780, 0.1766, 0.1737, 0.1780, 0.0981,
        0.1696, 0.1720, 0.1782, 0.0958, 0.1780, 0.1780, 0.1710, 0.1780, 0.1615,
        0.1621, 0.1753, 0.1780, 0.1719, 0.1780, 0.1780, 0.1777, 0.0935, 0.1781,
        0.1759, 0.0970, 0.1779, 0.0959, 0.1780, 0.1597, 0.1672, 0.1779, 0.1780,
        0.1632, 0.1780, 0.1536, 0.1769, 0.1779, 0.1780, 0.1780, 0.1359, 0.1753,
        0.1776, 0.1780, 0.1780, 0.1720, 0.1780, 0.1779, 0.1780, 0.1780, 0.1769,
        0.1780, 0.1779, 0.1771, 0.1780, 0.1779, 0.1749, 0.1780, 0.1777, 0.1775,
        0.1084, 0.1743, 0.1780, 0.1777, 0.1780, 0.1770, 0.1760, 0.1769, 0.1772,
        0.1780, 0.1779, 0.1776, 0.1780, 0.1718, 0.1753, 0.1780, 0.1196, 0.1747,
        0.1780, 0.0836, 0.1774, 0.1781, 0.1757, 0.1780, 0.1780, 0.1779, 0.1603,
        0.1449, 0.1705, 0.1779, 0.1772, 0.1780, 0.1687, 0.1780, 0.1737, 0.1630,
        0.1732, 0.1010, 0.1127, 0.1747, 0.1779, 0.1780, 0.1774, 0.1653, 0.1776,
        0.1765, 0.1780, 0.1775, 0.1780, 0.1663, 0.1776, 0.1780, 0.1779, 0.1083,
        0.1775, 0.1403, 0.1771, 0.1780, 0.1776, 0.1779, 0.1693, 0.1780, 0.1780,
        0.1775, 0.1780, 0.1780, 0.0938, 0.1780, 0.1780, 0.1779, 0.1776, 0.1780,
        0.1779, 0.1781, 0.1776, 0.1772, 0.1719, 0.1711, 0.1023, 0.1780, 0.0953,
        0.1780, 0.1780, 0.1780, 0.1772, 0.1780, 0.1780, 0.1649, 0.1760, 0.1780,
        0.1780, 0.1780, 0.1447, 0.0856, 0.1780, 0.1775, 0.1620, 0.1779, 0.1650,
        0.1707, 0.1682, 0.1780, 0.1780, 0.1495, 0.1780, 0.1780, 0.1752, 0.1774,
        0.0890, 0.1708, 0.0876, 0.1780, 0.1670, 0.1781, 0.1780, 0.1777, 0.1781,
        0.1643, 0.1780, 0.1171, 0.1780, 0.0855, 0.1742, 0.1776, 0.1780, 0.1627,
        0.1554, 0.1780, 0.1780, 0.1776, 0.1780, 0.1780, 0.1782, 0.1753, 0.1768,
        0.1780, 0.1727, 0.0850, 0.1698, 0.1753, 0.1780, 0.1780, 0.1779, 0.1780,
        0.1780, 0.1776, 0.1726, 0.1779, 0.1780, 0.1630, 0.1780, 0.0844, 0.1780,
        0.1779, 0.1780, 0.0930, 0.1709, 0.1775, 0.1780, 0.1780, 0.1761, 0.1780,
        0.1779, 0.1776, 0.1780, 0.1581, 0.1693, 0.0823, 0.1511, 0.0967, 0.1765,
        0.1779, 0.1777, 0.1780, 0.1750, 0.1777, 0.1633, 0.1781, 0.1776, 0.1780,
        0.1777, 0.1714, 0.1743, 0.1780, 0.1768, 0.1760, 0.1780, 0.1779, 0.0875,
        0.1766, 0.1776, 0.1755, 0.0909, 0.1774, 0.1776, 0.1769, 0.1780, 0.1780,
        0.1744, 0.1772, 0.1780, 0.1780, 0.1780, 0.1764, 0.1780, 0.1780, 0.1749,
        0.1175], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(5195.7285, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.2812, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7428.1436, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.3828, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5990.4126, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4922, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7715.4512, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4883, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8033.8931, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5430, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.1942, 0.1943, 0.1919, 0.1901, 0.1885, 0.1484, 0.1941, 0.1938, 0.1935,
        0.1941, 0.1803, 0.1849, 0.1849, 0.1945, 0.1942, 0.1943, 0.1909, 0.1942,
        0.0834, 0.1945, 0.1943, 0.1213, 0.0832, 0.0825, 0.1824, 0.1941, 0.1943,
        0.1910, 0.1915, 0.1943, 0.0895, 0.1937, 0.1895, 0.1943, 0.1940, 0.1936,
        0.1924, 0.1945, 0.1943, 0.1818, 0.1945, 0.0863, 0.1945, 0.1936, 0.1930,
        0.0889, 0.1753, 0.1943, 0.1943, 0.1934, 0.1945, 0.1365, 0.1852, 0.1932,
        0.1775, 0.1932, 0.1450, 0.1825, 0.1943, 0.1930, 0.1910, 0.1937, 0.1863,
        0.1937, 0.1396, 0.1943, 0.1945, 0.1930, 0.1945, 0.1943, 0.0848, 0.1943,
        0.1918, 0.1943, 0.1935, 0.1702, 0.1930, 0.0998, 0.1945, 0.1849, 0.1918,
        0.1941, 0.1066, 0.1943, 0.1937, 0.1309, 0.1942, 0.1937, 0.1934, 0.1935,
        0.1250, 0.0831, 0.1937, 0.1943, 0.1821, 0.0873, 0.0845, 0.1945, 0.0834,
        0.1943, 0.1942, 0.1943, 0.1940, 0.1931, 0.0914, 0.1945, 0.1943, 0.0836,
        0.1764, 0.0947, 0.1927, 0.1406, 0.1860, 0.1943, 0.1919, 0.1912, 0.1941,
        0.0833, 0.1924, 0.1876, 0.1945, 0.1943, 0.1945, 0.1943, 0.1799, 0.0939,
        0.1943, 0.1945, 0.1940, 0.1943, 0.1943, 0.1945, 0.1897, 0.1927, 0.1152,
        0.1919, 0.1936, 0.1825, 0.1903, 0.1945, 0.1935, 0.1842, 0.1940, 0.1937,
        0.1930, 0.1942, 0.1881, 0.1935, 0.1941, 0.1265, 0.1919, 0.1940, 0.1436,
        0.1847, 0.1943, 0.1831, 0.1943, 0.1943, 0.1926, 0.1945, 0.1942, 0.1943,
        0.1943, 0.1924, 0.1945, 0.1942, 0.1941, 0.1927, 0.1913, 0.1936, 0.1943,
        0.1943, 0.1927, 0.1943, 0.1936, 0.1344, 0.1943, 0.1940, 0.0839, 0.1864,
        0.1941, 0.1937, 0.1941, 0.1854, 0.1942, 0.1520, 0.1945, 0.1941, 0.1945,
        0.1411, 0.1927, 0.1942, 0.1942, 0.1384, 0.1926, 0.1912, 0.1918, 0.1940,
        0.1942, 0.1934, 0.1859, 0.1943, 0.1941, 0.1945, 0.1942, 0.0886, 0.1836,
        0.1832, 0.1943, 0.1943, 0.1927, 0.0866, 0.1942, 0.1945, 0.1945, 0.1943,
        0.1172, 0.1945, 0.1943, 0.1943, 0.1943, 0.1863, 0.1927, 0.1945, 0.1929,
        0.1024, 0.1945, 0.1943, 0.1945, 0.0833, 0.1926, 0.1741, 0.1943, 0.1942,
        0.1943, 0.1936, 0.1925, 0.1296, 0.1935, 0.1370, 0.1943, 0.1899, 0.1877,
        0.1831, 0.1715, 0.1943, 0.1868, 0.1188, 0.1945, 0.1945, 0.1869, 0.1906,
        0.0897, 0.1721, 0.1924, 0.1945, 0.1937, 0.1943, 0.1917, 0.1803, 0.1866,
        0.1941, 0.1940, 0.1251, 0.1932, 0.1890, 0.1943, 0.1945, 0.1282, 0.1937,
        0.0884, 0.1935, 0.1945, 0.1938, 0.1925, 0.1945, 0.0903, 0.1942, 0.0899,
        0.0844, 0.1945, 0.1938, 0.1354, 0.1946, 0.1941, 0.1926, 0.1945, 0.1934,
        0.1945, 0.1941, 0.1945, 0.1261, 0.1925, 0.1942, 0.1935, 0.0833, 0.1917,
        0.0825, 0.1899, 0.1943, 0.1941, 0.1693, 0.0881, 0.1813, 0.1901, 0.1907,
        0.1942, 0.1945, 0.1943, 0.1017, 0.1943, 0.1927, 0.1910, 0.1935, 0.1943,
        0.1349, 0.1943, 0.1010, 0.1171, 0.1942, 0.1926, 0.0850, 0.1945, 0.1943,
        0.1927, 0.1926, 0.1942, 0.1820, 0.1945, 0.1921, 0.1926, 0.1945, 0.1942,
        0.0846, 0.1929, 0.1906, 0.1926, 0.0901, 0.1938, 0.1942, 0.1935, 0.1942,
        0.1943, 0.1945, 0.1943, 0.1390, 0.1868, 0.0831, 0.1804, 0.1945, 0.1895,
        0.0885, 0.1940, 0.1945, 0.1924, 0.1942, 0.1945, 0.1686, 0.1945, 0.1837,
        0.1942, 0.1943, 0.1897, 0.1943, 0.1675, 0.1935, 0.1941, 0.1945, 0.1945,
        0.1920, 0.1945, 0.1945, 0.1942, 0.1945, 0.1930, 0.1901, 0.1945, 0.0887,
        0.1533, 0.1885, 0.1946, 0.0870, 0.1945, 0.1945, 0.1876, 0.1943, 0.1782,
        0.1786, 0.1917, 0.1943, 0.1884, 0.1945, 0.1943, 0.1941, 0.0861, 0.1946,
        0.1923, 0.0875, 0.1942, 0.0862, 0.1945, 0.1394, 0.1815, 0.1942, 0.1943,
        0.1409, 0.1945, 0.1378, 0.1932, 0.1942, 0.1943, 0.1943, 0.1140, 0.1918,
        0.1940, 0.1943, 0.1945, 0.1884, 0.1945, 0.1942, 0.1945, 0.1945, 0.1932,
        0.1943, 0.1942, 0.1935, 0.1945, 0.1942, 0.1913, 0.1943, 0.1941, 0.1938,
        0.0958, 0.1908, 0.1945, 0.1942, 0.1945, 0.1935, 0.1924, 0.1932, 0.1936,
        0.1943, 0.1942, 0.1940, 0.1943, 0.1881, 0.1917, 0.1945, 0.1212, 0.1910,
        0.1943, 0.0820, 0.1937, 0.1945, 0.1920, 0.1945, 0.1943, 0.1942, 0.1483,
        0.1204, 0.1869, 0.1942, 0.1936, 0.1945, 0.1851, 0.1943, 0.1901, 0.1793,
        0.1897, 0.0887, 0.1017, 0.1910, 0.1942, 0.1943, 0.1937, 0.1816, 0.1940,
        0.1929, 0.1945, 0.1938, 0.1945, 0.1826, 0.1940, 0.1943, 0.1943, 0.0970,
        0.1938, 0.1232, 0.1936, 0.1943, 0.1940, 0.1942, 0.1858, 0.1945, 0.1943,
        0.1938, 0.1943, 0.1945, 0.0885, 0.1945, 0.1945, 0.1942, 0.1940, 0.1943,
        0.1943, 0.1945, 0.1940, 0.1937, 0.1881, 0.1875, 0.0947, 0.1945, 0.0878,
        0.1943, 0.1945, 0.1943, 0.1936, 0.1943, 0.1943, 0.1818, 0.1925, 0.1943,
        0.1943, 0.1943, 0.1237, 0.0825, 0.1945, 0.1938, 0.1467, 0.1942, 0.1814,
        0.1871, 0.1846, 0.1943, 0.1943, 0.1254, 0.1943, 0.1945, 0.1915, 0.1937,
        0.0833, 0.1871, 0.0831, 0.1945, 0.1821, 0.1945, 0.1943, 0.1941, 0.1945,
        0.1807, 0.1943, 0.1097, 0.1945, 0.0828, 0.1906, 0.1940, 0.1945, 0.1792,
        0.1417, 0.1945, 0.1945, 0.1940, 0.1945, 0.1945, 0.1946, 0.1917, 0.1931,
        0.1943, 0.1891, 0.0826, 0.1862, 0.1917, 0.1945, 0.1943, 0.1942, 0.1943,
        0.1945, 0.1940, 0.1892, 0.1942, 0.1943, 0.1793, 0.1943, 0.0823, 0.1945,
        0.1943, 0.1942, 0.0862, 0.1873, 0.1938, 0.1945, 0.1945, 0.1925, 0.1945,
        0.1942, 0.1940, 0.1943, 0.1406, 0.1859, 0.0819, 0.1676, 0.0901, 0.1930,
        0.1942, 0.1941, 0.1943, 0.1914, 0.1941, 0.1797, 0.1945, 0.1941, 0.1943,
        0.1941, 0.1864, 0.1907, 0.1945, 0.1931, 0.1924, 0.1945, 0.1942, 0.0829,
        0.1930, 0.1940, 0.1919, 0.0844, 0.1937, 0.1940, 0.1932, 0.1943, 0.1943,
        0.1908, 0.1936, 0.1943, 0.1945, 0.1943, 0.1927, 0.1943, 0.1945, 0.1913,
        0.1064], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7225.7720, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4219, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7888.0723, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4453, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7908.9897, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.4414, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6849.8105, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5156, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6373.7910, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5508, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6376.5161, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5742, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2073, 0.2073, 0.2048, 0.2030, 0.2014, 0.1293, 0.2072, 0.2069, 0.2064,
        0.2072, 0.1934, 0.1979, 0.1980, 0.2074, 0.2073, 0.2073, 0.2039, 0.2073,
        0.0933, 0.2074, 0.2074, 0.1121, 0.0931, 0.0925, 0.1953, 0.2070, 0.2073,
        0.2040, 0.2045, 0.2073, 0.0955, 0.2067, 0.2024, 0.2073, 0.2069, 0.2067,
        0.2053, 0.2074, 0.2073, 0.1947, 0.2074, 0.0949, 0.2074, 0.2067, 0.2061,
        0.0944, 0.1884, 0.2074, 0.2073, 0.2064, 0.2074, 0.1192, 0.1981, 0.2062,
        0.1906, 0.2063, 0.1267, 0.1956, 0.2073, 0.2059, 0.2040, 0.2067, 0.1992,
        0.2067, 0.1236, 0.2073, 0.2074, 0.2059, 0.2074, 0.2074, 0.0940, 0.2074,
        0.2047, 0.2074, 0.2065, 0.1837, 0.2059, 0.0995, 0.2074, 0.1979, 0.2047,
        0.2072, 0.0992, 0.2074, 0.2067, 0.1248, 0.2072, 0.2067, 0.2063, 0.2064,
        0.1175, 0.0931, 0.2068, 0.2074, 0.1952, 0.0869, 0.0933, 0.2074, 0.0930,
        0.2073, 0.2073, 0.2074, 0.2070, 0.2062, 0.0961, 0.2074, 0.2074, 0.0934,
        0.1893, 0.0988, 0.2057, 0.1247, 0.1990, 0.2074, 0.2050, 0.2041, 0.2072,
        0.0929, 0.2053, 0.2006, 0.2074, 0.2073, 0.2074, 0.2074, 0.1930, 0.0952,
        0.2074, 0.2074, 0.2069, 0.2074, 0.2073, 0.2074, 0.2026, 0.2058, 0.1053,
        0.2050, 0.2067, 0.1954, 0.2032, 0.2074, 0.2064, 0.1971, 0.2069, 0.2068,
        0.2059, 0.2073, 0.2012, 0.2064, 0.2070, 0.1185, 0.2050, 0.2069, 0.1307,
        0.1976, 0.2074, 0.1960, 0.2073, 0.2074, 0.2057, 0.2074, 0.2072, 0.2074,
        0.2074, 0.2053, 0.2074, 0.2072, 0.2070, 0.2058, 0.2042, 0.2067, 0.2074,
        0.2073, 0.2057, 0.2074, 0.2067, 0.1277, 0.2074, 0.2069, 0.0927, 0.1993,
        0.2072, 0.2067, 0.2072, 0.1984, 0.2073, 0.1339, 0.2074, 0.2070, 0.2074,
        0.1329, 0.2058, 0.2072, 0.2073, 0.1235, 0.2056, 0.2042, 0.2047, 0.2069,
        0.2073, 0.2063, 0.1989, 0.2073, 0.2072, 0.2074, 0.2073, 0.0958, 0.1965,
        0.1963, 0.2073, 0.2074, 0.2057, 0.0952, 0.2073, 0.2074, 0.2074, 0.2073,
        0.1080, 0.2074, 0.2073, 0.2073, 0.2073, 0.1993, 0.2057, 0.2074, 0.2059,
        0.0996, 0.2074, 0.2073, 0.2074, 0.0927, 0.2056, 0.1871, 0.2074, 0.2072,
        0.2074, 0.2065, 0.2054, 0.1177, 0.2064, 0.1214, 0.2074, 0.2029, 0.2007,
        0.1962, 0.1846, 0.2074, 0.1998, 0.1040, 0.2074, 0.2074, 0.1998, 0.2035,
        0.0947, 0.1851, 0.2054, 0.2074, 0.2067, 0.2074, 0.2046, 0.1932, 0.1997,
        0.2070, 0.2069, 0.1113, 0.2062, 0.2019, 0.2073, 0.2074, 0.1133, 0.2067,
        0.0942, 0.2065, 0.2074, 0.2068, 0.2054, 0.2074, 0.0939, 0.2072, 0.0959,
        0.0934, 0.2074, 0.2069, 0.1171, 0.2075, 0.2072, 0.2057, 0.2075, 0.2064,
        0.2074, 0.2072, 0.2074, 0.1099, 0.2054, 0.2072, 0.2065, 0.0933, 0.2046,
        0.0925, 0.2029, 0.2074, 0.2070, 0.1823, 0.0943, 0.1943, 0.2031, 0.2036,
        0.2073, 0.2074, 0.2074, 0.1077, 0.2074, 0.2057, 0.2041, 0.2064, 0.2074,
        0.1171, 0.2074, 0.0981, 0.1030, 0.2073, 0.2056, 0.0939, 0.2074, 0.2074,
        0.2057, 0.2057, 0.2072, 0.1949, 0.2074, 0.2051, 0.2057, 0.2074, 0.2073,
        0.0934, 0.2059, 0.2035, 0.2056, 0.0956, 0.2069, 0.2073, 0.2064, 0.2072,
        0.2074, 0.2074, 0.2073, 0.1260, 0.1997, 0.0931, 0.1934, 0.2074, 0.2025,
        0.0952, 0.2069, 0.2074, 0.2053, 0.2073, 0.2074, 0.1823, 0.2074, 0.1968,
        0.2072, 0.2074, 0.2026, 0.2074, 0.1804, 0.2064, 0.2070, 0.2074, 0.2074,
        0.2050, 0.2074, 0.2074, 0.2072, 0.2074, 0.2059, 0.2030, 0.2074, 0.0958,
        0.1377, 0.2014, 0.2075, 0.0943, 0.2074, 0.2074, 0.2006, 0.2074, 0.1913,
        0.1915, 0.2046, 0.2074, 0.2013, 0.2074, 0.2073, 0.2070, 0.0948, 0.2075,
        0.2052, 0.0942, 0.2073, 0.0936, 0.2074, 0.1254, 0.1946, 0.2072, 0.2073,
        0.1282, 0.2074, 0.1221, 0.2063, 0.2072, 0.2074, 0.2074, 0.0999, 0.2047,
        0.2069, 0.2073, 0.2074, 0.2014, 0.2074, 0.2072, 0.2074, 0.2074, 0.2062,
        0.2073, 0.2073, 0.2065, 0.2074, 0.2073, 0.2042, 0.2073, 0.2070, 0.2068,
        0.0975, 0.2037, 0.2074, 0.2072, 0.2074, 0.2064, 0.2054, 0.2063, 0.2067,
        0.2073, 0.2073, 0.2070, 0.2074, 0.2010, 0.2047, 0.2074, 0.1043, 0.2041,
        0.2074, 0.0922, 0.2067, 0.2074, 0.2050, 0.2074, 0.2074, 0.2072, 0.1320,
        0.1045, 0.1998, 0.2072, 0.2065, 0.2074, 0.1980, 0.2074, 0.2031, 0.1924,
        0.2026, 0.0947, 0.0972, 0.2040, 0.2073, 0.2074, 0.2067, 0.1947, 0.2070,
        0.2058, 0.2074, 0.2069, 0.2074, 0.1956, 0.2070, 0.2074, 0.2073, 0.1011,
        0.2068, 0.1109, 0.2065, 0.2073, 0.2069, 0.2072, 0.1987, 0.2074, 0.2073,
        0.2068, 0.2074, 0.2074, 0.0964, 0.2074, 0.2074, 0.2072, 0.2069, 0.2074,
        0.2073, 0.2074, 0.2069, 0.2067, 0.2010, 0.2004, 0.0995, 0.2074, 0.0947,
        0.2074, 0.2074, 0.2073, 0.2065, 0.2074, 0.2074, 0.1948, 0.2054, 0.2074,
        0.2073, 0.2073, 0.1077, 0.0925, 0.2074, 0.2069, 0.1262, 0.2072, 0.1943,
        0.2002, 0.1976, 0.2074, 0.2073, 0.1095, 0.2074, 0.2074, 0.2046, 0.2067,
        0.0932, 0.2002, 0.0930, 0.2074, 0.1952, 0.2074, 0.2073, 0.2072, 0.2074,
        0.1936, 0.2073, 0.0977, 0.2074, 0.0930, 0.2035, 0.2070, 0.2074, 0.1921,
        0.1230, 0.2074, 0.2074, 0.2069, 0.2074, 0.2074, 0.2075, 0.2047, 0.2061,
        0.2073, 0.2020, 0.0930, 0.1992, 0.2047, 0.2074, 0.2073, 0.2072, 0.2073,
        0.2074, 0.2069, 0.2021, 0.2072, 0.2074, 0.1923, 0.2074, 0.0922, 0.2074,
        0.2073, 0.2073, 0.0942, 0.2002, 0.2069, 0.2074, 0.2074, 0.2056, 0.2074,
        0.2072, 0.2069, 0.2073, 0.1207, 0.1989, 0.0919, 0.1807, 0.0970, 0.2059,
        0.2073, 0.2070, 0.2074, 0.2043, 0.2070, 0.1927, 0.2074, 0.2070, 0.2074,
        0.2070, 0.1993, 0.2037, 0.2074, 0.2062, 0.2054, 0.2074, 0.2073, 0.0928,
        0.2059, 0.2069, 0.2048, 0.0934, 0.2067, 0.2069, 0.2062, 0.2073, 0.2073,
        0.2037, 0.2065, 0.2074, 0.2074, 0.2074, 0.2058, 0.2074, 0.2074, 0.2043,
        0.0959], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(4641.0488, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5977, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7883.3853, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5820, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2073, 0.2073, 0.2048, 0.2030, 0.2014, 0.1293, 0.2072, 0.2069, 0.2064,
        0.2072, 0.1934, 0.1979, 0.1980, 0.2074, 0.2073, 0.2073, 0.2039, 0.2073,
        0.0933, 0.2074, 0.2074, 0.1121, 0.0931, 0.0925, 0.1953, 0.2070, 0.2073,
        0.2040, 0.2045, 0.2073, 0.0955, 0.2067, 0.2024, 0.2073, 0.2069, 0.2067,
        0.2053, 0.2074, 0.2073, 0.1947, 0.2074, 0.0949, 0.2074, 0.2067, 0.2061,
        0.0944, 0.1884, 0.2074, 0.2073, 0.2064, 0.2074, 0.1192, 0.1981, 0.2062,
        0.1906, 0.2063, 0.1267, 0.1956, 0.2073, 0.2059, 0.2040, 0.2067, 0.1992,
        0.2067, 0.1236, 0.2073, 0.2074, 0.2059, 0.2074, 0.2074, 0.0940, 0.2074,
        0.2047, 0.2074, 0.2065, 0.1837, 0.2059, 0.0995, 0.2074, 0.1979, 0.2047,
        0.2072, 0.0992, 0.2074, 0.2067, 0.1248, 0.2072, 0.2067, 0.2063, 0.2064,
        0.1175, 0.0931, 0.2068, 0.2074, 0.1952, 0.0869, 0.0933, 0.2074, 0.0930,
        0.2073, 0.2073, 0.2074, 0.2070, 0.2062, 0.0961, 0.2074, 0.2074, 0.0934,
        0.1893, 0.0988, 0.2057, 0.1247, 0.1990, 0.2074, 0.2050, 0.2041, 0.2072,
        0.0929, 0.2053, 0.2006, 0.2074, 0.2073, 0.2074, 0.2074, 0.1930, 0.0952,
        0.2074, 0.2074, 0.2069, 0.2074, 0.2073, 0.2074, 0.2026, 0.2058, 0.1053,
        0.2050, 0.2067, 0.1954, 0.2032, 0.2074, 0.2064, 0.1971, 0.2069, 0.2068,
        0.2059, 0.2073, 0.2012, 0.2064, 0.2070, 0.1185, 0.2050, 0.2069, 0.1307,
        0.1976, 0.2074, 0.1960, 0.2073, 0.2074, 0.2057, 0.2074, 0.2072, 0.2074,
        0.2074, 0.2053, 0.2074, 0.2072, 0.2070, 0.2058, 0.2042, 0.2067, 0.2074,
        0.2073, 0.2057, 0.2074, 0.2067, 0.1277, 0.2074, 0.2069, 0.0927, 0.1993,
        0.2072, 0.2067, 0.2072, 0.1984, 0.2073, 0.1339, 0.2074, 0.2070, 0.2074,
        0.1329, 0.2058, 0.2072, 0.2073, 0.1235, 0.2056, 0.2042, 0.2047, 0.2069,
        0.2073, 0.2063, 0.1989, 0.2073, 0.2072, 0.2074, 0.2073, 0.0958, 0.1965,
        0.1963, 0.2073, 0.2074, 0.2057, 0.0952, 0.2073, 0.2074, 0.2074, 0.2073,
        0.1080, 0.2074, 0.2073, 0.2073, 0.2073, 0.1993, 0.2057, 0.2074, 0.2059,
        0.0996, 0.2074, 0.2073, 0.2074, 0.0927, 0.2056, 0.1871, 0.2074, 0.2072,
        0.2074, 0.2065, 0.2054, 0.1177, 0.2064, 0.1214, 0.2074, 0.2029, 0.2007,
        0.1962, 0.1846, 0.2074, 0.1998, 0.1040, 0.2074, 0.2074, 0.1998, 0.2035,
        0.0947, 0.1851, 0.2054, 0.2074, 0.2067, 0.2074, 0.2046, 0.1932, 0.1997,
        0.2070, 0.2069, 0.1113, 0.2062, 0.2019, 0.2073, 0.2074, 0.1133, 0.2067,
        0.0942, 0.2065, 0.2074, 0.2068, 0.2054, 0.2074, 0.0939, 0.2072, 0.0959,
        0.0934, 0.2074, 0.2069, 0.1171, 0.2075, 0.2072, 0.2057, 0.2075, 0.2064,
        0.2074, 0.2072, 0.2074, 0.1099, 0.2054, 0.2072, 0.2065, 0.0933, 0.2046,
        0.0925, 0.2029, 0.2074, 0.2070, 0.1823, 0.0943, 0.1943, 0.2031, 0.2036,
        0.2073, 0.2074, 0.2074, 0.1077, 0.2074, 0.2057, 0.2041, 0.2064, 0.2074,
        0.1171, 0.2074, 0.0981, 0.1030, 0.2073, 0.2056, 0.0939, 0.2074, 0.2074,
        0.2057, 0.2057, 0.2072, 0.1949, 0.2074, 0.2051, 0.2057, 0.2074, 0.2073,
        0.0934, 0.2059, 0.2035, 0.2056, 0.0956, 0.2069, 0.2073, 0.2064, 0.2072,
        0.2074, 0.2074, 0.2073, 0.1260, 0.1997, 0.0931, 0.1934, 0.2074, 0.2025,
        0.0952, 0.2069, 0.2074, 0.2053, 0.2073, 0.2074, 0.1823, 0.2074, 0.1968,
        0.2072, 0.2074, 0.2026, 0.2074, 0.1804, 0.2064, 0.2070, 0.2074, 0.2074,
        0.2050, 0.2074, 0.2074, 0.2072, 0.2074, 0.2059, 0.2030, 0.2074, 0.0958,
        0.1377, 0.2014, 0.2075, 0.0943, 0.2074, 0.2074, 0.2006, 0.2074, 0.1913,
        0.1915, 0.2046, 0.2074, 0.2013, 0.2074, 0.2073, 0.2070, 0.0948, 0.2075,
        0.2052, 0.0942, 0.2073, 0.0936, 0.2074, 0.1254, 0.1946, 0.2072, 0.2073,
        0.1282, 0.2074, 0.1221, 0.2063, 0.2072, 0.2074, 0.2074, 0.0999, 0.2047,
        0.2069, 0.2073, 0.2074, 0.2014, 0.2074, 0.2072, 0.2074, 0.2074, 0.2062,
        0.2073, 0.2073, 0.2065, 0.2074, 0.2073, 0.2042, 0.2073, 0.2070, 0.2068,
        0.0975, 0.2037, 0.2074, 0.2072, 0.2074, 0.2064, 0.2054, 0.2063, 0.2067,
        0.2073, 0.2073, 0.2070, 0.2074, 0.2010, 0.2047, 0.2074, 0.1043, 0.2041,
        0.2074, 0.0922, 0.2067, 0.2074, 0.2050, 0.2074, 0.2074, 0.2072, 0.1320,
        0.1045, 0.1998, 0.2072, 0.2065, 0.2074, 0.1980, 0.2074, 0.2031, 0.1924,
        0.2026, 0.0947, 0.0972, 0.2040, 0.2073, 0.2074, 0.2067, 0.1947, 0.2070,
        0.2058, 0.2074, 0.2069, 0.2074, 0.1956, 0.2070, 0.2074, 0.2073, 0.1011,
        0.2068, 0.1109, 0.2065, 0.2073, 0.2069, 0.2072, 0.1987, 0.2074, 0.2073,
        0.2068, 0.2074, 0.2074, 0.0964, 0.2074, 0.2074, 0.2072, 0.2069, 0.2074,
        0.2073, 0.2074, 0.2069, 0.2067, 0.2010, 0.2004, 0.0995, 0.2074, 0.0947,
        0.2074, 0.2074, 0.2073, 0.2065, 0.2074, 0.2074, 0.1948, 0.2054, 0.2074,
        0.2073, 0.2073, 0.1077, 0.0925, 0.2074, 0.2069, 0.1262, 0.2072, 0.1943,
        0.2002, 0.1976, 0.2074, 0.2073, 0.1095, 0.2074, 0.2074, 0.2046, 0.2067,
        0.0932, 0.2002, 0.0930, 0.2074, 0.1952, 0.2074, 0.2073, 0.2072, 0.2074,
        0.1936, 0.2073, 0.0977, 0.2074, 0.0930, 0.2035, 0.2070, 0.2074, 0.1921,
        0.1230, 0.2074, 0.2074, 0.2069, 0.2074, 0.2074, 0.2075, 0.2047, 0.2061,
        0.2073, 0.2020, 0.0930, 0.1992, 0.2047, 0.2074, 0.2073, 0.2072, 0.2073,
        0.2074, 0.2069, 0.2021, 0.2072, 0.2074, 0.1923, 0.2074, 0.0922, 0.2074,
        0.2073, 0.2073, 0.0942, 0.2002, 0.2069, 0.2074, 0.2074, 0.2056, 0.2074,
        0.2072, 0.2069, 0.2073, 0.1207, 0.1989, 0.0919, 0.1807, 0.0970, 0.2059,
        0.2073, 0.2070, 0.2074, 0.2043, 0.2070, 0.1927, 0.2074, 0.2070, 0.2074,
        0.2070, 0.1993, 0.2037, 0.2074, 0.2062, 0.2054, 0.2074, 0.2073, 0.0928,
        0.2059, 0.2069, 0.2048, 0.0934, 0.2067, 0.2069, 0.2062, 0.2073, 0.2073,
        0.2037, 0.2065, 0.2074, 0.2074, 0.2074, 0.2058, 0.2074, 0.2074, 0.2043,
        0.0959], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 17:24:05,113][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
loss: tensor(5330.1875, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.5977, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8750.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6133, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6272.5498, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6094, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7060.3662, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6172, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 17:28:10,991][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 17:28:10,992][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 17:28:11,068][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2023-09-12 17:28:34,676][valid][INFO] - {"epoch": 11, "valid_loss": "4.199", "valid_ntokens": "7899.4", "valid_nsentences": "55.2525", "valid_prob_perplexity": "70.429", "valid_code_perplexity": "64.883", "valid_temp": "1.944", "valid_loss_0": "4.059", "valid_loss_1": "0.128", "valid_loss_2": "0.012", "valid_accuracy": "0.33422", "valid_wps": "33405.8", "valid_wpb": "7899.4", "valid_bsz": "55.3", "valid_num_updates": "5708", "valid_best_loss": "4.161"}
[2023-09-12 17:28:34,678][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 5708 updates
[2023-09-12 17:28:34,679][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_last.pt
[2023-09-12 17:28:37,127][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_last.pt
[2023-09-12 17:28:37,182][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 11 @ 5708 updates, score 4.199) (writing took 2.504455009009689 seconds)
[2023-09-12 17:28:37,183][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2023-09-12 17:28:37,183][train][INFO] - {"epoch": 11, "train_loss": "4.513", "train_ntokens": "149400", "train_nsentences": "538.382", "train_prob_perplexity": "84.892", "train_code_perplexity": "80.431", "train_temp": "1.946", "train_loss_0": "4.376", "train_loss_1": "0.125", "train_loss_2": "0.012", "train_accuracy": "0.26854", "train_wps": "37117.8", "train_ups": "0.25", "train_wpb": "149400", "train_bsz": "538.4", "train_num_updates": "5708", "train_lr": "8.91875e-05", "train_gnorm": "0.623", "train_loss_scale": "4", "train_train_wall": "2059", "train_gb_free": "13.2", "train_wall": "22990"}
[2023-09-12 17:28:37,185][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 17:28:37,275][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2023-09-12 17:28:37,518][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 17:28:37,522][fairseq.trainer][INFO] - begin training epoch 12
[2023-09-12 17:28:37,522][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(9055.8242, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6328, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 17:34:43,040][train_inner][INFO] - {"epoch": 12, "update": 11.177, "loss": "4.49", "ntokens": "148930", "nsentences": "536.235", "prob_perplexity": "86.062", "code_perplexity": "79.607", "temp": "1.944", "loss_0": "4.354", "loss_1": "0.125", "loss_2": "0.011", "accuracy": "0.27177", "wps": "36327.2", "ups": "0.24", "wpb": "148930", "bsz": "536.2", "num_updates": "5800", "lr": "9.0625e-05", "gnorm": "0.594", "loss_scale": "4", "train_wall": "792", "gb_free": "12.6", "wall": "23356"}
Parameter containing:
tensor([0.2073, 0.2073, 0.2048, 0.2030, 0.2014, 0.1293, 0.2072, 0.2069, 0.2064,
        0.2072, 0.1934, 0.1979, 0.1980, 0.2074, 0.2073, 0.2073, 0.2039, 0.2073,
        0.0933, 0.2074, 0.2074, 0.1121, 0.0931, 0.0925, 0.1953, 0.2070, 0.2073,
        0.2040, 0.2045, 0.2073, 0.0955, 0.2067, 0.2024, 0.2073, 0.2069, 0.2067,
        0.2053, 0.2074, 0.2073, 0.1947, 0.2074, 0.0949, 0.2074, 0.2067, 0.2061,
        0.0944, 0.1884, 0.2074, 0.2073, 0.2064, 0.2074, 0.1192, 0.1981, 0.2062,
        0.1906, 0.2063, 0.1267, 0.1956, 0.2073, 0.2059, 0.2040, 0.2067, 0.1992,
        0.2067, 0.1236, 0.2073, 0.2074, 0.2059, 0.2074, 0.2074, 0.0940, 0.2074,
        0.2047, 0.2074, 0.2065, 0.1837, 0.2059, 0.0995, 0.2074, 0.1979, 0.2047,
        0.2072, 0.0992, 0.2074, 0.2067, 0.1248, 0.2072, 0.2067, 0.2063, 0.2064,
        0.1175, 0.0931, 0.2068, 0.2074, 0.1952, 0.0869, 0.0933, 0.2074, 0.0930,
        0.2073, 0.2073, 0.2074, 0.2070, 0.2062, 0.0961, 0.2074, 0.2074, 0.0934,
        0.1893, 0.0988, 0.2057, 0.1247, 0.1990, 0.2074, 0.2050, 0.2041, 0.2072,
        0.0929, 0.2053, 0.2006, 0.2074, 0.2073, 0.2074, 0.2074, 0.1930, 0.0952,
        0.2074, 0.2074, 0.2069, 0.2074, 0.2073, 0.2074, 0.2026, 0.2058, 0.1053,
        0.2050, 0.2067, 0.1954, 0.2032, 0.2074, 0.2064, 0.1971, 0.2069, 0.2068,
        0.2059, 0.2073, 0.2012, 0.2064, 0.2070, 0.1185, 0.2050, 0.2069, 0.1307,
        0.1976, 0.2074, 0.1960, 0.2073, 0.2074, 0.2057, 0.2074, 0.2072, 0.2074,
        0.2074, 0.2053, 0.2074, 0.2072, 0.2070, 0.2058, 0.2042, 0.2067, 0.2074,
        0.2073, 0.2057, 0.2074, 0.2067, 0.1277, 0.2074, 0.2069, 0.0927, 0.1993,
        0.2072, 0.2067, 0.2072, 0.1984, 0.2073, 0.1339, 0.2074, 0.2070, 0.2074,
        0.1329, 0.2058, 0.2072, 0.2073, 0.1235, 0.2056, 0.2042, 0.2047, 0.2069,
        0.2073, 0.2063, 0.1989, 0.2073, 0.2072, 0.2074, 0.2073, 0.0958, 0.1965,
        0.1963, 0.2073, 0.2074, 0.2057, 0.0952, 0.2073, 0.2074, 0.2074, 0.2073,
        0.1080, 0.2074, 0.2073, 0.2073, 0.2073, 0.1993, 0.2057, 0.2074, 0.2059,
        0.0996, 0.2074, 0.2073, 0.2074, 0.0927, 0.2056, 0.1871, 0.2074, 0.2072,
        0.2074, 0.2065, 0.2054, 0.1177, 0.2064, 0.1214, 0.2074, 0.2029, 0.2007,
        0.1962, 0.1846, 0.2074, 0.1998, 0.1040, 0.2074, 0.2074, 0.1998, 0.2035,
        0.0947, 0.1851, 0.2054, 0.2074, 0.2067, 0.2074, 0.2046, 0.1932, 0.1997,
        0.2070, 0.2069, 0.1113, 0.2062, 0.2019, 0.2073, 0.2074, 0.1133, 0.2067,
        0.0942, 0.2065, 0.2074, 0.2068, 0.2054, 0.2074, 0.0939, 0.2072, 0.0959,
        0.0934, 0.2074, 0.2069, 0.1171, 0.2075, 0.2072, 0.2057, 0.2075, 0.2064,
        0.2074, 0.2072, 0.2074, 0.1099, 0.2054, 0.2072, 0.2065, 0.0933, 0.2046,
        0.0925, 0.2029, 0.2074, 0.2070, 0.1823, 0.0943, 0.1943, 0.2031, 0.2036,
        0.2073, 0.2074, 0.2074, 0.1077, 0.2074, 0.2057, 0.2041, 0.2064, 0.2074,
        0.1171, 0.2074, 0.0981, 0.1030, 0.2073, 0.2056, 0.0939, 0.2074, 0.2074,
        0.2057, 0.2057, 0.2072, 0.1949, 0.2074, 0.2051, 0.2057, 0.2074, 0.2073,
        0.0934, 0.2059, 0.2035, 0.2056, 0.0956, 0.2069, 0.2073, 0.2064, 0.2072,
        0.2074, 0.2074, 0.2073, 0.1260, 0.1997, 0.0931, 0.1934, 0.2074, 0.2025,
        0.0952, 0.2069, 0.2074, 0.2053, 0.2073, 0.2074, 0.1823, 0.2074, 0.1968,
        0.2072, 0.2074, 0.2026, 0.2074, 0.1804, 0.2064, 0.2070, 0.2074, 0.2074,
        0.2050, 0.2074, 0.2074, 0.2072, 0.2074, 0.2059, 0.2030, 0.2074, 0.0958,
        0.1377, 0.2014, 0.2075, 0.0943, 0.2074, 0.2074, 0.2006, 0.2074, 0.1913,
        0.1915, 0.2046, 0.2074, 0.2013, 0.2074, 0.2073, 0.2070, 0.0948, 0.2075,
        0.2052, 0.0942, 0.2073, 0.0936, 0.2074, 0.1254, 0.1946, 0.2072, 0.2073,
        0.1282, 0.2074, 0.1221, 0.2063, 0.2072, 0.2074, 0.2074, 0.0999, 0.2047,
        0.2069, 0.2073, 0.2074, 0.2014, 0.2074, 0.2072, 0.2074, 0.2074, 0.2062,
        0.2073, 0.2073, 0.2065, 0.2074, 0.2073, 0.2042, 0.2073, 0.2070, 0.2068,
        0.0975, 0.2037, 0.2074, 0.2072, 0.2074, 0.2064, 0.2054, 0.2063, 0.2067,
        0.2073, 0.2073, 0.2070, 0.2074, 0.2010, 0.2047, 0.2074, 0.1043, 0.2041,
        0.2074, 0.0922, 0.2067, 0.2074, 0.2050, 0.2074, 0.2074, 0.2072, 0.1320,
        0.1045, 0.1998, 0.2072, 0.2065, 0.2074, 0.1980, 0.2074, 0.2031, 0.1924,
        0.2026, 0.0947, 0.0972, 0.2040, 0.2073, 0.2074, 0.2067, 0.1947, 0.2070,
        0.2058, 0.2074, 0.2069, 0.2074, 0.1956, 0.2070, 0.2074, 0.2073, 0.1011,
        0.2068, 0.1109, 0.2065, 0.2073, 0.2069, 0.2072, 0.1987, 0.2074, 0.2073,
        0.2068, 0.2074, 0.2074, 0.0964, 0.2074, 0.2074, 0.2072, 0.2069, 0.2074,
        0.2073, 0.2074, 0.2069, 0.2067, 0.2010, 0.2004, 0.0995, 0.2074, 0.0947,
        0.2074, 0.2074, 0.2073, 0.2065, 0.2074, 0.2074, 0.1948, 0.2054, 0.2074,
        0.2073, 0.2073, 0.1077, 0.0925, 0.2074, 0.2069, 0.1262, 0.2072, 0.1943,
        0.2002, 0.1976, 0.2074, 0.2073, 0.1095, 0.2074, 0.2074, 0.2046, 0.2067,
        0.0932, 0.2002, 0.0930, 0.2074, 0.1952, 0.2074, 0.2073, 0.2072, 0.2074,
        0.1936, 0.2073, 0.0977, 0.2074, 0.0930, 0.2035, 0.2070, 0.2074, 0.1921,
        0.1230, 0.2074, 0.2074, 0.2069, 0.2074, 0.2074, 0.2075, 0.2047, 0.2061,
        0.2073, 0.2020, 0.0930, 0.1992, 0.2047, 0.2074, 0.2073, 0.2072, 0.2073,
        0.2074, 0.2069, 0.2021, 0.2072, 0.2074, 0.1923, 0.2074, 0.0922, 0.2074,
        0.2073, 0.2073, 0.0942, 0.2002, 0.2069, 0.2074, 0.2074, 0.2056, 0.2074,
        0.2072, 0.2069, 0.2073, 0.1207, 0.1989, 0.0919, 0.1807, 0.0970, 0.2059,
        0.2073, 0.2070, 0.2074, 0.2043, 0.2070, 0.1927, 0.2074, 0.2070, 0.2074,
        0.2070, 0.1993, 0.2037, 0.2074, 0.2062, 0.2054, 0.2074, 0.2073, 0.0928,
        0.2059, 0.2069, 0.2048, 0.0934, 0.2067, 0.2069, 0.2062, 0.2073, 0.2073,
        0.2037, 0.2065, 0.2074, 0.2074, 0.2074, 0.2058, 0.2074, 0.2074, 0.2043,
        0.0959], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7646.2583, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6211, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7431.1841, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6211, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6816.0396, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6328, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6552.0376, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6367, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 17:36:33,827][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
loss: tensor(6873.8843, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6367, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2173, 0.2174, 0.2150, 0.2131, 0.2115, 0.1093, 0.2172, 0.2169, 0.2166,
        0.2172, 0.2035, 0.2079, 0.2080, 0.2174, 0.2173, 0.2174, 0.2140, 0.2173,
        0.1017, 0.2174, 0.2174, 0.1022, 0.1011, 0.1006, 0.2054, 0.2172, 0.2174,
        0.2141, 0.2146, 0.2174, 0.1053, 0.2168, 0.2125, 0.2173, 0.2170, 0.2167,
        0.2155, 0.2174, 0.2173, 0.2048, 0.2174, 0.1045, 0.2174, 0.2167, 0.2161,
        0.1042, 0.1984, 0.2174, 0.2174, 0.2164, 0.2174, 0.1037, 0.2083, 0.2163,
        0.2006, 0.2163, 0.1181, 0.2056, 0.2174, 0.2161, 0.2141, 0.2168, 0.2094,
        0.2168, 0.1123, 0.2174, 0.2174, 0.2161, 0.2174, 0.2174, 0.1029, 0.2174,
        0.2148, 0.2174, 0.2166, 0.1937, 0.2161, 0.1063, 0.2174, 0.2079, 0.2148,
        0.2172, 0.1035, 0.2174, 0.2168, 0.1171, 0.2173, 0.2168, 0.2164, 0.2166,
        0.1056, 0.1017, 0.2168, 0.2174, 0.2052, 0.0923, 0.1014, 0.2175, 0.1010,
        0.2174, 0.2173, 0.2174, 0.2170, 0.2162, 0.1055, 0.2174, 0.2174, 0.1014,
        0.1995, 0.1074, 0.2158, 0.1149, 0.2091, 0.2174, 0.2150, 0.2142, 0.2172,
        0.1011, 0.2155, 0.2107, 0.2175, 0.2174, 0.2175, 0.2174, 0.2030, 0.1044,
        0.2174, 0.2174, 0.2169, 0.2174, 0.2174, 0.2174, 0.2128, 0.2158, 0.0992,
        0.2150, 0.2167, 0.2056, 0.2134, 0.2175, 0.2164, 0.2073, 0.2170, 0.2168,
        0.2161, 0.2173, 0.2113, 0.2166, 0.2172, 0.1044, 0.2150, 0.2170, 0.1179,
        0.2078, 0.2174, 0.2062, 0.2174, 0.2174, 0.2157, 0.2174, 0.2173, 0.2174,
        0.2174, 0.2155, 0.2174, 0.2173, 0.2172, 0.2158, 0.2144, 0.2167, 0.2174,
        0.2174, 0.2158, 0.2174, 0.2167, 0.1169, 0.2174, 0.2170, 0.1010, 0.2095,
        0.2172, 0.2167, 0.2172, 0.2085, 0.2173, 0.1255, 0.2174, 0.2172, 0.2174,
        0.1240, 0.2158, 0.2173, 0.2173, 0.1194, 0.2157, 0.2142, 0.2147, 0.2170,
        0.2173, 0.2164, 0.2090, 0.2173, 0.2172, 0.2174, 0.2173, 0.1062, 0.2067,
        0.2063, 0.2174, 0.2174, 0.2158, 0.1040, 0.2173, 0.2174, 0.2175, 0.2173,
        0.1014, 0.2175, 0.2174, 0.2174, 0.2174, 0.2094, 0.2158, 0.2174, 0.2159,
        0.1054, 0.2174, 0.2174, 0.2175, 0.1010, 0.2157, 0.1973, 0.2174, 0.2172,
        0.2174, 0.2167, 0.2156, 0.1044, 0.2164, 0.1124, 0.2174, 0.2130, 0.2108,
        0.2063, 0.1947, 0.2174, 0.2098, 0.1003, 0.2174, 0.2174, 0.2100, 0.2136,
        0.1052, 0.1952, 0.2155, 0.2174, 0.2168, 0.2174, 0.2147, 0.2034, 0.2097,
        0.2172, 0.2170, 0.1066, 0.2163, 0.2120, 0.2174, 0.2174, 0.1043, 0.2167,
        0.1027, 0.2166, 0.2174, 0.2169, 0.2156, 0.2175, 0.1033, 0.2173, 0.1055,
        0.1022, 0.2174, 0.2169, 0.1028, 0.2175, 0.2172, 0.2157, 0.2175, 0.2164,
        0.2174, 0.2172, 0.2174, 0.0998, 0.2156, 0.2173, 0.2166, 0.1015, 0.2147,
        0.1008, 0.2130, 0.2174, 0.2172, 0.1924, 0.1046, 0.2043, 0.2131, 0.2137,
        0.2173, 0.2175, 0.2174, 0.1039, 0.2174, 0.2158, 0.2141, 0.2166, 0.2174,
        0.1022, 0.2174, 0.1078, 0.1017, 0.2173, 0.2157, 0.1025, 0.2174, 0.2174,
        0.2158, 0.2157, 0.2173, 0.2051, 0.2174, 0.2152, 0.2157, 0.2174, 0.2173,
        0.1022, 0.2159, 0.2136, 0.2157, 0.1041, 0.2169, 0.2173, 0.2166, 0.2173,
        0.2174, 0.2175, 0.2174, 0.1169, 0.2098, 0.1013, 0.2035, 0.2174, 0.2125,
        0.1041, 0.2170, 0.2174, 0.2155, 0.2173, 0.2174, 0.1923, 0.2174, 0.2068,
        0.2173, 0.2174, 0.2128, 0.2174, 0.1904, 0.2166, 0.2170, 0.2174, 0.2174,
        0.2151, 0.2174, 0.2174, 0.2173, 0.2174, 0.2161, 0.2131, 0.2174, 0.1044,
        0.1168, 0.2115, 0.2177, 0.1035, 0.2174, 0.2174, 0.2107, 0.2174, 0.2013,
        0.2017, 0.2147, 0.2174, 0.2114, 0.2174, 0.2174, 0.2172, 0.1048, 0.2175,
        0.2153, 0.1028, 0.2173, 0.1028, 0.2175, 0.1137, 0.2046, 0.2173, 0.2174,
        0.1119, 0.2174, 0.1060, 0.2163, 0.2173, 0.2174, 0.2174, 0.1021, 0.2148,
        0.2170, 0.2174, 0.2174, 0.2114, 0.2174, 0.2173, 0.2175, 0.2174, 0.2163,
        0.2174, 0.2173, 0.2166, 0.2174, 0.2173, 0.2144, 0.2174, 0.2172, 0.2169,
        0.1052, 0.2139, 0.2174, 0.2172, 0.2174, 0.2166, 0.2155, 0.2163, 0.2167,
        0.2174, 0.2173, 0.2170, 0.2174, 0.2112, 0.2147, 0.2174, 0.0996, 0.2141,
        0.2174, 0.1003, 0.2168, 0.2175, 0.2151, 0.2174, 0.2174, 0.2173, 0.1216,
        0.0981, 0.2100, 0.2173, 0.2167, 0.2174, 0.2081, 0.2174, 0.2131, 0.2024,
        0.2128, 0.1046, 0.1043, 0.2141, 0.2173, 0.2174, 0.2168, 0.2047, 0.2170,
        0.2159, 0.2174, 0.2169, 0.2174, 0.2057, 0.2170, 0.2174, 0.2174, 0.1077,
        0.2169, 0.1052, 0.2166, 0.2174, 0.2170, 0.2173, 0.2089, 0.2175, 0.2174,
        0.2169, 0.2174, 0.2175, 0.1052, 0.2174, 0.2174, 0.2173, 0.2170, 0.2174,
        0.2173, 0.2175, 0.2170, 0.2168, 0.2112, 0.2106, 0.1046, 0.2174, 0.1039,
        0.2174, 0.2174, 0.2174, 0.2167, 0.2174, 0.2174, 0.2048, 0.2156, 0.2174,
        0.2174, 0.2174, 0.0999, 0.1006, 0.2174, 0.2169, 0.1069, 0.2173, 0.2045,
        0.2102, 0.2076, 0.2174, 0.2174, 0.0994, 0.2174, 0.2174, 0.2146, 0.2168,
        0.1018, 0.2102, 0.1013, 0.2174, 0.2052, 0.2175, 0.2174, 0.2172, 0.2175,
        0.2037, 0.2174, 0.1021, 0.2174, 0.1014, 0.2136, 0.2170, 0.2174, 0.2023,
        0.1069, 0.2174, 0.2174, 0.2170, 0.2174, 0.2174, 0.2177, 0.2147, 0.2162,
        0.2174, 0.2122, 0.1015, 0.2092, 0.2147, 0.2174, 0.2174, 0.2173, 0.2174,
        0.2174, 0.2170, 0.2123, 0.2173, 0.2174, 0.2024, 0.2174, 0.1003, 0.2174,
        0.2173, 0.2173, 0.1035, 0.2103, 0.2169, 0.2174, 0.2174, 0.2156, 0.2174,
        0.2173, 0.2170, 0.2174, 0.1054, 0.2090, 0.1002, 0.1906, 0.1064, 0.2159,
        0.2173, 0.2172, 0.2174, 0.2145, 0.2172, 0.2028, 0.2175, 0.2172, 0.2174,
        0.2172, 0.2095, 0.2137, 0.2175, 0.2162, 0.2155, 0.2174, 0.2173, 0.1010,
        0.2161, 0.2170, 0.2150, 0.1014, 0.2168, 0.2170, 0.2163, 0.2174, 0.2174,
        0.2139, 0.2167, 0.2174, 0.2174, 0.2174, 0.2158, 0.2174, 0.2174, 0.2144,
        0.0983], device='cuda:0', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2073, 0.2073, 0.2048, 0.2030, 0.2014, 0.1293, 0.2072, 0.2069, 0.2064,
        0.2072, 0.1934, 0.1979, 0.1980, 0.2074, 0.2073, 0.2073, 0.2039, 0.2073,
        0.0933, 0.2074, 0.2074, 0.1121, 0.0931, 0.0925, 0.1953, 0.2070, 0.2073,
        0.2040, 0.2045, 0.2073, 0.0955, 0.2067, 0.2024, 0.2073, 0.2069, 0.2067,
        0.2053, 0.2074, 0.2073, 0.1947, 0.2074, 0.0949, 0.2074, 0.2067, 0.2061,
        0.0944, 0.1884, 0.2074, 0.2073, 0.2064, 0.2074, 0.1192, 0.1981, 0.2062,
        0.1906, 0.2063, 0.1267, 0.1956, 0.2073, 0.2059, 0.2040, 0.2067, 0.1992,
        0.2067, 0.1236, 0.2073, 0.2074, 0.2059, 0.2074, 0.2074, 0.0940, 0.2074,
        0.2047, 0.2074, 0.2065, 0.1837, 0.2059, 0.0995, 0.2074, 0.1979, 0.2047,
        0.2072, 0.0992, 0.2074, 0.2067, 0.1248, 0.2072, 0.2067, 0.2063, 0.2064,
        0.1175, 0.0931, 0.2068, 0.2074, 0.1952, 0.0869, 0.0933, 0.2074, 0.0930,
        0.2073, 0.2073, 0.2074, 0.2070, 0.2062, 0.0961, 0.2074, 0.2074, 0.0934,
        0.1893, 0.0988, 0.2057, 0.1247, 0.1990, 0.2074, 0.2050, 0.2041, 0.2072,
        0.0929, 0.2053, 0.2006, 0.2074, 0.2073, 0.2074, 0.2074, 0.1930, 0.0952,
        0.2074, 0.2074, 0.2069, 0.2074, 0.2073, 0.2074, 0.2026, 0.2058, 0.1053,
        0.2050, 0.2067, 0.1954, 0.2032, 0.2074, 0.2064, 0.1971, 0.2069, 0.2068,
        0.2059, 0.2073, 0.2012, 0.2064, 0.2070, 0.1185, 0.2050, 0.2069, 0.1307,
        0.1976, 0.2074, 0.1960, 0.2073, 0.2074, 0.2057, 0.2074, 0.2072, 0.2074,
        0.2074, 0.2053, 0.2074, 0.2072, 0.2070, 0.2058, 0.2042, 0.2067, 0.2074,
        0.2073, 0.2057, 0.2074, 0.2067, 0.1277, 0.2074, 0.2069, 0.0927, 0.1993,
        0.2072, 0.2067, 0.2072, 0.1984, 0.2073, 0.1339, 0.2074, 0.2070, 0.2074,
        0.1329, 0.2058, 0.2072, 0.2073, 0.1235, 0.2056, 0.2042, 0.2047, 0.2069,
        0.2073, 0.2063, 0.1989, 0.2073, 0.2072, 0.2074, 0.2073, 0.0958, 0.1965,
        0.1963, 0.2073, 0.2074, 0.2057, 0.0952, 0.2073, 0.2074, 0.2074, 0.2073,
        0.1080, 0.2074, 0.2073, 0.2073, 0.2073, 0.1993, 0.2057, 0.2074, 0.2059,
        0.0996, 0.2074, 0.2073, 0.2074, 0.0927, 0.2056, 0.1871, 0.2074, 0.2072,
        0.2074, 0.2065, 0.2054, 0.1177, 0.2064, 0.1214, 0.2074, 0.2029, 0.2007,
        0.1962, 0.1846, 0.2074, 0.1998, 0.1040, 0.2074, 0.2074, 0.1998, 0.2035,
        0.0947, 0.1851, 0.2054, 0.2074, 0.2067, 0.2074, 0.2046, 0.1932, 0.1997,
        0.2070, 0.2069, 0.1113, 0.2062, 0.2019, 0.2073, 0.2074, 0.1133, 0.2067,
        0.0942, 0.2065, 0.2074, 0.2068, 0.2054, 0.2074, 0.0939, 0.2072, 0.0959,
        0.0934, 0.2074, 0.2069, 0.1171, 0.2075, 0.2072, 0.2057, 0.2075, 0.2064,
        0.2074, 0.2072, 0.2074, 0.1099, 0.2054, 0.2072, 0.2065, 0.0933, 0.2046,
        0.0925, 0.2029, 0.2074, 0.2070, 0.1823, 0.0943, 0.1943, 0.2031, 0.2036,
        0.2073, 0.2074, 0.2074, 0.1077, 0.2074, 0.2057, 0.2041, 0.2064, 0.2074,
        0.1171, 0.2074, 0.0981, 0.1030, 0.2073, 0.2056, 0.0939, 0.2074, 0.2074,
        0.2057, 0.2057, 0.2072, 0.1949, 0.2074, 0.2051, 0.2057, 0.2074, 0.2073,
        0.0934, 0.2059, 0.2035, 0.2056, 0.0956, 0.2069, 0.2073, 0.2064, 0.2072,
        0.2074, 0.2074, 0.2073, 0.1260, 0.1997, 0.0931, 0.1934, 0.2074, 0.2025,
        0.0952, 0.2069, 0.2074, 0.2053, 0.2073, 0.2074, 0.1823, 0.2074, 0.1968,
        0.2072, 0.2074, 0.2026, 0.2074, 0.1804, 0.2064, 0.2070, 0.2074, 0.2074,
        0.2050, 0.2074, 0.2074, 0.2072, 0.2074, 0.2059, 0.2030, 0.2074, 0.0958,
        0.1377, 0.2014, 0.2075, 0.0943, 0.2074, 0.2074, 0.2006, 0.2074, 0.1913,
        0.1915, 0.2046, 0.2074, 0.2013, 0.2074, 0.2073, 0.2070, 0.0948, 0.2075,
        0.2052, 0.0942, 0.2073, 0.0936, 0.2074, 0.1254, 0.1946, 0.2072, 0.2073,
        0.1282, 0.2074, 0.1221, 0.2063, 0.2072, 0.2074, 0.2074, 0.0999, 0.2047,
        0.2069, 0.2073, 0.2074, 0.2014, 0.2074, 0.2072, 0.2074, 0.2074, 0.2062,
        0.2073, 0.2073, 0.2065, 0.2074, 0.2073, 0.2042, 0.2073, 0.2070, 0.2068,
        0.0975, 0.2037, 0.2074, 0.2072, 0.2074, 0.2064, 0.2054, 0.2063, 0.2067,
        0.2073, 0.2073, 0.2070, 0.2074, 0.2010, 0.2047, 0.2074, 0.1043, 0.2041,
        0.2074, 0.0922, 0.2067, 0.2074, 0.2050, 0.2074, 0.2074, 0.2072, 0.1320,
        0.1045, 0.1998, 0.2072, 0.2065, 0.2074, 0.1980, 0.2074, 0.2031, 0.1924,
        0.2026, 0.0947, 0.0972, 0.2040, 0.2073, 0.2074, 0.2067, 0.1947, 0.2070,
        0.2058, 0.2074, 0.2069, 0.2074, 0.1956, 0.2070, 0.2074, 0.2073, 0.1011,
        0.2068, 0.1109, 0.2065, 0.2073, 0.2069, 0.2072, 0.1987, 0.2074, 0.2073,
        0.2068, 0.2074, 0.2074, 0.0964, 0.2074, 0.2074, 0.2072, 0.2069, 0.2074,
        0.2073, 0.2074, 0.2069, 0.2067, 0.2010, 0.2004, 0.0995, 0.2074, 0.0947,
        0.2074, 0.2074, 0.2073, 0.2065, 0.2074, 0.2074, 0.1948, 0.2054, 0.2074,
        0.2073, 0.2073, 0.1077, 0.0925, 0.2074, 0.2069, 0.1262, 0.2072, 0.1943,
        0.2002, 0.1976, 0.2074, 0.2073, 0.1095, 0.2074, 0.2074, 0.2046, 0.2067,
        0.0932, 0.2002, 0.0930, 0.2074, 0.1952, 0.2074, 0.2073, 0.2072, 0.2074,
        0.1936, 0.2073, 0.0977, 0.2074, 0.0930, 0.2035, 0.2070, 0.2074, 0.1921,
        0.1230, 0.2074, 0.2074, 0.2069, 0.2074, 0.2074, 0.2075, 0.2047, 0.2061,
        0.2073, 0.2020, 0.0930, 0.1992, 0.2047, 0.2074, 0.2073, 0.2072, 0.2073,
        0.2074, 0.2069, 0.2021, 0.2072, 0.2074, 0.1923, 0.2074, 0.0922, 0.2074,
        0.2073, 0.2073, 0.0942, 0.2002, 0.2069, 0.2074, 0.2074, 0.2056, 0.2074,
        0.2072, 0.2069, 0.2073, 0.1207, 0.1989, 0.0919, 0.1807, 0.0970, 0.2059,
        0.2073, 0.2070, 0.2074, 0.2043, 0.2070, 0.1927, 0.2074, 0.2070, 0.2074,
        0.2070, 0.1993, 0.2037, 0.2074, 0.2062, 0.2054, 0.2074, 0.2073, 0.0928,
        0.2059, 0.2069, 0.2048, 0.0934, 0.2067, 0.2069, 0.2062, 0.2073, 0.2073,
        0.2037, 0.2065, 0.2074, 0.2074, 0.2074, 0.2058, 0.2074, 0.2074, 0.2043,
        0.0959], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7815.8491, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6250, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5272.0176, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6328, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7744.9688, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6367, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7172.6025, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6406, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2175, 0.2175, 0.2151, 0.2134, 0.2118, 0.1089, 0.2174, 0.2172, 0.2168,
        0.2174, 0.2036, 0.2081, 0.2083, 0.2177, 0.2175, 0.2177, 0.2141, 0.2175,
        0.1018, 0.2177, 0.2177, 0.1023, 0.1013, 0.1008, 0.2057, 0.2174, 0.2175,
        0.2144, 0.2147, 0.2177, 0.1057, 0.2169, 0.2126, 0.2175, 0.2172, 0.2169,
        0.2157, 0.2177, 0.2175, 0.2051, 0.2177, 0.1047, 0.2177, 0.2169, 0.2163,
        0.1046, 0.1986, 0.2177, 0.2177, 0.2167, 0.2177, 0.1033, 0.2085, 0.2164,
        0.2008, 0.2166, 0.1177, 0.2058, 0.2175, 0.2163, 0.2142, 0.2170, 0.2095,
        0.2169, 0.1122, 0.2177, 0.2177, 0.2162, 0.2177, 0.2177, 0.1030, 0.2177,
        0.2150, 0.2177, 0.2168, 0.1940, 0.2163, 0.1062, 0.2177, 0.2081, 0.2150,
        0.2174, 0.1038, 0.2177, 0.2169, 0.1170, 0.2175, 0.2169, 0.2167, 0.2167,
        0.1057, 0.1018, 0.2170, 0.2177, 0.2054, 0.0925, 0.1015, 0.2177, 0.1011,
        0.2175, 0.2175, 0.2177, 0.2173, 0.2164, 0.1058, 0.2177, 0.2177, 0.1015,
        0.1997, 0.1071, 0.2159, 0.1140, 0.2094, 0.2177, 0.2152, 0.2144, 0.2174,
        0.1013, 0.2156, 0.2108, 0.2177, 0.2177, 0.2177, 0.2177, 0.2032, 0.1048,
        0.2177, 0.2177, 0.2172, 0.2177, 0.2177, 0.2177, 0.2129, 0.2161, 0.0992,
        0.2152, 0.2169, 0.2058, 0.2135, 0.2177, 0.2167, 0.2075, 0.2172, 0.2170,
        0.2162, 0.2175, 0.2114, 0.2168, 0.2174, 0.1041, 0.2152, 0.2173, 0.1178,
        0.2079, 0.2177, 0.2064, 0.2175, 0.2177, 0.2159, 0.2177, 0.2174, 0.2177,
        0.2177, 0.2156, 0.2177, 0.2175, 0.2174, 0.2161, 0.2145, 0.2169, 0.2177,
        0.2175, 0.2161, 0.2177, 0.2169, 0.1160, 0.2177, 0.2173, 0.1011, 0.2096,
        0.2174, 0.2169, 0.2174, 0.2086, 0.2175, 0.1254, 0.2177, 0.2174, 0.2177,
        0.1238, 0.2161, 0.2175, 0.2175, 0.1194, 0.2158, 0.2145, 0.2150, 0.2172,
        0.2175, 0.2167, 0.2091, 0.2175, 0.2174, 0.2177, 0.2175, 0.1065, 0.2069,
        0.2065, 0.2177, 0.2177, 0.2161, 0.1042, 0.2175, 0.2177, 0.2177, 0.2175,
        0.1017, 0.2177, 0.2177, 0.2177, 0.2175, 0.2096, 0.2161, 0.2177, 0.2162,
        0.1057, 0.2177, 0.2175, 0.2177, 0.1011, 0.2159, 0.1974, 0.2177, 0.2174,
        0.2177, 0.2168, 0.2158, 0.1044, 0.2167, 0.1118, 0.2177, 0.2133, 0.2111,
        0.2064, 0.1949, 0.2177, 0.2101, 0.1004, 0.2177, 0.2177, 0.2102, 0.2137,
        0.1056, 0.1954, 0.2157, 0.2177, 0.2170, 0.2177, 0.2150, 0.2035, 0.2100,
        0.2173, 0.2173, 0.1061, 0.2164, 0.2123, 0.2177, 0.2177, 0.1044, 0.2169,
        0.1028, 0.2168, 0.2177, 0.2172, 0.2157, 0.2178, 0.1034, 0.2175, 0.1058,
        0.1024, 0.2177, 0.2172, 0.1026, 0.2178, 0.2174, 0.2159, 0.2178, 0.2167,
        0.2177, 0.2174, 0.2177, 0.0996, 0.2157, 0.2175, 0.2168, 0.1016, 0.2148,
        0.1010, 0.2133, 0.2177, 0.2173, 0.1925, 0.1049, 0.2046, 0.2134, 0.2140,
        0.2175, 0.2178, 0.2177, 0.1037, 0.2177, 0.2161, 0.2144, 0.2167, 0.2177,
        0.1020, 0.2177, 0.1081, 0.1019, 0.2175, 0.2158, 0.1027, 0.2177, 0.2177,
        0.2159, 0.2159, 0.2175, 0.2053, 0.2177, 0.2155, 0.2159, 0.2177, 0.2175,
        0.1024, 0.2162, 0.2137, 0.2159, 0.1043, 0.2172, 0.2175, 0.2168, 0.2174,
        0.2177, 0.2177, 0.2175, 0.1164, 0.2100, 0.1014, 0.2036, 0.2177, 0.2128,
        0.1042, 0.2173, 0.2177, 0.2157, 0.2175, 0.2177, 0.1925, 0.2177, 0.2070,
        0.2175, 0.2177, 0.2129, 0.2177, 0.1907, 0.2168, 0.2173, 0.2177, 0.2177,
        0.2152, 0.2177, 0.2177, 0.2175, 0.2177, 0.2163, 0.2134, 0.2177, 0.1046,
        0.1160, 0.2117, 0.2178, 0.1038, 0.2177, 0.2177, 0.2109, 0.2177, 0.2015,
        0.2018, 0.2150, 0.2177, 0.2115, 0.2177, 0.2175, 0.2174, 0.1052, 0.2178,
        0.2155, 0.1030, 0.2175, 0.1030, 0.2177, 0.1132, 0.2048, 0.2175, 0.2177,
        0.1116, 0.2177, 0.1056, 0.2166, 0.2175, 0.2177, 0.2177, 0.1024, 0.2151,
        0.2172, 0.2177, 0.2177, 0.2117, 0.2177, 0.2174, 0.2177, 0.2177, 0.2166,
        0.2177, 0.2175, 0.2168, 0.2177, 0.2175, 0.2146, 0.2175, 0.2174, 0.2170,
        0.1054, 0.2140, 0.2177, 0.2174, 0.2177, 0.2167, 0.2157, 0.2166, 0.2169,
        0.2175, 0.2175, 0.2173, 0.2177, 0.2114, 0.2150, 0.2177, 0.0997, 0.2144,
        0.2177, 0.1005, 0.2169, 0.2178, 0.2153, 0.2177, 0.2177, 0.2174, 0.1212,
        0.0981, 0.2102, 0.2175, 0.2169, 0.2177, 0.2084, 0.2177, 0.2134, 0.2026,
        0.2129, 0.1047, 0.1046, 0.2144, 0.2175, 0.2177, 0.2169, 0.2050, 0.2173,
        0.2161, 0.2177, 0.2172, 0.2177, 0.2058, 0.2173, 0.2177, 0.2175, 0.1077,
        0.2172, 0.1053, 0.2168, 0.2177, 0.2173, 0.2174, 0.2091, 0.2177, 0.2177,
        0.2172, 0.2177, 0.2177, 0.1055, 0.2177, 0.2177, 0.2175, 0.2173, 0.2177,
        0.2175, 0.2177, 0.2173, 0.2169, 0.2114, 0.2108, 0.1047, 0.2177, 0.1041,
        0.2177, 0.2177, 0.2177, 0.2168, 0.2177, 0.2177, 0.2051, 0.2157, 0.2177,
        0.2177, 0.2177, 0.0999, 0.1008, 0.2177, 0.2172, 0.1066, 0.2174, 0.2047,
        0.2104, 0.2079, 0.2177, 0.2175, 0.0992, 0.2177, 0.2177, 0.2148, 0.2170,
        0.1019, 0.2104, 0.1014, 0.2177, 0.2053, 0.2178, 0.2175, 0.2174, 0.2177,
        0.2040, 0.2175, 0.1022, 0.2177, 0.1016, 0.2139, 0.2173, 0.2177, 0.2024,
        0.1066, 0.2177, 0.2177, 0.2172, 0.2177, 0.2177, 0.2179, 0.2150, 0.2164,
        0.2175, 0.2124, 0.1016, 0.2095, 0.2150, 0.2177, 0.2177, 0.2175, 0.2177,
        0.2177, 0.2172, 0.2124, 0.2175, 0.2177, 0.2025, 0.2177, 0.1005, 0.2177,
        0.2175, 0.2175, 0.1038, 0.2106, 0.2172, 0.2177, 0.2177, 0.2158, 0.2177,
        0.2175, 0.2173, 0.2175, 0.1052, 0.2092, 0.1003, 0.1908, 0.1066, 0.2162,
        0.2175, 0.2174, 0.2177, 0.2147, 0.2174, 0.2030, 0.2177, 0.2173, 0.2177,
        0.2174, 0.2097, 0.2140, 0.2177, 0.2164, 0.2157, 0.2177, 0.2175, 0.1011,
        0.2163, 0.2173, 0.2152, 0.1016, 0.2170, 0.2173, 0.2164, 0.2177, 0.2175,
        0.2140, 0.2168, 0.2177, 0.2177, 0.2177, 0.2161, 0.2177, 0.2177, 0.2146,
        0.0984], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(6919.7925, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6406, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7425.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6406, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 17:47:58,267][train_inner][INFO] - {"epoch": 12, "update": 11.562, "loss": "4.468", "ntokens": "149798", "nsentences": "537.485", "prob_perplexity": "92.053", "code_perplexity": "83.233", "temp": "1.942", "loss_0": "4.334", "loss_1": "0.124", "loss_2": "0.01", "accuracy": "0.27161", "wps": "37674.2", "ups": "0.25", "wpb": "149798", "bsz": "537.5", "num_updates": "6000", "lr": "9.375e-05", "gnorm": "0.576", "loss_scale": "2", "train_wall": "794", "gb_free": "13", "wall": "24151"}
loss: tensor(7690.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 17:56:15,781][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
Parameter containing:
tensor([0.2190, 0.2191, 0.2167, 0.2148, 0.2133, 0.1063, 0.2189, 0.2186, 0.2183,
        0.2189, 0.2052, 0.2096, 0.2097, 0.2191, 0.2190, 0.2191, 0.2157, 0.2190,
        0.1022, 0.2191, 0.2191, 0.1033, 0.1016, 0.1011, 0.2072, 0.2189, 0.2191,
        0.2158, 0.2163, 0.2191, 0.1077, 0.2185, 0.2142, 0.2190, 0.2188, 0.2184,
        0.2172, 0.2191, 0.2190, 0.2065, 0.2191, 0.1054, 0.2191, 0.2184, 0.2178,
        0.1058, 0.2002, 0.2191, 0.2191, 0.2181, 0.2191, 0.1013, 0.2100, 0.2180,
        0.2023, 0.2180, 0.1102, 0.2073, 0.2191, 0.2178, 0.2158, 0.2185, 0.2111,
        0.2185, 0.1114, 0.2191, 0.2191, 0.2178, 0.2191, 0.2191, 0.1028, 0.2191,
        0.2166, 0.2191, 0.2183, 0.1954, 0.2178, 0.1047, 0.2191, 0.2096, 0.2166,
        0.2189, 0.1039, 0.2191, 0.2185, 0.1140, 0.2190, 0.2185, 0.2181, 0.2183,
        0.1052, 0.1022, 0.2185, 0.2191, 0.2069, 0.0951, 0.1017, 0.2192, 0.1015,
        0.2191, 0.2190, 0.2191, 0.2188, 0.2179, 0.1082, 0.2191, 0.2191, 0.1017,
        0.2012, 0.1044, 0.2175, 0.1071, 0.2108, 0.2191, 0.2167, 0.2159, 0.2189,
        0.1016, 0.2172, 0.2124, 0.2192, 0.2191, 0.2192, 0.2191, 0.2047, 0.1064,
        0.2191, 0.2191, 0.2186, 0.2191, 0.2191, 0.2191, 0.2145, 0.2175, 0.0992,
        0.2167, 0.2184, 0.2073, 0.2151, 0.2192, 0.2181, 0.2090, 0.2188, 0.2185,
        0.2178, 0.2190, 0.2130, 0.2183, 0.2189, 0.1025, 0.2167, 0.2188, 0.1163,
        0.2095, 0.2191, 0.2079, 0.2191, 0.2191, 0.2174, 0.2191, 0.2190, 0.2191,
        0.2191, 0.2172, 0.2191, 0.2190, 0.2189, 0.2175, 0.2161, 0.2184, 0.2191,
        0.2191, 0.2175, 0.2191, 0.2184, 0.1089, 0.2191, 0.2188, 0.1015, 0.2112,
        0.2189, 0.2184, 0.2189, 0.2102, 0.2190, 0.1189, 0.2191, 0.2189, 0.2191,
        0.1211, 0.2175, 0.2190, 0.2190, 0.1190, 0.2174, 0.2159, 0.2164, 0.2188,
        0.2190, 0.2181, 0.2107, 0.2190, 0.2189, 0.2191, 0.2190, 0.1088, 0.2084,
        0.2080, 0.2191, 0.2191, 0.2175, 0.1047, 0.2190, 0.2191, 0.2192, 0.2190,
        0.1027, 0.2192, 0.2191, 0.2191, 0.2191, 0.2111, 0.2175, 0.2191, 0.2177,
        0.1071, 0.2191, 0.2191, 0.2191, 0.1014, 0.2174, 0.1990, 0.2191, 0.2189,
        0.2191, 0.2184, 0.2173, 0.1049, 0.2181, 0.1070, 0.2191, 0.2147, 0.2125,
        0.2080, 0.1964, 0.2191, 0.2115, 0.1015, 0.2191, 0.2191, 0.2117, 0.2153,
        0.1080, 0.1969, 0.2172, 0.2191, 0.2185, 0.2191, 0.2164, 0.2051, 0.2114,
        0.2189, 0.2188, 0.1034, 0.2180, 0.2137, 0.2191, 0.2191, 0.1053, 0.2184,
        0.1027, 0.2183, 0.2191, 0.2186, 0.2173, 0.2192, 0.1035, 0.2190, 0.1076,
        0.1025, 0.2191, 0.2186, 0.1005, 0.2192, 0.2189, 0.2174, 0.2192, 0.2181,
        0.2191, 0.2189, 0.2191, 0.0991, 0.2173, 0.2190, 0.2183, 0.1021, 0.2164,
        0.1014, 0.2147, 0.2191, 0.2189, 0.1941, 0.1057, 0.2061, 0.2148, 0.2155,
        0.2190, 0.2192, 0.2191, 0.1024, 0.2191, 0.2175, 0.2158, 0.2183, 0.2191,
        0.1006, 0.2191, 0.1075, 0.1031, 0.2190, 0.2174, 0.1027, 0.2191, 0.2191,
        0.2175, 0.2174, 0.2190, 0.2068, 0.2191, 0.2169, 0.2174, 0.2191, 0.2190,
        0.1024, 0.2177, 0.2153, 0.2174, 0.1046, 0.2186, 0.2190, 0.2183, 0.2190,
        0.2191, 0.2192, 0.2191, 0.1099, 0.2115, 0.1017, 0.2052, 0.2191, 0.2142,
        0.1040, 0.2188, 0.2191, 0.2172, 0.2190, 0.2191, 0.1940, 0.2191, 0.2085,
        0.2190, 0.2191, 0.2145, 0.2191, 0.1921, 0.2183, 0.2188, 0.2191, 0.2191,
        0.2168, 0.2191, 0.2191, 0.2190, 0.2191, 0.2178, 0.2148, 0.2191, 0.1043,
        0.1102, 0.2133, 0.2194, 0.1046, 0.2191, 0.2191, 0.2124, 0.2191, 0.2030,
        0.2034, 0.2164, 0.2191, 0.2131, 0.2191, 0.2191, 0.2189, 0.1075, 0.2192,
        0.2170, 0.1034, 0.2190, 0.1029, 0.2192, 0.1084, 0.2063, 0.2190, 0.2191,
        0.1082, 0.2191, 0.1036, 0.2180, 0.2190, 0.2191, 0.2191, 0.1039, 0.2166,
        0.2188, 0.2191, 0.2191, 0.2131, 0.2191, 0.2190, 0.2191, 0.2191, 0.2180,
        0.2191, 0.2190, 0.2183, 0.2191, 0.2190, 0.2161, 0.2191, 0.2189, 0.2186,
        0.1066, 0.2156, 0.2191, 0.2189, 0.2191, 0.2183, 0.2172, 0.2180, 0.2184,
        0.2191, 0.2190, 0.2188, 0.2191, 0.2129, 0.2164, 0.2191, 0.1001, 0.2158,
        0.2191, 0.1010, 0.2185, 0.2192, 0.2168, 0.2191, 0.2191, 0.2190, 0.1172,
        0.0986, 0.2117, 0.2190, 0.2184, 0.2191, 0.2098, 0.2191, 0.2148, 0.2041,
        0.2145, 0.1047, 0.1056, 0.2158, 0.2190, 0.2191, 0.2185, 0.2064, 0.2188,
        0.2177, 0.2191, 0.2186, 0.2191, 0.2074, 0.2188, 0.2191, 0.2191, 0.1056,
        0.2186, 0.1062, 0.2183, 0.2191, 0.2188, 0.2190, 0.2106, 0.2192, 0.2191,
        0.2186, 0.2191, 0.2192, 0.1075, 0.2191, 0.2191, 0.2190, 0.2188, 0.2191,
        0.2190, 0.2192, 0.2188, 0.2185, 0.2129, 0.2123, 0.1054, 0.2191, 0.1046,
        0.2191, 0.2191, 0.2191, 0.2184, 0.2191, 0.2191, 0.2065, 0.2173, 0.2191,
        0.2191, 0.2191, 0.0997, 0.1012, 0.2191, 0.2186, 0.1045, 0.2190, 0.2062,
        0.2119, 0.2094, 0.2191, 0.2191, 0.0989, 0.2191, 0.2191, 0.2163, 0.2185,
        0.1022, 0.2119, 0.1016, 0.2191, 0.2069, 0.2192, 0.2191, 0.2189, 0.2192,
        0.2054, 0.2191, 0.1038, 0.2191, 0.1018, 0.2153, 0.2188, 0.2191, 0.2040,
        0.1052, 0.2191, 0.2191, 0.2188, 0.2191, 0.2191, 0.2194, 0.2164, 0.2179,
        0.2191, 0.2139, 0.1017, 0.2109, 0.2164, 0.2191, 0.2191, 0.2190, 0.2191,
        0.2191, 0.2188, 0.2140, 0.2190, 0.2191, 0.2041, 0.2191, 0.1010, 0.2191,
        0.2190, 0.2190, 0.1041, 0.2120, 0.2186, 0.2191, 0.2191, 0.2173, 0.2191,
        0.2190, 0.2188, 0.2191, 0.1036, 0.2107, 0.1008, 0.1923, 0.1068, 0.2177,
        0.2190, 0.2189, 0.2191, 0.2162, 0.2189, 0.2045, 0.2192, 0.2189, 0.2191,
        0.2189, 0.2112, 0.2155, 0.2192, 0.2179, 0.2172, 0.2191, 0.2190, 0.1015,
        0.2178, 0.2188, 0.2167, 0.1018, 0.2185, 0.2188, 0.2180, 0.2191, 0.2191,
        0.2156, 0.2184, 0.2191, 0.2191, 0.2191, 0.2175, 0.2191, 0.2191, 0.2161,
        0.0994], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(5039.1377, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8134.9282, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8862.0273, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: self.scaling_factor_for_vector: Parameter containing:
tensor([0.2200, 0.2200, 0.2175, 0.2158, 0.2142, 0.1028, 0.2198, 0.2196, 0.2192,
        0.2198, 0.2061, 0.2106, 0.2107, 0.2201, 0.2200, 0.2201, 0.2166, 0.2200,
        0.1017, 0.2201, 0.2201, 0.1030, 0.1014, 0.1010, 0.2081, 0.2198, 0.2200,
        0.2168, 0.2172, 0.2201, 0.1069, 0.2194, 0.2151, 0.2200, 0.2196, 0.2194,
        0.2181, 0.2201, 0.2200, 0.2075, 0.2201, 0.1044, 0.2201, 0.2194, 0.2188,
        0.1035, 0.2010, 0.2201, 0.2201, 0.2191, 0.2201, 0.1000, 0.2109, 0.2189,
        0.2032, 0.2190, 0.1050, 0.2083, 0.2201, 0.2188, 0.2167, 0.2195, 0.2119,
        0.2194, 0.1096, 0.2201, 0.2201, 0.2188, 0.2201, 0.2201, 0.1022, 0.2201,
        0.2174, 0.2201, 0.2192, 0.1964, 0.2188, 0.1029, 0.2201, 0.2106, 0.2174,
        0.2198, 0.1030, 0.2201, 0.2194, 0.1056, 0.2200, 0.2194, 0.2191, 0.2191,
        0.1031, 0.1017, 0.2195, 0.2201, 0.2079, 0.0969, 0.1015, 0.2201, 0.1013,
        0.2200, 0.2200, 0.2201, 0.2197, 0.2189, 0.1088, 0.2201, 0.2201, 0.1014,
        0.2021, 0.1035, 0.2184, 0.1039, 0.2118, 0.2201, 0.2177, 0.2168, 0.2198,
        0.1013, 0.2180, 0.2133, 0.2201, 0.2201, 0.2201, 0.2201, 0.2057, 0.1066,
        0.2201, 0.2201, 0.2196, 0.2201, 0.2201, 0.2201, 0.2155, 0.2185, 0.0995,
        0.2177, 0.2194, 0.2083, 0.2159, 0.2201, 0.2191, 0.2100, 0.2197, 0.2195,
        0.2188, 0.2200, 0.2139, 0.2192, 0.2198, 0.1014, 0.2177, 0.2197, 0.1069,
        0.2103, 0.2201, 0.2089, 0.2200, 0.2201, 0.2184, 0.2201, 0.2200, 0.2201,
        0.2201, 0.2180, 0.2201, 0.2200, 0.2198, 0.2185, 0.2169, 0.2194, 0.2201,
        0.2200, 0.2185, 0.2201, 0.2194, 0.1056, 0.2201, 0.2197, 0.1013, 0.2122,
        0.2198, 0.2194, 0.2198, 0.2111, 0.2200, 0.1083, 0.2201, 0.2198, 0.2201,
        0.1082, 0.2185, 0.2200, 0.2200, 0.1147, 0.2184, 0.2169, 0.2174, 0.2196,
        0.2200, 0.2191, 0.2117, 0.2200, 0.2198, 0.2201, 0.2200, 0.1097, 0.2094,
        0.2090, 0.2201, 0.2201, 0.2185, 0.1033, 0.2200, 0.2201, 0.2201, 0.2200,
        0.1027, 0.2201, 0.2201, 0.2201, 0.2200, 0.2120, 0.2185, 0.2201, 0.2186,
        0.1071, 0.2201, 0.2200, 0.2201, 0.1013, 0.2184, 0.2000, 0.2201, 0.2198,
        0.2201, 0.2192, 0.2183, 0.1038, 0.2191, 0.1050, 0.2201, 0.2157, 0.2135,
        0.2089, 0.1974, 0.2201, 0.2125, 0.1028, 0.2201, 0.2201, 0.2126, 0.2163,
        0.1088, 0.1979, 0.2181, 0.2201, 0.2195, 0.2201, 0.2174, 0.2059, 0.2124,
        0.2197, 0.2197, 0.1027, 0.2189, 0.2147, 0.2201, 0.2201, 0.1057, 0.2194,
        0.1021, 0.2192, 0.2201, 0.2196, 0.2181, 0.2202, 0.1030, 0.2200, 0.1069,
        0.1020, 0.2201, 0.2196, 0.0996, 0.2202, 0.2198, 0.2184, 0.2202, 0.2191,
        0.2201, 0.2198, 0.2201, 0.0994, 0.2181, 0.2200, 0.2192, 0.1015, 0.2173,
        0.1011, 0.2157, 0.2201, 0.2198, 0.1951, 0.1051, 0.2070, 0.2158, 0.2164,
        0.2200, 0.2202, 0.2201, 0.1017, 0.2201, 0.2185, 0.2168, 0.2191, 0.2201,
        0.0998, 0.2201, 0.1042, 0.1044, 0.2200, 0.2183, 0.1021, 0.2201, 0.2201,
        0.2184, 0.2184, 0.2200, 0.2078, 0.2201, 0.2179, 0.2184, 0.2201, 0.2200,
        0.1018, 0.2186, 0.2163, 0.2184, 0.1045, 0.2196, 0.2200, 0.2192, 0.2198,
        0.2201, 0.2201, 0.2200, 0.1044, 0.2125, 0.1014, 0.2061, 0.2201, 0.2152,
        0.1029, 0.2197, 0.2201, 0.2181, 0.2200, 0.2201, 0.1949, 0.2201, 0.2095,
        0.2200, 0.2201, 0.2153, 0.2201, 0.1931, 0.2192, 0.2197, 0.2201, 0.2201,
        0.2177, 0.2201, 0.2201, 0.2200, 0.2201, 0.2188, 0.2158, 0.2201, 0.1038,
        0.1047, 0.2141, 0.2203, 0.1045, 0.2201, 0.2201, 0.2134, 0.2201, 0.2040,
        0.2042, 0.2174, 0.2201, 0.2140, 0.2201, 0.2200, 0.2198, 0.1065, 0.2202,
        0.2180, 0.1026, 0.2200, 0.1020, 0.2201, 0.1042, 0.2073, 0.2200, 0.2201,
        0.1042, 0.2201, 0.1027, 0.2190, 0.2200, 0.2201, 0.2201, 0.1030, 0.2175,
        0.2196, 0.2201, 0.2201, 0.2141, 0.2201, 0.2198, 0.2201, 0.2201, 0.2190,
        0.2201, 0.2200, 0.2192, 0.2201, 0.2200, 0.2170, 0.2200, 0.2198, 0.2196,
        0.1053, 0.2164, 0.2201, 0.2198, 0.2201, 0.2191, 0.2181, 0.2190, 0.2194,
        0.2200, 0.2200, 0.2197, 0.2201, 0.2139, 0.2174, 0.2201, 0.1011, 0.2168,
        0.2201, 0.1008, 0.2194, 0.2202, 0.2178, 0.2201, 0.2201, 0.2198, 0.1112,
        0.0992, 0.2126, 0.2200, 0.2194, 0.2201, 0.2108, 0.2201, 0.2158, 0.2051,
        0.2153, 0.1033, 0.1038, 0.2168, 0.2200, 0.2201, 0.2195, 0.2074, 0.2197,
        0.2185, 0.2201, 0.2196, 0.2201, 0.2084, 0.2197, 0.2201, 0.2200, 0.1046,
        0.2196, 0.1047, 0.2192, 0.2201, 0.2197, 0.2200, 0.2115, 0.2201, 0.2201,
        0.2196, 0.2201, 0.2201, 0.1079, 0.2201, 0.2201, 0.2200, 0.2197, 0.2201,
        0.2200, 0.2201, 0.2197, 0.2194, 0.2139, 0.2133, 0.1042, 0.2201, 0.1040,
        0.2201, 0.2201, 0.2201, 0.2192, 0.2201, 0.2201, 0.2075, 0.2181, 0.2201,
        0.2201, 0.2201, 0.0998, 0.1011, 0.2201, 0.2196, 0.1022, 0.2198, 0.2072,
        0.2129, 0.2103, 0.2201, 0.2200, 0.0993, 0.2201, 0.2201, 0.2173, 0.2195,
        0.1017, 0.2129, 0.1013, 0.2201, 0.2078, 0.2202, 0.2200, 0.2198, 0.2201,
        0.2064, 0.2200, 0.1053, 0.2201, 0.1014, 0.2163, 0.2197, 0.2201, 0.2048,
        0.1040, 0.2201, 0.2201, 0.2197, 0.2201, 0.2201, 0.2203, 0.2174, 0.2189,
        0.2200, 0.2148, 0.1014, 0.2119, 0.2174, 0.2201, 0.2201, 0.2200, 0.2201,
        0.2201, 0.2196, 0.2148, 0.2200, 0.2201, 0.2051, 0.2201, 0.1008, 0.2201,
        0.2200, 0.2200, 0.1032, 0.2130, 0.2196, 0.2201, 0.2201, 0.2183, 0.2201,
        0.2200, 0.2197, 0.2200, 0.1021, 0.2117, 0.1008, 0.1932, 0.1043, 0.2186,
        0.2200, 0.2198, 0.2201, 0.2172, 0.2198, 0.2054, 0.2201, 0.2197, 0.2201,
        0.2198, 0.2122, 0.2164, 0.2201, 0.2189, 0.2181, 0.2201, 0.2200, 0.1012,
        0.2188, 0.2197, 0.2177, 0.1014, 0.2195, 0.2197, 0.2190, 0.2201, 0.2200,
        0.2164, 0.2194, 0.2201, 0.2201, 0.2201, 0.2185, 0.2201, 0.2201, 0.2170,
        0.1008], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(5541.2407, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7466.2896, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6251.3682, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 18:01:17,448][train_inner][INFO] - {"epoch": 12, "update": 11.948, "loss": "4.435", "ntokens": "149913", "nsentences": "540.83", "prob_perplexity": "107.573", "code_perplexity": "92.175", "temp": "1.94", "loss_0": "4.306", "loss_1": "0.12", "loss_2": "0.01", "accuracy": "0.27258", "wps": "37516.8", "ups": "0.25", "wpb": "149913", "bsz": "540.8", "num_updates": "6200", "lr": "9.6875e-05", "gnorm": "0.568", "loss_scale": "2", "train_wall": "798", "gb_free": "12.8", "wall": "24950"}
[2023-09-12 18:03:01,650][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 18:03:01,651][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 18:03:01,834][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
loss: tensor(6712.1211, device='cuda:0')
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16)
[2023-09-12 18:03:25,672][valid][INFO] - {"epoch": 12, "valid_loss": "4.117", "valid_ntokens": "7900", "valid_nsentences": "55.2525", "valid_prob_perplexity": "107.583", "valid_code_perplexity": "84.511", "valid_temp": "1.939", "valid_loss_0": "3.987", "valid_loss_1": "0.12", "valid_loss_2": "0.01", "valid_accuracy": "0.33923", "valid_wps": "33041.7", "valid_wpb": "7900", "valid_bsz": "55.3", "valid_num_updates": "6227", "valid_best_loss": "4.117"}
[2023-09-12 18:03:25,674][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 6227 updates
[2023-09-12 18:03:25,675][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 18:03:28,211][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 18:03:29,554][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 6227 updates, score 4.117) (writing took 3.8799001229926944 seconds)
[2023-09-12 18:03:29,555][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2023-09-12 18:03:29,555][train][INFO] - {"epoch": 12, "train_loss": "4.458", "train_ntokens": "149495", "train_nsentences": "538.38", "train_prob_perplexity": "98.527", "train_code_perplexity": "86.955", "train_temp": "1.941", "train_loss_0": "4.326", "train_loss_1": "0.122", "train_loss_2": "0.01", "train_accuracy": "0.27174", "train_wps": "37081.4", "train_ups": "0.25", "train_wpb": "149495", "train_bsz": "538.4", "train_num_updates": "6227", "train_lr": "9.72969e-05", "train_gnorm": "0.579", "train_loss_scale": "2", "train_train_wall": "2061", "train_gb_free": "15.1", "train_wall": "25082"}
[2023-09-12 18:03:29,557][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 18:03:29,678][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2023-09-12 18:03:29,918][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 18:03:29,921][fairseq.trainer][INFO] - begin training epoch 13
[2023-09-12 18:03:29,922][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([0.2153, 0.2153, 0.2130, 0.2112, 0.2096, 0.1132, 0.2152, 0.2150, 0.2146,
        0.2152, 0.2014, 0.2059, 0.2061, 0.2155, 0.2153, 0.2155, 0.2120, 0.2153,
        0.0993, 0.2155, 0.2155, 0.1062, 0.0990, 0.0984, 0.2035, 0.2152, 0.2153,
        0.2122, 0.2125, 0.2155, 0.1021, 0.2148, 0.2106, 0.2153, 0.2151, 0.2147,
        0.2135, 0.2155, 0.2153, 0.2029, 0.2155, 0.1014, 0.2155, 0.2147, 0.2141,
        0.1009, 0.1964, 0.2155, 0.2155, 0.2145, 0.2155, 0.1069, 0.2063, 0.2144,
        0.1986, 0.2144, 0.1202, 0.2036, 0.2155, 0.2141, 0.2122, 0.2148, 0.2073,
        0.2148, 0.1135, 0.2155, 0.2155, 0.2141, 0.2155, 0.2155, 0.1003, 0.2155,
        0.2129, 0.2155, 0.2146, 0.1918, 0.2141, 0.1056, 0.2155, 0.2059, 0.2129,
        0.2152, 0.1005, 0.2155, 0.2148, 0.1181, 0.2153, 0.2147, 0.2145, 0.2146,
        0.1062, 0.0993, 0.2148, 0.2155, 0.2032, 0.0903, 0.0992, 0.2155, 0.0988,
        0.2155, 0.2153, 0.2155, 0.2151, 0.2142, 0.1022, 0.2155, 0.2155, 0.0992,
        0.1975, 0.1058, 0.2139, 0.1233, 0.2072, 0.2155, 0.2130, 0.2122, 0.2152,
        0.0989, 0.2135, 0.2086, 0.2155, 0.2155, 0.2156, 0.2155, 0.2010, 0.1013,
        0.2155, 0.2155, 0.2150, 0.2155, 0.2155, 0.2155, 0.2108, 0.2139, 0.1008,
        0.2130, 0.2147, 0.2036, 0.2114, 0.2155, 0.2145, 0.2053, 0.2151, 0.2148,
        0.2141, 0.2153, 0.2092, 0.2146, 0.2152, 0.1082, 0.2130, 0.2151, 0.1186,
        0.2057, 0.2155, 0.2042, 0.2155, 0.2155, 0.2137, 0.2155, 0.2153, 0.2155,
        0.2155, 0.2134, 0.2155, 0.2153, 0.2152, 0.2139, 0.2124, 0.2147, 0.2155,
        0.2155, 0.2139, 0.2155, 0.2147, 0.1226, 0.2155, 0.2151, 0.0989, 0.2075,
        0.2152, 0.2147, 0.2152, 0.2065, 0.2153, 0.1272, 0.2155, 0.2152, 0.2155,
        0.1252, 0.2139, 0.2153, 0.2153, 0.1191, 0.2137, 0.2123, 0.2128, 0.2151,
        0.2153, 0.2145, 0.2070, 0.2153, 0.2152, 0.2155, 0.2153, 0.1031, 0.2047,
        0.2043, 0.2155, 0.2155, 0.2139, 0.1011, 0.2153, 0.2155, 0.2155, 0.2153,
        0.0997, 0.2156, 0.2155, 0.2155, 0.2153, 0.2074, 0.2139, 0.2155, 0.2140,
        0.1030, 0.2155, 0.2155, 0.2155, 0.0988, 0.2137, 0.1953, 0.2155, 0.2152,
        0.2155, 0.2147, 0.2136, 0.1058, 0.2145, 0.1168, 0.2155, 0.2111, 0.2089,
        0.2042, 0.1927, 0.2155, 0.2079, 0.0995, 0.2155, 0.2155, 0.2080, 0.2117,
        0.1021, 0.1932, 0.2135, 0.2155, 0.2148, 0.2155, 0.2128, 0.2013, 0.2078,
        0.2152, 0.2151, 0.1083, 0.2144, 0.2101, 0.2155, 0.2155, 0.1041, 0.2147,
        0.1003, 0.2146, 0.2155, 0.2150, 0.2136, 0.2156, 0.1005, 0.2153, 0.1025,
        0.0995, 0.2155, 0.2150, 0.1058, 0.2156, 0.2152, 0.2137, 0.2156, 0.2145,
        0.2155, 0.2152, 0.2155, 0.1019, 0.2136, 0.2153, 0.2146, 0.0991, 0.2126,
        0.0986, 0.2111, 0.2155, 0.2152, 0.1904, 0.1012, 0.2024, 0.2112, 0.2118,
        0.2153, 0.2156, 0.2155, 0.1064, 0.2155, 0.2139, 0.2122, 0.2145, 0.2155,
        0.1046, 0.2155, 0.1050, 0.1007, 0.2153, 0.2137, 0.0999, 0.2155, 0.2155,
        0.2139, 0.2137, 0.2153, 0.2031, 0.2155, 0.2133, 0.2137, 0.2155, 0.2153,
        0.0994, 0.2140, 0.2117, 0.2137, 0.1013, 0.2150, 0.2153, 0.2146, 0.2153,
        0.2155, 0.2155, 0.2155, 0.1211, 0.2079, 0.0991, 0.2015, 0.2155, 0.2106,
        0.1014, 0.2151, 0.2155, 0.2135, 0.2153, 0.2155, 0.1903, 0.2155, 0.2048,
        0.2153, 0.2155, 0.2108, 0.2155, 0.1885, 0.2146, 0.2151, 0.2155, 0.2155,
        0.2131, 0.2155, 0.2155, 0.2153, 0.2155, 0.2141, 0.2112, 0.2155, 0.1022,
        0.1247, 0.2095, 0.2157, 0.1008, 0.2155, 0.2155, 0.2087, 0.2155, 0.1993,
        0.1996, 0.2128, 0.2155, 0.2094, 0.2155, 0.2155, 0.2152, 0.1014, 0.2156,
        0.2134, 0.1000, 0.2153, 0.0999, 0.2155, 0.1194, 0.2026, 0.2153, 0.2155,
        0.1158, 0.2155, 0.1096, 0.2144, 0.2153, 0.2155, 0.2155, 0.0999, 0.2129,
        0.2151, 0.2155, 0.2155, 0.2095, 0.2155, 0.2153, 0.2155, 0.2155, 0.2144,
        0.2155, 0.2153, 0.2146, 0.2155, 0.2153, 0.2124, 0.2155, 0.2152, 0.2150,
        0.1024, 0.2118, 0.2155, 0.2152, 0.2155, 0.2145, 0.2135, 0.2144, 0.2147,
        0.2155, 0.2153, 0.2151, 0.2155, 0.2092, 0.2128, 0.2155, 0.0994, 0.2122,
        0.2155, 0.0984, 0.2148, 0.2156, 0.2131, 0.2155, 0.2155, 0.2153, 0.1244,
        0.0986, 0.2080, 0.2153, 0.2147, 0.2155, 0.2062, 0.2155, 0.2112, 0.2004,
        0.2107, 0.1017, 0.1014, 0.2122, 0.2153, 0.2155, 0.2148, 0.2028, 0.2151,
        0.2140, 0.2155, 0.2150, 0.2155, 0.2037, 0.2151, 0.2155, 0.2153, 0.1066,
        0.2150, 0.1044, 0.2146, 0.2155, 0.2151, 0.2153, 0.2069, 0.2155, 0.2155,
        0.2150, 0.2155, 0.2155, 0.1019, 0.2155, 0.2155, 0.2153, 0.2151, 0.2155,
        0.2153, 0.2156, 0.2151, 0.2147, 0.2092, 0.2086, 0.1025, 0.2155, 0.1012,
        0.2155, 0.2155, 0.2155, 0.2147, 0.2155, 0.2155, 0.2029, 0.2135, 0.2155,
        0.2155, 0.2155, 0.1010, 0.0986, 0.2155, 0.2150, 0.1100, 0.2153, 0.2025,
        0.2083, 0.2057, 0.2155, 0.2155, 0.1012, 0.2155, 0.2155, 0.2126, 0.2148,
        0.0994, 0.2083, 0.0991, 0.2155, 0.2032, 0.2156, 0.2155, 0.2152, 0.2156,
        0.2018, 0.2153, 0.1003, 0.2155, 0.0991, 0.2117, 0.2151, 0.2155, 0.2002,
        0.1118, 0.2155, 0.2155, 0.2151, 0.2155, 0.2155, 0.2157, 0.2128, 0.2142,
        0.2153, 0.2102, 0.0992, 0.2073, 0.2128, 0.2155, 0.2155, 0.2153, 0.2155,
        0.2155, 0.2151, 0.2103, 0.2153, 0.2155, 0.2004, 0.2155, 0.0983, 0.2155,
        0.2153, 0.2153, 0.1005, 0.2084, 0.2150, 0.2155, 0.2155, 0.2136, 0.2155,
        0.2153, 0.2151, 0.2155, 0.1078, 0.2070, 0.0981, 0.1886, 0.1034, 0.2140,
        0.2153, 0.2152, 0.2155, 0.2125, 0.2152, 0.2008, 0.2156, 0.2151, 0.2155,
        0.2152, 0.2075, 0.2118, 0.2155, 0.2142, 0.2135, 0.2155, 0.2153, 0.0988,
        0.2141, 0.2151, 0.2130, 0.0992, 0.2148, 0.2151, 0.2144, 0.2155, 0.2155,
        0.2119, 0.2147, 0.2155, 0.2155, 0.2155, 0.2139, 0.2155, 0.2155, 0.2124,
        0.0974], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7963.4160, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6367, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6991.4629, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6328, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6216.3750, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6406, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7195.7461, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7385.0264, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8843.5664, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7828.8013, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7145.5117, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8103.7407, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5622.3940, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6970.0063, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5398.6802, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7224.0068, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5956.3965, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7362.3091, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2202, 0.2202, 0.2178, 0.2161, 0.2145, 0.1013, 0.2201, 0.2198, 0.2195,
        0.2201, 0.2063, 0.2108, 0.2109, 0.2203, 0.2202, 0.2203, 0.2168, 0.2202,
        0.1013, 0.2203, 0.2203, 0.1025, 0.1012, 0.1008, 0.2084, 0.2201, 0.2202,
        0.2170, 0.2174, 0.2203, 0.1049, 0.2196, 0.2153, 0.2202, 0.2198, 0.2196,
        0.2184, 0.2203, 0.2202, 0.2078, 0.2203, 0.1036, 0.2203, 0.2196, 0.2190,
        0.1025, 0.2013, 0.2203, 0.2203, 0.2194, 0.2203, 0.0997, 0.2112, 0.2191,
        0.2035, 0.2192, 0.1038, 0.2085, 0.2203, 0.2190, 0.2169, 0.2197, 0.2122,
        0.2196, 0.1075, 0.2203, 0.2203, 0.2190, 0.2203, 0.2203, 0.1018, 0.2203,
        0.2177, 0.2203, 0.2195, 0.1967, 0.2190, 0.1021, 0.2203, 0.2108, 0.2177,
        0.2201, 0.1025, 0.2203, 0.2196, 0.1045, 0.2202, 0.2196, 0.2194, 0.2194,
        0.1014, 0.1014, 0.2197, 0.2203, 0.2081, 0.0974, 0.1012, 0.2203, 0.1011,
        0.2202, 0.2202, 0.2203, 0.2200, 0.2191, 0.1068, 0.2203, 0.2203, 0.1012,
        0.2024, 0.1034, 0.2186, 0.1035, 0.2120, 0.2203, 0.2179, 0.2170, 0.2201,
        0.1010, 0.2183, 0.2135, 0.2203, 0.2203, 0.2203, 0.2203, 0.2059, 0.1061,
        0.2203, 0.2203, 0.2198, 0.2203, 0.2203, 0.2203, 0.2157, 0.2188, 0.0995,
        0.2179, 0.2196, 0.2085, 0.2162, 0.2203, 0.2194, 0.2102, 0.2200, 0.2197,
        0.2189, 0.2202, 0.2141, 0.2195, 0.2201, 0.1010, 0.2179, 0.2200, 0.1034,
        0.2106, 0.2203, 0.2091, 0.2202, 0.2203, 0.2186, 0.2203, 0.2202, 0.2203,
        0.2203, 0.2183, 0.2203, 0.2202, 0.2201, 0.2188, 0.2172, 0.2196, 0.2203,
        0.2202, 0.2188, 0.2203, 0.2196, 0.1047, 0.2203, 0.2200, 0.1011, 0.2123,
        0.2201, 0.2196, 0.2201, 0.2113, 0.2202, 0.1042, 0.2203, 0.2201, 0.2203,
        0.1049, 0.2188, 0.2202, 0.2202, 0.1121, 0.2185, 0.2172, 0.2177, 0.2198,
        0.2202, 0.2194, 0.2119, 0.2202, 0.2201, 0.2203, 0.2202, 0.1088, 0.2096,
        0.2092, 0.2203, 0.2203, 0.2188, 0.1027, 0.2202, 0.2203, 0.2203, 0.2202,
        0.1030, 0.2203, 0.2203, 0.2203, 0.2202, 0.2123, 0.2188, 0.2203, 0.2189,
        0.1071, 0.2203, 0.2202, 0.2203, 0.1010, 0.2186, 0.2002, 0.2203, 0.2201,
        0.2203, 0.2195, 0.2185, 0.1030, 0.2194, 0.1040, 0.2203, 0.2159, 0.2137,
        0.2091, 0.1976, 0.2203, 0.2128, 0.1030, 0.2203, 0.2203, 0.2129, 0.2166,
        0.1080, 0.1981, 0.2184, 0.2203, 0.2197, 0.2203, 0.2177, 0.2062, 0.2126,
        0.2200, 0.2200, 0.1027, 0.2191, 0.2150, 0.2203, 0.2203, 0.1050, 0.2196,
        0.1019, 0.2195, 0.2203, 0.2198, 0.2184, 0.2205, 0.1024, 0.2202, 0.1044,
        0.1016, 0.2203, 0.2198, 0.0995, 0.2205, 0.2201, 0.2186, 0.2205, 0.2194,
        0.2203, 0.2201, 0.2203, 0.0995, 0.2184, 0.2202, 0.2195, 0.1012, 0.2175,
        0.1009, 0.2159, 0.2203, 0.2201, 0.1953, 0.1045, 0.2073, 0.2161, 0.2167,
        0.2202, 0.2205, 0.2203, 0.1013, 0.2203, 0.2188, 0.2170, 0.2194, 0.2203,
        0.0996, 0.2203, 0.1030, 0.1039, 0.2202, 0.2185, 0.1017, 0.2203, 0.2203,
        0.2186, 0.2186, 0.2202, 0.2080, 0.2203, 0.2181, 0.2186, 0.2203, 0.2202,
        0.1014, 0.2189, 0.2166, 0.2186, 0.1039, 0.2198, 0.2202, 0.2195, 0.2201,
        0.2203, 0.2203, 0.2202, 0.1038, 0.2128, 0.1011, 0.2063, 0.2203, 0.2155,
        0.1023, 0.2200, 0.2203, 0.2184, 0.2202, 0.2203, 0.1952, 0.2203, 0.2097,
        0.2202, 0.2203, 0.2156, 0.2203, 0.1934, 0.2195, 0.2200, 0.2203, 0.2203,
        0.2179, 0.2203, 0.2203, 0.2202, 0.2203, 0.2190, 0.2161, 0.2203, 0.1035,
        0.1029, 0.2144, 0.2206, 0.1046, 0.2203, 0.2203, 0.2136, 0.2203, 0.2042,
        0.2045, 0.2177, 0.2203, 0.2142, 0.2203, 0.2202, 0.2201, 0.1047, 0.2205,
        0.2183, 0.1021, 0.2202, 0.1016, 0.2203, 0.1030, 0.2075, 0.2202, 0.2203,
        0.1030, 0.2203, 0.1024, 0.2192, 0.2202, 0.2203, 0.2203, 0.1027, 0.2178,
        0.2198, 0.2203, 0.2203, 0.2144, 0.2203, 0.2201, 0.2203, 0.2203, 0.2192,
        0.2203, 0.2202, 0.2195, 0.2203, 0.2202, 0.2173, 0.2202, 0.2201, 0.2197,
        0.1041, 0.2167, 0.2203, 0.2201, 0.2203, 0.2194, 0.2184, 0.2192, 0.2196,
        0.2202, 0.2202, 0.2200, 0.2203, 0.2141, 0.2177, 0.2203, 0.1014, 0.2170,
        0.2203, 0.1006, 0.2196, 0.2205, 0.2180, 0.2203, 0.2203, 0.2201, 0.1035,
        0.0994, 0.2129, 0.2202, 0.2196, 0.2203, 0.2111, 0.2203, 0.2161, 0.2053,
        0.2156, 0.1027, 0.1030, 0.2170, 0.2202, 0.2203, 0.2197, 0.2076, 0.2200,
        0.2188, 0.2203, 0.2198, 0.2203, 0.2086, 0.2200, 0.2203, 0.2202, 0.1038,
        0.2198, 0.1033, 0.2195, 0.2203, 0.2200, 0.2201, 0.2118, 0.2203, 0.2203,
        0.2198, 0.2203, 0.2203, 0.1072, 0.2203, 0.2203, 0.2202, 0.2200, 0.2203,
        0.2202, 0.2203, 0.2200, 0.2196, 0.2141, 0.2135, 0.1031, 0.2203, 0.1034,
        0.2203, 0.2203, 0.2203, 0.2195, 0.2203, 0.2203, 0.2078, 0.2184, 0.2203,
        0.2203, 0.2203, 0.0998, 0.1008, 0.2203, 0.2198, 0.1014, 0.2201, 0.2074,
        0.2131, 0.2106, 0.2203, 0.2202, 0.0994, 0.2203, 0.2203, 0.2175, 0.2197,
        0.1016, 0.2131, 0.1010, 0.2203, 0.2080, 0.2205, 0.2202, 0.2201, 0.2203,
        0.2067, 0.2202, 0.1052, 0.2203, 0.1011, 0.2166, 0.2200, 0.2203, 0.2051,
        0.1037, 0.2203, 0.2203, 0.2200, 0.2203, 0.2203, 0.2206, 0.2177, 0.2191,
        0.2202, 0.2151, 0.1011, 0.2122, 0.2177, 0.2203, 0.2203, 0.2202, 0.2203,
        0.2203, 0.2198, 0.2151, 0.2202, 0.2203, 0.2053, 0.2203, 0.1006, 0.2203,
        0.2202, 0.2202, 0.1025, 0.2133, 0.2198, 0.2203, 0.2203, 0.2185, 0.2203,
        0.2202, 0.2200, 0.2202, 0.1013, 0.2119, 0.1007, 0.1935, 0.1028, 0.2189,
        0.2202, 0.2201, 0.2203, 0.2174, 0.2201, 0.2057, 0.2203, 0.2200, 0.2203,
        0.2201, 0.2124, 0.2167, 0.2203, 0.2191, 0.2184, 0.2203, 0.2202, 0.1010,
        0.2190, 0.2200, 0.2179, 0.1011, 0.2197, 0.2200, 0.2191, 0.2203, 0.2202,
        0.2167, 0.2195, 0.2203, 0.2203, 0.2203, 0.2188, 0.2203, 0.2203, 0.2173,
        0.1017], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 18:15:00,987][train_inner][INFO] - {"epoch": 13, "update": 12.332, "loss": "4.421", "ntokens": "148750", "nsentences": "533.65", "prob_perplexity": "127.587", "code_perplexity": "107.597", "temp": "1.938", "loss_0": "4.295", "loss_1": "0.116", "loss_2": "0.01", "accuracy": "0.27042", "wps": "36124.6", "ups": "0.24", "wpb": "148750", "bsz": "533.6", "num_updates": "6400", "lr": "0.0001", "gnorm": "0.522", "loss_scale": "4", "train_wall": "794", "gb_free": "13", "wall": "25774"}
[2023-09-12 18:16:59,936][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
Parameter containing:
tensor([0.2084, 0.2084, 0.2059, 0.2041, 0.2025, 0.1273, 0.2083, 0.2079, 0.2075,
        0.2083, 0.1945, 0.1990, 0.1990, 0.2085, 0.2083, 0.2084, 0.2050, 0.2083,
        0.0939, 0.2085, 0.2084, 0.1121, 0.0938, 0.0931, 0.1964, 0.2081, 0.2084,
        0.2051, 0.2056, 0.2084, 0.0963, 0.2078, 0.2035, 0.2084, 0.2080, 0.2078,
        0.2064, 0.2085, 0.2084, 0.1958, 0.2085, 0.0958, 0.2085, 0.2076, 0.2070,
        0.0953, 0.1895, 0.2084, 0.2084, 0.2074, 0.2085, 0.1176, 0.1992, 0.2073,
        0.1915, 0.2073, 0.1254, 0.1965, 0.2084, 0.2070, 0.2051, 0.2078, 0.2003,
        0.2078, 0.1220, 0.2084, 0.2085, 0.2070, 0.2085, 0.2085, 0.0949, 0.2084,
        0.2058, 0.2085, 0.2075, 0.1847, 0.2070, 0.1002, 0.2085, 0.1990, 0.2058,
        0.2083, 0.0981, 0.2084, 0.2078, 0.1241, 0.2083, 0.2078, 0.2074, 0.2075,
        0.1162, 0.0939, 0.2078, 0.2084, 0.1962, 0.0869, 0.0941, 0.2085, 0.0938,
        0.2084, 0.2083, 0.2084, 0.2080, 0.2072, 0.0969, 0.2085, 0.2084, 0.0941,
        0.1904, 0.0996, 0.2068, 0.1243, 0.2001, 0.2084, 0.2059, 0.2052, 0.2083,
        0.0936, 0.2064, 0.2017, 0.2085, 0.2084, 0.2085, 0.2084, 0.1941, 0.0958,
        0.2085, 0.2085, 0.2080, 0.2085, 0.2084, 0.2085, 0.2037, 0.2068, 0.1052,
        0.2059, 0.2076, 0.1965, 0.2043, 0.2085, 0.2075, 0.1982, 0.2080, 0.2078,
        0.2070, 0.2084, 0.2023, 0.2075, 0.2081, 0.1176, 0.2059, 0.2080, 0.1299,
        0.1987, 0.2084, 0.1971, 0.2084, 0.2084, 0.2067, 0.2085, 0.2083, 0.2084,
        0.2085, 0.2064, 0.2085, 0.2083, 0.2081, 0.2069, 0.2053, 0.2076, 0.2085,
        0.2084, 0.2068, 0.2085, 0.2078, 0.1273, 0.2085, 0.2080, 0.0936, 0.2004,
        0.2083, 0.2078, 0.2081, 0.1995, 0.2083, 0.1327, 0.2085, 0.2081, 0.2085,
        0.1328, 0.2068, 0.2083, 0.2083, 0.1230, 0.2067, 0.2052, 0.2058, 0.2080,
        0.2083, 0.2074, 0.2000, 0.2084, 0.2081, 0.2085, 0.2084, 0.0967, 0.1976,
        0.1974, 0.2084, 0.2084, 0.2068, 0.0961, 0.2083, 0.2085, 0.2085, 0.2084,
        0.1062, 0.2085, 0.2084, 0.2084, 0.2084, 0.2003, 0.2068, 0.2085, 0.2069,
        0.0999, 0.2085, 0.2084, 0.2085, 0.0933, 0.2067, 0.1882, 0.2084, 0.2083,
        0.2084, 0.2076, 0.2065, 0.1160, 0.2075, 0.1204, 0.2085, 0.2040, 0.2018,
        0.1973, 0.1857, 0.2085, 0.2008, 0.1029, 0.2085, 0.2085, 0.2009, 0.2046,
        0.0956, 0.1862, 0.2064, 0.2085, 0.2078, 0.2085, 0.2057, 0.1943, 0.2007,
        0.2081, 0.2080, 0.1108, 0.2073, 0.2030, 0.2084, 0.2085, 0.1119, 0.2078,
        0.0952, 0.2075, 0.2085, 0.2079, 0.2065, 0.2085, 0.0948, 0.2083, 0.0968,
        0.0942, 0.2085, 0.2080, 0.1156, 0.2086, 0.2083, 0.2068, 0.2086, 0.2074,
        0.2085, 0.2081, 0.2085, 0.1087, 0.2065, 0.2083, 0.2075, 0.0940, 0.2057,
        0.0932, 0.2040, 0.2084, 0.2081, 0.1833, 0.0952, 0.1953, 0.2041, 0.2047,
        0.2083, 0.2085, 0.2084, 0.1085, 0.2084, 0.2068, 0.2052, 0.2075, 0.2085,
        0.1155, 0.2084, 0.0989, 0.1021, 0.2083, 0.2067, 0.0947, 0.2085, 0.2085,
        0.2068, 0.2067, 0.2083, 0.1960, 0.2085, 0.2062, 0.2067, 0.2085, 0.2083,
        0.0943, 0.2069, 0.2046, 0.2067, 0.0964, 0.2079, 0.2083, 0.2075, 0.2083,
        0.2084, 0.2085, 0.2084, 0.1246, 0.2008, 0.0939, 0.1945, 0.2085, 0.2035,
        0.0960, 0.2080, 0.2085, 0.2064, 0.2083, 0.2085, 0.1832, 0.2085, 0.1978,
        0.2083, 0.2084, 0.2037, 0.2084, 0.1815, 0.2075, 0.2081, 0.2085, 0.2085,
        0.2061, 0.2085, 0.2085, 0.2083, 0.2085, 0.2070, 0.2041, 0.2085, 0.0966,
        0.1365, 0.2025, 0.2086, 0.0952, 0.2085, 0.2085, 0.2017, 0.2084, 0.1923,
        0.1926, 0.2057, 0.2085, 0.2024, 0.2085, 0.2084, 0.2081, 0.0957, 0.2086,
        0.2063, 0.0951, 0.2083, 0.0944, 0.2085, 0.1246, 0.1956, 0.2083, 0.2084,
        0.1274, 0.2085, 0.1208, 0.2073, 0.2083, 0.2084, 0.2084, 0.0992, 0.2058,
        0.2080, 0.2084, 0.2085, 0.2024, 0.2085, 0.2083, 0.2085, 0.2085, 0.2073,
        0.2084, 0.2084, 0.2075, 0.2085, 0.2084, 0.2053, 0.2084, 0.2081, 0.2079,
        0.0981, 0.2048, 0.2085, 0.2083, 0.2085, 0.2075, 0.2064, 0.2074, 0.2076,
        0.2084, 0.2084, 0.2080, 0.2084, 0.2021, 0.2057, 0.2085, 0.1032, 0.2052,
        0.2085, 0.0930, 0.2078, 0.2085, 0.2061, 0.2085, 0.2084, 0.2083, 0.1306,
        0.1035, 0.2009, 0.2083, 0.2076, 0.2085, 0.1991, 0.2084, 0.2041, 0.1934,
        0.2037, 0.0957, 0.0977, 0.2051, 0.2083, 0.2084, 0.2078, 0.1957, 0.2080,
        0.2069, 0.2085, 0.2079, 0.2085, 0.1967, 0.2081, 0.2084, 0.2084, 0.1019,
        0.2079, 0.1100, 0.2076, 0.2084, 0.2080, 0.2083, 0.1998, 0.2085, 0.2084,
        0.2079, 0.2084, 0.2085, 0.0970, 0.2085, 0.2085, 0.2083, 0.2080, 0.2084,
        0.2084, 0.2085, 0.2080, 0.2078, 0.2021, 0.2015, 0.1001, 0.2085, 0.0956,
        0.2085, 0.2085, 0.2084, 0.2076, 0.2084, 0.2084, 0.1958, 0.2065, 0.2084,
        0.2084, 0.2084, 0.1066, 0.0931, 0.2085, 0.2079, 0.1243, 0.2083, 0.1954,
        0.2013, 0.1986, 0.2084, 0.2084, 0.1084, 0.2084, 0.2085, 0.2056, 0.2078,
        0.0939, 0.2012, 0.0937, 0.2085, 0.1963, 0.2085, 0.2084, 0.2081, 0.2085,
        0.1947, 0.2084, 0.0978, 0.2085, 0.0938, 0.2046, 0.2080, 0.2085, 0.1932,
        0.1217, 0.2085, 0.2085, 0.2080, 0.2085, 0.2085, 0.2086, 0.2057, 0.2072,
        0.2084, 0.2031, 0.0938, 0.2002, 0.2057, 0.2085, 0.2084, 0.2083, 0.2084,
        0.2085, 0.2080, 0.2032, 0.2083, 0.2084, 0.1934, 0.2085, 0.0929, 0.2085,
        0.2084, 0.2083, 0.0951, 0.2013, 0.2079, 0.2085, 0.2085, 0.2067, 0.2085,
        0.2083, 0.2080, 0.2084, 0.1191, 0.2000, 0.0926, 0.1816, 0.0980, 0.2070,
        0.2083, 0.2081, 0.2085, 0.2054, 0.2081, 0.1938, 0.2085, 0.2081, 0.2085,
        0.2081, 0.2004, 0.2048, 0.2085, 0.2072, 0.2064, 0.2085, 0.2083, 0.0935,
        0.2070, 0.2080, 0.2059, 0.0942, 0.2078, 0.2080, 0.2073, 0.2084, 0.2084,
        0.2048, 0.2076, 0.2084, 0.2085, 0.2084, 0.2068, 0.2084, 0.2085, 0.2054,
        0.0959], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7524.9497, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6172, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9313.8652, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6289, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5578.8296, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6328, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5651.9995, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6406, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5381.6958, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5429.9053, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7307.2583, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5368.1021, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7520.5884, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6853.0049, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2203, 0.2203, 0.2179, 0.2162, 0.2146, 0.1001, 0.2202, 0.2200, 0.2196,
        0.2202, 0.2064, 0.2109, 0.2111, 0.2205, 0.2203, 0.2205, 0.2169, 0.2203,
        0.1009, 0.2205, 0.2205, 0.1016, 0.1008, 0.1005, 0.2085, 0.2202, 0.2203,
        0.2172, 0.2175, 0.2205, 0.1030, 0.2197, 0.2155, 0.2203, 0.2200, 0.2197,
        0.2185, 0.2205, 0.2203, 0.2079, 0.2205, 0.1027, 0.2205, 0.2197, 0.2191,
        0.1017, 0.2014, 0.2205, 0.2205, 0.2195, 0.2205, 0.0994, 0.2113, 0.2192,
        0.2036, 0.2194, 0.1028, 0.2086, 0.2205, 0.2191, 0.2170, 0.2198, 0.2123,
        0.2197, 0.1041, 0.2205, 0.2205, 0.2191, 0.2205, 0.2205, 0.1013, 0.2205,
        0.2178, 0.2205, 0.2196, 0.1968, 0.2191, 0.1013, 0.2205, 0.2109, 0.2178,
        0.2202, 0.1017, 0.2205, 0.2197, 0.1034, 0.2203, 0.2197, 0.2195, 0.2195,
        0.1002, 0.1009, 0.2198, 0.2205, 0.2083, 0.0977, 0.1008, 0.2205, 0.1007,
        0.2203, 0.2203, 0.2205, 0.2201, 0.2192, 0.1043, 0.2205, 0.2205, 0.1008,
        0.2025, 0.1033, 0.2188, 0.1021, 0.2122, 0.2205, 0.2180, 0.2172, 0.2202,
        0.1007, 0.2184, 0.2136, 0.2205, 0.2205, 0.2205, 0.2205, 0.2061, 0.1054,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2205, 0.2158, 0.2189, 0.0994,
        0.2180, 0.2197, 0.2086, 0.2163, 0.2205, 0.2195, 0.2103, 0.2201, 0.2198,
        0.2191, 0.2203, 0.2142, 0.2196, 0.2202, 0.1002, 0.2180, 0.2201, 0.1029,
        0.2107, 0.2205, 0.2092, 0.2203, 0.2205, 0.2188, 0.2205, 0.2203, 0.2205,
        0.2205, 0.2184, 0.2205, 0.2203, 0.2202, 0.2189, 0.2173, 0.2197, 0.2205,
        0.2203, 0.2189, 0.2205, 0.2197, 0.1027, 0.2205, 0.2201, 0.1008, 0.2125,
        0.2202, 0.2197, 0.2202, 0.2114, 0.2203, 0.1013, 0.2205, 0.2202, 0.2205,
        0.1028, 0.2189, 0.2203, 0.2203, 0.1078, 0.2186, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2195, 0.2120, 0.2203, 0.2202, 0.2205, 0.2203, 0.1070, 0.2097,
        0.2094, 0.2205, 0.2205, 0.2189, 0.1015, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1027, 0.2205, 0.2205, 0.2205, 0.2203, 0.2122, 0.2189, 0.2205, 0.2190,
        0.1050, 0.2205, 0.2203, 0.2205, 0.1007, 0.2188, 0.2003, 0.2205, 0.2202,
        0.2205, 0.2196, 0.2186, 0.1021, 0.2195, 0.1025, 0.2205, 0.2161, 0.2139,
        0.2092, 0.1978, 0.2205, 0.2129, 0.1026, 0.2205, 0.2205, 0.2130, 0.2167,
        0.1055, 0.1982, 0.2185, 0.2205, 0.2198, 0.2205, 0.2178, 0.2063, 0.2128,
        0.2201, 0.2201, 0.1023, 0.2192, 0.2151, 0.2205, 0.2205, 0.1031, 0.2197,
        0.1015, 0.2196, 0.2205, 0.2200, 0.2185, 0.2206, 0.1017, 0.2203, 0.1026,
        0.1011, 0.2205, 0.2200, 0.0994, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2202, 0.2205, 0.0994, 0.2185, 0.2203, 0.2196, 0.1008, 0.2177,
        0.1006, 0.2161, 0.2205, 0.2202, 0.1954, 0.1031, 0.2074, 0.2162, 0.2168,
        0.2203, 0.2206, 0.2205, 0.1010, 0.2205, 0.2189, 0.2172, 0.2195, 0.2205,
        0.0994, 0.2205, 0.1021, 0.1028, 0.2203, 0.2186, 0.1011, 0.2205, 0.2205,
        0.2188, 0.2188, 0.2203, 0.2081, 0.2205, 0.2183, 0.2188, 0.2205, 0.2203,
        0.1010, 0.2190, 0.2167, 0.2188, 0.1033, 0.2200, 0.2203, 0.2196, 0.2202,
        0.2205, 0.2205, 0.2203, 0.1033, 0.2129, 0.1008, 0.2064, 0.2205, 0.2156,
        0.1016, 0.2201, 0.2205, 0.2185, 0.2203, 0.2205, 0.1953, 0.2205, 0.2098,
        0.2203, 0.2205, 0.2157, 0.2205, 0.1935, 0.2196, 0.2201, 0.2205, 0.2205,
        0.2180, 0.2205, 0.2205, 0.2203, 0.2205, 0.2191, 0.2162, 0.2205, 0.1024,
        0.1018, 0.2145, 0.2207, 0.1044, 0.2205, 0.2205, 0.2137, 0.2205, 0.2043,
        0.2046, 0.2178, 0.2205, 0.2144, 0.2205, 0.2203, 0.2202, 0.1042, 0.2206,
        0.2184, 0.1013, 0.2203, 0.1010, 0.2205, 0.1019, 0.2076, 0.2203, 0.2205,
        0.1019, 0.2205, 0.1017, 0.2194, 0.2203, 0.2205, 0.2205, 0.1023, 0.2179,
        0.2200, 0.2205, 0.2205, 0.2145, 0.2205, 0.2202, 0.2205, 0.2205, 0.2194,
        0.2205, 0.2203, 0.2196, 0.2205, 0.2203, 0.2174, 0.2203, 0.2202, 0.2198,
        0.1027, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2203, 0.2203, 0.2201, 0.2205, 0.2142, 0.2178, 0.2205, 0.1009, 0.2172,
        0.2205, 0.1004, 0.2197, 0.2206, 0.2181, 0.2205, 0.2205, 0.2202, 0.1026,
        0.0993, 0.2130, 0.2203, 0.2197, 0.2205, 0.2112, 0.2205, 0.2162, 0.2054,
        0.2157, 0.1017, 0.1023, 0.2172, 0.2203, 0.2205, 0.2198, 0.2078, 0.2201,
        0.2189, 0.2205, 0.2200, 0.2205, 0.2087, 0.2201, 0.2205, 0.2203, 0.1039,
        0.2200, 0.1025, 0.2196, 0.2205, 0.2201, 0.2202, 0.2119, 0.2205, 0.2205,
        0.2200, 0.2205, 0.2205, 0.1050, 0.2205, 0.2205, 0.2203, 0.2201, 0.2205,
        0.2203, 0.2205, 0.2201, 0.2197, 0.2142, 0.2136, 0.1022, 0.2205, 0.1027,
        0.2205, 0.2205, 0.2205, 0.2196, 0.2205, 0.2205, 0.2079, 0.2185, 0.2205,
        0.2205, 0.2205, 0.0995, 0.1006, 0.2205, 0.2200, 0.1007, 0.2202, 0.2075,
        0.2133, 0.2107, 0.2205, 0.2203, 0.0994, 0.2205, 0.2205, 0.2177, 0.2198,
        0.1010, 0.2133, 0.1006, 0.2205, 0.2081, 0.2206, 0.2203, 0.2202, 0.2205,
        0.2068, 0.2203, 0.1030, 0.2205, 0.1007, 0.2167, 0.2201, 0.2205, 0.2052,
        0.1025, 0.2205, 0.2205, 0.2201, 0.2205, 0.2205, 0.2207, 0.2178, 0.2192,
        0.2203, 0.2152, 0.1007, 0.2123, 0.2178, 0.2205, 0.2205, 0.2203, 0.2205,
        0.2205, 0.2200, 0.2152, 0.2203, 0.2205, 0.2054, 0.2205, 0.1004, 0.2205,
        0.2203, 0.2203, 0.1017, 0.2134, 0.2200, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2203, 0.2201, 0.2203, 0.1002, 0.2120, 0.1004, 0.1936, 0.1017, 0.2190,
        0.2203, 0.2202, 0.2205, 0.2175, 0.2202, 0.2058, 0.2205, 0.2201, 0.2205,
        0.2202, 0.2125, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2203, 0.1006,
        0.2191, 0.2201, 0.2180, 0.1008, 0.2198, 0.2201, 0.2192, 0.2205, 0.2203,
        0.2168, 0.2196, 0.2205, 0.2205, 0.2205, 0.2189, 0.2205, 0.2205, 0.2174,
        0.1024], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 18:28:17,947][train_inner][INFO] - {"epoch": 13, "update": 12.718, "loss": "4.355", "ntokens": "149847", "nsentences": "539.115", "prob_perplexity": "138.034", "code_perplexity": "116.869", "temp": "1.936", "loss_0": "4.233", "loss_1": "0.113", "loss_2": "0.01", "accuracy": "0.27807", "wps": "37604.7", "ups": "0.25", "wpb": "149847", "bsz": "539.1", "num_updates": "6600", "lr": "0.000103125", "gnorm": "0.522", "loss_scale": "2", "train_wall": "796", "gb_free": "13", "wall": "26570"}
loss: tensor(7715.6792, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 18:34:23,523][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2023-09-12 18:38:00,609][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 18:38:00,611][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 18:38:00,757][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2023-09-12 18:38:24,331][valid][INFO] - {"epoch": 13, "valid_loss": "4.024", "valid_ntokens": "7873.68", "valid_nsentences": "55.2525", "valid_prob_perplexity": "131.596", "valid_code_perplexity": "107.653", "valid_temp": "1.934", "valid_loss_0": "3.9", "valid_loss_1": "0.114", "valid_loss_2": "0.01", "valid_accuracy": "0.34676", "valid_wps": "33020.5", "valid_wpb": "7873.7", "valid_bsz": "55.3", "valid_num_updates": "6746", "valid_best_loss": "4.024"}
[2023-09-12 18:38:24,333][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 6746 updates
[2023-09-12 18:38:24,334][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 18:38:26,881][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 18:38:28,356][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 6746 updates, score 4.024) (writing took 4.0226932659279555 seconds)
[2023-09-12 18:38:28,356][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2023-09-12 18:38:28,356][train][INFO] - {"epoch": 13, "train_loss": "4.366", "train_ntokens": "149470", "train_nsentences": "538.401", "train_prob_perplexity": "137.216", "train_code_perplexity": "115.948", "train_temp": "1.936", "train_loss_0": "4.243", "train_loss_1": "0.113", "train_loss_2": "0.01", "train_accuracy": "0.27665", "train_wps": "36961.5", "train_ups": "0.25", "train_wpb": "149470", "train_bsz": "538.4", "train_num_updates": "6746", "train_lr": "0.000105406", "train_gnorm": "0.524", "train_loss_scale": "2", "train_train_wall": "2067", "train_gb_free": "15.5", "train_wall": "27181"}
[2023-09-12 18:38:28,358][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 18:38:28,445][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2023-09-12 18:38:28,673][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 18:38:28,676][fairseq.trainer][INFO] - begin training epoch 14
[2023-09-12 18:38:28,676][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 18:42:03,287][train_inner][INFO] - {"epoch": 14, "update": 13.104, "loss": "4.318", "ntokens": "149086", "nsentences": "541.475", "prob_perplexity": "146.849", "code_perplexity": "123.886", "temp": "1.934", "loss_0": "4.197", "loss_1": "0.111", "loss_2": "0.01", "accuracy": "0.28176", "wps": "36127.3", "ups": "0.24", "wpb": "149086", "bsz": "541.5", "num_updates": "6800", "lr": "0.00010625", "gnorm": "0.546", "loss_scale": "2", "train_wall": "796", "gb_free": "12.7", "wall": "27396"}
Parameter containing:
tensor([0.2203, 0.2203, 0.2179, 0.2162, 0.2146, 0.1002, 0.2202, 0.2200, 0.2196,
        0.2202, 0.2064, 0.2109, 0.2111, 0.2205, 0.2203, 0.2205, 0.2169, 0.2203,
        0.1010, 0.2205, 0.2205, 0.1017, 0.1008, 0.1005, 0.2085, 0.2202, 0.2203,
        0.2172, 0.2175, 0.2205, 0.1031, 0.2197, 0.2155, 0.2203, 0.2200, 0.2197,
        0.2185, 0.2205, 0.2203, 0.2079, 0.2205, 0.1028, 0.2205, 0.2197, 0.2191,
        0.1017, 0.2014, 0.2205, 0.2205, 0.2195, 0.2205, 0.0994, 0.2113, 0.2192,
        0.2036, 0.2194, 0.1028, 0.2086, 0.2205, 0.2191, 0.2170, 0.2198, 0.2123,
        0.2197, 0.1043, 0.2205, 0.2205, 0.2190, 0.2205, 0.2205, 0.1013, 0.2205,
        0.2178, 0.2205, 0.2196, 0.1968, 0.2191, 0.1014, 0.2205, 0.2109, 0.2178,
        0.2202, 0.1018, 0.2205, 0.2197, 0.1035, 0.2203, 0.2197, 0.2195, 0.2195,
        0.1003, 0.1009, 0.2198, 0.2205, 0.2083, 0.0977, 0.1009, 0.2205, 0.1007,
        0.2203, 0.2203, 0.2205, 0.2201, 0.2192, 0.1044, 0.2205, 0.2205, 0.1008,
        0.2025, 0.1032, 0.2188, 0.1022, 0.2122, 0.2205, 0.2180, 0.2172, 0.2202,
        0.1007, 0.2184, 0.2136, 0.2205, 0.2205, 0.2205, 0.2205, 0.2061, 0.1055,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2205, 0.2158, 0.2189, 0.0994,
        0.2180, 0.2197, 0.2086, 0.2163, 0.2205, 0.2195, 0.2103, 0.2201, 0.2198,
        0.2190, 0.2203, 0.2142, 0.2196, 0.2202, 0.1003, 0.2180, 0.2201, 0.1029,
        0.2107, 0.2205, 0.2092, 0.2203, 0.2205, 0.2188, 0.2205, 0.2202, 0.2205,
        0.2205, 0.2184, 0.2205, 0.2203, 0.2202, 0.2189, 0.2173, 0.2197, 0.2205,
        0.2203, 0.2189, 0.2205, 0.2197, 0.1027, 0.2205, 0.2201, 0.1008, 0.2124,
        0.2202, 0.2197, 0.2202, 0.2114, 0.2203, 0.1014, 0.2205, 0.2202, 0.2205,
        0.1030, 0.2189, 0.2203, 0.2203, 0.1082, 0.2186, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2195, 0.2120, 0.2203, 0.2202, 0.2205, 0.2203, 0.1072, 0.2097,
        0.2094, 0.2205, 0.2205, 0.2189, 0.1016, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1027, 0.2205, 0.2205, 0.2205, 0.2203, 0.2122, 0.2189, 0.2205, 0.2190,
        0.1050, 0.2205, 0.2203, 0.2205, 0.1007, 0.2188, 0.2003, 0.2205, 0.2202,
        0.2205, 0.2196, 0.2186, 0.1022, 0.2195, 0.1025, 0.2205, 0.2161, 0.2139,
        0.2092, 0.1978, 0.2205, 0.2129, 0.1026, 0.2205, 0.2205, 0.2130, 0.2167,
        0.1058, 0.1982, 0.2185, 0.2205, 0.2198, 0.2205, 0.2178, 0.2063, 0.2128,
        0.2201, 0.2201, 0.1024, 0.2192, 0.2151, 0.2205, 0.2205, 0.1033, 0.2197,
        0.1015, 0.2196, 0.2205, 0.2200, 0.2185, 0.2206, 0.1017, 0.2203, 0.1027,
        0.1011, 0.2205, 0.2200, 0.0994, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2202, 0.2205, 0.0994, 0.2185, 0.2203, 0.2196, 0.1008, 0.2177,
        0.1006, 0.2161, 0.2205, 0.2201, 0.1954, 0.1032, 0.2074, 0.2162, 0.2168,
        0.2203, 0.2206, 0.2205, 0.1010, 0.2205, 0.2189, 0.2172, 0.2195, 0.2205,
        0.0994, 0.2205, 0.1021, 0.1028, 0.2203, 0.2186, 0.1012, 0.2205, 0.2205,
        0.2188, 0.2188, 0.2203, 0.2081, 0.2205, 0.2183, 0.2188, 0.2205, 0.2203,
        0.1010, 0.2190, 0.2167, 0.2188, 0.1034, 0.2200, 0.2203, 0.2196, 0.2202,
        0.2205, 0.2205, 0.2203, 0.1033, 0.2129, 0.1008, 0.2064, 0.2205, 0.2156,
        0.1016, 0.2201, 0.2205, 0.2185, 0.2203, 0.2205, 0.1953, 0.2205, 0.2098,
        0.2203, 0.2205, 0.2157, 0.2205, 0.1935, 0.2196, 0.2201, 0.2205, 0.2205,
        0.2180, 0.2205, 0.2205, 0.2203, 0.2205, 0.2191, 0.2162, 0.2205, 0.1024,
        0.1019, 0.2145, 0.2206, 0.1044, 0.2205, 0.2205, 0.2137, 0.2205, 0.2043,
        0.2046, 0.2178, 0.2205, 0.2144, 0.2205, 0.2203, 0.2202, 0.1043, 0.2206,
        0.2184, 0.1013, 0.2203, 0.1010, 0.2205, 0.1019, 0.2076, 0.2203, 0.2205,
        0.1019, 0.2205, 0.1017, 0.2194, 0.2203, 0.2205, 0.2205, 0.1023, 0.2179,
        0.2200, 0.2205, 0.2205, 0.2145, 0.2205, 0.2202, 0.2205, 0.2205, 0.2194,
        0.2205, 0.2203, 0.2196, 0.2205, 0.2203, 0.2174, 0.2203, 0.2202, 0.2198,
        0.1027, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2203, 0.2203, 0.2201, 0.2205, 0.2142, 0.2178, 0.2205, 0.1009, 0.2172,
        0.2205, 0.1004, 0.2197, 0.2206, 0.2181, 0.2205, 0.2205, 0.2202, 0.1026,
        0.0993, 0.2130, 0.2203, 0.2197, 0.2205, 0.2112, 0.2205, 0.2162, 0.2054,
        0.2157, 0.1017, 0.1023, 0.2172, 0.2203, 0.2205, 0.2197, 0.2078, 0.2201,
        0.2189, 0.2205, 0.2200, 0.2205, 0.2087, 0.2201, 0.2205, 0.2203, 0.1039,
        0.2200, 0.1026, 0.2196, 0.2205, 0.2201, 0.2202, 0.2119, 0.2205, 0.2205,
        0.2200, 0.2205, 0.2205, 0.1051, 0.2205, 0.2205, 0.2203, 0.2201, 0.2205,
        0.2203, 0.2205, 0.2201, 0.2197, 0.2142, 0.2136, 0.1022, 0.2205, 0.1027,
        0.2205, 0.2205, 0.2205, 0.2196, 0.2205, 0.2205, 0.2079, 0.2185, 0.2205,
        0.2205, 0.2205, 0.0996, 0.1006, 0.2205, 0.2200, 0.1007, 0.2202, 0.2075,
        0.2133, 0.2107, 0.2205, 0.2203, 0.0994, 0.2205, 0.2205, 0.2177, 0.2198,
        0.1010, 0.2133, 0.1006, 0.2205, 0.2081, 0.2206, 0.2203, 0.2202, 0.2205,
        0.2068, 0.2203, 0.1031, 0.2205, 0.1008, 0.2167, 0.2201, 0.2205, 0.2052,
        0.1025, 0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2207, 0.2178, 0.2192,
        0.2203, 0.2152, 0.1008, 0.2123, 0.2178, 0.2205, 0.2205, 0.2203, 0.2205,
        0.2205, 0.2200, 0.2152, 0.2203, 0.2205, 0.2054, 0.2205, 0.1004, 0.2205,
        0.2203, 0.2203, 0.1017, 0.2134, 0.2200, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2203, 0.2201, 0.2203, 0.1002, 0.2120, 0.1004, 0.1936, 0.1017, 0.2190,
        0.2203, 0.2202, 0.2205, 0.2175, 0.2202, 0.2058, 0.2205, 0.2201, 0.2205,
        0.2202, 0.2125, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2203, 0.1006,
        0.2191, 0.2201, 0.2180, 0.1008, 0.2198, 0.2201, 0.2192, 0.2205, 0.2203,
        0.2168, 0.2196, 0.2205, 0.2205, 0.2205, 0.2189, 0.2205, 0.2205, 0.2174,
        0.1024], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(6786.5229, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5607.2729, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6898.1426, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7932.2930, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2201, 0.2201, 0.2177, 0.2158, 0.2142, 0.1025, 0.2200, 0.2197, 0.2192,
        0.2200, 0.2062, 0.2107, 0.2108, 0.2202, 0.2201, 0.2201, 0.2167, 0.2201,
        0.1017, 0.2202, 0.2201, 0.1029, 0.1014, 0.1010, 0.2081, 0.2198, 0.2201,
        0.2168, 0.2173, 0.2201, 0.1067, 0.2195, 0.2152, 0.2201, 0.2197, 0.2195,
        0.2181, 0.2202, 0.2201, 0.2075, 0.2202, 0.1043, 0.2202, 0.2195, 0.2189,
        0.1034, 0.2012, 0.2201, 0.2201, 0.2191, 0.2202, 0.1000, 0.2109, 0.2190,
        0.2032, 0.2191, 0.1049, 0.2084, 0.2201, 0.2188, 0.2168, 0.2195, 0.2120,
        0.2195, 0.1093, 0.2201, 0.2202, 0.2188, 0.2202, 0.2202, 0.1022, 0.2202,
        0.2175, 0.2202, 0.2192, 0.1965, 0.2188, 0.1027, 0.2202, 0.2107, 0.2175,
        0.2200, 0.1029, 0.2201, 0.2195, 0.1053, 0.2200, 0.2195, 0.2191, 0.2192,
        0.1028, 0.1017, 0.2196, 0.2202, 0.2080, 0.0970, 0.1014, 0.2202, 0.1013,
        0.2201, 0.2200, 0.2201, 0.2197, 0.2189, 0.1085, 0.2202, 0.2202, 0.1014,
        0.2021, 0.1035, 0.2185, 0.1038, 0.2118, 0.2201, 0.2177, 0.2169, 0.2200,
        0.1013, 0.2181, 0.2134, 0.2202, 0.2201, 0.2202, 0.2201, 0.2058, 0.1066,
        0.2202, 0.2202, 0.2197, 0.2202, 0.2201, 0.2202, 0.2155, 0.2186, 0.0995,
        0.2177, 0.2195, 0.2083, 0.2161, 0.2202, 0.2192, 0.2100, 0.2197, 0.2195,
        0.2188, 0.2201, 0.2140, 0.2192, 0.2198, 0.1013, 0.2177, 0.2197, 0.1054,
        0.2104, 0.2202, 0.2089, 0.2201, 0.2201, 0.2185, 0.2202, 0.2200, 0.2201,
        0.2202, 0.2181, 0.2202, 0.2200, 0.2198, 0.2186, 0.2170, 0.2194, 0.2202,
        0.2201, 0.2185, 0.2202, 0.2195, 0.1053, 0.2202, 0.2197, 0.1013, 0.2122,
        0.2200, 0.2195, 0.2198, 0.2112, 0.2200, 0.1072, 0.2202, 0.2198, 0.2202,
        0.1073, 0.2185, 0.2200, 0.2200, 0.1139, 0.2184, 0.2170, 0.2175, 0.2197,
        0.2201, 0.2191, 0.2117, 0.2201, 0.2198, 0.2202, 0.2201, 0.1097, 0.2094,
        0.2091, 0.2201, 0.2201, 0.2185, 0.1031, 0.2201, 0.2202, 0.2202, 0.2201,
        0.1027, 0.2202, 0.2201, 0.2201, 0.2201, 0.2120, 0.2185, 0.2202, 0.2188,
        0.1072, 0.2202, 0.2201, 0.2202, 0.1013, 0.2184, 0.2000, 0.2202, 0.2200,
        0.2201, 0.2194, 0.2183, 0.1038, 0.2192, 0.1048, 0.2202, 0.2157, 0.2135,
        0.2090, 0.1975, 0.2202, 0.2126, 0.1030, 0.2202, 0.2202, 0.2126, 0.2163,
        0.1088, 0.1980, 0.2181, 0.2202, 0.2195, 0.2202, 0.2174, 0.2061, 0.2124,
        0.2198, 0.2197, 0.1027, 0.2190, 0.2147, 0.2201, 0.2202, 0.1057, 0.2195,
        0.1021, 0.2192, 0.2202, 0.2196, 0.2183, 0.2202, 0.1029, 0.2200, 0.1066,
        0.1020, 0.2202, 0.2197, 0.0996, 0.2203, 0.2200, 0.2185, 0.2203, 0.2191,
        0.2202, 0.2198, 0.2202, 0.0994, 0.2183, 0.2200, 0.2194, 0.1015, 0.2174,
        0.1012, 0.2157, 0.2202, 0.2198, 0.1951, 0.1050, 0.2072, 0.2158, 0.2164,
        0.2200, 0.2202, 0.2201, 0.1017, 0.2201, 0.2185, 0.2169, 0.2192, 0.2202,
        0.0997, 0.2202, 0.1039, 0.1044, 0.2200, 0.2184, 0.1021, 0.2202, 0.2202,
        0.2185, 0.2184, 0.2200, 0.2078, 0.2202, 0.2179, 0.2185, 0.2202, 0.2201,
        0.1018, 0.2186, 0.2163, 0.2184, 0.1044, 0.2197, 0.2201, 0.2192, 0.2200,
        0.2202, 0.2202, 0.2201, 0.1042, 0.2125, 0.1014, 0.2062, 0.2202, 0.2152,
        0.1027, 0.2197, 0.2202, 0.2181, 0.2200, 0.2202, 0.1951, 0.2202, 0.2096,
        0.2200, 0.2201, 0.2155, 0.2201, 0.1932, 0.2192, 0.2197, 0.2202, 0.2202,
        0.2178, 0.2202, 0.2202, 0.2200, 0.2202, 0.2188, 0.2158, 0.2202, 0.1036,
        0.1044, 0.2142, 0.2203, 0.1045, 0.2202, 0.2202, 0.2134, 0.2201, 0.2040,
        0.2043, 0.2174, 0.2202, 0.2141, 0.2202, 0.2201, 0.2198, 0.1063, 0.2203,
        0.2180, 0.1025, 0.2201, 0.1020, 0.2202, 0.1041, 0.2074, 0.2200, 0.2201,
        0.1039, 0.2202, 0.1027, 0.2190, 0.2200, 0.2202, 0.2201, 0.1029, 0.2175,
        0.2197, 0.2201, 0.2202, 0.2142, 0.2202, 0.2200, 0.2202, 0.2202, 0.2190,
        0.2201, 0.2201, 0.2192, 0.2202, 0.2201, 0.2170, 0.2201, 0.2198, 0.2196,
        0.1051, 0.2166, 0.2202, 0.2200, 0.2202, 0.2192, 0.2183, 0.2191, 0.2194,
        0.2201, 0.2201, 0.2197, 0.2201, 0.2139, 0.2174, 0.2202, 0.1012, 0.2169,
        0.2202, 0.1008, 0.2195, 0.2202, 0.2178, 0.2202, 0.2202, 0.2200, 0.1096,
        0.0993, 0.2126, 0.2200, 0.2194, 0.2202, 0.2108, 0.2201, 0.2158, 0.2051,
        0.2155, 0.1033, 0.1038, 0.2168, 0.2200, 0.2202, 0.2195, 0.2074, 0.2198,
        0.2186, 0.2202, 0.2196, 0.2202, 0.2084, 0.2198, 0.2201, 0.2201, 0.1043,
        0.2196, 0.1046, 0.2194, 0.2201, 0.2197, 0.2200, 0.2115, 0.2202, 0.2201,
        0.2196, 0.2202, 0.2202, 0.1077, 0.2202, 0.2202, 0.2200, 0.2197, 0.2201,
        0.2201, 0.2202, 0.2197, 0.2195, 0.2139, 0.2133, 0.1040, 0.2202, 0.1039,
        0.2202, 0.2202, 0.2201, 0.2194, 0.2201, 0.2201, 0.2075, 0.2183, 0.2201,
        0.2201, 0.2201, 0.0998, 0.1011, 0.2202, 0.2196, 0.1021, 0.2200, 0.2072,
        0.2130, 0.2104, 0.2202, 0.2201, 0.0994, 0.2201, 0.2202, 0.2174, 0.2195,
        0.1018, 0.2130, 0.1012, 0.2202, 0.2079, 0.2202, 0.2201, 0.2200, 0.2202,
        0.2064, 0.2201, 0.1053, 0.2202, 0.1014, 0.2163, 0.2198, 0.2202, 0.2050,
        0.1039, 0.2202, 0.2202, 0.2197, 0.2202, 0.2202, 0.2203, 0.2174, 0.2189,
        0.2201, 0.2148, 0.1014, 0.2119, 0.2175, 0.2202, 0.2201, 0.2200, 0.2201,
        0.2202, 0.2197, 0.2150, 0.2200, 0.2201, 0.2051, 0.2202, 0.1008, 0.2202,
        0.2201, 0.2200, 0.1030, 0.2130, 0.2196, 0.2202, 0.2202, 0.2184, 0.2202,
        0.2200, 0.2197, 0.2201, 0.1019, 0.2117, 0.1008, 0.1932, 0.1040, 0.2188,
        0.2200, 0.2198, 0.2202, 0.2172, 0.2198, 0.2056, 0.2202, 0.2198, 0.2202,
        0.2198, 0.2122, 0.2166, 0.2202, 0.2190, 0.2181, 0.2202, 0.2200, 0.1013,
        0.2188, 0.2197, 0.2177, 0.1014, 0.2195, 0.2197, 0.2190, 0.2201, 0.2201,
        0.2166, 0.2194, 0.2201, 0.2202, 0.2201, 0.2185, 0.2201, 0.2202, 0.2172,
        0.1010], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(8796.1260, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6856.8657, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8096.5684, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5243.0669, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6662.2173, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7849.1787, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7505.7798, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5044.8755, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7920.1733, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6919.3687, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7599.6216, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7963.2715, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7347.6895, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 18:51:43,550][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2023-09-12 18:55:12,765][train_inner][INFO] - {"epoch": 14, "update": 13.489, "loss": "4.279", "ntokens": "149614", "nsentences": "539.755", "prob_perplexity": "156.375", "code_perplexity": "131.378", "temp": "1.932", "loss_0": "4.161", "loss_1": "0.109", "loss_2": "0.01", "accuracy": "0.28554", "wps": "37902.1", "ups": "0.25", "wpb": "149614", "bsz": "539.8", "num_updates": "7000", "lr": "0.000109375", "gnorm": "0.49", "loss_scale": "2", "train_wall": "788", "gb_free": "12.9", "wall": "28185"}
loss: tensor(8019.3193, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:08:30,391][train_inner][INFO] - {"epoch": 14, "update": 13.873, "loss": "4.238", "ntokens": "149731", "nsentences": "538.395", "prob_perplexity": "163.954", "code_perplexity": "137.8", "temp": "1.93", "loss_0": "4.121", "loss_1": "0.107", "loss_2": "0.01", "accuracy": "0.29022", "wps": "37544.1", "ups": "0.25", "wpb": "149731", "bsz": "538.4", "num_updates": "7200", "lr": "0.0001125", "gnorm": "0.522", "loss_scale": "2", "train_wall": "796", "gb_free": "12.6", "wall": "28983"}
Parameter containing:
tensor([0.2203, 0.2203, 0.2180, 0.2162, 0.2146, 0.0996, 0.2202, 0.2200, 0.2196,
        0.2202, 0.2065, 0.2109, 0.2111, 0.2205, 0.2203, 0.2205, 0.2170, 0.2203,
        0.1008, 0.2205, 0.2205, 0.1013, 0.1007, 0.1004, 0.2085, 0.2202, 0.2203,
        0.2172, 0.2175, 0.2203, 0.1019, 0.2198, 0.2156, 0.2203, 0.2200, 0.2197,
        0.2185, 0.2205, 0.2203, 0.2079, 0.2205, 0.1024, 0.2205, 0.2197, 0.2191,
        0.1013, 0.2014, 0.2205, 0.2205, 0.2195, 0.2205, 0.0993, 0.2113, 0.2194,
        0.2036, 0.2194, 0.1019, 0.2086, 0.2205, 0.2191, 0.2172, 0.2198, 0.2123,
        0.2197, 0.1021, 0.2205, 0.2205, 0.2191, 0.2205, 0.2205, 0.1011, 0.2205,
        0.2179, 0.2205, 0.2196, 0.1968, 0.2191, 0.1008, 0.2205, 0.2109, 0.2179,
        0.2202, 0.1012, 0.2205, 0.2197, 0.1026, 0.2203, 0.2197, 0.2195, 0.2195,
        0.0997, 0.1008, 0.2198, 0.2205, 0.2083, 0.0978, 0.1007, 0.2205, 0.1006,
        0.2205, 0.2203, 0.2205, 0.2201, 0.2192, 0.1035, 0.2205, 0.2205, 0.1006,
        0.2025, 0.1033, 0.2189, 0.1016, 0.2122, 0.2205, 0.2180, 0.2172, 0.2202,
        0.1006, 0.2185, 0.2136, 0.2205, 0.2205, 0.2206, 0.2205, 0.2061, 0.1035,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2205, 0.2158, 0.2189, 0.0993,
        0.2179, 0.2197, 0.2086, 0.2164, 0.2205, 0.2195, 0.2103, 0.2201, 0.2198,
        0.2191, 0.2203, 0.2142, 0.2196, 0.2202, 0.0997, 0.2180, 0.2201, 0.1025,
        0.2108, 0.2205, 0.2092, 0.2205, 0.2205, 0.2188, 0.2205, 0.2203, 0.2205,
        0.2205, 0.2184, 0.2205, 0.2203, 0.2202, 0.2189, 0.2174, 0.2197, 0.2205,
        0.2205, 0.2189, 0.2205, 0.2197, 0.1019, 0.2205, 0.2201, 0.1006, 0.2125,
        0.2202, 0.2197, 0.2202, 0.2115, 0.2203, 0.1000, 0.2205, 0.2202, 0.2205,
        0.1019, 0.2189, 0.2203, 0.2203, 0.1038, 0.2188, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2195, 0.2120, 0.2203, 0.2202, 0.2205, 0.2203, 0.1052, 0.2097,
        0.2094, 0.2205, 0.2205, 0.2189, 0.1012, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1021, 0.2205, 0.2205, 0.2205, 0.2203, 0.2114, 0.2189, 0.2205, 0.2190,
        0.1028, 0.2205, 0.2205, 0.2205, 0.1006, 0.2188, 0.2003, 0.2205, 0.2202,
        0.2205, 0.2196, 0.2186, 0.1019, 0.2195, 0.1022, 0.2205, 0.2161, 0.2139,
        0.2092, 0.1978, 0.2205, 0.2129, 0.1024, 0.2205, 0.2205, 0.2130, 0.2167,
        0.1031, 0.1984, 0.2185, 0.2205, 0.2198, 0.2205, 0.2178, 0.2064, 0.2128,
        0.2201, 0.2201, 0.1021, 0.2194, 0.2151, 0.2205, 0.2205, 0.1010, 0.2197,
        0.1014, 0.2196, 0.2205, 0.2200, 0.2186, 0.2206, 0.1016, 0.2203, 0.1017,
        0.1009, 0.2205, 0.2200, 0.0992, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2202, 0.2205, 0.0994, 0.2186, 0.2203, 0.2196, 0.1007, 0.2177,
        0.1005, 0.2161, 0.2205, 0.2202, 0.1954, 0.1024, 0.2074, 0.2162, 0.2168,
        0.2203, 0.2206, 0.2205, 0.1011, 0.2205, 0.2189, 0.2172, 0.2195, 0.2205,
        0.0992, 0.2205, 0.1016, 0.1021, 0.2203, 0.2188, 0.1010, 0.2205, 0.2205,
        0.2188, 0.2188, 0.2203, 0.2081, 0.2205, 0.2183, 0.2188, 0.2205, 0.2203,
        0.1008, 0.2190, 0.2167, 0.2188, 0.1028, 0.2200, 0.2203, 0.2196, 0.2202,
        0.2205, 0.2205, 0.2203, 0.1028, 0.2129, 0.1007, 0.2065, 0.2205, 0.2156,
        0.1013, 0.2201, 0.2205, 0.2185, 0.2203, 0.2205, 0.1954, 0.2205, 0.2098,
        0.2203, 0.2205, 0.2158, 0.2205, 0.1935, 0.2196, 0.2201, 0.2205, 0.2205,
        0.2181, 0.2205, 0.2205, 0.2203, 0.2205, 0.2191, 0.2162, 0.2205, 0.1016,
        0.1014, 0.2145, 0.2207, 0.1041, 0.2205, 0.2205, 0.2137, 0.2205, 0.2043,
        0.2046, 0.2178, 0.2205, 0.2144, 0.2205, 0.2203, 0.2202, 0.1033, 0.2206,
        0.2184, 0.1010, 0.2203, 0.1008, 0.2205, 0.1014, 0.2076, 0.2203, 0.2205,
        0.1014, 0.2205, 0.1013, 0.2194, 0.2203, 0.2205, 0.2205, 0.1019, 0.2179,
        0.2200, 0.2205, 0.2205, 0.2145, 0.2205, 0.2203, 0.2205, 0.2205, 0.2194,
        0.2205, 0.2203, 0.2196, 0.2205, 0.2203, 0.2174, 0.2205, 0.2202, 0.2200,
        0.1020, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2205, 0.2203, 0.2201, 0.2205, 0.2142, 0.2178, 0.2205, 0.1008, 0.2172,
        0.2205, 0.1003, 0.2197, 0.2206, 0.2181, 0.2205, 0.2205, 0.2203, 0.1022,
        0.0992, 0.2130, 0.2203, 0.2197, 0.2205, 0.2112, 0.2205, 0.2162, 0.2054,
        0.2157, 0.1015, 0.1017, 0.2172, 0.2203, 0.2205, 0.2198, 0.2078, 0.2201,
        0.2190, 0.2205, 0.2200, 0.2205, 0.2087, 0.2201, 0.2205, 0.2203, 0.1034,
        0.2200, 0.1022, 0.2196, 0.2205, 0.2201, 0.2203, 0.2119, 0.2205, 0.2205,
        0.2200, 0.2205, 0.2205, 0.1034, 0.2205, 0.2205, 0.2203, 0.2201, 0.2205,
        0.2203, 0.2205, 0.2201, 0.2197, 0.2142, 0.2136, 0.1019, 0.2205, 0.1022,
        0.2205, 0.2205, 0.2205, 0.2196, 0.2205, 0.2205, 0.2079, 0.2185, 0.2205,
        0.2205, 0.2205, 0.0994, 0.1004, 0.2205, 0.2200, 0.1004, 0.2202, 0.2075,
        0.2133, 0.2107, 0.2205, 0.2205, 0.0993, 0.2205, 0.2205, 0.2177, 0.2198,
        0.1008, 0.2133, 0.1004, 0.2205, 0.2081, 0.2206, 0.2203, 0.2202, 0.2205,
        0.2068, 0.2203, 0.1020, 0.2205, 0.1006, 0.2167, 0.2201, 0.2205, 0.2052,
        0.1019, 0.2205, 0.2205, 0.2201, 0.2205, 0.2205, 0.2207, 0.2178, 0.2192,
        0.2203, 0.2152, 0.1006, 0.2123, 0.2178, 0.2205, 0.2205, 0.2203, 0.2205,
        0.2205, 0.2201, 0.2153, 0.2203, 0.2205, 0.2054, 0.2205, 0.1003, 0.2205,
        0.2203, 0.2203, 0.1014, 0.2134, 0.2200, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2203, 0.2201, 0.2205, 0.0996, 0.2120, 0.1003, 0.1936, 0.1012, 0.2190,
        0.2203, 0.2202, 0.2205, 0.2175, 0.2202, 0.2058, 0.2206, 0.2201, 0.2205,
        0.2202, 0.2125, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2203, 0.1005,
        0.2191, 0.2201, 0.2180, 0.1006, 0.2198, 0.2201, 0.2194, 0.2205, 0.2203,
        0.2169, 0.2197, 0.2205, 0.2205, 0.2205, 0.2189, 0.2205, 0.2205, 0.2174,
        0.1022], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6110.4448, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7226.6099, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5896.0815, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8692.9775, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6360.1904, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(5148.8306, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:11:47,620][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
loss: tensor(5861.8311, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:12:48,768][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 19:12:48,769][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 19:12:48,987][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2023-09-12 19:13:12,255][valid][INFO] - {"epoch": 14, "valid_loss": "3.926", "valid_ntokens": "7894.15", "valid_nsentences": "55.2525", "valid_prob_perplexity": "155.767", "valid_code_perplexity": "128.632", "valid_temp": "1.929", "valid_loss_0": "3.807", "valid_loss_1": "0.109", "valid_loss_2": "0.01", "valid_accuracy": "0.35733", "valid_wps": "33817.4", "valid_wpb": "7894.2", "valid_bsz": "55.3", "valid_num_updates": "7265", "valid_best_loss": "3.926"}
[2023-09-12 19:13:12,257][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 7265 updates
[2023-09-12 19:13:12,258][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 19:13:14,851][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 19:13:16,311][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 7265 updates, score 3.926) (writing took 4.053951964946464 seconds)
[2023-09-12 19:13:16,311][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2023-09-12 19:13:16,312][train][INFO] - {"epoch": 14, "train_loss": "4.261", "train_ntokens": "149416", "train_nsentences": "538.428", "train_prob_perplexity": "160.157", "train_code_perplexity": "134.653", "train_temp": "1.931", "train_loss_0": "4.143", "train_loss_1": "0.108", "train_loss_2": "0.01", "train_accuracy": "0.28761", "train_wps": "37140.2", "train_ups": "0.25", "train_wpb": "149416", "train_bsz": "538.4", "train_num_updates": "7265", "train_lr": "0.000113516", "train_gnorm": "0.512", "train_loss_scale": "2", "train_train_wall": "2057", "train_gb_free": "13.2", "train_wall": "29269"}
[2023-09-12 19:13:16,314][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 19:13:16,415][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2023-09-12 19:13:16,674][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 19:13:16,677][fairseq.trainer][INFO] - begin training epoch 15
[2023-09-12 19:13:16,678][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(6481.6343, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:20:49,064][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(5306.0205, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:22:17,529][train_inner][INFO] - {"epoch": 15, "update": 14.261, "loss": "4.218", "ntokens": "148929", "nsentences": "538.015", "prob_perplexity": "171.274", "code_perplexity": "144.133", "temp": "1.928", "loss_0": "4.102", "loss_1": "0.106", "loss_2": "0.01", "accuracy": "0.29188", "wps": "36010.7", "ups": "0.24", "wpb": "148929", "bsz": "538", "num_updates": "7400", "lr": "0.000115625", "gnorm": "0.504", "loss_scale": "1", "train_wall": "798", "gb_free": "12.9", "wall": "29810"}
Parameter containing:
tensor([0.2203, 0.2203, 0.2180, 0.2162, 0.2146, 0.0996, 0.2202, 0.2200, 0.2196,
        0.2202, 0.2065, 0.2109, 0.2111, 0.2205, 0.2203, 0.2205, 0.2170, 0.2203,
        0.1008, 0.2205, 0.2205, 0.1013, 0.1007, 0.1004, 0.2085, 0.2202, 0.2203,
        0.2172, 0.2175, 0.2203, 0.1019, 0.2198, 0.2156, 0.2203, 0.2200, 0.2197,
        0.2185, 0.2205, 0.2203, 0.2079, 0.2205, 0.1025, 0.2205, 0.2197, 0.2191,
        0.1013, 0.2014, 0.2205, 0.2205, 0.2195, 0.2205, 0.0993, 0.2113, 0.2194,
        0.2036, 0.2194, 0.1019, 0.2086, 0.2205, 0.2191, 0.2172, 0.2198, 0.2123,
        0.2197, 0.1022, 0.2205, 0.2205, 0.2191, 0.2205, 0.2205, 0.1011, 0.2205,
        0.2179, 0.2205, 0.2196, 0.1968, 0.2191, 0.1008, 0.2205, 0.2109, 0.2179,
        0.2202, 0.1012, 0.2205, 0.2197, 0.1026, 0.2203, 0.2197, 0.2195, 0.2196,
        0.0997, 0.1008, 0.2198, 0.2205, 0.2083, 0.0978, 0.1007, 0.2205, 0.1006,
        0.2205, 0.2203, 0.2205, 0.2201, 0.2192, 0.1035, 0.2205, 0.2205, 0.1006,
        0.2025, 0.1033, 0.2189, 0.1016, 0.2122, 0.2205, 0.2180, 0.2172, 0.2202,
        0.1006, 0.2185, 0.2137, 0.2205, 0.2205, 0.2206, 0.2205, 0.2061, 0.1036,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2205, 0.2158, 0.2189, 0.0993,
        0.2180, 0.2197, 0.2086, 0.2164, 0.2205, 0.2195, 0.2103, 0.2201, 0.2198,
        0.2191, 0.2203, 0.2142, 0.2196, 0.2202, 0.0998, 0.2180, 0.2201, 0.1025,
        0.2108, 0.2205, 0.2092, 0.2205, 0.2205, 0.2188, 0.2205, 0.2203, 0.2205,
        0.2205, 0.2184, 0.2205, 0.2203, 0.2202, 0.2189, 0.2174, 0.2197, 0.2205,
        0.2205, 0.2189, 0.2205, 0.2197, 0.1019, 0.2205, 0.2201, 0.1006, 0.2125,
        0.2202, 0.2197, 0.2202, 0.2115, 0.2203, 0.1000, 0.2205, 0.2202, 0.2205,
        0.1020, 0.2189, 0.2203, 0.2203, 0.1039, 0.2188, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2195, 0.2120, 0.2203, 0.2202, 0.2205, 0.2203, 0.1054, 0.2097,
        0.2094, 0.2205, 0.2205, 0.2189, 0.1012, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1021, 0.2205, 0.2205, 0.2205, 0.2203, 0.2115, 0.2189, 0.2205, 0.2190,
        0.1029, 0.2205, 0.2205, 0.2205, 0.1006, 0.2188, 0.2003, 0.2205, 0.2202,
        0.2205, 0.2197, 0.2186, 0.1019, 0.2195, 0.1022, 0.2205, 0.2161, 0.2139,
        0.2094, 0.1978, 0.2205, 0.2129, 0.1024, 0.2205, 0.2205, 0.2130, 0.2167,
        0.1032, 0.1984, 0.2185, 0.2205, 0.2198, 0.2205, 0.2178, 0.2064, 0.2128,
        0.2201, 0.2201, 0.1021, 0.2194, 0.2151, 0.2205, 0.2205, 0.1011, 0.2197,
        0.1014, 0.2196, 0.2205, 0.2200, 0.2186, 0.2206, 0.1016, 0.2203, 0.1017,
        0.1009, 0.2205, 0.2200, 0.0992, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2202, 0.2205, 0.0994, 0.2186, 0.2203, 0.2196, 0.1007, 0.2177,
        0.1005, 0.2161, 0.2205, 0.2202, 0.1954, 0.1024, 0.2074, 0.2162, 0.2168,
        0.2203, 0.2206, 0.2205, 0.1011, 0.2205, 0.2189, 0.2172, 0.2195, 0.2205,
        0.0992, 0.2205, 0.1016, 0.1021, 0.2203, 0.2188, 0.1010, 0.2205, 0.2205,
        0.2189, 0.2188, 0.2203, 0.2081, 0.2205, 0.2183, 0.2188, 0.2205, 0.2203,
        0.1008, 0.2190, 0.2167, 0.2188, 0.1028, 0.2200, 0.2203, 0.2196, 0.2202,
        0.2205, 0.2205, 0.2205, 0.1029, 0.2129, 0.1007, 0.2065, 0.2205, 0.2156,
        0.1013, 0.2201, 0.2205, 0.2185, 0.2203, 0.2205, 0.1954, 0.2205, 0.2098,
        0.2203, 0.2205, 0.2158, 0.2205, 0.1935, 0.2196, 0.2201, 0.2205, 0.2205,
        0.2181, 0.2205, 0.2205, 0.2203, 0.2205, 0.2191, 0.2162, 0.2205, 0.1017,
        0.1014, 0.2145, 0.2207, 0.1041, 0.2205, 0.2205, 0.2137, 0.2205, 0.2043,
        0.2046, 0.2178, 0.2205, 0.2145, 0.2205, 0.2203, 0.2202, 0.1033, 0.2206,
        0.2184, 0.1010, 0.2203, 0.1008, 0.2205, 0.1014, 0.2076, 0.2203, 0.2205,
        0.1014, 0.2205, 0.1013, 0.2194, 0.2203, 0.2205, 0.2205, 0.1019, 0.2179,
        0.2200, 0.2205, 0.2205, 0.2145, 0.2205, 0.2203, 0.2205, 0.2205, 0.2194,
        0.2205, 0.2203, 0.2196, 0.2205, 0.2203, 0.2174, 0.2205, 0.2202, 0.2200,
        0.1020, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2205, 0.2203, 0.2201, 0.2205, 0.2142, 0.2178, 0.2205, 0.1008, 0.2172,
        0.2205, 0.1003, 0.2197, 0.2206, 0.2181, 0.2205, 0.2205, 0.2203, 0.1022,
        0.0992, 0.2130, 0.2203, 0.2197, 0.2205, 0.2112, 0.2205, 0.2162, 0.2054,
        0.2157, 0.1016, 0.1017, 0.2172, 0.2203, 0.2205, 0.2198, 0.2078, 0.2201,
        0.2190, 0.2205, 0.2200, 0.2205, 0.2087, 0.2201, 0.2205, 0.2203, 0.1035,
        0.2200, 0.1022, 0.2196, 0.2205, 0.2201, 0.2203, 0.2119, 0.2205, 0.2205,
        0.2200, 0.2205, 0.2205, 0.1035, 0.2205, 0.2205, 0.2203, 0.2201, 0.2205,
        0.2203, 0.2205, 0.2201, 0.2197, 0.2142, 0.2136, 0.1019, 0.2205, 0.1022,
        0.2205, 0.2205, 0.2205, 0.2197, 0.2205, 0.2205, 0.2079, 0.2185, 0.2205,
        0.2205, 0.2205, 0.0994, 0.1004, 0.2205, 0.2200, 0.1005, 0.2202, 0.2075,
        0.2133, 0.2107, 0.2205, 0.2205, 0.0993, 0.2205, 0.2205, 0.2177, 0.2198,
        0.1008, 0.2133, 0.1004, 0.2205, 0.2081, 0.2206, 0.2205, 0.2202, 0.2205,
        0.2068, 0.2203, 0.1020, 0.2205, 0.1006, 0.2167, 0.2201, 0.2205, 0.2053,
        0.1019, 0.2205, 0.2205, 0.2201, 0.2205, 0.2205, 0.2207, 0.2178, 0.2192,
        0.2203, 0.2152, 0.1006, 0.2123, 0.2178, 0.2205, 0.2205, 0.2203, 0.2205,
        0.2205, 0.2201, 0.2153, 0.2203, 0.2205, 0.2054, 0.2205, 0.1003, 0.2205,
        0.2203, 0.2203, 0.1014, 0.2134, 0.2200, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2203, 0.2201, 0.2205, 0.0997, 0.2120, 0.1003, 0.1936, 0.1012, 0.2190,
        0.2203, 0.2202, 0.2205, 0.2175, 0.2202, 0.2058, 0.2206, 0.2201, 0.2205,
        0.2202, 0.2125, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2203, 0.1005,
        0.2191, 0.2201, 0.2180, 0.1006, 0.2198, 0.2201, 0.2194, 0.2205, 0.2203,
        0.2169, 0.2197, 0.2205, 0.2205, 0.2205, 0.2189, 0.2205, 0.2205, 0.2174,
        0.1023], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7371.3301, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6367.1655, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7400.0879, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8292.1582, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5212.6489, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6783.5610, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7464.3633, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7520.1733, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7617.6934, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8378.7246, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6489.9717, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8266.9180, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(4790.9844, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8765.9746, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:35:23,823][train_inner][INFO] - {"epoch": 15, "update": 14.645, "loss": "4.184", "ntokens": "149817", "nsentences": "538.405", "prob_perplexity": "179.331", "code_perplexity": "151.554", "temp": "1.926", "loss_0": "4.07", "loss_1": "0.104", "loss_2": "0.01", "accuracy": "0.29549", "wps": "38107.1", "ups": "0.25", "wpb": "149817", "bsz": "538.4", "num_updates": "7600", "lr": "0.00011875", "gnorm": "0.479", "loss_scale": "1", "train_wall": "785", "gb_free": "12.6", "wall": "30596"}
Parameter containing:
tensor([0.2203, 0.2203, 0.2179, 0.2161, 0.2145, 0.0993, 0.2202, 0.2200, 0.2195,
        0.2202, 0.2064, 0.2109, 0.2111, 0.2205, 0.2203, 0.2203, 0.2169, 0.2203,
        0.1005, 0.2205, 0.2205, 0.1010, 0.1004, 0.1003, 0.2084, 0.2201, 0.2203,
        0.2170, 0.2175, 0.2202, 0.1014, 0.2197, 0.2155, 0.2203, 0.2200, 0.2197,
        0.2184, 0.2205, 0.2203, 0.2079, 0.2205, 0.1018, 0.2205, 0.2197, 0.2191,
        0.1009, 0.2014, 0.2205, 0.2203, 0.2194, 0.2205, 0.0992, 0.2113, 0.2192,
        0.2036, 0.2194, 0.1014, 0.2085, 0.2203, 0.2190, 0.2170, 0.2197, 0.2123,
        0.2197, 0.1008, 0.2203, 0.2205, 0.2190, 0.2205, 0.2205, 0.1008, 0.2205,
        0.2178, 0.2205, 0.2195, 0.1968, 0.2190, 0.1003, 0.2205, 0.2109, 0.2178,
        0.2202, 0.1008, 0.2203, 0.2197, 0.1021, 0.2202, 0.2197, 0.2194, 0.2195,
        0.0993, 0.1006, 0.2197, 0.2205, 0.2083, 0.0978, 0.1005, 0.2205, 0.1003,
        0.2203, 0.2202, 0.2203, 0.2201, 0.2192, 0.1028, 0.2205, 0.2205, 0.1004,
        0.2024, 0.1030, 0.2188, 0.1011, 0.2120, 0.2203, 0.2179, 0.2172, 0.2202,
        0.1003, 0.2184, 0.2136, 0.2205, 0.2203, 0.2205, 0.2203, 0.2061, 0.1020,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2203, 0.2205, 0.2157, 0.2189, 0.0991,
        0.2178, 0.2197, 0.2085, 0.2163, 0.2205, 0.2195, 0.2102, 0.2200, 0.2198,
        0.2190, 0.2203, 0.2142, 0.2195, 0.2201, 0.0994, 0.2180, 0.2200, 0.1022,
        0.2107, 0.2205, 0.2092, 0.2203, 0.2205, 0.2188, 0.2205, 0.2202, 0.2203,
        0.2205, 0.2184, 0.2205, 0.2202, 0.2201, 0.2189, 0.2173, 0.2197, 0.2205,
        0.2203, 0.2188, 0.2205, 0.2197, 0.1014, 0.2205, 0.2200, 0.1005, 0.2124,
        0.2202, 0.2197, 0.2201, 0.2114, 0.2203, 0.0994, 0.2205, 0.2201, 0.2205,
        0.1015, 0.2188, 0.2202, 0.2202, 0.1022, 0.2186, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2194, 0.2119, 0.2203, 0.2201, 0.2205, 0.2203, 0.1031, 0.2096,
        0.2094, 0.2203, 0.2203, 0.2188, 0.1008, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1016, 0.2205, 0.2203, 0.2203, 0.2203, 0.2014, 0.2188, 0.2205, 0.2190,
        0.1019, 0.2205, 0.2203, 0.2205, 0.1003, 0.2186, 0.2002, 0.2205, 0.2202,
        0.2203, 0.2196, 0.2185, 0.1016, 0.2195, 0.1019, 0.2205, 0.2159, 0.2137,
        0.2092, 0.1978, 0.2205, 0.2129, 0.1021, 0.2203, 0.2205, 0.2129, 0.2166,
        0.1017, 0.1982, 0.2184, 0.2205, 0.2197, 0.2205, 0.2177, 0.2063, 0.2128,
        0.2201, 0.2200, 0.1017, 0.2192, 0.2150, 0.2203, 0.2205, 0.0999, 0.2197,
        0.1014, 0.2195, 0.2205, 0.2198, 0.2185, 0.2205, 0.1013, 0.2202, 0.1012,
        0.1007, 0.2203, 0.2200, 0.0991, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2201, 0.2205, 0.0992, 0.2185, 0.2202, 0.2196, 0.1005, 0.2177,
        0.1003, 0.2159, 0.2205, 0.2201, 0.1953, 0.1016, 0.2074, 0.2162, 0.2167,
        0.2202, 0.2205, 0.2205, 0.1014, 0.2205, 0.2188, 0.2172, 0.2195, 0.2205,
        0.0991, 0.2205, 0.1011, 0.1017, 0.2202, 0.2186, 0.1008, 0.2205, 0.2205,
        0.2188, 0.2186, 0.2202, 0.2080, 0.2205, 0.2181, 0.2188, 0.2205, 0.2203,
        0.1006, 0.2189, 0.2166, 0.2186, 0.1021, 0.2200, 0.2203, 0.2195, 0.2202,
        0.2205, 0.2205, 0.2203, 0.1027, 0.2128, 0.1005, 0.2064, 0.2205, 0.2156,
        0.1009, 0.2200, 0.2205, 0.2184, 0.2202, 0.2205, 0.1953, 0.2205, 0.2098,
        0.2202, 0.2205, 0.2157, 0.2203, 0.1935, 0.2195, 0.2200, 0.2205, 0.2205,
        0.2180, 0.2205, 0.2205, 0.2202, 0.2205, 0.2190, 0.2161, 0.2205, 0.1011,
        0.1011, 0.2145, 0.2206, 0.1036, 0.2205, 0.2205, 0.2136, 0.2205, 0.2043,
        0.2046, 0.2177, 0.2205, 0.2144, 0.2205, 0.2203, 0.2201, 0.1021, 0.2206,
        0.2183, 0.1006, 0.2203, 0.1005, 0.2205, 0.1013, 0.2075, 0.2202, 0.2203,
        0.1011, 0.2205, 0.1009, 0.2192, 0.2202, 0.2205, 0.2203, 0.1014, 0.2178,
        0.2200, 0.2203, 0.2205, 0.2145, 0.2205, 0.2202, 0.2205, 0.2205, 0.2192,
        0.2203, 0.2203, 0.2195, 0.2205, 0.2203, 0.2173, 0.2203, 0.2201, 0.2198,
        0.1013, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2203, 0.2203, 0.2200, 0.2203, 0.2141, 0.2178, 0.2205, 0.1005, 0.2172,
        0.2205, 0.1001, 0.2197, 0.2205, 0.2180, 0.2205, 0.2205, 0.2202, 0.1019,
        0.0991, 0.2129, 0.2202, 0.2196, 0.2205, 0.2111, 0.2203, 0.2161, 0.2054,
        0.2157, 0.1014, 0.1013, 0.2170, 0.2202, 0.2205, 0.2197, 0.2078, 0.2201,
        0.2189, 0.2205, 0.2198, 0.2205, 0.2086, 0.2201, 0.2205, 0.2203, 0.1021,
        0.2198, 0.1018, 0.2196, 0.2203, 0.2200, 0.2202, 0.2118, 0.2205, 0.2203,
        0.2198, 0.2205, 0.2205, 0.1024, 0.2205, 0.2205, 0.2202, 0.2200, 0.2203,
        0.2203, 0.2205, 0.2200, 0.2197, 0.2141, 0.2135, 0.1016, 0.2205, 0.1017,
        0.2205, 0.2205, 0.2203, 0.2196, 0.2205, 0.2203, 0.2079, 0.2185, 0.2205,
        0.2203, 0.2203, 0.0992, 0.1002, 0.2205, 0.2198, 0.1005, 0.2202, 0.2074,
        0.2133, 0.2107, 0.2205, 0.2203, 0.0992, 0.2203, 0.2205, 0.2177, 0.2197,
        0.1005, 0.2133, 0.1002, 0.2205, 0.2081, 0.2205, 0.2203, 0.2202, 0.2205,
        0.2067, 0.2203, 0.1014, 0.2205, 0.1004, 0.2166, 0.2201, 0.2205, 0.2052,
        0.1013, 0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2206, 0.2178, 0.2191,
        0.2203, 0.2151, 0.1003, 0.2123, 0.2178, 0.2205, 0.2203, 0.2202, 0.2203,
        0.2205, 0.2200, 0.2152, 0.2202, 0.2203, 0.2053, 0.2205, 0.1001, 0.2205,
        0.2203, 0.2202, 0.1011, 0.2133, 0.2200, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2202, 0.2200, 0.2203, 0.0993, 0.2119, 0.1001, 0.1936, 0.1008, 0.2190,
        0.2202, 0.2201, 0.2205, 0.2174, 0.2201, 0.2058, 0.2205, 0.2201, 0.2205,
        0.2201, 0.2124, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2202, 0.1003,
        0.2190, 0.2200, 0.2179, 0.1003, 0.2197, 0.2200, 0.2192, 0.2203, 0.2203,
        0.2168, 0.2196, 0.2205, 0.2205, 0.2205, 0.2188, 0.2205, 0.2205, 0.2174,
        0.1014], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(5707.2148, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7356.2710, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7199.3857, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:44:28,024][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(7575.2021, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 19:47:32,592][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 19:47:32,593][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 19:47:32,662][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2023-09-12 19:47:56,395][valid][INFO] - {"epoch": 15, "valid_loss": "3.857", "valid_ntokens": "7910.23", "valid_nsentences": "55.2525", "valid_prob_perplexity": "180.796", "valid_code_perplexity": "143.781", "valid_temp": "1.924", "valid_loss_0": "3.744", "valid_loss_1": "0.103", "valid_loss_2": "0.011", "valid_accuracy": "0.3637", "valid_wps": "33208.8", "valid_wpb": "7910.2", "valid_bsz": "55.3", "valid_num_updates": "7784", "valid_best_loss": "3.857"}
[2023-09-12 19:47:56,396][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 7784 updates
[2023-09-12 19:47:56,397][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 19:47:58,862][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 19:48:00,237][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 7784 updates, score 3.857) (writing took 3.840907071949914 seconds)
[2023-09-12 19:48:00,238][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2023-09-12 19:48:00,238][train][INFO] - {"epoch": 15, "train_loss": "4.181", "train_ntokens": "149487", "train_nsentences": "538.441", "train_prob_perplexity": "180.161", "train_code_perplexity": "152.052", "train_temp": "1.926", "train_loss_0": "4.067", "train_loss_1": "0.104", "train_loss_2": "0.01", "train_accuracy": "0.29576", "train_wps": "37229.6", "train_ups": "0.25", "train_wpb": "149487", "train_bsz": "538.4", "train_num_updates": "7784", "train_lr": "0.000121625", "train_gnorm": "0.492", "train_loss_scale": "1", "train_train_wall": "2052", "train_gb_free": "13", "train_wall": "31353"}
[2023-09-12 19:48:00,241][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 19:48:00,351][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2023-09-12 19:48:00,576][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 19:48:00,579][fairseq.trainer][INFO] - begin training epoch 16
[2023-09-12 19:48:00,579][fairseq_cli.train][INFO] - Start iterating over samples
[2023-09-12 19:49:03,767][train_inner][INFO] - {"epoch": 16, "update": 15.031, "loss": "4.157", "ntokens": "149040", "nsentences": "537.265", "prob_perplexity": "186.71", "code_perplexity": "157.739", "temp": "1.924", "loss_0": "4.044", "loss_1": "0.102", "loss_2": "0.01", "accuracy": "0.29815", "wps": "36353.8", "ups": "0.24", "wpb": "149040", "bsz": "537.3", "num_updates": "7800", "lr": "0.000121875", "gnorm": "0.517", "loss_scale": "1", "train_wall": "790", "gb_free": "12.9", "wall": "31416"}
loss: tensor(7358.9995, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7322.6196, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7250.3286, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6767.0195, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7657.1357, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6086.6523, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6007.7251, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5891.7437, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6754.3921, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7057.7710, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(9254.0293, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7447.0811, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7350.9097, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7723.0073, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6532.5503, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7122.4937, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6241.1768, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7142.6099, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6868.7495, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6893.6958, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5896.7607, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7221.9448, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3721.5552, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5028.2876, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6747.6987, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7295.2002, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4480.4800, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7835.2656, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7211.6577, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6409.4575, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6971.2563, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6488.2471, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6765.3989, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 20:02:15,594][train_inner][INFO] - {"epoch": 16, "update": 15.415, "loss": "4.127", "ntokens": "149687", "nsentences": "540.13", "prob_perplexity": "194.106", "code_perplexity": "165.097", "temp": "1.923", "loss_0": "4.016", "loss_1": "0.101", "loss_2": "0.011", "accuracy": "0.30103", "wps": "37807.9", "ups": "0.25", "wpb": "149687", "bsz": "540.1", "num_updates": "8000", "lr": "0.000125", "gnorm": "0.495", "loss_scale": "2", "train_wall": "791", "gb_free": "12.9", "wall": "32208"}
[2023-09-12 20:03:07,615][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Parameter containing:
tensor([0.2203, 0.2203, 0.2179, 0.2162, 0.2146, 0.0994, 0.2202, 0.2200, 0.2196,
        0.2202, 0.2064, 0.2109, 0.2111, 0.2205, 0.2203, 0.2205, 0.2169, 0.2203,
        0.1006, 0.2205, 0.2205, 0.1010, 0.1005, 0.1003, 0.2085, 0.2202, 0.2203,
        0.2172, 0.2175, 0.2203, 0.1016, 0.2197, 0.2155, 0.2203, 0.2200, 0.2197,
        0.2185, 0.2205, 0.2203, 0.2079, 0.2205, 0.1020, 0.2205, 0.2197, 0.2191,
        0.1010, 0.2014, 0.2205, 0.2205, 0.2195, 0.2205, 0.0992, 0.2113, 0.2192,
        0.2036, 0.2194, 0.1016, 0.2086, 0.2203, 0.2191, 0.2170, 0.2198, 0.2123,
        0.2197, 0.1010, 0.2205, 0.2205, 0.2190, 0.2205, 0.2205, 0.1008, 0.2205,
        0.2178, 0.2205, 0.2196, 0.1968, 0.2191, 0.1004, 0.2205, 0.2109, 0.2178,
        0.2202, 0.1010, 0.2205, 0.2197, 0.1023, 0.2203, 0.2197, 0.2195, 0.2195,
        0.0994, 0.1006, 0.2198, 0.2205, 0.2083, 0.0978, 0.1005, 0.2205, 0.1005,
        0.2203, 0.2203, 0.2205, 0.2201, 0.2192, 0.1030, 0.2205, 0.2205, 0.1005,
        0.2025, 0.1033, 0.2188, 0.1012, 0.2122, 0.2205, 0.2180, 0.2172, 0.2202,
        0.1003, 0.2184, 0.2136, 0.2205, 0.2203, 0.2205, 0.2205, 0.2061, 0.1024,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2205, 0.2158, 0.2189, 0.0992,
        0.2179, 0.2197, 0.2086, 0.2163, 0.2205, 0.2195, 0.2103, 0.2200, 0.2198,
        0.2190, 0.2203, 0.2142, 0.2196, 0.2202, 0.0995, 0.2180, 0.2201, 0.1024,
        0.2107, 0.2205, 0.2092, 0.2203, 0.2205, 0.2188, 0.2205, 0.2202, 0.2205,
        0.2205, 0.2184, 0.2205, 0.2203, 0.2202, 0.2189, 0.2173, 0.2197, 0.2205,
        0.2203, 0.2189, 0.2205, 0.2197, 0.1016, 0.2205, 0.2200, 0.1005, 0.2124,
        0.2202, 0.2197, 0.2202, 0.2114, 0.2203, 0.0995, 0.2205, 0.2202, 0.2205,
        0.1016, 0.2189, 0.2202, 0.2203, 0.1026, 0.2186, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2195, 0.2120, 0.2203, 0.2202, 0.2205, 0.2203, 0.1038, 0.2097,
        0.2094, 0.2205, 0.2205, 0.2189, 0.1010, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1017, 0.2205, 0.2205, 0.2205, 0.2203, 0.2095, 0.2189, 0.2205, 0.2190,
        0.1024, 0.2205, 0.2203, 0.2205, 0.1004, 0.2188, 0.2003, 0.2205, 0.2202,
        0.2205, 0.2196, 0.2186, 0.1017, 0.2195, 0.1021, 0.2205, 0.2161, 0.2139,
        0.2092, 0.1978, 0.2205, 0.2129, 0.1022, 0.2205, 0.2205, 0.2130, 0.2166,
        0.1021, 0.1982, 0.2185, 0.2205, 0.2198, 0.2205, 0.2178, 0.2063, 0.2128,
        0.2201, 0.2201, 0.1017, 0.2192, 0.2151, 0.2205, 0.2205, 0.1002, 0.2197,
        0.1014, 0.2196, 0.2205, 0.2200, 0.2185, 0.2206, 0.1014, 0.2203, 0.1013,
        0.1008, 0.2205, 0.2200, 0.0992, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2202, 0.2205, 0.0992, 0.2185, 0.2203, 0.2196, 0.1005, 0.2177,
        0.1003, 0.2161, 0.2205, 0.2201, 0.1954, 0.1020, 0.2074, 0.2162, 0.2168,
        0.2203, 0.2206, 0.2205, 0.1012, 0.2205, 0.2189, 0.2172, 0.2195, 0.2205,
        0.0992, 0.2205, 0.1013, 0.1018, 0.2203, 0.2186, 0.1009, 0.2205, 0.2205,
        0.2188, 0.2188, 0.2203, 0.2081, 0.2205, 0.2183, 0.2188, 0.2205, 0.2203,
        0.1006, 0.2190, 0.2167, 0.2188, 0.1023, 0.2200, 0.2203, 0.2195, 0.2202,
        0.2205, 0.2205, 0.2203, 0.1028, 0.2129, 0.1006, 0.2064, 0.2205, 0.2156,
        0.1010, 0.2201, 0.2205, 0.2185, 0.2203, 0.2205, 0.1953, 0.2205, 0.2098,
        0.2203, 0.2205, 0.2157, 0.2205, 0.1935, 0.2196, 0.2201, 0.2205, 0.2205,
        0.2180, 0.2205, 0.2205, 0.2203, 0.2205, 0.2191, 0.2162, 0.2205, 0.1013,
        0.1013, 0.2145, 0.2206, 0.1039, 0.2205, 0.2205, 0.2137, 0.2205, 0.2043,
        0.2046, 0.2178, 0.2205, 0.2144, 0.2205, 0.2203, 0.2202, 0.1024, 0.2206,
        0.2183, 0.1007, 0.2203, 0.1006, 0.2205, 0.1013, 0.2076, 0.2203, 0.2203,
        0.1011, 0.2205, 0.1010, 0.2194, 0.2203, 0.2205, 0.2205, 0.1016, 0.2179,
        0.2200, 0.2205, 0.2205, 0.2145, 0.2205, 0.2202, 0.2205, 0.2205, 0.2192,
        0.2205, 0.2203, 0.2196, 0.2205, 0.2203, 0.2174, 0.2203, 0.2201, 0.2198,
        0.1015, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2203, 0.2203, 0.2201, 0.2205, 0.2142, 0.2178, 0.2205, 0.1006, 0.2172,
        0.2205, 0.1001, 0.2197, 0.2205, 0.2181, 0.2205, 0.2205, 0.2202, 0.1021,
        0.0992, 0.2130, 0.2203, 0.2197, 0.2205, 0.2112, 0.2205, 0.2162, 0.2054,
        0.2157, 0.1014, 0.1015, 0.2172, 0.2203, 0.2205, 0.2197, 0.2078, 0.2201,
        0.2189, 0.2205, 0.2200, 0.2205, 0.2087, 0.2201, 0.2205, 0.2203, 0.1025,
        0.2200, 0.1020, 0.2196, 0.2205, 0.2200, 0.2202, 0.2119, 0.2205, 0.2205,
        0.2200, 0.2205, 0.2205, 0.1026, 0.2205, 0.2205, 0.2202, 0.2201, 0.2205,
        0.2203, 0.2205, 0.2201, 0.2197, 0.2142, 0.2136, 0.1017, 0.2205, 0.1019,
        0.2205, 0.2205, 0.2205, 0.2196, 0.2205, 0.2205, 0.2079, 0.2185, 0.2205,
        0.2205, 0.2205, 0.0992, 0.1002, 0.2205, 0.2200, 0.1004, 0.2202, 0.2075,
        0.2133, 0.2107, 0.2205, 0.2203, 0.0992, 0.2205, 0.2205, 0.2177, 0.2198,
        0.1006, 0.2133, 0.1002, 0.2205, 0.2081, 0.2206, 0.2203, 0.2202, 0.2205,
        0.2068, 0.2203, 0.1015, 0.2205, 0.1005, 0.2167, 0.2201, 0.2205, 0.2052,
        0.1015, 0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2207, 0.2178, 0.2192,
        0.2203, 0.2152, 0.1004, 0.2123, 0.2178, 0.2205, 0.2205, 0.2202, 0.2205,
        0.2205, 0.2200, 0.2152, 0.2203, 0.2205, 0.2054, 0.2205, 0.1001, 0.2205,
        0.2203, 0.2203, 0.1013, 0.2134, 0.2200, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2203, 0.2201, 0.2203, 0.0993, 0.2120, 0.1002, 0.1936, 0.1010, 0.2190,
        0.2203, 0.2202, 0.2205, 0.2175, 0.2202, 0.2058, 0.2205, 0.2201, 0.2205,
        0.2202, 0.2125, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2203, 0.1003,
        0.2191, 0.2201, 0.2180, 0.1004, 0.2198, 0.2201, 0.2192, 0.2205, 0.2203,
        0.2168, 0.2196, 0.2205, 0.2205, 0.2205, 0.2189, 0.2205, 0.2205, 0.2174,
        0.1018], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6220.3438, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5650.8384, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7230.3418, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7418.1543, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7370.4736, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5155.6831, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7611.3276, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7177.1182, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7553.4868, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6921.0767, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3712.5872, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6820.9526, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5711.8555, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7203.5967, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6795.8071, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 20:14:55,355][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
[2023-09-12 20:15:34,297][train_inner][INFO] - {"epoch": 16, "update": 15.802, "loss": "4.097", "ntokens": "149833", "nsentences": "539.82", "prob_perplexity": "200.534", "code_perplexity": "171.111", "temp": "1.921", "loss_0": "3.987", "loss_1": "0.099", "loss_2": "0.011", "accuracy": "0.30409", "wps": "37519.2", "ups": "0.25", "wpb": "149833", "bsz": "539.8", "num_updates": "8200", "lr": "0.000128125", "gnorm": "0.484", "loss_scale": "0.5", "train_wall": "798", "gb_free": "13", "wall": "33007"}
loss: tensor(6986.6733, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6868.0620, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3094.2383, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 20:22:18,752][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 20:22:18,753][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 20:22:18,949][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2023-09-12 20:22:42,775][valid][INFO] - {"epoch": 16, "valid_loss": "3.756", "valid_ntokens": "7907.7", "valid_nsentences": "55.2525", "valid_prob_perplexity": "179.666", "valid_code_perplexity": "151.299", "valid_temp": "1.919", "valid_loss_0": "3.641", "valid_loss_1": "0.104", "valid_loss_2": "0.011", "valid_accuracy": "0.379", "valid_wps": "33093.8", "valid_wpb": "7907.7", "valid_bsz": "55.3", "valid_num_updates": "8303", "valid_best_loss": "3.756"}
[2023-09-12 20:22:42,776][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 8303 updates
[2023-09-12 20:22:42,777][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 20:22:45,286][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 20:22:46,646][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 8303 updates, score 3.756) (writing took 3.8702873639995232 seconds)
[2023-09-12 20:22:46,647][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2023-09-12 20:22:46,647][train][INFO] - {"epoch": 16, "train_loss": "4.108", "train_ntokens": "149430", "train_nsentences": "538.312", "train_prob_perplexity": "198.26", "train_code_perplexity": "168.93", "train_temp": "1.921", "train_loss_0": "3.998", "train_loss_1": "0.1", "train_loss_2": "0.011", "train_accuracy": "0.30301", "train_wps": "37171.1", "train_ups": "0.25", "train_wpb": "149430", "train_bsz": "538.3", "train_num_updates": "8303", "train_lr": "0.000129734", "train_gnorm": "0.488", "train_loss_scale": "0.5", "train_train_wall": "2055", "train_gb_free": "13.2", "train_wall": "33439"}
[2023-09-12 20:22:46,649][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 20:22:46,764][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2023-09-12 20:22:46,989][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 20:22:46,992][fairseq.trainer][INFO] - begin training epoch 17
[2023-09-12 20:22:46,992][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(6167.6060, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7125.4375, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2253, 0.2253, 0.2229, 0.2212, 0.2196, 0.1042, 0.2252, 0.2250, 0.2246,
        0.2252, 0.2115, 0.2159, 0.2161, 0.2255, 0.2253, 0.2255, 0.2219, 0.2253,
        0.1074, 0.2255, 0.2255, 0.1081, 0.1072, 0.1074, 0.2135, 0.2252, 0.2253,
        0.2222, 0.2225, 0.2252, 0.1084, 0.2247, 0.2205, 0.2253, 0.2250, 0.2247,
        0.2235, 0.2255, 0.2253, 0.2129, 0.2255, 0.1084, 0.2255, 0.2247, 0.2241,
        0.1082, 0.2065, 0.2255, 0.2255, 0.2245, 0.2255, 0.1042, 0.2163, 0.2242,
        0.2086, 0.2244, 0.1082, 0.2136, 0.2253, 0.2241, 0.2220, 0.2249, 0.2173,
        0.2247, 0.1066, 0.2255, 0.2255, 0.2240, 0.2255, 0.2255, 0.1080, 0.2255,
        0.2228, 0.2255, 0.2246, 0.2019, 0.2241, 0.1056, 0.2255, 0.2159, 0.2228,
        0.2252, 0.1079, 0.2255, 0.2247, 0.1082, 0.2253, 0.2247, 0.2245, 0.2245,
        0.1042, 0.1077, 0.2249, 0.2255, 0.2133, 0.1006, 0.1074, 0.2255, 0.1076,
        0.2253, 0.2253, 0.2255, 0.2251, 0.2242, 0.1088, 0.2255, 0.2255, 0.1076,
        0.2075, 0.1090, 0.2238, 0.1082, 0.2172, 0.2255, 0.2230, 0.2222, 0.2252,
        0.1074, 0.2234, 0.2186, 0.2255, 0.2253, 0.2255, 0.2255, 0.2111, 0.1085,
        0.2255, 0.2255, 0.2250, 0.2255, 0.2253, 0.2255, 0.2208, 0.2239, 0.1042,
        0.2228, 0.2247, 0.2136, 0.2213, 0.2255, 0.2245, 0.2153, 0.2250, 0.2249,
        0.2240, 0.2253, 0.2192, 0.2246, 0.2252, 0.1044, 0.2230, 0.2250, 0.1083,
        0.2157, 0.2255, 0.2142, 0.2253, 0.2255, 0.2238, 0.2255, 0.2252, 0.2255,
        0.2255, 0.2234, 0.2255, 0.2253, 0.2252, 0.2239, 0.2223, 0.2247, 0.2255,
        0.2253, 0.2239, 0.2255, 0.2247, 0.1083, 0.2255, 0.2250, 0.1080, 0.2175,
        0.2252, 0.2247, 0.2252, 0.2166, 0.2253, 0.1042, 0.2255, 0.2252, 0.2255,
        0.1083, 0.2239, 0.2252, 0.2253, 0.1086, 0.2236, 0.2223, 0.2228, 0.2250,
        0.2253, 0.2245, 0.2170, 0.2253, 0.2252, 0.2255, 0.2253, 0.1088, 0.2147,
        0.2144, 0.2255, 0.2253, 0.2239, 0.1080, 0.2253, 0.2255, 0.2255, 0.2253,
        0.1085, 0.2255, 0.2255, 0.2253, 0.2253, 0.1425, 0.2239, 0.2255, 0.2240,
        0.1086, 0.2255, 0.2253, 0.2255, 0.1077, 0.2238, 0.2053, 0.2255, 0.2252,
        0.2255, 0.2246, 0.2236, 0.1084, 0.2245, 0.1085, 0.2255, 0.2211, 0.2189,
        0.2142, 0.2028, 0.2255, 0.2179, 0.1087, 0.2253, 0.2255, 0.2180, 0.2217,
        0.1086, 0.2034, 0.2235, 0.2255, 0.2249, 0.2255, 0.2228, 0.2114, 0.2178,
        0.2251, 0.2251, 0.1079, 0.2242, 0.2201, 0.2255, 0.2255, 0.1047, 0.2247,
        0.1082, 0.2246, 0.2255, 0.2250, 0.2235, 0.2256, 0.1082, 0.2253, 0.1083,
        0.1078, 0.2253, 0.2250, 0.1042, 0.2256, 0.2252, 0.2238, 0.2256, 0.2245,
        0.2255, 0.2252, 0.2255, 0.1042, 0.2235, 0.2253, 0.2246, 0.1077, 0.2227,
        0.1072, 0.2211, 0.2255, 0.2251, 0.2004, 0.1083, 0.2124, 0.2212, 0.2218,
        0.2253, 0.2255, 0.2255, 0.1080, 0.2255, 0.2239, 0.2222, 0.2245, 0.2255,
        0.1041, 0.2255, 0.1082, 0.1084, 0.2253, 0.2236, 0.1081, 0.2255, 0.2255,
        0.2238, 0.2238, 0.2253, 0.2131, 0.2255, 0.2233, 0.2238, 0.2255, 0.2253,
        0.1079, 0.2240, 0.2217, 0.2238, 0.1085, 0.2250, 0.2253, 0.2245, 0.2252,
        0.2255, 0.2255, 0.2253, 0.1080, 0.2179, 0.1079, 0.2115, 0.2255, 0.2206,
        0.1080, 0.2251, 0.2255, 0.2235, 0.2253, 0.2255, 0.2004, 0.2255, 0.2148,
        0.2253, 0.2255, 0.2207, 0.2255, 0.1985, 0.2246, 0.2251, 0.2255, 0.2255,
        0.2230, 0.2255, 0.2255, 0.2253, 0.2255, 0.2241, 0.2212, 0.2255, 0.1082,
        0.1082, 0.2195, 0.2256, 0.1091, 0.2255, 0.2255, 0.2188, 0.2255, 0.2094,
        0.2096, 0.2228, 0.2255, 0.2194, 0.2255, 0.2253, 0.2252, 0.1088, 0.2256,
        0.2233, 0.1080, 0.2253, 0.1077, 0.2255, 0.1083, 0.2120, 0.2253, 0.2253,
        0.1081, 0.2255, 0.1081, 0.2244, 0.2252, 0.2255, 0.2255, 0.1083, 0.2229,
        0.2250, 0.2255, 0.2255, 0.2195, 0.2255, 0.2252, 0.2255, 0.2255, 0.2242,
        0.2255, 0.2253, 0.2246, 0.2255, 0.2253, 0.2224, 0.2253, 0.2251, 0.2249,
        0.1083, 0.2218, 0.2255, 0.2252, 0.2255, 0.2245, 0.2235, 0.2244, 0.2247,
        0.2253, 0.2253, 0.2251, 0.2255, 0.2192, 0.2228, 0.2255, 0.1076, 0.2222,
        0.2255, 0.1069, 0.2247, 0.2255, 0.2231, 0.2255, 0.2255, 0.2252, 0.1082,
        0.1042, 0.2180, 0.2253, 0.2247, 0.2255, 0.2162, 0.2255, 0.2212, 0.2104,
        0.2207, 0.1082, 0.1082, 0.2222, 0.2253, 0.2255, 0.2247, 0.2128, 0.2251,
        0.2239, 0.2255, 0.2250, 0.2255, 0.2137, 0.2251, 0.2255, 0.2253, 0.1088,
        0.2250, 0.1083, 0.2246, 0.2253, 0.2250, 0.2252, 0.2169, 0.2255, 0.2255,
        0.2250, 0.2255, 0.2255, 0.1085, 0.2255, 0.2255, 0.2252, 0.2250, 0.2255,
        0.2253, 0.2255, 0.2251, 0.2247, 0.2192, 0.2186, 0.1083, 0.2255, 0.1084,
        0.2255, 0.2255, 0.2255, 0.2246, 0.2255, 0.2255, 0.2129, 0.2235, 0.2255,
        0.2255, 0.2255, 0.1042, 0.1071, 0.2255, 0.2250, 0.1067, 0.2252, 0.2125,
        0.2183, 0.2157, 0.2255, 0.2253, 0.1042, 0.2255, 0.2255, 0.2227, 0.2249,
        0.1072, 0.2183, 0.1071, 0.2255, 0.2124, 0.2255, 0.2253, 0.2252, 0.2255,
        0.2118, 0.2253, 0.1083, 0.2255, 0.1075, 0.2217, 0.2251, 0.2255, 0.2102,
        0.1085, 0.2255, 0.2255, 0.2250, 0.2255, 0.2255, 0.2257, 0.2228, 0.2242,
        0.2253, 0.2202, 0.1075, 0.2173, 0.2228, 0.2255, 0.2255, 0.2252, 0.2255,
        0.2255, 0.2250, 0.2202, 0.2253, 0.2255, 0.2104, 0.2255, 0.1069, 0.2255,
        0.2253, 0.2253, 0.1082, 0.2184, 0.2250, 0.2255, 0.2255, 0.2236, 0.2255,
        0.2253, 0.2251, 0.2253, 0.1042, 0.2170, 0.1070, 0.1986, 0.1081, 0.2240,
        0.2253, 0.2252, 0.2255, 0.2225, 0.2252, 0.2108, 0.2255, 0.2251, 0.2255,
        0.2252, 0.2175, 0.2218, 0.2255, 0.2242, 0.2235, 0.2255, 0.2253, 0.1073,
        0.2241, 0.2251, 0.2230, 0.1075, 0.2249, 0.2251, 0.2242, 0.2255, 0.2253,
        0.2218, 0.2246, 0.2255, 0.2255, 0.2255, 0.2239, 0.2255, 0.2255, 0.2224,
        0.1085], device='cuda:2', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2203, 0.2203, 0.2179, 0.2162, 0.2146, 0.0993, 0.2202, 0.2200, 0.2195,
        0.2202, 0.2064, 0.2109, 0.2111, 0.2205, 0.2203, 0.2205, 0.2169, 0.2203,
        0.1005, 0.2205, 0.2205, 0.1010, 0.1005, 0.1003, 0.2085, 0.2202, 0.2203,
        0.2172, 0.2175, 0.2202, 0.1013, 0.2197, 0.2155, 0.2203, 0.2200, 0.2197,
        0.2184, 0.2205, 0.2203, 0.2079, 0.2205, 0.1015, 0.2205, 0.2197, 0.2191,
        0.1010, 0.2014, 0.2205, 0.2203, 0.2195, 0.2205, 0.0992, 0.2113, 0.2192,
        0.2036, 0.2194, 0.1014, 0.2086, 0.2203, 0.2190, 0.2170, 0.2197, 0.2123,
        0.2197, 0.1008, 0.2203, 0.2205, 0.2190, 0.2205, 0.2205, 0.1008, 0.2205,
        0.2178, 0.2205, 0.2196, 0.1968, 0.2190, 0.1003, 0.2205, 0.2109, 0.2178,
        0.2202, 0.1008, 0.2205, 0.2197, 0.1019, 0.2203, 0.2197, 0.2195, 0.2195,
        0.0992, 0.1006, 0.2198, 0.2205, 0.2083, 0.0978, 0.1005, 0.2205, 0.1004,
        0.2203, 0.2203, 0.2205, 0.2201, 0.2192, 0.1028, 0.2205, 0.2205, 0.1005,
        0.2025, 0.1028, 0.2188, 0.1010, 0.2122, 0.2205, 0.2180, 0.2172, 0.2202,
        0.1003, 0.2184, 0.2136, 0.2205, 0.2203, 0.2205, 0.2205, 0.2061, 0.1018,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2203, 0.2205, 0.2157, 0.2189, 0.0991,
        0.2178, 0.2197, 0.2086, 0.2163, 0.2205, 0.2195, 0.2103, 0.2200, 0.2198,
        0.2190, 0.2203, 0.2142, 0.2195, 0.2201, 0.0994, 0.2180, 0.2200, 0.1022,
        0.2107, 0.2205, 0.2092, 0.2203, 0.2205, 0.2188, 0.2205, 0.2202, 0.2205,
        0.2205, 0.2184, 0.2205, 0.2203, 0.2201, 0.2189, 0.2173, 0.2197, 0.2205,
        0.2203, 0.2189, 0.2205, 0.2197, 0.1014, 0.2205, 0.2200, 0.1006, 0.2124,
        0.2202, 0.2197, 0.2202, 0.2114, 0.2203, 0.0993, 0.2205, 0.2201, 0.2205,
        0.1014, 0.2189, 0.2202, 0.2203, 0.1021, 0.2186, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2195, 0.2119, 0.2203, 0.2202, 0.2205, 0.2203, 0.1030, 0.2097,
        0.2094, 0.2205, 0.2203, 0.2188, 0.1009, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1015, 0.2205, 0.2205, 0.2203, 0.2203, 0.1882, 0.2188, 0.2205, 0.2190,
        0.1018, 0.2205, 0.2203, 0.2205, 0.1004, 0.2186, 0.2003, 0.2205, 0.2202,
        0.2205, 0.2196, 0.2186, 0.1016, 0.2195, 0.1018, 0.2205, 0.2159, 0.2139,
        0.2092, 0.1978, 0.2205, 0.2129, 0.1021, 0.2203, 0.2205, 0.2130, 0.2166,
        0.1018, 0.1982, 0.2185, 0.2205, 0.2197, 0.2205, 0.2177, 0.2063, 0.2128,
        0.2201, 0.2200, 0.1017, 0.2192, 0.2151, 0.2203, 0.2205, 0.0998, 0.2197,
        0.1014, 0.2196, 0.2205, 0.2200, 0.2185, 0.2205, 0.1013, 0.2203, 0.1011,
        0.1007, 0.2203, 0.2200, 0.0991, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2202, 0.2205, 0.0992, 0.2185, 0.2202, 0.2196, 0.1005, 0.2177,
        0.1003, 0.2161, 0.2205, 0.2201, 0.1954, 0.1016, 0.2074, 0.2162, 0.2168,
        0.2203, 0.2205, 0.2205, 0.1016, 0.2205, 0.2189, 0.2172, 0.2195, 0.2205,
        0.0991, 0.2205, 0.1011, 0.1016, 0.2203, 0.2186, 0.1008, 0.2205, 0.2205,
        0.2188, 0.2188, 0.2202, 0.2081, 0.2205, 0.2183, 0.2188, 0.2205, 0.2203,
        0.1006, 0.2190, 0.2166, 0.2188, 0.1021, 0.2200, 0.2203, 0.2195, 0.2202,
        0.2205, 0.2205, 0.2203, 0.1024, 0.2128, 0.1006, 0.2064, 0.2205, 0.2156,
        0.1009, 0.2200, 0.2205, 0.2184, 0.2203, 0.2205, 0.1953, 0.2205, 0.2098,
        0.2203, 0.2205, 0.2157, 0.2205, 0.1935, 0.2196, 0.2201, 0.2205, 0.2205,
        0.2180, 0.2205, 0.2205, 0.2202, 0.2205, 0.2191, 0.2161, 0.2205, 0.1011,
        0.1011, 0.2145, 0.2206, 0.1033, 0.2205, 0.2205, 0.2137, 0.2205, 0.2043,
        0.2046, 0.2178, 0.2205, 0.2144, 0.2205, 0.2203, 0.2202, 0.1020, 0.2206,
        0.2183, 0.1007, 0.2203, 0.1006, 0.2205, 0.1013, 0.2072, 0.2202, 0.2203,
        0.1011, 0.2205, 0.1008, 0.2194, 0.2202, 0.2205, 0.2205, 0.1013, 0.2178,
        0.2200, 0.2205, 0.2205, 0.2145, 0.2205, 0.2202, 0.2205, 0.2205, 0.2192,
        0.2203, 0.2203, 0.2196, 0.2205, 0.2203, 0.2174, 0.2203, 0.2201, 0.2198,
        0.1014, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2203, 0.2203, 0.2201, 0.2205, 0.2142, 0.2178, 0.2205, 0.1006, 0.2172,
        0.2205, 0.1002, 0.2197, 0.2205, 0.2180, 0.2205, 0.2205, 0.2202, 0.1019,
        0.0991, 0.2130, 0.2203, 0.2197, 0.2205, 0.2112, 0.2205, 0.2162, 0.2054,
        0.2157, 0.1014, 0.1013, 0.2172, 0.2203, 0.2205, 0.2197, 0.2078, 0.2201,
        0.2189, 0.2205, 0.2200, 0.2205, 0.2086, 0.2201, 0.2205, 0.2203, 0.1021,
        0.2200, 0.1016, 0.2196, 0.2203, 0.2200, 0.2202, 0.2119, 0.2205, 0.2203,
        0.2198, 0.2205, 0.2205, 0.1022, 0.2205, 0.2205, 0.2202, 0.2200, 0.2205,
        0.2203, 0.2205, 0.2201, 0.2197, 0.2142, 0.2136, 0.1014, 0.2205, 0.1015,
        0.2205, 0.2205, 0.2203, 0.2196, 0.2205, 0.2205, 0.2079, 0.2185, 0.2205,
        0.2205, 0.2203, 0.0992, 0.1002, 0.2205, 0.2200, 0.1006, 0.2202, 0.2075,
        0.2133, 0.2107, 0.2205, 0.2203, 0.0992, 0.2205, 0.2205, 0.2177, 0.2197,
        0.1005, 0.2133, 0.1002, 0.2205, 0.2081, 0.2205, 0.2203, 0.2202, 0.2205,
        0.2068, 0.2203, 0.1014, 0.2205, 0.1004, 0.2167, 0.2201, 0.2205, 0.2052,
        0.1013, 0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2207, 0.2178, 0.2191,
        0.2203, 0.2152, 0.1004, 0.2123, 0.2178, 0.2205, 0.2203, 0.2202, 0.2205,
        0.2205, 0.2200, 0.2152, 0.2203, 0.2205, 0.2054, 0.2205, 0.1002, 0.2205,
        0.2203, 0.2203, 0.1011, 0.2134, 0.2200, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2203, 0.2201, 0.2203, 0.0992, 0.2120, 0.1002, 0.1936, 0.1009, 0.2190,
        0.2203, 0.2201, 0.2205, 0.2175, 0.2202, 0.2058, 0.2205, 0.2201, 0.2205,
        0.2202, 0.2125, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2203, 0.1003,
        0.2191, 0.2200, 0.2179, 0.1003, 0.2198, 0.2200, 0.2192, 0.2205, 0.2203,
        0.2168, 0.2196, 0.2205, 0.2205, 0.2205, 0.2189, 0.2205, 0.2205, 0.2174,
        0.1014], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7068.5264, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4392.5693, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7662.2646, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7095.0723, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6847.5005, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(7050.9302, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6893.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6935.6572, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7250.4697, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6094, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 20:29:12,764][train_inner][INFO] - {"epoch": 17, "update": 16.186, "loss": "4.077", "ntokens": "148968", "nsentences": "534.73", "prob_perplexity": "203.522", "code_perplexity": "175.004", "temp": "1.919", "loss_0": "3.967", "loss_1": "0.098", "loss_2": "0.011", "accuracy": "0.30663", "wps": "36401.8", "ups": "0.24", "wpb": "148968", "bsz": "534.7", "num_updates": "8400", "lr": "0.00013125", "gnorm": "0.484", "loss_scale": "0.5", "train_wall": "789", "gb_free": "12.6", "wall": "33825"}
Parameter containing:
tensor([0.2203, 0.2203, 0.2179, 0.2161, 0.2145, 0.0992, 0.2202, 0.2200, 0.2195,
        0.2202, 0.2064, 0.2109, 0.2111, 0.2205, 0.2203, 0.2203, 0.2169, 0.2203,
        0.1005, 0.2205, 0.2205, 0.1010, 0.1004, 0.1003, 0.2084, 0.2201, 0.2203,
        0.2170, 0.2175, 0.2202, 0.1013, 0.2197, 0.2155, 0.2203, 0.2200, 0.2197,
        0.2184, 0.2205, 0.2203, 0.2079, 0.2205, 0.1016, 0.2205, 0.2197, 0.2191,
        0.1009, 0.2014, 0.2205, 0.2203, 0.2194, 0.2205, 0.0992, 0.2113, 0.2192,
        0.2036, 0.2194, 0.1014, 0.2085, 0.2203, 0.2190, 0.2170, 0.2197, 0.2123,
        0.2197, 0.1008, 0.2203, 0.2205, 0.2190, 0.2205, 0.2205, 0.1008, 0.2205,
        0.2178, 0.2205, 0.2195, 0.1968, 0.2190, 0.1003, 0.2205, 0.2109, 0.2178,
        0.2202, 0.1008, 0.2203, 0.2197, 0.1020, 0.2202, 0.2197, 0.2194, 0.2195,
        0.0992, 0.1006, 0.2197, 0.2205, 0.2083, 0.0978, 0.1005, 0.2205, 0.1004,
        0.2203, 0.2202, 0.2203, 0.2201, 0.2192, 0.1028, 0.2205, 0.2205, 0.1004,
        0.2024, 0.1029, 0.2188, 0.1010, 0.2120, 0.2203, 0.2179, 0.2172, 0.2202,
        0.1003, 0.2184, 0.2136, 0.2205, 0.2203, 0.2205, 0.2203, 0.2061, 0.1019,
        0.2205, 0.2205, 0.2200, 0.2205, 0.2203, 0.2205, 0.2157, 0.2189, 0.0991,
        0.2178, 0.2197, 0.2085, 0.2163, 0.2205, 0.2195, 0.2102, 0.2200, 0.2198,
        0.2190, 0.2203, 0.2142, 0.2195, 0.2201, 0.0994, 0.2180, 0.2200, 0.1022,
        0.2107, 0.2205, 0.2092, 0.2203, 0.2205, 0.2188, 0.2205, 0.2202, 0.2203,
        0.2205, 0.2184, 0.2205, 0.2202, 0.2201, 0.2189, 0.2173, 0.2197, 0.2205,
        0.2203, 0.2188, 0.2205, 0.2197, 0.1014, 0.2205, 0.2200, 0.1005, 0.2124,
        0.2202, 0.2197, 0.2201, 0.2114, 0.2203, 0.0993, 0.2205, 0.2201, 0.2205,
        0.1014, 0.2188, 0.2202, 0.2202, 0.1021, 0.2186, 0.2173, 0.2178, 0.2200,
        0.2203, 0.2194, 0.2119, 0.2203, 0.2201, 0.2205, 0.2203, 0.1030, 0.2096,
        0.2094, 0.2203, 0.2203, 0.2188, 0.1008, 0.2203, 0.2205, 0.2205, 0.2203,
        0.1014, 0.2205, 0.2203, 0.2203, 0.2203, 0.1968, 0.2188, 0.2205, 0.2190,
        0.1018, 0.2205, 0.2203, 0.2205, 0.1003, 0.2186, 0.2002, 0.2205, 0.2202,
        0.2203, 0.2196, 0.2185, 0.1016, 0.2195, 0.1018, 0.2205, 0.2159, 0.2137,
        0.2092, 0.1978, 0.2205, 0.2129, 0.1021, 0.2203, 0.2205, 0.2129, 0.2166,
        0.1017, 0.1982, 0.2184, 0.2205, 0.2197, 0.2205, 0.2177, 0.2063, 0.2128,
        0.2201, 0.2200, 0.1017, 0.2192, 0.2150, 0.2203, 0.2205, 0.0998, 0.2197,
        0.1014, 0.2195, 0.2205, 0.2198, 0.2185, 0.2205, 0.1013, 0.2202, 0.1011,
        0.1006, 0.2203, 0.2200, 0.0991, 0.2206, 0.2202, 0.2188, 0.2206, 0.2195,
        0.2205, 0.2201, 0.2205, 0.0992, 0.2185, 0.2202, 0.2196, 0.1005, 0.2177,
        0.1003, 0.2159, 0.2205, 0.2201, 0.1953, 0.1015, 0.2074, 0.2162, 0.2167,
        0.2202, 0.2205, 0.2205, 0.1014, 0.2205, 0.2188, 0.2172, 0.2195, 0.2205,
        0.0991, 0.2205, 0.1011, 0.1016, 0.2202, 0.2186, 0.1008, 0.2205, 0.2205,
        0.2188, 0.2186, 0.2202, 0.2080, 0.2205, 0.2181, 0.2188, 0.2205, 0.2203,
        0.1006, 0.2189, 0.2166, 0.2186, 0.1020, 0.2200, 0.2203, 0.2195, 0.2202,
        0.2205, 0.2205, 0.2203, 0.1025, 0.2128, 0.1005, 0.2064, 0.2205, 0.2156,
        0.1009, 0.2200, 0.2205, 0.2184, 0.2202, 0.2205, 0.1953, 0.2205, 0.2098,
        0.2202, 0.2205, 0.2157, 0.2203, 0.1935, 0.2195, 0.2200, 0.2205, 0.2205,
        0.2180, 0.2205, 0.2205, 0.2202, 0.2205, 0.2190, 0.2161, 0.2205, 0.1011,
        0.1011, 0.2145, 0.2206, 0.1035, 0.2205, 0.2205, 0.2136, 0.2205, 0.2043,
        0.2046, 0.2177, 0.2205, 0.2144, 0.2205, 0.2203, 0.2201, 0.1020, 0.2206,
        0.2183, 0.1006, 0.2203, 0.1005, 0.2205, 0.1013, 0.2075, 0.2202, 0.2203,
        0.1011, 0.2205, 0.1008, 0.2192, 0.2202, 0.2205, 0.2203, 0.1013, 0.2178,
        0.2200, 0.2203, 0.2205, 0.2145, 0.2205, 0.2202, 0.2205, 0.2205, 0.2192,
        0.2203, 0.2203, 0.2195, 0.2205, 0.2203, 0.2173, 0.2203, 0.2201, 0.2198,
        0.1013, 0.2168, 0.2205, 0.2202, 0.2205, 0.2195, 0.2185, 0.2194, 0.2197,
        0.2203, 0.2203, 0.2200, 0.2203, 0.2141, 0.2178, 0.2205, 0.1005, 0.2172,
        0.2205, 0.1001, 0.2197, 0.2205, 0.2180, 0.2205, 0.2205, 0.2202, 0.1018,
        0.0991, 0.2129, 0.2202, 0.2196, 0.2205, 0.2111, 0.2203, 0.2161, 0.2054,
        0.2157, 0.1014, 0.1013, 0.2170, 0.2202, 0.2205, 0.2197, 0.2078, 0.2201,
        0.2189, 0.2205, 0.2198, 0.2205, 0.2086, 0.2201, 0.2205, 0.2203, 0.1021,
        0.2198, 0.1017, 0.2196, 0.2203, 0.2200, 0.2202, 0.2118, 0.2205, 0.2203,
        0.2198, 0.2205, 0.2205, 0.1022, 0.2205, 0.2205, 0.2202, 0.2200, 0.2203,
        0.2203, 0.2205, 0.2200, 0.2197, 0.2141, 0.2135, 0.1014, 0.2205, 0.1015,
        0.2205, 0.2205, 0.2203, 0.2196, 0.2205, 0.2203, 0.2079, 0.2185, 0.2205,
        0.2203, 0.2203, 0.0992, 0.1002, 0.2205, 0.2198, 0.1005, 0.2202, 0.2074,
        0.2133, 0.2107, 0.2205, 0.2203, 0.0991, 0.2203, 0.2205, 0.2177, 0.2197,
        0.1005, 0.2133, 0.1002, 0.2205, 0.2081, 0.2205, 0.2203, 0.2202, 0.2205,
        0.2067, 0.2203, 0.1014, 0.2205, 0.1004, 0.2166, 0.2201, 0.2205, 0.2052,
        0.1013, 0.2205, 0.2205, 0.2200, 0.2205, 0.2205, 0.2206, 0.2178, 0.2191,
        0.2203, 0.2151, 0.1003, 0.2123, 0.2178, 0.2205, 0.2203, 0.2202, 0.2203,
        0.2205, 0.2200, 0.2152, 0.2202, 0.2203, 0.2053, 0.2205, 0.1001, 0.2205,
        0.2203, 0.2202, 0.1011, 0.2133, 0.2198, 0.2205, 0.2205, 0.2186, 0.2205,
        0.2202, 0.2200, 0.2203, 0.0992, 0.2119, 0.1001, 0.1936, 0.1008, 0.2190,
        0.2202, 0.2201, 0.2205, 0.2174, 0.2201, 0.2058, 0.2205, 0.2201, 0.2205,
        0.2201, 0.2124, 0.2168, 0.2205, 0.2192, 0.2185, 0.2205, 0.2202, 0.1003,
        0.2190, 0.2200, 0.2179, 0.1003, 0.2197, 0.2200, 0.2192, 0.2203, 0.2203,
        0.2168, 0.2196, 0.2205, 0.2205, 0.2205, 0.2188, 0.2205, 0.2205, 0.2174,
        0.1014], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7360.2974, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6858.3901, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7546.9897, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6780.4927, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6723.4673, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7729.1118, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7038.9097, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7119.9478, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5981.9697, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7158.1821, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(8168.4541, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7022.4458, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7518.4561, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6866.6255, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6359.9180, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6445, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6009.5317, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2284, 0.2284, 0.2260, 0.2241, 0.2227, 0.1073, 0.2283, 0.2280, 0.2275,
        0.2283, 0.2145, 0.2190, 0.2191, 0.2285, 0.2284, 0.2284, 0.2250, 0.2284,
        0.1115, 0.2285, 0.2285, 0.1131, 0.1113, 0.1114, 0.2166, 0.2281, 0.2284,
        0.2252, 0.2256, 0.2283, 0.1130, 0.2278, 0.2235, 0.2284, 0.2280, 0.2278,
        0.2264, 0.2285, 0.2284, 0.2159, 0.2285, 0.1130, 0.2285, 0.2278, 0.2272,
        0.1123, 0.2095, 0.2285, 0.2284, 0.2275, 0.2285, 0.1073, 0.2194, 0.2273,
        0.2117, 0.2274, 0.1128, 0.2167, 0.2284, 0.2271, 0.2251, 0.2278, 0.2203,
        0.2278, 0.1102, 0.2284, 0.2285, 0.2271, 0.2285, 0.2285, 0.1121, 0.2285,
        0.2258, 0.2285, 0.2277, 0.2048, 0.2271, 0.1089, 0.2285, 0.2190, 0.2258,
        0.2283, 0.1122, 0.2285, 0.2278, 0.1132, 0.2283, 0.2278, 0.2275, 0.2275,
        0.1073, 0.1118, 0.2278, 0.2285, 0.2163, 0.1030, 0.1117, 0.2285, 0.1117,
        0.2284, 0.2284, 0.2285, 0.2281, 0.2273, 0.1136, 0.2285, 0.2285, 0.1115,
        0.2106, 0.1137, 0.2268, 0.1125, 0.2201, 0.2285, 0.2260, 0.2252, 0.2283,
        0.1113, 0.2264, 0.2217, 0.2285, 0.2284, 0.2285, 0.2285, 0.2141, 0.1130,
        0.2285, 0.2285, 0.2280, 0.2285, 0.2284, 0.2285, 0.2238, 0.2269, 0.1073,
        0.2258, 0.2278, 0.2167, 0.2244, 0.2285, 0.2275, 0.2184, 0.2280, 0.2279,
        0.2271, 0.2284, 0.2223, 0.2275, 0.2281, 0.1074, 0.2261, 0.2280, 0.1133,
        0.2188, 0.2285, 0.2173, 0.2284, 0.2285, 0.2268, 0.2285, 0.2283, 0.2285,
        0.2285, 0.2264, 0.2285, 0.2283, 0.2281, 0.2269, 0.2253, 0.2278, 0.2285,
        0.2284, 0.2269, 0.2285, 0.2278, 0.1127, 0.2285, 0.2280, 0.1124, 0.2205,
        0.2283, 0.2278, 0.2283, 0.2195, 0.2284, 0.1073, 0.2285, 0.2281, 0.2285,
        0.1128, 0.2269, 0.2283, 0.2284, 0.1130, 0.2267, 0.2253, 0.2258, 0.2280,
        0.2284, 0.2274, 0.2200, 0.2284, 0.2283, 0.2285, 0.2284, 0.1135, 0.2177,
        0.2174, 0.2284, 0.2284, 0.2268, 0.1122, 0.2284, 0.2285, 0.2285, 0.2284,
        0.1131, 0.2285, 0.2284, 0.2284, 0.2284, 0.1309, 0.2268, 0.2285, 0.2271,
        0.1133, 0.2285, 0.2284, 0.2285, 0.1116, 0.2267, 0.2084, 0.2285, 0.2283,
        0.2285, 0.2277, 0.2266, 0.1131, 0.2275, 0.1129, 0.2285, 0.2240, 0.2219,
        0.2173, 0.2058, 0.2285, 0.2209, 0.1135, 0.2284, 0.2285, 0.2211, 0.2246,
        0.1133, 0.2063, 0.2266, 0.2285, 0.2278, 0.2285, 0.2257, 0.2144, 0.2208,
        0.2281, 0.2280, 0.1128, 0.2273, 0.2231, 0.2284, 0.2285, 0.1077, 0.2278,
        0.1132, 0.2277, 0.2285, 0.2279, 0.2266, 0.2285, 0.1127, 0.2283, 0.1127,
        0.1118, 0.2284, 0.2280, 0.1073, 0.2286, 0.2283, 0.2268, 0.2286, 0.2275,
        0.2285, 0.2283, 0.2285, 0.1073, 0.2266, 0.2283, 0.2277, 0.1116, 0.2257,
        0.1112, 0.2241, 0.2285, 0.2281, 0.2035, 0.1132, 0.2155, 0.2242, 0.2247,
        0.2284, 0.2285, 0.2285, 0.1132, 0.2285, 0.2269, 0.2252, 0.2275, 0.2285,
        0.1074, 0.2285, 0.1124, 0.1129, 0.2284, 0.2267, 0.1123, 0.2285, 0.2285,
        0.2268, 0.2268, 0.2283, 0.2162, 0.2285, 0.2263, 0.2268, 0.2285, 0.2284,
        0.1121, 0.2271, 0.2246, 0.2268, 0.1133, 0.2280, 0.2284, 0.2275, 0.2283,
        0.2285, 0.2285, 0.2284, 0.1125, 0.2208, 0.1119, 0.2145, 0.2285, 0.2236,
        0.1126, 0.2280, 0.2285, 0.2264, 0.2284, 0.2285, 0.2034, 0.2285, 0.2179,
        0.2283, 0.2285, 0.2238, 0.2285, 0.2015, 0.2275, 0.2280, 0.2285, 0.2285,
        0.2261, 0.2285, 0.2285, 0.2283, 0.2285, 0.2272, 0.2241, 0.2285, 0.1121,
        0.1129, 0.2225, 0.2286, 0.1140, 0.2285, 0.2285, 0.2217, 0.2285, 0.2124,
        0.2126, 0.2258, 0.2285, 0.2224, 0.2285, 0.2284, 0.2281, 0.1135, 0.2286,
        0.2263, 0.1118, 0.2284, 0.1116, 0.2285, 0.1129, 0.2151, 0.2283, 0.2284,
        0.1126, 0.2285, 0.1121, 0.2274, 0.2283, 0.2285, 0.2285, 0.1133, 0.2258,
        0.2280, 0.2284, 0.2285, 0.2225, 0.2285, 0.2283, 0.2285, 0.2285, 0.2273,
        0.2284, 0.2284, 0.2277, 0.2285, 0.2284, 0.2253, 0.2284, 0.2281, 0.2279,
        0.1125, 0.2249, 0.2285, 0.2283, 0.2285, 0.2275, 0.2266, 0.2274, 0.2278,
        0.2284, 0.2284, 0.2281, 0.2285, 0.2223, 0.2258, 0.2285, 0.1116, 0.2252,
        0.2285, 0.1107, 0.2278, 0.2285, 0.2261, 0.2285, 0.2285, 0.2283, 0.1130,
        0.1073, 0.2211, 0.2284, 0.2277, 0.2285, 0.2191, 0.2285, 0.2242, 0.2135,
        0.2238, 0.1131, 0.1127, 0.2251, 0.2284, 0.2285, 0.2278, 0.2158, 0.2281,
        0.2269, 0.2285, 0.2280, 0.2285, 0.2167, 0.2281, 0.2285, 0.2284, 0.1132,
        0.2280, 0.1130, 0.2277, 0.2284, 0.2280, 0.2283, 0.2200, 0.2285, 0.2284,
        0.2279, 0.2285, 0.2285, 0.1129, 0.2285, 0.2285, 0.2283, 0.2280, 0.2285,
        0.2284, 0.2285, 0.2280, 0.2278, 0.2223, 0.2217, 0.1127, 0.2285, 0.1129,
        0.2285, 0.2285, 0.2284, 0.2277, 0.2285, 0.2285, 0.2159, 0.2266, 0.2285,
        0.2284, 0.2284, 0.1073, 0.1109, 0.2285, 0.2280, 0.1116, 0.2283, 0.2156,
        0.2213, 0.2188, 0.2285, 0.2284, 0.1073, 0.2285, 0.2285, 0.2257, 0.2278,
        0.1115, 0.2213, 0.1110, 0.2285, 0.2150, 0.2285, 0.2284, 0.2283, 0.2285,
        0.2148, 0.2284, 0.1133, 0.2285, 0.1116, 0.2247, 0.2281, 0.2285, 0.2133,
        0.1127, 0.2285, 0.2285, 0.2280, 0.2285, 0.2285, 0.2286, 0.2258, 0.2272,
        0.2284, 0.2233, 0.1112, 0.2203, 0.2258, 0.2285, 0.2284, 0.2283, 0.2284,
        0.2285, 0.2280, 0.2233, 0.2283, 0.2285, 0.2135, 0.2285, 0.1107, 0.2285,
        0.2284, 0.2284, 0.1125, 0.2214, 0.2280, 0.2285, 0.2285, 0.2267, 0.2285,
        0.2283, 0.2280, 0.2284, 0.1073, 0.2201, 0.1107, 0.2017, 0.1124, 0.2271,
        0.2284, 0.2281, 0.2285, 0.2256, 0.2283, 0.2139, 0.2285, 0.2281, 0.2285,
        0.2283, 0.2206, 0.2249, 0.2285, 0.2273, 0.2266, 0.2285, 0.2284, 0.1111,
        0.2271, 0.2280, 0.2260, 0.1113, 0.2279, 0.2280, 0.2273, 0.2284, 0.2284,
        0.2249, 0.2277, 0.2285, 0.2285, 0.2285, 0.2269, 0.2285, 0.2285, 0.2255,
        0.1129], device='cuda:3', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(5523.4536, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7141.3877, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6484, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6093.1714, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7907.1562, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6725.2744, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6677.9678, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4884.2598, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 20:42:31,554][train_inner][INFO] - {"epoch": 17, "update": 16.57, "loss": "4.039", "ntokens": "149976", "nsentences": "539.23", "prob_perplexity": "205.473", "code_perplexity": "181.552", "temp": "1.917", "loss_0": "3.931", "loss_1": "0.098", "loss_2": "0.011", "accuracy": "0.31195", "wps": "37550.8", "ups": "0.25", "wpb": "149976", "bsz": "539.2", "num_updates": "8600", "lr": "0.000134375", "gnorm": "0.464", "loss_scale": "1", "train_wall": "798", "gb_free": "13", "wall": "34624"}
loss: tensor(3671.2891, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4829.6016, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 20:50:20,996][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(4839.9839, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4686.0127, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7715.0122, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 20:55:43,363][train_inner][INFO] - {"epoch": 17, "update": 16.956, "loss": "4.018", "ntokens": "149619", "nsentences": "539.805", "prob_perplexity": "213.736", "code_perplexity": "186.845", "temp": "1.915", "loss_0": "3.911", "loss_1": "0.096", "loss_2": "0.011", "accuracy": "0.31392", "wps": "37791.6", "ups": "0.25", "wpb": "149618", "bsz": "539.8", "num_updates": "8800", "lr": "0.0001375", "gnorm": "0.483", "loss_scale": "1", "train_wall": "791", "gb_free": "12.7", "wall": "35416"}
Parameter containing:
tensor([0.2257, 0.2258, 0.2234, 0.2216, 0.2200, 0.1047, 0.2257, 0.2253, 0.2250,
        0.2256, 0.2119, 0.2164, 0.2166, 0.2260, 0.2257, 0.2258, 0.2224, 0.2257,
        0.1080, 0.2260, 0.2258, 0.1088, 0.1079, 0.1080, 0.2139, 0.2256, 0.2258,
        0.2225, 0.2230, 0.2257, 0.1091, 0.2252, 0.2209, 0.2258, 0.2255, 0.2252,
        0.2239, 0.2260, 0.2258, 0.2133, 0.2260, 0.1091, 0.2260, 0.2251, 0.2245,
        0.1087, 0.2069, 0.2258, 0.2258, 0.2249, 0.2260, 0.1046, 0.2167, 0.2247,
        0.2091, 0.2247, 0.1089, 0.2140, 0.2258, 0.2245, 0.2225, 0.2252, 0.2178,
        0.2252, 0.1072, 0.2258, 0.2260, 0.2245, 0.2260, 0.2258, 0.1086, 0.2258,
        0.2233, 0.2258, 0.2250, 0.2023, 0.2245, 0.1060, 0.2260, 0.2164, 0.2233,
        0.2256, 0.1086, 0.2258, 0.2252, 0.1089, 0.2257, 0.2252, 0.2249, 0.2250,
        0.1046, 0.1083, 0.2252, 0.2258, 0.2137, 0.1010, 0.1080, 0.2260, 0.1082,
        0.2258, 0.2257, 0.2258, 0.2255, 0.2246, 0.1094, 0.2260, 0.2258, 0.1082,
        0.2079, 0.1096, 0.2242, 0.1088, 0.2175, 0.2258, 0.2234, 0.2227, 0.2256,
        0.1080, 0.2239, 0.2191, 0.2260, 0.2258, 0.2260, 0.2258, 0.2115, 0.1091,
        0.2258, 0.2260, 0.2255, 0.2258, 0.2258, 0.2260, 0.2212, 0.2242, 0.1046,
        0.2233, 0.2251, 0.2140, 0.2218, 0.2260, 0.2250, 0.2157, 0.2255, 0.2252,
        0.2245, 0.2257, 0.2197, 0.2250, 0.2256, 0.1048, 0.2234, 0.2255, 0.1089,
        0.2162, 0.2258, 0.2146, 0.2258, 0.2258, 0.2241, 0.2260, 0.2257, 0.2258,
        0.2258, 0.2239, 0.2260, 0.2257, 0.2256, 0.2242, 0.2228, 0.2251, 0.2258,
        0.2258, 0.2242, 0.2258, 0.2251, 0.1089, 0.2258, 0.2255, 0.1086, 0.2179,
        0.2256, 0.2252, 0.2256, 0.2169, 0.2257, 0.1047, 0.2260, 0.2256, 0.2260,
        0.1089, 0.2242, 0.2257, 0.2257, 0.1093, 0.2241, 0.2227, 0.2233, 0.2255,
        0.2257, 0.2249, 0.2174, 0.2258, 0.2256, 0.2260, 0.2257, 0.1094, 0.2151,
        0.2148, 0.2258, 0.2258, 0.2242, 0.1087, 0.2257, 0.2260, 0.2260, 0.2258,
        0.1091, 0.2260, 0.2258, 0.2258, 0.2258, 0.1406, 0.2242, 0.2260, 0.2244,
        0.1093, 0.2260, 0.2258, 0.2260, 0.1083, 0.2241, 0.2057, 0.2258, 0.2257,
        0.2258, 0.2251, 0.2240, 0.1091, 0.2250, 0.1091, 0.2258, 0.2214, 0.2192,
        0.2147, 0.2032, 0.2258, 0.2184, 0.1093, 0.2258, 0.2260, 0.2184, 0.2220,
        0.1093, 0.2037, 0.2239, 0.2260, 0.2252, 0.2258, 0.2231, 0.2118, 0.2181,
        0.2256, 0.2255, 0.1085, 0.2247, 0.2205, 0.2258, 0.2260, 0.1051, 0.2252,
        0.1089, 0.2250, 0.2260, 0.2253, 0.2240, 0.2260, 0.1089, 0.2257, 0.1090,
        0.1083, 0.2258, 0.2253, 0.1046, 0.2261, 0.2256, 0.2242, 0.2260, 0.2249,
        0.2260, 0.2256, 0.2260, 0.1046, 0.2240, 0.2257, 0.2250, 0.1083, 0.2231,
        0.1078, 0.2214, 0.2258, 0.2256, 0.2008, 0.1090, 0.2129, 0.2216, 0.2222,
        0.2257, 0.2260, 0.2258, 0.1087, 0.2258, 0.2242, 0.2227, 0.2250, 0.2258,
        0.1046, 0.2258, 0.1088, 0.1091, 0.2257, 0.2241, 0.1088, 0.2260, 0.2258,
        0.2242, 0.2241, 0.2257, 0.2135, 0.2258, 0.2236, 0.2241, 0.2260, 0.2257,
        0.1086, 0.2244, 0.2220, 0.2241, 0.1091, 0.2253, 0.2257, 0.2249, 0.2257,
        0.2258, 0.2260, 0.2258, 0.1086, 0.2183, 0.1085, 0.2119, 0.2260, 0.2209,
        0.1087, 0.2255, 0.2260, 0.2239, 0.2257, 0.2260, 0.2008, 0.2260, 0.2153,
        0.2257, 0.2258, 0.2212, 0.2258, 0.1990, 0.2250, 0.2255, 0.2260, 0.2260,
        0.2235, 0.2260, 0.2260, 0.2257, 0.2260, 0.2245, 0.2216, 0.2260, 0.1088,
        0.1089, 0.2198, 0.2261, 0.1098, 0.2260, 0.2260, 0.2191, 0.2258, 0.2098,
        0.2101, 0.2231, 0.2258, 0.2198, 0.2260, 0.2258, 0.2256, 0.1094, 0.2261,
        0.2238, 0.1084, 0.2257, 0.1083, 0.2260, 0.1089, 0.2125, 0.2257, 0.2258,
        0.1088, 0.2260, 0.1087, 0.2247, 0.2257, 0.2258, 0.2258, 0.1089, 0.2233,
        0.2255, 0.2258, 0.2260, 0.2198, 0.2260, 0.2257, 0.2260, 0.2260, 0.2247,
        0.2258, 0.2257, 0.2250, 0.2260, 0.2257, 0.2228, 0.2258, 0.2256, 0.2253,
        0.1089, 0.2223, 0.2260, 0.2257, 0.2260, 0.2250, 0.2239, 0.2247, 0.2251,
        0.2258, 0.2257, 0.2255, 0.2258, 0.2196, 0.2231, 0.2260, 0.1080, 0.2227,
        0.2258, 0.1075, 0.2252, 0.2260, 0.2235, 0.2260, 0.2258, 0.2257, 0.1088,
        0.1046, 0.2184, 0.2257, 0.2251, 0.2260, 0.2166, 0.2258, 0.2216, 0.2109,
        0.2212, 0.1089, 0.1088, 0.2225, 0.2257, 0.2258, 0.2252, 0.2133, 0.2255,
        0.2244, 0.2260, 0.2253, 0.2260, 0.2141, 0.2255, 0.2258, 0.2258, 0.1094,
        0.2253, 0.1090, 0.2251, 0.2258, 0.2255, 0.2257, 0.2173, 0.2260, 0.2258,
        0.2253, 0.2258, 0.2260, 0.1092, 0.2260, 0.2260, 0.2257, 0.2255, 0.2258,
        0.2258, 0.2260, 0.2255, 0.2252, 0.2196, 0.2190, 0.1090, 0.2260, 0.1090,
        0.2258, 0.2260, 0.2258, 0.2251, 0.2258, 0.2258, 0.2134, 0.2240, 0.2258,
        0.2258, 0.2258, 0.1047, 0.1077, 0.2260, 0.2253, 0.1074, 0.2257, 0.2129,
        0.2188, 0.2162, 0.2258, 0.2258, 0.1046, 0.2258, 0.2260, 0.2230, 0.2252,
        0.1079, 0.2186, 0.1077, 0.2260, 0.2126, 0.2260, 0.2258, 0.2256, 0.2260,
        0.2122, 0.2258, 0.1089, 0.2260, 0.1081, 0.2220, 0.2255, 0.2260, 0.2107,
        0.1092, 0.2260, 0.2260, 0.2255, 0.2260, 0.2260, 0.2261, 0.2231, 0.2246,
        0.2258, 0.2206, 0.1079, 0.2177, 0.2231, 0.2260, 0.2258, 0.2257, 0.2258,
        0.2260, 0.2255, 0.2207, 0.2257, 0.2258, 0.2108, 0.2258, 0.1074, 0.2260,
        0.2258, 0.2257, 0.1089, 0.2188, 0.2253, 0.2260, 0.2260, 0.2241, 0.2260,
        0.2257, 0.2255, 0.2258, 0.1047, 0.2174, 0.1075, 0.1991, 0.1087, 0.2245,
        0.2257, 0.2256, 0.2258, 0.2229, 0.2256, 0.2113, 0.2260, 0.2256, 0.2258,
        0.2256, 0.2179, 0.2223, 0.2260, 0.2246, 0.2239, 0.2260, 0.2257, 0.1079,
        0.2245, 0.2255, 0.2234, 0.1080, 0.2252, 0.2255, 0.2247, 0.2258, 0.2258,
        0.2223, 0.2251, 0.2258, 0.2260, 0.2258, 0.2242, 0.2258, 0.2260, 0.2229,
        0.1092], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(6634.6714, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5684.1211, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7164.3774, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6323.0205, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7247.9302, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6123.2339, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7435.8818, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7357.0991, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6038.4463, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2299, 0.2299, 0.2275, 0.2257, 0.2241, 0.1089, 0.2297, 0.2295, 0.2291,
        0.2297, 0.2161, 0.2206, 0.2206, 0.2300, 0.2299, 0.2300, 0.2266, 0.2299,
        0.1124, 0.2300, 0.2300, 0.1158, 0.1125, 0.1125, 0.2180, 0.2297, 0.2299,
        0.2267, 0.2272, 0.2299, 0.1138, 0.2294, 0.2251, 0.2299, 0.2295, 0.2292,
        0.2280, 0.2300, 0.2299, 0.2174, 0.2300, 0.1143, 0.2300, 0.2292, 0.2286,
        0.1132, 0.2111, 0.2300, 0.2300, 0.2290, 0.2300, 0.1089, 0.2208, 0.2289,
        0.2131, 0.2289, 0.1138, 0.2181, 0.2300, 0.2286, 0.2267, 0.2294, 0.2219,
        0.2292, 0.1125, 0.2300, 0.2300, 0.2286, 0.2300, 0.2300, 0.1132, 0.2300,
        0.2274, 0.2300, 0.2291, 0.2064, 0.2286, 0.1110, 0.2300, 0.2206, 0.2274,
        0.2297, 0.1130, 0.2300, 0.2292, 0.1150, 0.2299, 0.2292, 0.2290, 0.2291,
        0.1089, 0.1127, 0.2294, 0.2300, 0.2178, 0.1045, 0.1127, 0.2300, 0.1127,
        0.2300, 0.2299, 0.2300, 0.2296, 0.2288, 0.1160, 0.2300, 0.2300, 0.1125,
        0.2120, 0.1149, 0.2284, 0.1136, 0.2217, 0.2300, 0.2275, 0.2268, 0.2297,
        0.1125, 0.2280, 0.2233, 0.2300, 0.2300, 0.2301, 0.2300, 0.2157, 0.1144,
        0.2300, 0.2300, 0.2295, 0.2300, 0.2300, 0.2300, 0.2253, 0.2284, 0.1089,
        0.2273, 0.2292, 0.2181, 0.2260, 0.2300, 0.2290, 0.2198, 0.2296, 0.2294,
        0.2286, 0.2299, 0.2239, 0.2291, 0.2297, 0.1089, 0.2275, 0.2295, 0.1160,
        0.2203, 0.2300, 0.2188, 0.2300, 0.2300, 0.2283, 0.2300, 0.2299, 0.2300,
        0.2300, 0.2279, 0.2300, 0.2299, 0.2297, 0.2284, 0.2269, 0.2292, 0.2300,
        0.2300, 0.2284, 0.2300, 0.2292, 0.1140, 0.2300, 0.2296, 0.1135, 0.2220,
        0.2297, 0.2292, 0.2297, 0.2211, 0.2299, 0.1089, 0.2300, 0.2297, 0.2300,
        0.1136, 0.2284, 0.2299, 0.2299, 0.1138, 0.2283, 0.2268, 0.2273, 0.2295,
        0.2299, 0.2290, 0.2216, 0.2299, 0.2297, 0.2300, 0.2299, 0.1163, 0.2192,
        0.2189, 0.2300, 0.2299, 0.2284, 0.1135, 0.2299, 0.2300, 0.2300, 0.2299,
        0.1141, 0.2300, 0.2300, 0.2300, 0.2299, 0.1238, 0.2284, 0.2300, 0.2285,
        0.1145, 0.2300, 0.2300, 0.2300, 0.1124, 0.2283, 0.2098, 0.2300, 0.2297,
        0.2300, 0.2292, 0.2281, 0.1144, 0.2290, 0.1143, 0.2300, 0.2256, 0.2234,
        0.2189, 0.2074, 0.2300, 0.2224, 0.1147, 0.2300, 0.2300, 0.2225, 0.2262,
        0.1143, 0.2079, 0.2280, 0.2300, 0.2294, 0.2300, 0.2273, 0.2159, 0.2223,
        0.2296, 0.2296, 0.1155, 0.2289, 0.2246, 0.2300, 0.2300, 0.1091, 0.2292,
        0.1148, 0.2291, 0.2300, 0.2295, 0.2281, 0.2301, 0.1133, 0.2299, 0.1138,
        0.1129, 0.2300, 0.2295, 0.1089, 0.2301, 0.2297, 0.2283, 0.2301, 0.2290,
        0.2300, 0.2297, 0.2300, 0.1089, 0.2281, 0.2299, 0.2291, 0.1126, 0.2272,
        0.1123, 0.2256, 0.2300, 0.2297, 0.2050, 0.1148, 0.2169, 0.2257, 0.2263,
        0.2299, 0.2301, 0.2300, 0.1154, 0.2300, 0.2284, 0.2267, 0.2290, 0.2300,
        0.1089, 0.2300, 0.1135, 0.1141, 0.2299, 0.2283, 0.1131, 0.2300, 0.2300,
        0.2284, 0.2283, 0.2299, 0.2177, 0.2300, 0.2278, 0.2283, 0.2300, 0.2299,
        0.1132, 0.2285, 0.2262, 0.2283, 0.1148, 0.2295, 0.2299, 0.2290, 0.2297,
        0.2300, 0.2300, 0.2300, 0.1157, 0.2224, 0.1130, 0.2161, 0.2300, 0.2251,
        0.1138, 0.2296, 0.2300, 0.2280, 0.2299, 0.2300, 0.2050, 0.2300, 0.2194,
        0.2299, 0.2300, 0.2253, 0.2300, 0.2031, 0.2291, 0.2296, 0.2300, 0.2300,
        0.2277, 0.2300, 0.2300, 0.2299, 0.2300, 0.2286, 0.2257, 0.2300, 0.1129,
        0.1143, 0.2240, 0.2302, 0.1157, 0.2300, 0.2300, 0.2233, 0.2300, 0.2139,
        0.2141, 0.2273, 0.2300, 0.2240, 0.2300, 0.2299, 0.2297, 0.1147, 0.2301,
        0.2279, 0.1127, 0.2299, 0.1127, 0.2300, 0.1142, 0.2167, 0.2299, 0.2300,
        0.1136, 0.2300, 0.1130, 0.2289, 0.2299, 0.2300, 0.2300, 0.1158, 0.2274,
        0.2295, 0.2300, 0.2300, 0.2240, 0.2300, 0.2299, 0.2300, 0.2300, 0.2289,
        0.2300, 0.2299, 0.2291, 0.2300, 0.2299, 0.2269, 0.2300, 0.2297, 0.2295,
        0.1133, 0.2264, 0.2300, 0.2297, 0.2300, 0.2290, 0.2280, 0.2289, 0.2292,
        0.2300, 0.2299, 0.2296, 0.2300, 0.2238, 0.2273, 0.2300, 0.1128, 0.2267,
        0.2300, 0.1119, 0.2294, 0.2301, 0.2277, 0.2300, 0.2300, 0.2299, 0.1150,
        0.1089, 0.2225, 0.2299, 0.2292, 0.2300, 0.2207, 0.2300, 0.2257, 0.2150,
        0.2253, 0.1145, 0.1136, 0.2267, 0.2299, 0.2300, 0.2294, 0.2173, 0.2296,
        0.2285, 0.2300, 0.2295, 0.2300, 0.2183, 0.2296, 0.2300, 0.2299, 0.1138,
        0.2295, 0.1140, 0.2291, 0.2300, 0.2296, 0.2299, 0.2214, 0.2300, 0.2300,
        0.2295, 0.2300, 0.2300, 0.1138, 0.2300, 0.2300, 0.2299, 0.2296, 0.2300,
        0.2299, 0.2300, 0.2296, 0.2292, 0.2238, 0.2231, 0.1136, 0.2300, 0.1140,
        0.2300, 0.2300, 0.2300, 0.2292, 0.2300, 0.2300, 0.2174, 0.2280, 0.2300,
        0.2300, 0.2300, 0.1089, 0.1120, 0.2300, 0.2295, 0.1152, 0.2297, 0.2170,
        0.2228, 0.2202, 0.2300, 0.2300, 0.1089, 0.2300, 0.2300, 0.2272, 0.2294,
        0.1124, 0.2228, 0.1121, 0.2300, 0.2164, 0.2301, 0.2299, 0.2297, 0.2300,
        0.2163, 0.2299, 0.1144, 0.2300, 0.1125, 0.2262, 0.2296, 0.2300, 0.2148,
        0.1138, 0.2300, 0.2300, 0.2296, 0.2300, 0.2300, 0.2302, 0.2273, 0.2288,
        0.2299, 0.2247, 0.1124, 0.2218, 0.2273, 0.2300, 0.2300, 0.2299, 0.2300,
        0.2300, 0.2296, 0.2249, 0.2299, 0.2300, 0.2150, 0.2300, 0.1119, 0.2300,
        0.2299, 0.2299, 0.1133, 0.2229, 0.2295, 0.2300, 0.2300, 0.2281, 0.2300,
        0.2299, 0.2296, 0.2300, 0.1089, 0.2216, 0.1118, 0.2031, 0.1136, 0.2285,
        0.2299, 0.2297, 0.2300, 0.2271, 0.2297, 0.2155, 0.2301, 0.2296, 0.2300,
        0.2297, 0.2220, 0.2263, 0.2300, 0.2288, 0.2280, 0.2300, 0.2299, 0.1122,
        0.2286, 0.2296, 0.2275, 0.1124, 0.2294, 0.2296, 0.2289, 0.2300, 0.2299,
        0.2264, 0.2292, 0.2300, 0.2300, 0.2300, 0.2284, 0.2300, 0.2300, 0.2269,
        0.1135], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(4998.1230, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6991.7100, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7449.0405, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2260, 0.2260, 0.2235, 0.2217, 0.2201, 0.1048, 0.2258, 0.2256, 0.2251,
        0.2258, 0.2120, 0.2166, 0.2167, 0.2261, 0.2260, 0.2260, 0.2225, 0.2260,
        0.1083, 0.2261, 0.2260, 0.1090, 0.1082, 0.1082, 0.2140, 0.2257, 0.2260,
        0.2227, 0.2231, 0.2258, 0.1093, 0.2253, 0.2211, 0.2260, 0.2256, 0.2253,
        0.2240, 0.2261, 0.2260, 0.2135, 0.2261, 0.1093, 0.2261, 0.2253, 0.2247,
        0.1089, 0.2070, 0.2260, 0.2260, 0.2250, 0.2261, 0.1048, 0.2169, 0.2249,
        0.2092, 0.2250, 0.1091, 0.2142, 0.2260, 0.2246, 0.2227, 0.2253, 0.2179,
        0.2253, 0.1074, 0.2260, 0.2261, 0.2246, 0.2261, 0.2261, 0.1088, 0.2261,
        0.2234, 0.2261, 0.2251, 0.2024, 0.2246, 0.1062, 0.2261, 0.2166, 0.2234,
        0.2258, 0.1088, 0.2260, 0.2253, 0.1091, 0.2258, 0.2253, 0.2250, 0.2251,
        0.1048, 0.1085, 0.2253, 0.2261, 0.2139, 0.1010, 0.1083, 0.2261, 0.1085,
        0.2260, 0.2258, 0.2260, 0.2256, 0.2247, 0.1097, 0.2261, 0.2261, 0.1083,
        0.2081, 0.1099, 0.2244, 0.1091, 0.2177, 0.2260, 0.2235, 0.2228, 0.2258,
        0.1083, 0.2240, 0.2192, 0.2261, 0.2260, 0.2261, 0.2260, 0.2117, 0.1094,
        0.2261, 0.2261, 0.2256, 0.2261, 0.2260, 0.2261, 0.2213, 0.2245, 0.1047,
        0.2234, 0.2253, 0.2141, 0.2219, 0.2261, 0.2251, 0.2158, 0.2256, 0.2253,
        0.2246, 0.2260, 0.2198, 0.2251, 0.2257, 0.1049, 0.2235, 0.2256, 0.1092,
        0.2163, 0.2261, 0.2148, 0.2260, 0.2260, 0.2244, 0.2261, 0.2258, 0.2260,
        0.2261, 0.2240, 0.2261, 0.2258, 0.2257, 0.2245, 0.2229, 0.2252, 0.2261,
        0.2260, 0.2244, 0.2261, 0.2253, 0.1091, 0.2261, 0.2256, 0.1088, 0.2180,
        0.2258, 0.2253, 0.2257, 0.2170, 0.2258, 0.1048, 0.2261, 0.2257, 0.2261,
        0.1092, 0.2244, 0.2258, 0.2258, 0.1095, 0.2242, 0.2229, 0.2234, 0.2256,
        0.2260, 0.2250, 0.2175, 0.2260, 0.2257, 0.2261, 0.2260, 0.1096, 0.2152,
        0.2150, 0.2260, 0.2260, 0.2244, 0.1089, 0.2260, 0.2261, 0.2261, 0.2260,
        0.1093, 0.2261, 0.2260, 0.2260, 0.2260, 0.1401, 0.2244, 0.2261, 0.2246,
        0.1095, 0.2261, 0.2260, 0.2261, 0.1085, 0.2242, 0.2058, 0.2261, 0.2258,
        0.2260, 0.2252, 0.2241, 0.1093, 0.2251, 0.1093, 0.2261, 0.2216, 0.2194,
        0.2148, 0.2034, 0.2261, 0.2185, 0.1096, 0.2260, 0.2261, 0.2185, 0.2222,
        0.1095, 0.2039, 0.2240, 0.2261, 0.2253, 0.2261, 0.2233, 0.2119, 0.2184,
        0.2257, 0.2256, 0.1088, 0.2249, 0.2206, 0.2260, 0.2261, 0.1052, 0.2253,
        0.1091, 0.2251, 0.2261, 0.2255, 0.2241, 0.2261, 0.1091, 0.2258, 0.1093,
        0.1086, 0.2260, 0.2256, 0.1047, 0.2262, 0.2258, 0.2244, 0.2262, 0.2250,
        0.2261, 0.2257, 0.2261, 0.1048, 0.2241, 0.2258, 0.2252, 0.1085, 0.2233,
        0.1080, 0.2216, 0.2261, 0.2257, 0.2010, 0.1093, 0.2130, 0.2218, 0.2223,
        0.2258, 0.2261, 0.2260, 0.1089, 0.2260, 0.2244, 0.2228, 0.2251, 0.2261,
        0.1047, 0.2260, 0.1090, 0.1093, 0.2258, 0.2242, 0.1089, 0.2261, 0.2261,
        0.2244, 0.2242, 0.2258, 0.2136, 0.2261, 0.2238, 0.2244, 0.2261, 0.2260,
        0.1088, 0.2245, 0.2222, 0.2242, 0.1094, 0.2256, 0.2260, 0.2250, 0.2258,
        0.2260, 0.2261, 0.2260, 0.1088, 0.2184, 0.1087, 0.2120, 0.2261, 0.2212,
        0.1089, 0.2256, 0.2261, 0.2240, 0.2258, 0.2261, 0.2009, 0.2261, 0.2155,
        0.2258, 0.2260, 0.2213, 0.2260, 0.1991, 0.2251, 0.2256, 0.2261, 0.2261,
        0.2236, 0.2261, 0.2261, 0.2258, 0.2261, 0.2246, 0.2217, 0.2261, 0.1090,
        0.1091, 0.2201, 0.2262, 0.1100, 0.2261, 0.2261, 0.2192, 0.2260, 0.2100,
        0.2102, 0.2233, 0.2261, 0.2200, 0.2261, 0.2260, 0.2257, 0.1096, 0.2262,
        0.2239, 0.1086, 0.2260, 0.1085, 0.2261, 0.1092, 0.2126, 0.2258, 0.2260,
        0.1090, 0.2261, 0.1089, 0.2249, 0.2258, 0.2261, 0.2260, 0.1092, 0.2234,
        0.2256, 0.2260, 0.2261, 0.2201, 0.2261, 0.2258, 0.2261, 0.2261, 0.2249,
        0.2260, 0.2260, 0.2251, 0.2261, 0.2260, 0.2229, 0.2260, 0.2257, 0.2255,
        0.1092, 0.2224, 0.2261, 0.2258, 0.2261, 0.2251, 0.2241, 0.2250, 0.2252,
        0.2260, 0.2260, 0.2256, 0.2260, 0.2197, 0.2234, 0.2261, 0.1083, 0.2228,
        0.2261, 0.1078, 0.2253, 0.2261, 0.2236, 0.2261, 0.2261, 0.2258, 0.1090,
        0.1047, 0.2185, 0.2258, 0.2252, 0.2261, 0.2167, 0.2260, 0.2217, 0.2111,
        0.2213, 0.1091, 0.1091, 0.2227, 0.2258, 0.2261, 0.2253, 0.2134, 0.2257,
        0.2245, 0.2261, 0.2255, 0.2261, 0.2142, 0.2257, 0.2260, 0.2260, 0.1096,
        0.2255, 0.1093, 0.2252, 0.2260, 0.2256, 0.2258, 0.2174, 0.2261, 0.2260,
        0.2255, 0.2261, 0.2261, 0.1094, 0.2261, 0.2261, 0.2258, 0.2256, 0.2260,
        0.2260, 0.2261, 0.2256, 0.2253, 0.2197, 0.2191, 0.1093, 0.2261, 0.1093,
        0.2261, 0.2261, 0.2260, 0.2252, 0.2260, 0.2260, 0.2135, 0.2241, 0.2260,
        0.2260, 0.2260, 0.1048, 0.1078, 0.2261, 0.2255, 0.1075, 0.2258, 0.2130,
        0.2189, 0.2163, 0.2261, 0.2260, 0.1047, 0.2260, 0.2261, 0.2233, 0.2253,
        0.1082, 0.2189, 0.1079, 0.2261, 0.2128, 0.2261, 0.2260, 0.2258, 0.2261,
        0.2123, 0.2260, 0.1092, 0.2261, 0.1083, 0.2222, 0.2257, 0.2261, 0.2108,
        0.1094, 0.2261, 0.2261, 0.2256, 0.2261, 0.2261, 0.2262, 0.2234, 0.2247,
        0.2260, 0.2207, 0.1082, 0.2179, 0.2234, 0.2261, 0.2260, 0.2258, 0.2260,
        0.2261, 0.2256, 0.2208, 0.2258, 0.2260, 0.2109, 0.2261, 0.1077, 0.2261,
        0.2260, 0.2258, 0.1091, 0.2189, 0.2255, 0.2261, 0.2261, 0.2242, 0.2261,
        0.2258, 0.2256, 0.2260, 0.1048, 0.2175, 0.1077, 0.1992, 0.1089, 0.2246,
        0.2258, 0.2257, 0.2261, 0.2230, 0.2257, 0.2114, 0.2261, 0.2257, 0.2261,
        0.2257, 0.2180, 0.2224, 0.2261, 0.2249, 0.2240, 0.2261, 0.2258, 0.1081,
        0.2246, 0.2256, 0.2235, 0.1083, 0.2253, 0.2256, 0.2249, 0.2260, 0.2260,
        0.2224, 0.2252, 0.2260, 0.2261, 0.2260, 0.2244, 0.2260, 0.2261, 0.2230,
        0.1094], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(7270.9644, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6202.9907, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6556.5938, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6874.2070, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7028.7090, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4796.7476, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(3978.9158, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2305, 0.2305, 0.2280, 0.2263, 0.2247, 0.1093, 0.2303, 0.2301, 0.2297,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2306, 0.2271, 0.2305,
        0.1119, 0.2306, 0.2306, 0.1148, 0.1118, 0.1119, 0.2186, 0.2303, 0.2305,
        0.2273, 0.2277, 0.2303, 0.1131, 0.2299, 0.2257, 0.2305, 0.2301, 0.2299,
        0.2286, 0.2306, 0.2305, 0.2180, 0.2306, 0.1127, 0.2306, 0.2299, 0.2292,
        0.1125, 0.2117, 0.2306, 0.2306, 0.2296, 0.2306, 0.1094, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1127, 0.2188, 0.2305, 0.2292, 0.2272, 0.2300, 0.2224,
        0.2299, 0.1149, 0.2306, 0.2306, 0.2291, 0.2306, 0.2306, 0.1122, 0.2306,
        0.2279, 0.2306, 0.2297, 0.2070, 0.2292, 0.1133, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1121, 0.2306, 0.2299, 0.1131, 0.2305, 0.2299, 0.2296, 0.2296,
        0.1093, 0.1120, 0.2300, 0.2306, 0.2184, 0.1062, 0.1121, 0.2306, 0.1121,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1140, 0.2306, 0.2306, 0.1119,
        0.2126, 0.1135, 0.2289, 0.1127, 0.2223, 0.2306, 0.2281, 0.2273, 0.2303,
        0.1119, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2306, 0.2162, 0.1129,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2305, 0.2306, 0.2260, 0.2290, 0.1093,
        0.2279, 0.2299, 0.2188, 0.2264, 0.2306, 0.2296, 0.2205, 0.2301, 0.2300,
        0.2291, 0.2305, 0.2244, 0.2297, 0.2303, 0.1094, 0.2281, 0.2301, 0.1160,
        0.2208, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2305, 0.2303, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1127, 0.2306, 0.2301, 0.1125, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2217, 0.2305, 0.1094, 0.2306, 0.2303, 0.2306,
        0.1125, 0.2290, 0.2303, 0.2305, 0.1128, 0.2288, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2296, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1141, 0.2198,
        0.2195, 0.2306, 0.2305, 0.2290, 0.1124, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1127, 0.2306, 0.2306, 0.2305, 0.2305, 0.1164, 0.2290, 0.2306, 0.2291,
        0.1132, 0.2306, 0.2305, 0.2306, 0.1119, 0.2289, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1132, 0.2296, 0.1128, 0.2306, 0.2262, 0.2240,
        0.2195, 0.2079, 0.2306, 0.2230, 0.1131, 0.2305, 0.2306, 0.2231, 0.2268,
        0.1130, 0.2085, 0.2286, 0.2306, 0.2300, 0.2306, 0.2279, 0.2166, 0.2229,
        0.2302, 0.2302, 0.1146, 0.2294, 0.2252, 0.2306, 0.2306, 0.1096, 0.2299,
        0.1136, 0.2297, 0.2306, 0.2301, 0.2286, 0.2307, 0.1125, 0.2305, 0.1127,
        0.1120, 0.2305, 0.2301, 0.1093, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1093, 0.2286, 0.2305, 0.2297, 0.1119, 0.2278,
        0.1118, 0.2262, 0.2306, 0.2302, 0.2056, 0.1133, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2307, 0.2306, 0.1135, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1093, 0.2306, 0.1125, 0.1129, 0.2305, 0.2288, 0.1122, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2305, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1122, 0.2291, 0.2268, 0.2289, 0.1134, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1151, 0.2230, 0.1121, 0.2167, 0.2306, 0.2257,
        0.1127, 0.2302, 0.2306, 0.2286, 0.2305, 0.2306, 0.2056, 0.2306, 0.2200,
        0.2305, 0.2306, 0.2258, 0.2306, 0.2036, 0.2297, 0.2302, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2305, 0.2306, 0.2292, 0.2263, 0.2306, 0.1120,
        0.1130, 0.2246, 0.2307, 0.1139, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2303, 0.1134, 0.2307,
        0.2285, 0.1119, 0.2305, 0.1119, 0.2306, 0.1132, 0.2173, 0.2305, 0.2305,
        0.1124, 0.2306, 0.1121, 0.2295, 0.2303, 0.2306, 0.2306, 0.1158, 0.2280,
        0.2301, 0.2306, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2306, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2302, 0.2300,
        0.1126, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1120, 0.2273,
        0.2306, 0.1115, 0.2299, 0.2306, 0.2283, 0.2306, 0.2306, 0.2303, 0.1128,
        0.1093, 0.2231, 0.2305, 0.2299, 0.2306, 0.2213, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1132, 0.1127, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1131,
        0.2301, 0.1130, 0.2297, 0.2305, 0.2301, 0.2303, 0.2220, 0.2306, 0.2306,
        0.2300, 0.2306, 0.2306, 0.1127, 0.2306, 0.2306, 0.2303, 0.2302, 0.2306,
        0.2305, 0.2306, 0.2302, 0.2299, 0.2244, 0.2238, 0.1125, 0.2306, 0.1129,
        0.2306, 0.2306, 0.2306, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2306, 0.2306, 0.1094, 0.1115, 0.2306, 0.2301, 0.1139, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1094, 0.2306, 0.2306, 0.2278, 0.2300,
        0.1118, 0.2234, 0.1116, 0.2306, 0.2170, 0.2307, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1131, 0.2306, 0.1119, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1127, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2308, 0.2279, 0.2294,
        0.2305, 0.2253, 0.1117, 0.2224, 0.2279, 0.2306, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2301, 0.2253, 0.2305, 0.2306, 0.2156, 0.2306, 0.1114, 0.2306,
        0.2305, 0.2305, 0.1124, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2305, 0.2302, 0.2305, 0.1093, 0.2222, 0.1114, 0.2037, 0.1125, 0.2291,
        0.2305, 0.2303, 0.2306, 0.2277, 0.2303, 0.2159, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1117,
        0.2292, 0.2302, 0.2281, 0.1118, 0.2300, 0.2302, 0.2294, 0.2306, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1127], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 20:57:10,731][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 20:57:10,732][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 20:57:10,906][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
loss: tensor(4449.6729, device='cuda:0')
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16)
[2023-09-12 20:57:34,643][valid][INFO] - {"epoch": 17, "valid_loss": "3.708", "valid_ntokens": "7923.12", "valid_nsentences": "55.2525", "valid_prob_perplexity": "202.336", "valid_code_perplexity": "170.939", "valid_temp": "1.914", "valid_loss_0": "3.598", "valid_loss_1": "0.099", "valid_loss_2": "0.012", "valid_accuracy": "0.38399", "valid_wps": "33216.7", "valid_wpb": "7923.1", "valid_bsz": "55.3", "valid_num_updates": "8823", "valid_best_loss": "3.708"}
[2023-09-12 20:57:34,645][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 8823 updates
[2023-09-12 20:57:34,646][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 20:57:37,115][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 20:57:38,458][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 8823 updates, score 3.708) (writing took 3.8131321880500764 seconds)
[2023-09-12 20:57:38,458][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2023-09-12 20:57:38,459][train][INFO] - {"epoch": 17, "train_loss": "4.037", "train_ntokens": "149509", "train_nsentences": "538.479", "train_prob_perplexity": "208.737", "train_code_perplexity": "182.883", "train_temp": "1.916", "train_loss_0": "3.929", "train_loss_1": "0.097", "train_loss_2": "0.011", "train_accuracy": "0.31186", "train_wps": "37166.1", "train_ups": "0.25", "train_wpb": "149509", "train_bsz": "538.5", "train_num_updates": "8823", "train_lr": "0.000137859", "train_gnorm": "0.483", "train_loss_scale": "1", "train_train_wall": "2060", "train_gb_free": "13.6", "train_wall": "35531"}
[2023-09-12 20:57:38,460][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 20:57:38,547][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2023-09-12 20:57:38,778][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 20:57:38,781][fairseq.trainer][INFO] - begin training epoch 18
[2023-09-12 20:57:38,781][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([0.2305, 0.2305, 0.2280, 0.2263, 0.2247, 0.1093, 0.2303, 0.2301, 0.2297,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2306, 0.2271, 0.2305,
        0.1119, 0.2306, 0.2306, 0.1148, 0.1118, 0.1119, 0.2186, 0.2303, 0.2305,
        0.2273, 0.2277, 0.2303, 0.1131, 0.2299, 0.2257, 0.2305, 0.2301, 0.2299,
        0.2286, 0.2306, 0.2305, 0.2180, 0.2306, 0.1127, 0.2306, 0.2299, 0.2292,
        0.1125, 0.2117, 0.2306, 0.2306, 0.2296, 0.2306, 0.1094, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1127, 0.2188, 0.2305, 0.2292, 0.2272, 0.2300, 0.2224,
        0.2299, 0.1149, 0.2306, 0.2306, 0.2291, 0.2306, 0.2306, 0.1122, 0.2306,
        0.2279, 0.2306, 0.2297, 0.2070, 0.2292, 0.1133, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1121, 0.2306, 0.2299, 0.1131, 0.2305, 0.2299, 0.2296, 0.2296,
        0.1093, 0.1120, 0.2300, 0.2306, 0.2184, 0.1062, 0.1121, 0.2306, 0.1121,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1140, 0.2306, 0.2306, 0.1119,
        0.2126, 0.1135, 0.2289, 0.1127, 0.2223, 0.2306, 0.2281, 0.2273, 0.2303,
        0.1119, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2306, 0.2162, 0.1129,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2305, 0.2306, 0.2260, 0.2290, 0.1093,
        0.2279, 0.2299, 0.2188, 0.2264, 0.2306, 0.2296, 0.2205, 0.2301, 0.2300,
        0.2291, 0.2305, 0.2244, 0.2297, 0.2303, 0.1094, 0.2281, 0.2301, 0.1160,
        0.2208, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2305, 0.2303, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1127, 0.2306, 0.2301, 0.1125, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2217, 0.2305, 0.1094, 0.2306, 0.2303, 0.2306,
        0.1125, 0.2290, 0.2303, 0.2305, 0.1128, 0.2288, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2296, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1141, 0.2198,
        0.2195, 0.2306, 0.2305, 0.2290, 0.1124, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1127, 0.2306, 0.2306, 0.2305, 0.2305, 0.1164, 0.2290, 0.2306, 0.2291,
        0.1132, 0.2306, 0.2305, 0.2306, 0.1119, 0.2289, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1132, 0.2296, 0.1128, 0.2306, 0.2262, 0.2240,
        0.2195, 0.2079, 0.2306, 0.2230, 0.1131, 0.2305, 0.2306, 0.2231, 0.2268,
        0.1130, 0.2085, 0.2286, 0.2306, 0.2300, 0.2306, 0.2279, 0.2166, 0.2229,
        0.2302, 0.2302, 0.1146, 0.2294, 0.2252, 0.2306, 0.2306, 0.1096, 0.2299,
        0.1136, 0.2297, 0.2306, 0.2301, 0.2286, 0.2307, 0.1125, 0.2305, 0.1127,
        0.1120, 0.2305, 0.2301, 0.1093, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1093, 0.2286, 0.2305, 0.2297, 0.1119, 0.2278,
        0.1118, 0.2262, 0.2306, 0.2302, 0.2056, 0.1133, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2307, 0.2306, 0.1135, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1093, 0.2306, 0.1125, 0.1129, 0.2305, 0.2288, 0.1122, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2305, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1122, 0.2291, 0.2268, 0.2289, 0.1134, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1151, 0.2230, 0.1121, 0.2167, 0.2306, 0.2257,
        0.1127, 0.2302, 0.2306, 0.2286, 0.2305, 0.2306, 0.2056, 0.2306, 0.2200,
        0.2305, 0.2306, 0.2258, 0.2306, 0.2036, 0.2297, 0.2302, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2305, 0.2306, 0.2292, 0.2263, 0.2306, 0.1120,
        0.1130, 0.2246, 0.2307, 0.1139, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2303, 0.1134, 0.2307,
        0.2285, 0.1119, 0.2305, 0.1119, 0.2306, 0.1132, 0.2173, 0.2305, 0.2305,
        0.1124, 0.2306, 0.1121, 0.2295, 0.2303, 0.2306, 0.2306, 0.1158, 0.2280,
        0.2301, 0.2306, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2306, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2302, 0.2300,
        0.1126, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1120, 0.2273,
        0.2306, 0.1115, 0.2299, 0.2306, 0.2283, 0.2306, 0.2306, 0.2303, 0.1128,
        0.1093, 0.2231, 0.2305, 0.2299, 0.2306, 0.2213, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1132, 0.1127, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1131,
        0.2301, 0.1130, 0.2297, 0.2305, 0.2301, 0.2303, 0.2220, 0.2306, 0.2306,
        0.2300, 0.2306, 0.2306, 0.1127, 0.2306, 0.2306, 0.2303, 0.2302, 0.2306,
        0.2305, 0.2306, 0.2302, 0.2299, 0.2244, 0.2238, 0.1125, 0.2306, 0.1129,
        0.2306, 0.2306, 0.2306, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2306, 0.2306, 0.1094, 0.1115, 0.2306, 0.2301, 0.1139, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1094, 0.2306, 0.2306, 0.2278, 0.2300,
        0.1118, 0.2234, 0.1116, 0.2306, 0.2170, 0.2307, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1131, 0.2306, 0.1119, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1127, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2308, 0.2279, 0.2294,
        0.2305, 0.2253, 0.1117, 0.2224, 0.2279, 0.2306, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2301, 0.2253, 0.2305, 0.2306, 0.2156, 0.2306, 0.1114, 0.2306,
        0.2305, 0.2305, 0.1124, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2305, 0.2302, 0.2305, 0.1093, 0.2222, 0.1114, 0.2037, 0.1125, 0.2291,
        0.2305, 0.2303, 0.2306, 0.2277, 0.2303, 0.2159, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1117,
        0.2292, 0.2302, 0.2281, 0.1118, 0.2300, 0.2302, 0.2294, 0.2306, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1127], device='cuda:2', dtype=torch.float16, requires_grad=True)
self.scaling_factor_for_vector: loss: tensor(5262.9043, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6271.4736, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 21:08:38,131][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2023-09-12 21:09:27,002][train_inner][INFO] - {"epoch": 18, "update": 17.342, "loss": "4.005", "ntokens": "149197", "nsentences": "537.92", "prob_perplexity": "220.674", "code_perplexity": "191.951", "temp": "1.913", "loss_0": "3.899", "loss_1": "0.095", "loss_2": "0.012", "accuracy": "0.31513", "wps": "36228.8", "ups": "0.24", "wpb": "149197", "bsz": "537.9", "num_updates": "9000", "lr": "0.000140625", "gnorm": "0.466", "loss_scale": "1", "train_wall": "794", "gb_free": "12.7", "wall": "36240"}
loss: tensor(7275.5020, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2305, 0.2305, 0.2281, 0.2263, 0.2247, 0.1093, 0.2303, 0.2301, 0.2297,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2306, 0.2271, 0.2305,
        0.1115, 0.2306, 0.2306, 0.1140, 0.1115, 0.1115, 0.2186, 0.2303, 0.2305,
        0.2273, 0.2277, 0.2305, 0.1125, 0.2299, 0.2257, 0.2305, 0.2301, 0.2299,
        0.2286, 0.2306, 0.2305, 0.2180, 0.2306, 0.1121, 0.2306, 0.2299, 0.2292,
        0.1121, 0.2117, 0.2306, 0.2306, 0.2296, 0.2306, 0.1093, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1122, 0.2188, 0.2306, 0.2292, 0.2272, 0.2300, 0.2225,
        0.2299, 0.1141, 0.2306, 0.2306, 0.2292, 0.2306, 0.2306, 0.1118, 0.2306,
        0.2280, 0.2306, 0.2297, 0.2070, 0.2292, 0.1140, 0.2306, 0.2211, 0.2280,
        0.2303, 0.1117, 0.2306, 0.2299, 0.1126, 0.2305, 0.2299, 0.2296, 0.2296,
        0.1093, 0.1116, 0.2300, 0.2306, 0.2184, 0.1064, 0.1118, 0.2306, 0.1117,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1134, 0.2306, 0.2306, 0.1115,
        0.2126, 0.1128, 0.2289, 0.1122, 0.2223, 0.2306, 0.2281, 0.2273, 0.2303,
        0.1115, 0.2285, 0.2239, 0.2306, 0.2306, 0.2306, 0.2306, 0.2163, 0.1123,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2306, 0.2260, 0.2290, 0.1093,
        0.2279, 0.2299, 0.2188, 0.2266, 0.2306, 0.2296, 0.2205, 0.2302, 0.2300,
        0.2292, 0.2305, 0.2244, 0.2297, 0.2303, 0.1094, 0.2281, 0.2301, 0.1145,
        0.2209, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2305, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2305, 0.2303, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1122, 0.2306, 0.2302, 0.1121, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2217, 0.2305, 0.1093, 0.2306, 0.2303, 0.2306,
        0.1120, 0.2290, 0.2305, 0.2305, 0.1123, 0.2289, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2296, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1133, 0.2198,
        0.2195, 0.2306, 0.2305, 0.2290, 0.1120, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1123, 0.2306, 0.2306, 0.2306, 0.2305, 0.1136, 0.2290, 0.2306, 0.2291,
        0.1126, 0.2306, 0.2305, 0.2306, 0.1116, 0.2289, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1127, 0.2296, 0.1125, 0.2306, 0.2262, 0.2240,
        0.2195, 0.2080, 0.2306, 0.2230, 0.1125, 0.2305, 0.2306, 0.2231, 0.2268,
        0.1124, 0.2085, 0.2286, 0.2306, 0.2300, 0.2306, 0.2279, 0.2166, 0.2229,
        0.2302, 0.2302, 0.1135, 0.2294, 0.2252, 0.2306, 0.2306, 0.1095, 0.2299,
        0.1127, 0.2297, 0.2306, 0.2301, 0.2286, 0.2307, 0.1120, 0.2305, 0.1122,
        0.1117, 0.2306, 0.2301, 0.1093, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1093, 0.2286, 0.2305, 0.2297, 0.1115, 0.2278,
        0.1114, 0.2262, 0.2306, 0.2302, 0.2056, 0.1129, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2307, 0.2306, 0.1132, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1093, 0.2306, 0.1121, 0.1125, 0.2305, 0.2288, 0.1117, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2305, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1119, 0.2291, 0.2268, 0.2289, 0.1127, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1133, 0.2230, 0.1117, 0.2167, 0.2306, 0.2257,
        0.1122, 0.2302, 0.2306, 0.2286, 0.2305, 0.2306, 0.2054, 0.2306, 0.2200,
        0.2305, 0.2306, 0.2260, 0.2306, 0.2037, 0.2297, 0.2302, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2305, 0.2306, 0.2292, 0.2263, 0.2306, 0.1116,
        0.1125, 0.2246, 0.2307, 0.1133, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2303, 0.1125, 0.2307,
        0.2285, 0.1116, 0.2305, 0.1115, 0.2306, 0.1125, 0.2173, 0.2305, 0.2306,
        0.1121, 0.2306, 0.1117, 0.2295, 0.2305, 0.2306, 0.2306, 0.1149, 0.2280,
        0.2301, 0.2306, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2295,
        0.2306, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2303, 0.2300,
        0.1121, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1116, 0.2273,
        0.2306, 0.1111, 0.2299, 0.2307, 0.2283, 0.2306, 0.2306, 0.2303, 0.1122,
        0.1093, 0.2231, 0.2305, 0.2299, 0.2306, 0.2213, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1126, 0.1121, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1123,
        0.2301, 0.1124, 0.2297, 0.2306, 0.2302, 0.2303, 0.2220, 0.2306, 0.2306,
        0.2300, 0.2306, 0.2306, 0.1122, 0.2306, 0.2306, 0.2305, 0.2302, 0.2306,
        0.2305, 0.2306, 0.2302, 0.2299, 0.2244, 0.2238, 0.1121, 0.2306, 0.1124,
        0.2306, 0.2306, 0.2306, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2306, 0.2306, 0.1093, 0.1111, 0.2306, 0.2301, 0.1133, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1093, 0.2306, 0.2306, 0.2278, 0.2300,
        0.1115, 0.2234, 0.1111, 0.2306, 0.2170, 0.2307, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1124, 0.2306, 0.1116, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1122, 0.2306, 0.2306, 0.2302, 0.2306, 0.2306, 0.2308, 0.2279, 0.2294,
        0.2305, 0.2253, 0.1113, 0.2224, 0.2279, 0.2306, 0.2306, 0.2305, 0.2306,
        0.2306, 0.2301, 0.2255, 0.2305, 0.2306, 0.2156, 0.2306, 0.1111, 0.2306,
        0.2305, 0.2305, 0.1119, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2305, 0.2302, 0.2305, 0.1093, 0.2222, 0.1111, 0.2037, 0.1122, 0.2291,
        0.2305, 0.2303, 0.2306, 0.2277, 0.2303, 0.2161, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1114,
        0.2292, 0.2302, 0.2281, 0.1113, 0.2300, 0.2302, 0.2294, 0.2306, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1122], device='cuda:0', dtype=torch.float16, requires_grad=True)
loss: tensor(7074.7148, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3631.4968, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6744.4248, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4744.3311, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 21:22:37,625][train_inner][INFO] - {"epoch": 18, "update": 17.726, "loss": "3.977", "ntokens": "149548", "nsentences": "539.345", "prob_perplexity": "227.424", "code_perplexity": "198.012", "temp": "1.911", "loss_0": "3.873", "loss_1": "0.093", "loss_2": "0.011", "accuracy": "0.31778", "wps": "37830.6", "ups": "0.25", "wpb": "149548", "bsz": "539.3", "num_updates": "9200", "lr": "0.00014375", "gnorm": "0.488", "loss_scale": "1", "train_wall": "789", "gb_free": "13", "wall": "37030"}
[2023-09-12 21:26:06,177][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
loss: tensor(6924.3828, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6963.4468, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6909.4805, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 21:31:54,441][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 21:31:54,442][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 21:31:54,647][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2023-09-12 21:32:18,579][valid][INFO] - {"epoch": 18, "valid_loss": "3.635", "valid_ntokens": "7860.56", "valid_nsentences": "55.2525", "valid_prob_perplexity": "215.824", "valid_code_perplexity": "184.439", "valid_temp": "1.909", "valid_loss_0": "3.528", "valid_loss_1": "0.095", "valid_loss_2": "0.012", "valid_accuracy": "0.39408", "valid_wps": "32757.9", "valid_wpb": "7860.6", "valid_bsz": "55.3", "valid_num_updates": "9342", "valid_best_loss": "3.635"}
[2023-09-12 21:32:18,581][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 9342 updates
[2023-09-12 21:32:18,582][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 21:32:21,220][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_best.pt
[2023-09-12 21:32:22,702][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 9342 updates, score 3.635) (writing took 4.121200880967081 seconds)
[2023-09-12 21:32:22,702][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2023-09-12 21:32:22,703][train][INFO] - {"epoch": 18, "train_loss": "3.981", "train_ntokens": "149446", "train_nsentences": "538.453", "train_prob_perplexity": "226.736", "train_code_perplexity": "197.604", "train_temp": "1.911", "train_loss_0": "3.876", "train_loss_1": "0.093", "train_loss_2": "0.011", "train_accuracy": "0.3175", "train_wps": "37213.7", "train_ups": "0.25", "train_wpb": "149446", "train_bsz": "538.5", "train_num_updates": "9342", "train_lr": "0.000145969", "train_gnorm": "0.478", "train_loss_scale": "1", "train_train_wall": "2052", "train_gb_free": "13.3", "train_wall": "37615"}
[2023-09-12 21:32:22,706][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 21:32:22,797][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2023-09-12 21:32:23,023][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 21:32:23,026][fairseq.trainer][INFO] - begin training epoch 19
[2023-09-12 21:32:23,027][fairseq_cli.train][INFO] - Start iterating over samples
loss: tensor(6465.8145, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 21:36:16,339][train_inner][INFO] - {"epoch": 19, "update": 18.111, "loss": "3.964", "ntokens": "149122", "nsentences": "536.255", "prob_perplexity": "232.663", "code_perplexity": "203.351", "temp": "1.909", "loss_0": "3.86", "loss_1": "0.092", "loss_2": "0.012", "accuracy": "0.31899", "wps": "36428.4", "ups": "0.24", "wpb": "149122", "bsz": "536.3", "num_updates": "9400", "lr": "0.000146875", "gnorm": "0.477", "loss_scale": "1", "train_wall": "789", "gb_free": "13", "wall": "37849"}
loss: tensor(6299.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2303, 0.2305, 0.2280, 0.2262, 0.2246, 0.1091, 0.2302, 0.2300, 0.2296,
        0.2302, 0.2166, 0.2211, 0.2212, 0.2306, 0.2303, 0.2305, 0.2271, 0.2303,
        0.1112, 0.2306, 0.2305, 0.1131, 0.1112, 0.1113, 0.2185, 0.2302, 0.2305,
        0.2272, 0.2277, 0.2303, 0.1121, 0.2299, 0.2256, 0.2305, 0.2301, 0.2299,
        0.2285, 0.2306, 0.2305, 0.2180, 0.2306, 0.1118, 0.2306, 0.2297, 0.2291,
        0.1117, 0.2115, 0.2305, 0.2305, 0.2295, 0.2306, 0.1091, 0.2213, 0.2294,
        0.2137, 0.2294, 0.1119, 0.2186, 0.2305, 0.2291, 0.2271, 0.2299, 0.2224,
        0.2299, 0.1130, 0.2305, 0.2306, 0.2291, 0.2306, 0.2305, 0.1116, 0.2305,
        0.2279, 0.2305, 0.2296, 0.2069, 0.2291, 0.1133, 0.2306, 0.2211, 0.2279,
        0.2302, 0.1114, 0.2305, 0.2299, 0.1122, 0.2303, 0.2299, 0.2295, 0.2296,
        0.1091, 0.1113, 0.2299, 0.2305, 0.2184, 0.1063, 0.1116, 0.2306, 0.1115,
        0.2305, 0.2303, 0.2305, 0.2301, 0.2292, 0.1128, 0.2306, 0.2305, 0.1115,
        0.2126, 0.1124, 0.2289, 0.1119, 0.2222, 0.2305, 0.2280, 0.2273, 0.2302,
        0.1113, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2305, 0.2162, 0.1119,
        0.2305, 0.2306, 0.2301, 0.2305, 0.2305, 0.2306, 0.2258, 0.2289, 0.1091,
        0.2279, 0.2297, 0.2186, 0.2264, 0.2306, 0.2296, 0.2203, 0.2301, 0.2299,
        0.2291, 0.2303, 0.2244, 0.2296, 0.2302, 0.1092, 0.2280, 0.2301, 0.1137,
        0.2208, 0.2305, 0.2194, 0.2305, 0.2305, 0.2288, 0.2306, 0.2303, 0.2305,
        0.2305, 0.2285, 0.2306, 0.2303, 0.2302, 0.2289, 0.2274, 0.2297, 0.2305,
        0.2305, 0.2289, 0.2305, 0.2297, 0.1119, 0.2305, 0.2301, 0.1115, 0.2225,
        0.2302, 0.2299, 0.2302, 0.2216, 0.2303, 0.1091, 0.2306, 0.2302, 0.2306,
        0.1117, 0.2289, 0.2303, 0.2303, 0.1119, 0.2288, 0.2273, 0.2279, 0.2301,
        0.2303, 0.2295, 0.2220, 0.2305, 0.2302, 0.2306, 0.2303, 0.1130, 0.2197,
        0.2195, 0.2305, 0.2305, 0.2289, 0.1117, 0.2303, 0.2306, 0.2306, 0.2305,
        0.1119, 0.2306, 0.2305, 0.2305, 0.2305, 0.1125, 0.2289, 0.2306, 0.2290,
        0.1121, 0.2306, 0.2305, 0.2306, 0.1114, 0.2288, 0.2104, 0.2305, 0.2303,
        0.2305, 0.2297, 0.2286, 0.1124, 0.2296, 0.1122, 0.2305, 0.2261, 0.2239,
        0.2194, 0.2079, 0.2305, 0.2230, 0.1122, 0.2305, 0.2306, 0.2230, 0.2267,
        0.1119, 0.2084, 0.2285, 0.2306, 0.2299, 0.2305, 0.2278, 0.2164, 0.2229,
        0.2302, 0.2301, 0.1128, 0.2294, 0.2251, 0.2305, 0.2306, 0.1093, 0.2299,
        0.1125, 0.2296, 0.2306, 0.2300, 0.2286, 0.2306, 0.1116, 0.2303, 0.1118,
        0.1115, 0.2305, 0.2300, 0.1091, 0.2307, 0.2302, 0.2289, 0.2306, 0.2295,
        0.2306, 0.2302, 0.2306, 0.1091, 0.2286, 0.2303, 0.2296, 0.1112, 0.2278,
        0.1112, 0.2261, 0.2305, 0.2302, 0.2056, 0.1124, 0.2175, 0.2262, 0.2268,
        0.2303, 0.2306, 0.2305, 0.1128, 0.2305, 0.2289, 0.2273, 0.2296, 0.2305,
        0.1091, 0.2305, 0.1118, 0.1121, 0.2303, 0.2288, 0.1115, 0.2306, 0.2305,
        0.2289, 0.2288, 0.2303, 0.2181, 0.2305, 0.2283, 0.2288, 0.2306, 0.2303,
        0.1116, 0.2290, 0.2267, 0.2288, 0.1124, 0.2300, 0.2303, 0.2295, 0.2303,
        0.2305, 0.2306, 0.2305, 0.1126, 0.2229, 0.1115, 0.2166, 0.2306, 0.2256,
        0.1118, 0.2301, 0.2306, 0.2285, 0.2303, 0.2306, 0.2054, 0.2306, 0.2200,
        0.2303, 0.2305, 0.2258, 0.2305, 0.2036, 0.2296, 0.2301, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2303, 0.2306, 0.2291, 0.2262, 0.2306, 0.1113,
        0.1122, 0.2246, 0.2307, 0.1129, 0.2306, 0.2306, 0.2238, 0.2305, 0.2145,
        0.2147, 0.2278, 0.2305, 0.2245, 0.2306, 0.2305, 0.2302, 0.1120, 0.2307,
        0.2284, 0.1113, 0.2303, 0.1113, 0.2306, 0.1119, 0.2172, 0.2303, 0.2305,
        0.1116, 0.2306, 0.1115, 0.2294, 0.2303, 0.2305, 0.2305, 0.1138, 0.2279,
        0.2301, 0.2305, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2305, 0.2303, 0.2296, 0.2306, 0.2303, 0.2274, 0.2305, 0.2302, 0.2300,
        0.1118, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2285, 0.2294, 0.2297,
        0.2305, 0.2303, 0.2301, 0.2305, 0.2242, 0.2278, 0.2306, 0.1113, 0.2273,
        0.2305, 0.1110, 0.2299, 0.2306, 0.2281, 0.2306, 0.2305, 0.2303, 0.1118,
        0.1091, 0.2230, 0.2303, 0.2297, 0.2306, 0.2212, 0.2305, 0.2262, 0.2156,
        0.2258, 0.1122, 0.1119, 0.2272, 0.2303, 0.2305, 0.2299, 0.2179, 0.2301,
        0.2290, 0.2306, 0.2300, 0.2306, 0.2188, 0.2301, 0.2305, 0.2305, 0.1120,
        0.2300, 0.1121, 0.2297, 0.2305, 0.2301, 0.2303, 0.2219, 0.2306, 0.2305,
        0.2299, 0.2305, 0.2306, 0.1119, 0.2306, 0.2305, 0.2303, 0.2301, 0.2305,
        0.2305, 0.2306, 0.2301, 0.2299, 0.2242, 0.2236, 0.1116, 0.2306, 0.1120,
        0.2305, 0.2306, 0.2305, 0.2297, 0.2305, 0.2305, 0.2180, 0.2286, 0.2305,
        0.2305, 0.2305, 0.1092, 0.1109, 0.2306, 0.2300, 0.1128, 0.2303, 0.2175,
        0.2234, 0.2208, 0.2305, 0.2305, 0.1091, 0.2305, 0.2306, 0.2277, 0.2299,
        0.1112, 0.2234, 0.1110, 0.2306, 0.2169, 0.2306, 0.2305, 0.2302, 0.2306,
        0.2168, 0.2305, 0.1120, 0.2306, 0.1115, 0.2267, 0.2301, 0.2306, 0.2153,
        0.1118, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2307, 0.2278, 0.2292,
        0.2305, 0.2252, 0.1111, 0.2224, 0.2278, 0.2306, 0.2305, 0.2303, 0.2305,
        0.2306, 0.2301, 0.2253, 0.2303, 0.2305, 0.2155, 0.2305, 0.1110, 0.2306,
        0.2305, 0.2303, 0.1116, 0.2234, 0.2300, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2303, 0.2301, 0.2305, 0.1091, 0.2220, 0.1109, 0.2037, 0.1118, 0.2291,
        0.2303, 0.2302, 0.2305, 0.2275, 0.2302, 0.2159, 0.2306, 0.2302, 0.2305,
        0.2302, 0.2225, 0.2269, 0.2306, 0.2292, 0.2285, 0.2305, 0.2303, 0.1111,
        0.2291, 0.2301, 0.2280, 0.1111, 0.2299, 0.2301, 0.2294, 0.2305, 0.2305,
        0.2269, 0.2297, 0.2305, 0.2306, 0.2305, 0.2289, 0.2305, 0.2306, 0.2275,
        0.1119], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 21:46:08,326][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Parameter containing:
tensor([0.2305, 0.2305, 0.2281, 0.2263, 0.2247, 0.1093, 0.2303, 0.2301, 0.2297,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2306, 0.2271, 0.2305,
        0.1118, 0.2306, 0.2306, 0.1147, 0.1118, 0.1118, 0.2186, 0.2303, 0.2305,
        0.2273, 0.2277, 0.2305, 0.1130, 0.2299, 0.2257, 0.2305, 0.2301, 0.2299,
        0.2286, 0.2306, 0.2305, 0.2180, 0.2306, 0.1127, 0.2306, 0.2299, 0.2292,
        0.1125, 0.2117, 0.2306, 0.2306, 0.2296, 0.2306, 0.1093, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1127, 0.2188, 0.2306, 0.2292, 0.2272, 0.2300, 0.2224,
        0.2299, 0.1148, 0.2306, 0.2306, 0.2292, 0.2306, 0.2306, 0.1122, 0.2306,
        0.2279, 0.2306, 0.2297, 0.2070, 0.2292, 0.1135, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1121, 0.2306, 0.2299, 0.1130, 0.2305, 0.2299, 0.2296, 0.2296,
        0.1093, 0.1120, 0.2300, 0.2306, 0.2184, 0.1062, 0.1120, 0.2306, 0.1120,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1137, 0.2306, 0.2306, 0.1119,
        0.2126, 0.1134, 0.2289, 0.1126, 0.2223, 0.2306, 0.2281, 0.2273, 0.2303,
        0.1119, 0.2285, 0.2238, 0.2306, 0.2306, 0.2306, 0.2306, 0.2163, 0.1128,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2306, 0.2260, 0.2290, 0.1093,
        0.2279, 0.2299, 0.2188, 0.2264, 0.2306, 0.2296, 0.2205, 0.2302, 0.2300,
        0.2292, 0.2305, 0.2244, 0.2297, 0.2303, 0.1094, 0.2281, 0.2301, 0.1157,
        0.2209, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2305, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2305, 0.2303, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1126, 0.2306, 0.2302, 0.1125, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2217, 0.2305, 0.1094, 0.2306, 0.2303, 0.2306,
        0.1125, 0.2290, 0.2305, 0.2305, 0.1127, 0.2289, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2296, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1138, 0.2198,
        0.2195, 0.2306, 0.2305, 0.2290, 0.1124, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1127, 0.2306, 0.2306, 0.2306, 0.2305, 0.1160, 0.2290, 0.2306, 0.2291,
        0.1132, 0.2306, 0.2305, 0.2306, 0.1119, 0.2289, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1131, 0.2296, 0.1128, 0.2306, 0.2262, 0.2240,
        0.2195, 0.2080, 0.2306, 0.2230, 0.1130, 0.2305, 0.2306, 0.2231, 0.2268,
        0.1130, 0.2085, 0.2286, 0.2306, 0.2300, 0.2306, 0.2279, 0.2166, 0.2229,
        0.2302, 0.2302, 0.1143, 0.2294, 0.2252, 0.2306, 0.2306, 0.1096, 0.2299,
        0.1136, 0.2297, 0.2306, 0.2301, 0.2286, 0.2307, 0.1124, 0.2305, 0.1127,
        0.1120, 0.2306, 0.2301, 0.1093, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1093, 0.2286, 0.2305, 0.2297, 0.1119, 0.2278,
        0.1117, 0.2262, 0.2306, 0.2302, 0.2056, 0.1133, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2307, 0.2306, 0.1135, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1093, 0.2306, 0.1125, 0.1129, 0.2305, 0.2288, 0.1121, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2305, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1122, 0.2291, 0.2268, 0.2289, 0.1133, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1149, 0.2230, 0.1121, 0.2167, 0.2306, 0.2257,
        0.1126, 0.2302, 0.2306, 0.2286, 0.2305, 0.2306, 0.2056, 0.2306, 0.2200,
        0.2305, 0.2306, 0.2260, 0.2306, 0.2037, 0.2297, 0.2302, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2305, 0.2306, 0.2292, 0.2263, 0.2306, 0.1121,
        0.1131, 0.2246, 0.2307, 0.1138, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2303, 0.1132, 0.2307,
        0.2285, 0.1119, 0.2305, 0.1119, 0.2306, 0.1131, 0.2173, 0.2305, 0.2306,
        0.1124, 0.2306, 0.1121, 0.2295, 0.2305, 0.2306, 0.2306, 0.1157, 0.2280,
        0.2301, 0.2306, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2295,
        0.2306, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2303, 0.2300,
        0.1126, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1120, 0.2273,
        0.2306, 0.1114, 0.2299, 0.2307, 0.2283, 0.2306, 0.2306, 0.2303, 0.1127,
        0.1093, 0.2231, 0.2305, 0.2299, 0.2306, 0.2213, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1132, 0.1127, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1130,
        0.2301, 0.1128, 0.2297, 0.2306, 0.2302, 0.2303, 0.2220, 0.2306, 0.2306,
        0.2300, 0.2306, 0.2306, 0.1126, 0.2306, 0.2306, 0.2305, 0.2302, 0.2306,
        0.2305, 0.2306, 0.2302, 0.2299, 0.2244, 0.2238, 0.1125, 0.2306, 0.1129,
        0.2306, 0.2306, 0.2306, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2306, 0.2306, 0.1094, 0.1115, 0.2306, 0.2301, 0.1139, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1094, 0.2306, 0.2306, 0.2278, 0.2300,
        0.1118, 0.2234, 0.1116, 0.2306, 0.2170, 0.2307, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1130, 0.2306, 0.1119, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1126, 0.2306, 0.2306, 0.2302, 0.2306, 0.2306, 0.2308, 0.2279, 0.2294,
        0.2305, 0.2253, 0.1117, 0.2224, 0.2279, 0.2306, 0.2306, 0.2305, 0.2306,
        0.2306, 0.2301, 0.2255, 0.2305, 0.2306, 0.2156, 0.2306, 0.1114, 0.2306,
        0.2305, 0.2305, 0.1123, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2305, 0.2302, 0.2305, 0.1093, 0.2222, 0.1113, 0.2037, 0.1125, 0.2291,
        0.2305, 0.2303, 0.2306, 0.2277, 0.2303, 0.2161, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1117,
        0.2292, 0.2302, 0.2281, 0.1118, 0.2300, 0.2302, 0.2294, 0.2306, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1127], device='cuda:2', dtype=torch.float16, requires_grad=True)
loss: tensor(7383.7031, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6979.6543, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5964.9482, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7096.4902, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6219.4048, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5080.3789, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6386.5078, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6988.9370, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7250.1074, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5960.6836, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6937.7705, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6303.1963, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5462.2808, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7059.7358, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6539.3730, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7957.7119, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6742.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 21:49:40,394][train_inner][INFO] - {"epoch": 19, "update": 18.497, "loss": "3.95", "ntokens": "149951", "nsentences": "537.525", "prob_perplexity": "240.961", "code_perplexity": "211.794", "temp": "1.907", "loss_0": "3.848", "loss_1": "0.09", "loss_2": "0.012", "accuracy": "0.31893", "wps": "37298.7", "ups": "0.25", "wpb": "149951", "bsz": "537.5", "num_updates": "9600", "lr": "0.00015", "gnorm": "0.455", "loss_scale": "1", "train_wall": "803", "gb_free": "12.4", "wall": "38653"}
Parameter containing:
tensor([0.2305, 0.2305, 0.2280, 0.2263, 0.2247, 0.1093, 0.2303, 0.2301, 0.2297,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2306, 0.2271, 0.2305,
        0.1119, 0.2306, 0.2306, 0.1148, 0.1118, 0.1119, 0.2186, 0.2303, 0.2305,
        0.2273, 0.2277, 0.2303, 0.1131, 0.2299, 0.2257, 0.2305, 0.2301, 0.2299,
        0.2286, 0.2306, 0.2305, 0.2180, 0.2306, 0.1127, 0.2306, 0.2299, 0.2292,
        0.1125, 0.2117, 0.2306, 0.2306, 0.2296, 0.2306, 0.1094, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1127, 0.2188, 0.2305, 0.2292, 0.2272, 0.2300, 0.2224,
        0.2299, 0.1149, 0.2306, 0.2306, 0.2291, 0.2306, 0.2306, 0.1122, 0.2306,
        0.2279, 0.2306, 0.2297, 0.2070, 0.2292, 0.1133, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1121, 0.2306, 0.2299, 0.1131, 0.2305, 0.2299, 0.2296, 0.2296,
        0.1093, 0.1120, 0.2300, 0.2306, 0.2184, 0.1062, 0.1121, 0.2306, 0.1121,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1140, 0.2306, 0.2306, 0.1119,
        0.2126, 0.1135, 0.2289, 0.1127, 0.2223, 0.2306, 0.2281, 0.2273, 0.2303,
        0.1119, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2306, 0.2162, 0.1129,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2305, 0.2306, 0.2260, 0.2290, 0.1093,
        0.2279, 0.2299, 0.2188, 0.2264, 0.2306, 0.2296, 0.2205, 0.2301, 0.2300,
        0.2291, 0.2305, 0.2244, 0.2297, 0.2303, 0.1094, 0.2281, 0.2301, 0.1160,
        0.2208, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2305, 0.2303, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1127, 0.2306, 0.2301, 0.1125, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2217, 0.2305, 0.1094, 0.2306, 0.2303, 0.2306,
        0.1125, 0.2290, 0.2303, 0.2305, 0.1128, 0.2288, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2296, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1141, 0.2198,
        0.2195, 0.2306, 0.2305, 0.2290, 0.1124, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1127, 0.2306, 0.2306, 0.2305, 0.2305, 0.1164, 0.2290, 0.2306, 0.2291,
        0.1132, 0.2306, 0.2305, 0.2306, 0.1119, 0.2289, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1132, 0.2296, 0.1128, 0.2306, 0.2262, 0.2240,
        0.2195, 0.2079, 0.2306, 0.2230, 0.1131, 0.2305, 0.2306, 0.2231, 0.2268,
        0.1130, 0.2085, 0.2286, 0.2306, 0.2300, 0.2306, 0.2279, 0.2166, 0.2229,
        0.2302, 0.2302, 0.1146, 0.2294, 0.2252, 0.2306, 0.2306, 0.1096, 0.2299,
        0.1136, 0.2297, 0.2306, 0.2301, 0.2286, 0.2307, 0.1125, 0.2305, 0.1127,
        0.1120, 0.2305, 0.2301, 0.1093, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1093, 0.2286, 0.2305, 0.2297, 0.1119, 0.2278,
        0.1118, 0.2262, 0.2306, 0.2302, 0.2056, 0.1133, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2307, 0.2306, 0.1135, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1093, 0.2306, 0.1125, 0.1129, 0.2305, 0.2288, 0.1122, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2305, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1122, 0.2291, 0.2268, 0.2289, 0.1134, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1151, 0.2230, 0.1121, 0.2167, 0.2306, 0.2257,
        0.1127, 0.2302, 0.2306, 0.2286, 0.2305, 0.2306, 0.2056, 0.2306, 0.2200,
        0.2305, 0.2306, 0.2258, 0.2306, 0.2036, 0.2297, 0.2302, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2305, 0.2306, 0.2292, 0.2263, 0.2306, 0.1120,
        0.1130, 0.2246, 0.2307, 0.1139, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2303, 0.1134, 0.2307,
        0.2285, 0.1119, 0.2305, 0.1119, 0.2306, 0.1132, 0.2173, 0.2305, 0.2305,
        0.1124, 0.2306, 0.1121, 0.2295, 0.2303, 0.2306, 0.2306, 0.1158, 0.2280,
        0.2301, 0.2306, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2306, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2302, 0.2300,
        0.1126, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1120, 0.2273,
        0.2306, 0.1115, 0.2299, 0.2306, 0.2283, 0.2306, 0.2306, 0.2303, 0.1128,
        0.1093, 0.2231, 0.2305, 0.2299, 0.2306, 0.2213, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1132, 0.1127, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1131,
        0.2301, 0.1130, 0.2297, 0.2305, 0.2301, 0.2303, 0.2220, 0.2306, 0.2306,
        0.2300, 0.2306, 0.2306, 0.1127, 0.2306, 0.2306, 0.2303, 0.2302, 0.2306,
        0.2305, 0.2306, 0.2302, 0.2299, 0.2244, 0.2238, 0.1125, 0.2306, 0.1129,
        0.2306, 0.2306, 0.2306, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2306, 0.2306, 0.1094, 0.1115, 0.2306, 0.2301, 0.1139, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1094, 0.2306, 0.2306, 0.2278, 0.2300,
        0.1118, 0.2234, 0.1116, 0.2306, 0.2170, 0.2307, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1131, 0.2306, 0.1119, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1127, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2308, 0.2279, 0.2294,
        0.2305, 0.2253, 0.1117, 0.2224, 0.2279, 0.2306, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2301, 0.2253, 0.2305, 0.2306, 0.2156, 0.2306, 0.1114, 0.2306,
        0.2305, 0.2305, 0.1124, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2305, 0.2302, 0.2305, 0.1093, 0.2222, 0.1114, 0.2037, 0.1125, 0.2291,
        0.2305, 0.2303, 0.2306, 0.2277, 0.2303, 0.2159, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1117,
        0.2292, 0.2302, 0.2281, 0.1118, 0.2300, 0.2302, 0.2294, 0.2306, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1127], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7604.5107, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5284.3330, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4469.0518, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6566.4609, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6293.5864, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3690.7124, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5294.1240, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6559.8218, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7254.5591, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7644.6240, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5384.8521, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6496.6035, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7023.5811, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(5699.4878, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5427.2559, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6749.3188, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6817.3721, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6452.9712, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 22:02:52,428][train_inner][INFO] - {"epoch": 19, "update": 18.881, "loss": "3.924", "ntokens": "149522", "nsentences": "540.17", "prob_perplexity": "247.134", "code_perplexity": "217.751", "temp": "1.905", "loss_0": "3.824", "loss_1": "0.089", "loss_2": "0.012", "accuracy": "0.32136", "wps": "37756.6", "ups": "0.25", "wpb": "149522", "bsz": "540.2", "num_updates": "9800", "lr": "0.000153125", "gnorm": "0.455", "loss_scale": "1", "train_wall": "791", "gb_free": "13", "wall": "39445"}
[2023-09-12 22:03:31,275][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Parameter containing:
tensor([0.2305, 0.2305, 0.2280, 0.2263, 0.2247, 0.1093, 0.2303, 0.2301, 0.2297,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2306, 0.2271, 0.2305,
        0.1119, 0.2306, 0.2306, 0.1148, 0.1118, 0.1119, 0.2186, 0.2303, 0.2305,
        0.2273, 0.2277, 0.2303, 0.1131, 0.2299, 0.2257, 0.2305, 0.2301, 0.2299,
        0.2286, 0.2306, 0.2305, 0.2180, 0.2306, 0.1127, 0.2306, 0.2299, 0.2292,
        0.1125, 0.2117, 0.2306, 0.2306, 0.2296, 0.2306, 0.1094, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1127, 0.2188, 0.2305, 0.2292, 0.2272, 0.2300, 0.2224,
        0.2299, 0.1149, 0.2306, 0.2306, 0.2291, 0.2306, 0.2306, 0.1122, 0.2306,
        0.2279, 0.2306, 0.2297, 0.2070, 0.2292, 0.1133, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1121, 0.2306, 0.2299, 0.1131, 0.2305, 0.2299, 0.2296, 0.2296,
        0.1093, 0.1120, 0.2300, 0.2306, 0.2184, 0.1062, 0.1121, 0.2306, 0.1121,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1140, 0.2306, 0.2306, 0.1119,
        0.2126, 0.1135, 0.2289, 0.1127, 0.2223, 0.2306, 0.2281, 0.2273, 0.2303,
        0.1119, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2306, 0.2162, 0.1129,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2305, 0.2306, 0.2260, 0.2290, 0.1093,
        0.2279, 0.2299, 0.2188, 0.2264, 0.2306, 0.2296, 0.2205, 0.2301, 0.2300,
        0.2291, 0.2305, 0.2244, 0.2297, 0.2303, 0.1094, 0.2281, 0.2301, 0.1160,
        0.2208, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2305, 0.2303, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1127, 0.2306, 0.2301, 0.1125, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2217, 0.2305, 0.1094, 0.2306, 0.2303, 0.2306,
        0.1125, 0.2290, 0.2303, 0.2305, 0.1128, 0.2288, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2296, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1141, 0.2198,
        0.2195, 0.2306, 0.2305, 0.2290, 0.1124, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1127, 0.2306, 0.2306, 0.2305, 0.2305, 0.1164, 0.2290, 0.2306, 0.2291,
        0.1132, 0.2306, 0.2305, 0.2306, 0.1119, 0.2289, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1132, 0.2296, 0.1128, 0.2306, 0.2262, 0.2240,
        0.2195, 0.2079, 0.2306, 0.2230, 0.1131, 0.2305, 0.2306, 0.2231, 0.2268,
        0.1130, 0.2085, 0.2286, 0.2306, 0.2300, 0.2306, 0.2279, 0.2166, 0.2229,
        0.2302, 0.2302, 0.1146, 0.2294, 0.2252, 0.2306, 0.2306, 0.1096, 0.2299,
        0.1136, 0.2297, 0.2306, 0.2301, 0.2286, 0.2307, 0.1125, 0.2305, 0.1127,
        0.1120, 0.2305, 0.2301, 0.1093, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1093, 0.2286, 0.2305, 0.2297, 0.1119, 0.2278,
        0.1118, 0.2262, 0.2306, 0.2302, 0.2056, 0.1133, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2307, 0.2306, 0.1135, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1093, 0.2306, 0.1125, 0.1129, 0.2305, 0.2288, 0.1122, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2305, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1122, 0.2291, 0.2268, 0.2289, 0.1134, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1151, 0.2230, 0.1121, 0.2167, 0.2306, 0.2257,
        0.1127, 0.2302, 0.2306, 0.2286, 0.2305, 0.2306, 0.2056, 0.2306, 0.2200,
        0.2305, 0.2306, 0.2258, 0.2306, 0.2036, 0.2297, 0.2302, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2305, 0.2306, 0.2292, 0.2263, 0.2306, 0.1120,
        0.1130, 0.2246, 0.2307, 0.1139, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2303, 0.1134, 0.2307,
        0.2285, 0.1119, 0.2305, 0.1119, 0.2306, 0.1132, 0.2173, 0.2305, 0.2305,
        0.1124, 0.2306, 0.1121, 0.2295, 0.2303, 0.2306, 0.2306, 0.1158, 0.2280,
        0.2301, 0.2306, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2306, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2302, 0.2300,
        0.1126, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1120, 0.2273,
        0.2306, 0.1115, 0.2299, 0.2306, 0.2283, 0.2306, 0.2306, 0.2303, 0.1128,
        0.1093, 0.2231, 0.2305, 0.2299, 0.2306, 0.2213, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1132, 0.1127, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1131,
        0.2301, 0.1130, 0.2297, 0.2305, 0.2301, 0.2303, 0.2220, 0.2306, 0.2306,
        0.2300, 0.2306, 0.2306, 0.1127, 0.2306, 0.2306, 0.2303, 0.2302, 0.2306,
        0.2305, 0.2306, 0.2302, 0.2299, 0.2244, 0.2238, 0.1125, 0.2306, 0.1129,
        0.2306, 0.2306, 0.2306, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2306, 0.2306, 0.1094, 0.1115, 0.2306, 0.2301, 0.1139, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1094, 0.2306, 0.2306, 0.2278, 0.2300,
        0.1118, 0.2234, 0.1116, 0.2306, 0.2170, 0.2307, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1131, 0.2306, 0.1119, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1127, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2308, 0.2279, 0.2294,
        0.2305, 0.2253, 0.1117, 0.2224, 0.2279, 0.2306, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2301, 0.2253, 0.2305, 0.2306, 0.2156, 0.2306, 0.1114, 0.2306,
        0.2305, 0.2305, 0.1124, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2305, 0.2302, 0.2305, 0.1093, 0.2222, 0.1114, 0.2037, 0.1125, 0.2291,
        0.2305, 0.2303, 0.2306, 0.2277, 0.2303, 0.2159, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1117,
        0.2292, 0.2302, 0.2281, 0.1118, 0.2300, 0.2302, 0.2294, 0.2306, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1127], device='cuda:1', dtype=torch.float16, requires_grad=True)
loss: tensor(6767.0063, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6393.2935, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7137.1226, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6728.6113, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5041.8174, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6707.9746, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5756.4946, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6829.8818, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3780.0659, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7266.7373, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6763.2500, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(3468.8301, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6971.8281, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6595.2065, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5079.0669, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7414.8203, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
[2023-09-12 22:06:57,565][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2023-09-12 22:06:57,566][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 22:06:57,689][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2023-09-12 22:07:20,953][valid][INFO] - {"epoch": 19, "valid_loss": "3.646", "valid_ntokens": "7913.52", "valid_nsentences": "55.2525", "valid_prob_perplexity": "225.36", "valid_code_perplexity": "195.66", "valid_temp": "1.904", "valid_loss_0": "3.54", "valid_loss_1": "0.093", "valid_loss_2": "0.012", "valid_accuracy": "0.3866", "valid_wps": "33915.6", "valid_wpb": "7913.5", "valid_bsz": "55.3", "valid_num_updates": "9861", "valid_best_loss": "3.635"}
[2023-09-12 22:07:20,955][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 9861 updates
[2023-09-12 22:07:20,956][fairseq.trainer][INFO] - Saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_last.pt
[2023-09-12 22:07:23,483][fairseq.trainer][INFO] - Finished saving checkpoint to /home/Workspace/fairseq/outputs/2023-09-12/11-05-19/checkpoints/checkpoint_last.pt
[2023-09-12 22:07:23,531][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 19 @ 9861 updates, score 3.646) (writing took 2.576504602096975 seconds)
[2023-09-12 22:07:23,532][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2023-09-12 22:07:23,532][train][INFO] - {"epoch": 19, "train_loss": "3.939", "train_ntokens": "149510", "train_nsentences": "538.36", "train_prob_perplexity": "243.597", "train_code_perplexity": "214.28", "train_temp": "1.906", "train_loss_0": "3.838", "train_loss_1": "0.089", "train_loss_2": "0.012", "train_accuracy": "0.32023", "train_wps": "36935.8", "train_ups": "0.25", "train_wpb": "149510", "train_bsz": "538.4", "train_num_updates": "9861", "train_lr": "0.000154078", "train_gnorm": "0.456", "train_loss_scale": "1", "train_train_wall": "2071", "train_gb_free": "13.2", "train_wall": "39716"}
[2023-09-12 22:07:23,534][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2023-09-12 22:07:23,624][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2023-09-12 22:07:23,873][fairseq.data.iterators][INFO] - grouped total_num_itrs = 521
[2023-09-12 22:07:23,876][fairseq.trainer][INFO] - begin training epoch 20
[2023-09-12 22:07:23,876][fairseq_cli.train][INFO] - Start iterating over samples
Parameter containing:
tensor([0.2303, 0.2303, 0.2280, 0.2262, 0.2246, 0.1091, 0.2302, 0.2300, 0.2296,
        0.2302, 0.2166, 0.2211, 0.2212, 0.2305, 0.2303, 0.2305, 0.2271, 0.2303,
        0.1110, 0.2305, 0.2305, 0.1128, 0.1110, 0.1111, 0.2185, 0.2302, 0.2303,
        0.2272, 0.2277, 0.2303, 0.1119, 0.2299, 0.2256, 0.2303, 0.2301, 0.2297,
        0.2285, 0.2305, 0.2303, 0.2179, 0.2305, 0.1115, 0.2305, 0.2297, 0.2291,
        0.1116, 0.2115, 0.2305, 0.2305, 0.2295, 0.2305, 0.1091, 0.2213, 0.2294,
        0.2137, 0.2294, 0.1115, 0.2186, 0.2305, 0.2291, 0.2271, 0.2299, 0.2224,
        0.2299, 0.1127, 0.2305, 0.2305, 0.2291, 0.2305, 0.2305, 0.1113, 0.2305,
        0.2279, 0.2305, 0.2296, 0.2069, 0.2291, 0.1129, 0.2305, 0.2211, 0.2279,
        0.2302, 0.1112, 0.2305, 0.2299, 0.1120, 0.2303, 0.2299, 0.2295, 0.2296,
        0.1091, 0.1111, 0.2299, 0.2305, 0.2184, 0.1063, 0.1114, 0.2305, 0.1112,
        0.2305, 0.2303, 0.2305, 0.2301, 0.2292, 0.1125, 0.2305, 0.2305, 0.1111,
        0.2125, 0.1121, 0.2289, 0.1118, 0.2222, 0.2305, 0.2280, 0.2273, 0.2302,
        0.1111, 0.2285, 0.2238, 0.2305, 0.2305, 0.2306, 0.2305, 0.2162, 0.1118,
        0.2305, 0.2305, 0.2300, 0.2305, 0.2305, 0.2305, 0.2258, 0.2289, 0.1091,
        0.2278, 0.2297, 0.2186, 0.2264, 0.2305, 0.2295, 0.2203, 0.2301, 0.2299,
        0.2291, 0.2303, 0.2244, 0.2296, 0.2302, 0.1091, 0.2280, 0.2300, 0.1133,
        0.2208, 0.2305, 0.2192, 0.2305, 0.2305, 0.2288, 0.2305, 0.2303, 0.2305,
        0.2305, 0.2285, 0.2305, 0.2303, 0.2302, 0.2289, 0.2274, 0.2297, 0.2305,
        0.2305, 0.2289, 0.2305, 0.2297, 0.1118, 0.2305, 0.2301, 0.1115, 0.2225,
        0.2302, 0.2297, 0.2302, 0.2216, 0.2303, 0.1091, 0.2305, 0.2302, 0.2305,
        0.1115, 0.2289, 0.2303, 0.2303, 0.1116, 0.2288, 0.2273, 0.2278, 0.2301,
        0.2303, 0.2295, 0.2220, 0.2303, 0.2302, 0.2305, 0.2303, 0.1129, 0.2197,
        0.2195, 0.2305, 0.2303, 0.2289, 0.1114, 0.2303, 0.2305, 0.2305, 0.2303,
        0.1118, 0.2306, 0.2305, 0.2305, 0.2303, 0.1124, 0.2289, 0.2305, 0.2290,
        0.1118, 0.2305, 0.2305, 0.2305, 0.1112, 0.2288, 0.2103, 0.2305, 0.2302,
        0.2305, 0.2297, 0.2286, 0.1123, 0.2295, 0.1119, 0.2305, 0.2261, 0.2239,
        0.2194, 0.2079, 0.2305, 0.2230, 0.1118, 0.2305, 0.2305, 0.2230, 0.2267,
        0.1117, 0.2084, 0.2285, 0.2305, 0.2299, 0.2305, 0.2278, 0.2164, 0.2228,
        0.2302, 0.2301, 0.1125, 0.2294, 0.2251, 0.2305, 0.2305, 0.1093, 0.2297,
        0.1122, 0.2296, 0.2305, 0.2300, 0.2286, 0.2306, 0.1114, 0.2303, 0.1115,
        0.1112, 0.2305, 0.2300, 0.1091, 0.2306, 0.2302, 0.2288, 0.2306, 0.2295,
        0.2305, 0.2302, 0.2305, 0.1091, 0.2286, 0.2303, 0.2296, 0.1110, 0.2278,
        0.1109, 0.2261, 0.2305, 0.2302, 0.2056, 0.1122, 0.2175, 0.2262, 0.2268,
        0.2303, 0.2306, 0.2305, 0.1127, 0.2305, 0.2289, 0.2272, 0.2295, 0.2305,
        0.1090, 0.2305, 0.1116, 0.1119, 0.2303, 0.2288, 0.1113, 0.2305, 0.2305,
        0.2289, 0.2288, 0.2303, 0.2181, 0.2305, 0.2283, 0.2288, 0.2305, 0.2303,
        0.1114, 0.2290, 0.2267, 0.2288, 0.1122, 0.2300, 0.2303, 0.2295, 0.2303,
        0.2305, 0.2305, 0.2305, 0.1123, 0.2229, 0.1113, 0.2166, 0.2305, 0.2256,
        0.1116, 0.2301, 0.2305, 0.2285, 0.2303, 0.2305, 0.2054, 0.2305, 0.2200,
        0.2303, 0.2305, 0.2258, 0.2305, 0.2036, 0.2296, 0.2301, 0.2305, 0.2305,
        0.2281, 0.2305, 0.2305, 0.2303, 0.2305, 0.2291, 0.2262, 0.2305, 0.1111,
        0.1120, 0.2245, 0.2307, 0.1127, 0.2305, 0.2305, 0.2238, 0.2305, 0.2145,
        0.2147, 0.2278, 0.2305, 0.2245, 0.2305, 0.2305, 0.2302, 0.1117, 0.2306,
        0.2284, 0.1111, 0.2303, 0.1111, 0.2305, 0.1119, 0.2172, 0.2303, 0.2305,
        0.1113, 0.2305, 0.1111, 0.2294, 0.2303, 0.2305, 0.2305, 0.1134, 0.2279,
        0.2301, 0.2305, 0.2305, 0.2245, 0.2305, 0.2303, 0.2305, 0.2305, 0.2294,
        0.2305, 0.2303, 0.2296, 0.2305, 0.2303, 0.2274, 0.2305, 0.2302, 0.2300,
        0.1114, 0.2269, 0.2305, 0.2302, 0.2305, 0.2295, 0.2285, 0.2294, 0.2297,
        0.2305, 0.2303, 0.2301, 0.2305, 0.2242, 0.2278, 0.2305, 0.1111, 0.2272,
        0.2305, 0.1108, 0.2299, 0.2306, 0.2281, 0.2305, 0.2305, 0.2303, 0.1115,
        0.1091, 0.2230, 0.2303, 0.2297, 0.2305, 0.2212, 0.2305, 0.2262, 0.2156,
        0.2258, 0.1122, 0.1117, 0.2272, 0.2303, 0.2305, 0.2299, 0.2179, 0.2301,
        0.2290, 0.2305, 0.2300, 0.2305, 0.2188, 0.2301, 0.2305, 0.2303, 0.1117,
        0.2300, 0.1119, 0.2296, 0.2305, 0.2301, 0.2303, 0.2219, 0.2305, 0.2305,
        0.2299, 0.2305, 0.2305, 0.1116, 0.2305, 0.2305, 0.2303, 0.2301, 0.2305,
        0.2303, 0.2305, 0.2301, 0.2297, 0.2242, 0.2236, 0.1115, 0.2305, 0.1116,
        0.2305, 0.2305, 0.2305, 0.2297, 0.2305, 0.2305, 0.2180, 0.2286, 0.2305,
        0.2305, 0.2305, 0.1091, 0.1107, 0.2305, 0.2300, 0.1124, 0.2303, 0.2175,
        0.2234, 0.2207, 0.2305, 0.2305, 0.1091, 0.2305, 0.2305, 0.2277, 0.2299,
        0.1110, 0.2233, 0.1108, 0.2305, 0.2169, 0.2306, 0.2305, 0.2302, 0.2306,
        0.2168, 0.2303, 0.1119, 0.2305, 0.1111, 0.2267, 0.2301, 0.2305, 0.2153,
        0.1116, 0.2305, 0.2305, 0.2301, 0.2305, 0.2305, 0.2307, 0.2278, 0.2292,
        0.2303, 0.2252, 0.1109, 0.2223, 0.2278, 0.2305, 0.2305, 0.2303, 0.2305,
        0.2305, 0.2301, 0.2253, 0.2303, 0.2305, 0.2155, 0.2305, 0.1107, 0.2305,
        0.2303, 0.2303, 0.1112, 0.2234, 0.2300, 0.2305, 0.2305, 0.2286, 0.2305,
        0.2303, 0.2301, 0.2305, 0.1091, 0.2220, 0.1106, 0.2037, 0.1116, 0.2290,
        0.2303, 0.2302, 0.2305, 0.2275, 0.2302, 0.2159, 0.2306, 0.2301, 0.2305,
        0.2302, 0.2225, 0.2268, 0.2305, 0.2292, 0.2285, 0.2305, 0.2303, 0.1109,
        0.2291, 0.2301, 0.2280, 0.1109, 0.2299, 0.2301, 0.2294, 0.2305, 0.2305,
        0.2269, 0.2297, 0.2305, 0.2305, 0.2305, 0.2289, 0.2305, 0.2305, 0.2274,
        0.1118], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(4238.4331, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7625.9507, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6749.9668, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: loss: tensor(6405.2124, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6308.4365, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2305, 0.2305, 0.2280, 0.2263, 0.2247, 0.1091, 0.2303, 0.2301, 0.2296,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2305, 0.2271, 0.2305,
        0.1111, 0.2306, 0.2306, 0.1125, 0.1111, 0.1113, 0.2186, 0.2302, 0.2305,
        0.2273, 0.2277, 0.2303, 0.1119, 0.2299, 0.2256, 0.2305, 0.2301, 0.2299,
        0.2285, 0.2306, 0.2305, 0.2180, 0.2306, 0.1116, 0.2306, 0.2299, 0.2292,
        0.1117, 0.2117, 0.2306, 0.2305, 0.2296, 0.2306, 0.1091, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1116, 0.2188, 0.2305, 0.2291, 0.2272, 0.2299, 0.2224,
        0.2299, 0.1125, 0.2305, 0.2306, 0.2291, 0.2306, 0.2306, 0.1114, 0.2306,
        0.2279, 0.2306, 0.2297, 0.2067, 0.2291, 0.1127, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1113, 0.2306, 0.2299, 0.1121, 0.2303, 0.2299, 0.2296, 0.2296,
        0.1091, 0.1112, 0.2299, 0.2306, 0.2184, 0.1065, 0.1115, 0.2306, 0.1113,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1127, 0.2306, 0.2306, 0.1113,
        0.2126, 0.1121, 0.2289, 0.1119, 0.2223, 0.2306, 0.2280, 0.2273, 0.2303,
        0.1112, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2306, 0.2162, 0.1118,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2305, 0.2306, 0.2258, 0.2290, 0.1091,
        0.2279, 0.2299, 0.2188, 0.2264, 0.2306, 0.2296, 0.2205, 0.2301, 0.2300,
        0.2291, 0.2305, 0.2244, 0.2296, 0.2302, 0.1092, 0.2281, 0.2301, 0.1131,
        0.2208, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2303, 0.2302, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1118, 0.2306, 0.2301, 0.1116, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2216, 0.2305, 0.1091, 0.2306, 0.2302, 0.2306,
        0.1116, 0.2290, 0.2303, 0.2305, 0.1116, 0.2288, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2295, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1127, 0.2198,
        0.2195, 0.2305, 0.2305, 0.2289, 0.1116, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1119, 0.2306, 0.2305, 0.2305, 0.2305, 0.1122, 0.2289, 0.2306, 0.2291,
        0.1119, 0.2306, 0.2305, 0.2306, 0.1114, 0.2288, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1124, 0.2296, 0.1120, 0.2306, 0.2261, 0.2240,
        0.2194, 0.2080, 0.2306, 0.2230, 0.1118, 0.2305, 0.2306, 0.2231, 0.2267,
        0.1118, 0.2085, 0.2286, 0.2306, 0.2299, 0.2306, 0.2278, 0.2166, 0.2229,
        0.2302, 0.2301, 0.1125, 0.2294, 0.2252, 0.2305, 0.2306, 0.1094, 0.2299,
        0.1122, 0.2297, 0.2306, 0.2300, 0.2286, 0.2306, 0.1115, 0.2303, 0.1116,
        0.1113, 0.2305, 0.2301, 0.1091, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1091, 0.2286, 0.2303, 0.2297, 0.1111, 0.2278,
        0.1111, 0.2262, 0.2306, 0.2302, 0.2056, 0.1122, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2306, 0.2305, 0.1125, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1091, 0.2306, 0.1116, 0.1119, 0.2305, 0.2288, 0.1115, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2303, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1115, 0.2291, 0.2267, 0.2289, 0.1122, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1121, 0.2230, 0.1113, 0.2167, 0.2306, 0.2257,
        0.1116, 0.2301, 0.2306, 0.2285, 0.2305, 0.2306, 0.2054, 0.2306, 0.2189,
        0.2303, 0.2306, 0.2258, 0.2306, 0.2037, 0.2296, 0.2301, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2303, 0.2306, 0.2292, 0.2262, 0.2306, 0.1112,
        0.1121, 0.2246, 0.2307, 0.1127, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2302, 0.1118, 0.2307,
        0.2284, 0.1112, 0.2305, 0.1112, 0.2306, 0.1119, 0.2173, 0.2303, 0.2305,
        0.1115, 0.2306, 0.1113, 0.2295, 0.2303, 0.2306, 0.2306, 0.1130, 0.2279,
        0.2301, 0.2305, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2305, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2302, 0.2300,
        0.1116, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1112, 0.2273,
        0.2306, 0.1110, 0.2299, 0.2306, 0.2281, 0.2306, 0.2306, 0.2303, 0.1115,
        0.1091, 0.2231, 0.2305, 0.2297, 0.2306, 0.2198, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1121, 0.1117, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1118,
        0.2301, 0.1120, 0.2297, 0.2305, 0.2301, 0.2303, 0.2220, 0.2306, 0.2305,
        0.2300, 0.2306, 0.2306, 0.1116, 0.2306, 0.2305, 0.2303, 0.2301, 0.2306,
        0.2305, 0.2306, 0.2301, 0.2299, 0.2244, 0.2238, 0.1116, 0.2306, 0.1118,
        0.2306, 0.2306, 0.2305, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2305, 0.2305, 0.1092, 0.1108, 0.2306, 0.2301, 0.1125, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1091, 0.2306, 0.2306, 0.2278, 0.2299,
        0.1111, 0.2234, 0.1110, 0.2306, 0.2169, 0.2306, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1118, 0.2306, 0.1113, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1116, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2307, 0.2279, 0.2292,
        0.2305, 0.2253, 0.1110, 0.2224, 0.2279, 0.2306, 0.2305, 0.2303, 0.2305,
        0.2306, 0.2301, 0.2253, 0.2303, 0.2306, 0.2156, 0.2306, 0.1109, 0.2306,
        0.2305, 0.2305, 0.1113, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2303, 0.2301, 0.2305, 0.1091, 0.2222, 0.1108, 0.2037, 0.1116, 0.2291,
        0.2305, 0.2302, 0.2306, 0.2277, 0.2302, 0.2159, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1111,
        0.2292, 0.2301, 0.2280, 0.1110, 0.2300, 0.2301, 0.2294, 0.2305, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1118], device='cuda:0', dtype=torch.float16, requires_grad=True)
[2023-09-12 22:16:34,493][train_inner][INFO] - {"epoch": 20, "update": 19.267, "loss": "3.904", "ntokens": "148989", "nsentences": "536.695", "prob_perplexity": "252.427", "code_perplexity": "223.565", "temp": "1.903", "loss_0": "3.805", "loss_1": "0.087", "loss_2": "0.012", "accuracy": "0.32479", "wps": "36247.4", "ups": "0.24", "wpb": "148989", "bsz": "536.7", "num_updates": "10000", "lr": "0.00015625", "gnorm": "0.462", "loss_scale": "1", "train_wall": "794", "gb_free": "13", "wall": "40267"}
[2023-09-12 22:16:42,441][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
Traceback (most recent call last):
  File "/root/anaconda3/bin/fairseq-hydra-train", line 8, in <module>
    sys.exit(cli_main())
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 87, in cli_main
    hydra_main()
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/main.py", line 32, in decorated_main
    _run_hydra(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 346, in _run_hydra
    run_and_report(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 198, in run_and_report
    return func()
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/utils.py", line 347, in <lambda>
    lambda: hydra.run(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 107, in run
    return run_job(
  File "/root/anaconda3/lib/python3.9/site-packages/hydra/core/utils.py", line 129, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 27, in hydra_main
    _hydra_main(cfg)
  File "/home/Workspace/fairseq/fairseq_cli/hydra_train.py", line 56, in _hydra_main
    distributed_utils.call_main(cfg, pre_main, **kwargs)
  File "/home/Workspace/fairseq/fairseq/distributed/utils.py", line 379, in call_main
    torch.multiprocessing.spawn(
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/root/anaconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 109, in join
    ready = multiprocessing.connection.wait(
  File "/root/anaconda3/lib/python3.9/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/root/anaconda3/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
Process SpawnProcess-3:
Process SpawnProcess-2:
Process SpawnProcess-4:
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
loss: tensor(6231.7236, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(5278.3926, device='cuda:0', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
loss: tensor(3951.8999, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6374.4106, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(4089.5454, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6117.2656, device='cuda:1', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:1', dtype=torch.float16, grad_fn=<SumBackward0>)
self.scaling_factor_for_vector: Parameter containing:
tensor([0.2305, 0.2305, 0.2280, 0.2262, 0.2246, 0.1091, 0.2303, 0.2300, 0.2296,
        0.2302, 0.2166, 0.2211, 0.2212, 0.2306, 0.2303, 0.2305, 0.2271, 0.2303,
        0.1111, 0.2306, 0.2305, 0.1124, 0.1111, 0.1113, 0.2186, 0.2302, 0.2305,
        0.2272, 0.2277, 0.2303, 0.1119, 0.2299, 0.2256, 0.2305, 0.2301, 0.2297,
        0.2285, 0.2306, 0.2305, 0.2180, 0.2306, 0.1116, 0.2306, 0.2297, 0.2291,
        0.1116, 0.2115, 0.2305, 0.2305, 0.2295, 0.2306, 0.1091, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1116, 0.2188, 0.2305, 0.2291, 0.2271, 0.2299, 0.2224,
        0.2299, 0.1125, 0.2305, 0.2306, 0.2291, 0.2306, 0.2306, 0.1114, 0.2305,
        0.2279, 0.2306, 0.2296, 0.2067, 0.2291, 0.1127, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1113, 0.2305, 0.2299, 0.1120, 0.2303, 0.2299, 0.2295, 0.2296,
        0.1091, 0.1112, 0.2299, 0.2305, 0.2184, 0.1064, 0.1115, 0.2306, 0.1113,
        0.2305, 0.2303, 0.2305, 0.2301, 0.2292, 0.1127, 0.2306, 0.2305, 0.1113,
        0.2126, 0.1121, 0.2289, 0.1118, 0.2222, 0.2305, 0.2280, 0.2273, 0.2303,
        0.1112, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2305, 0.2162, 0.1118,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2305, 0.2306, 0.2258, 0.2290, 0.1091,
        0.2279, 0.2297, 0.2186, 0.2264, 0.2306, 0.2296, 0.2205, 0.2301, 0.2299,
        0.2291, 0.2305, 0.2244, 0.2296, 0.2302, 0.1092, 0.2280, 0.2301, 0.1130,
        0.2208, 0.2305, 0.2194, 0.2305, 0.2305, 0.2289, 0.2306, 0.2303, 0.2305,
        0.2306, 0.2285, 0.2306, 0.2303, 0.2302, 0.2290, 0.2274, 0.2297, 0.2305,
        0.2305, 0.2289, 0.2306, 0.2299, 0.1118, 0.2306, 0.2301, 0.1116, 0.2225,
        0.2303, 0.2299, 0.2302, 0.2216, 0.2303, 0.1091, 0.2306, 0.2302, 0.2306,
        0.1116, 0.2289, 0.2303, 0.2303, 0.1116, 0.2288, 0.2274, 0.2279, 0.2301,
        0.2303, 0.2295, 0.2220, 0.2305, 0.2302, 0.2306, 0.2303, 0.1127, 0.2197,
        0.2195, 0.2305, 0.2303, 0.2289, 0.1115, 0.2303, 0.2306, 0.2306, 0.2305,
        0.1119, 0.2306, 0.2305, 0.2305, 0.2305, 0.1122, 0.2289, 0.2306, 0.2290,
        0.1119, 0.2306, 0.2305, 0.2306, 0.1114, 0.2288, 0.2104, 0.2305, 0.2303,
        0.2305, 0.2297, 0.2286, 0.1124, 0.2296, 0.1120, 0.2306, 0.2261, 0.2239,
        0.2194, 0.2079, 0.2306, 0.2230, 0.1118, 0.2305, 0.2306, 0.2230, 0.2267,
        0.1118, 0.2085, 0.2285, 0.2306, 0.2299, 0.2306, 0.2278, 0.2164, 0.2229,
        0.2302, 0.2301, 0.1124, 0.2294, 0.2251, 0.2305, 0.2306, 0.1094, 0.2299,
        0.1122, 0.2296, 0.2306, 0.2300, 0.2286, 0.2306, 0.1115, 0.2303, 0.1116,
        0.1113, 0.2305, 0.2301, 0.1091, 0.2307, 0.2303, 0.2289, 0.2306, 0.2295,
        0.2306, 0.2302, 0.2306, 0.1091, 0.2286, 0.2303, 0.2296, 0.1111, 0.2278,
        0.1111, 0.2261, 0.2305, 0.2302, 0.2056, 0.1122, 0.2175, 0.2262, 0.2268,
        0.2303, 0.2306, 0.2305, 0.1125, 0.2305, 0.2289, 0.2273, 0.2296, 0.2306,
        0.1091, 0.2305, 0.1116, 0.1119, 0.2303, 0.2288, 0.1114, 0.2306, 0.2306,
        0.2289, 0.2288, 0.2303, 0.2183, 0.2306, 0.2283, 0.2288, 0.2306, 0.2303,
        0.1115, 0.2290, 0.2267, 0.2288, 0.1122, 0.2300, 0.2303, 0.2295, 0.2303,
        0.2305, 0.2306, 0.2305, 0.1121, 0.2229, 0.1113, 0.2166, 0.2306, 0.2257,
        0.1116, 0.2301, 0.2306, 0.2285, 0.2303, 0.2306, 0.2054, 0.2306, 0.2189,
        0.2303, 0.2305, 0.2258, 0.2305, 0.2036, 0.2296, 0.2301, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2303, 0.2306, 0.2291, 0.2262, 0.2306, 0.1111,
        0.1121, 0.2246, 0.2307, 0.1127, 0.2306, 0.2306, 0.2238, 0.2305, 0.2145,
        0.2147, 0.2278, 0.2305, 0.2245, 0.2306, 0.2305, 0.2302, 0.1118, 0.2307,
        0.2284, 0.1112, 0.2303, 0.1112, 0.2306, 0.1119, 0.2172, 0.2303, 0.2305,
        0.1115, 0.2306, 0.1113, 0.2294, 0.2303, 0.2305, 0.2305, 0.1130, 0.2279,
        0.2301, 0.2305, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2305, 0.2305, 0.2296, 0.2306, 0.2305, 0.2274, 0.2305, 0.2302, 0.2300,
        0.1115, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2297,
        0.2305, 0.2305, 0.2301, 0.2305, 0.2242, 0.2278, 0.2306, 0.1111, 0.2273,
        0.2305, 0.1110, 0.2299, 0.2306, 0.2281, 0.2306, 0.2305, 0.2303, 0.1115,
        0.1091, 0.2231, 0.2303, 0.2297, 0.2306, 0.2198, 0.2305, 0.2262, 0.2156,
        0.2258, 0.1121, 0.1117, 0.2272, 0.2303, 0.2305, 0.2299, 0.2179, 0.2301,
        0.2290, 0.2306, 0.2300, 0.2306, 0.2188, 0.2302, 0.2305, 0.2305, 0.1118,
        0.2300, 0.1120, 0.2297, 0.2305, 0.2301, 0.2303, 0.2219, 0.2306, 0.2305,
        0.2299, 0.2305, 0.2306, 0.1116, 0.2306, 0.2305, 0.2303, 0.2301, 0.2305,
        0.2305, 0.2306, 0.2301, 0.2299, 0.2242, 0.2236, 0.1116, 0.2306, 0.1118,
        0.2306, 0.2306, 0.2305, 0.2297, 0.2305, 0.2305, 0.2180, 0.2286, 0.2305,
        0.2305, 0.2305, 0.1092, 0.1108, 0.2306, 0.2300, 0.1125, 0.2303, 0.2175,
        0.2234, 0.2208, 0.2305, 0.2305, 0.1091, 0.2305, 0.2306, 0.2278, 0.2299,
        0.1111, 0.2234, 0.1110, 0.2306, 0.2169, 0.2306, 0.2305, 0.2302, 0.2306,
        0.2169, 0.2305, 0.1118, 0.2306, 0.1113, 0.2267, 0.2301, 0.2306, 0.2153,
        0.1116, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2307, 0.2278, 0.2292,
        0.2305, 0.2252, 0.1110, 0.2224, 0.2279, 0.2306, 0.2305, 0.2303, 0.2305,
        0.2306, 0.2301, 0.2253, 0.2303, 0.2305, 0.2156, 0.2306, 0.1109, 0.2306,
        0.2305, 0.2303, 0.1113, 0.2234, 0.2300, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2303, 0.2301, 0.2305, 0.1091, 0.2222, 0.1108, 0.2037, 0.1116, 0.2291,
        0.2303, 0.2302, 0.2306, 0.2275, 0.2302, 0.2159, 0.2306, 0.2302, 0.2306,
        0.2302, 0.2227, 0.2269, 0.2306, 0.2294, 0.2285, 0.2306, 0.2303, 0.1110,
        0.2291, 0.2301, 0.2280, 0.1110, 0.2299, 0.2301, 0.2294, 0.2305, 0.2305,
        0.2269, 0.2297, 0.2305, 0.2306, 0.2305, 0.2289, 0.2305, 0.2306, 0.2275,
        0.1118], device='cuda:1', dtype=torch.float16, requires_grad=True)
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
loss: tensor(3910.1296, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7264.9019, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7634.0576, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7633.1821, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6696.5981, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(7330.6577, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6523, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
loss: tensor(6103.9326, device='cuda:2', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:2', dtype=torch.float16, grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/root/anaconda3/lib/python3.9/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/root/anaconda3/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/root/anaconda3/lib/python3.9/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Parameter containing:
tensor([0.2305, 0.2305, 0.2280, 0.2263, 0.2247, 0.1092, 0.2303, 0.2301, 0.2297,
        0.2303, 0.2167, 0.2211, 0.2212, 0.2306, 0.2305, 0.2306, 0.2271, 0.2305,
        0.1112, 0.2306, 0.2306, 0.1126, 0.1111, 0.1113, 0.2186, 0.2303, 0.2305,
        0.2273, 0.2277, 0.2303, 0.1120, 0.2299, 0.2257, 0.2305, 0.2301, 0.2299,
        0.2286, 0.2306, 0.2305, 0.2180, 0.2306, 0.1116, 0.2306, 0.2299, 0.2292,
        0.1117, 0.2117, 0.2306, 0.2306, 0.2296, 0.2306, 0.1092, 0.2214, 0.2294,
        0.2137, 0.2295, 0.1116, 0.2188, 0.2305, 0.2292, 0.2272, 0.2300, 0.2224,
        0.2299, 0.1125, 0.2306, 0.2306, 0.2291, 0.2306, 0.2306, 0.1115, 0.2306,
        0.2279, 0.2306, 0.2297, 0.2068, 0.2292, 0.1128, 0.2306, 0.2211, 0.2279,
        0.2303, 0.1113, 0.2306, 0.2299, 0.1121, 0.2305, 0.2299, 0.2296, 0.2296,
        0.1092, 0.1113, 0.2300, 0.2306, 0.2184, 0.1065, 0.1115, 0.2306, 0.1113,
        0.2305, 0.2305, 0.2306, 0.2302, 0.2294, 0.1127, 0.2306, 0.2306, 0.1114,
        0.2126, 0.1122, 0.2289, 0.1119, 0.2223, 0.2306, 0.2281, 0.2273, 0.2303,
        0.1113, 0.2285, 0.2238, 0.2306, 0.2305, 0.2306, 0.2306, 0.2163, 0.1118,
        0.2306, 0.2306, 0.2301, 0.2306, 0.2305, 0.2306, 0.2260, 0.2290, 0.1091,
        0.2279, 0.2299, 0.2188, 0.2264, 0.2306, 0.2296, 0.2205, 0.2301, 0.2300,
        0.2291, 0.2305, 0.2244, 0.2297, 0.2303, 0.1093, 0.2281, 0.2301, 0.1132,
        0.2208, 0.2306, 0.2194, 0.2305, 0.2306, 0.2289, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2285, 0.2306, 0.2305, 0.2303, 0.2290, 0.2274, 0.2299, 0.2306,
        0.2305, 0.2290, 0.2306, 0.2299, 0.1119, 0.2306, 0.2301, 0.1117, 0.2227,
        0.2303, 0.2299, 0.2303, 0.2217, 0.2305, 0.1092, 0.2306, 0.2303, 0.2306,
        0.1116, 0.2290, 0.2303, 0.2305, 0.1117, 0.2288, 0.2274, 0.2279, 0.2301,
        0.2305, 0.2296, 0.2222, 0.2305, 0.2303, 0.2306, 0.2305, 0.1128, 0.2198,
        0.2195, 0.2306, 0.2305, 0.2290, 0.1116, 0.2305, 0.2306, 0.2306, 0.2305,
        0.1119, 0.2306, 0.2306, 0.2305, 0.2305, 0.1123, 0.2290, 0.2306, 0.2291,
        0.1119, 0.2306, 0.2305, 0.2306, 0.1114, 0.2289, 0.2104, 0.2306, 0.2303,
        0.2306, 0.2297, 0.2288, 0.1124, 0.2296, 0.1121, 0.2306, 0.2262, 0.2240,
        0.2195, 0.2080, 0.2306, 0.2230, 0.1118, 0.2305, 0.2306, 0.2231, 0.2268,
        0.1118, 0.2085, 0.2286, 0.2306, 0.2300, 0.2306, 0.2279, 0.2166, 0.2229,
        0.2302, 0.2302, 0.1125, 0.2294, 0.2252, 0.2306, 0.2306, 0.1094, 0.2299,
        0.1122, 0.2297, 0.2306, 0.2301, 0.2286, 0.2307, 0.1115, 0.2305, 0.1116,
        0.1113, 0.2305, 0.2301, 0.1091, 0.2307, 0.2303, 0.2289, 0.2307, 0.2296,
        0.2306, 0.2303, 0.2306, 0.1092, 0.2286, 0.2305, 0.2297, 0.1112, 0.2278,
        0.1111, 0.2262, 0.2306, 0.2302, 0.2056, 0.1122, 0.2175, 0.2263, 0.2269,
        0.2305, 0.2306, 0.2306, 0.1126, 0.2306, 0.2290, 0.2273, 0.2296, 0.2306,
        0.1091, 0.2306, 0.1116, 0.1120, 0.2305, 0.2288, 0.1115, 0.2306, 0.2306,
        0.2289, 0.2289, 0.2303, 0.2183, 0.2306, 0.2284, 0.2289, 0.2306, 0.2305,
        0.1115, 0.2291, 0.2268, 0.2289, 0.1123, 0.2301, 0.2305, 0.2296, 0.2303,
        0.2306, 0.2306, 0.2305, 0.1121, 0.2230, 0.1113, 0.2167, 0.2306, 0.2257,
        0.1116, 0.2302, 0.2306, 0.2286, 0.2305, 0.2306, 0.2056, 0.2306, 0.2189,
        0.2305, 0.2306, 0.2258, 0.2306, 0.2037, 0.2297, 0.2302, 0.2306, 0.2306,
        0.2281, 0.2306, 0.2306, 0.2303, 0.2306, 0.2292, 0.2263, 0.2306, 0.1112,
        0.1121, 0.2246, 0.2307, 0.1128, 0.2306, 0.2306, 0.2239, 0.2306, 0.2145,
        0.2147, 0.2279, 0.2306, 0.2245, 0.2306, 0.2305, 0.2303, 0.1118, 0.2307,
        0.2285, 0.1113, 0.2305, 0.1113, 0.2306, 0.1119, 0.2173, 0.2305, 0.2305,
        0.1115, 0.2306, 0.1113, 0.2295, 0.2303, 0.2306, 0.2306, 0.1130, 0.2280,
        0.2301, 0.2306, 0.2306, 0.2246, 0.2306, 0.2303, 0.2306, 0.2306, 0.2294,
        0.2306, 0.2305, 0.2297, 0.2306, 0.2305, 0.2275, 0.2305, 0.2302, 0.2300,
        0.1116, 0.2269, 0.2306, 0.2303, 0.2306, 0.2296, 0.2286, 0.2295, 0.2299,
        0.2305, 0.2305, 0.2302, 0.2306, 0.2244, 0.2279, 0.2306, 0.1112, 0.2273,
        0.2306, 0.1110, 0.2299, 0.2306, 0.2283, 0.2306, 0.2306, 0.2303, 0.1116,
        0.1092, 0.2231, 0.2305, 0.2299, 0.2306, 0.2200, 0.2306, 0.2263, 0.2156,
        0.2258, 0.1122, 0.1118, 0.2273, 0.2305, 0.2306, 0.2299, 0.2179, 0.2302,
        0.2290, 0.2306, 0.2301, 0.2306, 0.2189, 0.2302, 0.2306, 0.2305, 0.1118,
        0.2301, 0.1121, 0.2297, 0.2305, 0.2301, 0.2303, 0.2220, 0.2306, 0.2305,
        0.2300, 0.2306, 0.2306, 0.1117, 0.2306, 0.2306, 0.2303, 0.2301, 0.2306,
        0.2305, 0.2306, 0.2302, 0.2299, 0.2244, 0.2238, 0.1116, 0.2306, 0.1118,
        0.2306, 0.2306, 0.2306, 0.2297, 0.2306, 0.2306, 0.2180, 0.2286, 0.2306,
        0.2306, 0.2306, 0.1093, 0.1108, 0.2306, 0.2301, 0.1125, 0.2303, 0.2177,
        0.2234, 0.2208, 0.2306, 0.2305, 0.1092, 0.2306, 0.2306, 0.2278, 0.2300,
        0.1111, 0.2234, 0.1110, 0.2306, 0.2169, 0.2306, 0.2305, 0.2303, 0.2306,
        0.2169, 0.2305, 0.1119, 0.2306, 0.1113, 0.2268, 0.2302, 0.2306, 0.2155,
        0.1116, 0.2306, 0.2306, 0.2301, 0.2306, 0.2306, 0.2308, 0.2279, 0.2294,
        0.2305, 0.2253, 0.1110, 0.2224, 0.2279, 0.2306, 0.2306, 0.2303, 0.2306,
        0.2306, 0.2301, 0.2253, 0.2305, 0.2306, 0.2156, 0.2306, 0.1110, 0.2306,
        0.2305, 0.2305, 0.1114, 0.2235, 0.2301, 0.2306, 0.2306, 0.2288, 0.2306,
        0.2305, 0.2302, 0.2305, 0.1091, 0.2222, 0.1108, 0.2039, 0.1116, 0.2291,
        0.2305, 0.2303, 0.2306, 0.2277, 0.2303, 0.2161, 0.2306, 0.2302, 0.2306,
        0.2303, 0.2227, 0.2269, 0.2306, 0.2294, 0.2286, 0.2306, 0.2305, 0.1111,
        0.2292, 0.2302, 0.2281, 0.1111, 0.2300, 0.2302, 0.2294, 0.2306, 0.2305,
        0.2269, 0.2297, 0.2306, 0.2306, 0.2306, 0.2290, 0.2306, 0.2306, 0.2275,
        0.1118], device='cuda:3', dtype=torch.float16, requires_grad=True)
loss: tensor(7264.7593, device='cuda:3', grad_fn=<NllLossBackward0>)
loss_ent_max: tensor(-4.6562, device='cuda:3', dtype=torch.float16, grad_fn=<SumBackward0>)
/root/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
